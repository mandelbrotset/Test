diff --git a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
index b56a22e..0813713 100644
--- a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
+++ b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
@@ -15,8 +15,15 @@ import org.gradle.api.logging.LogLevel
 import org.gradle.api.logging.Logger
 import org.junit.runner.Description
 
+import java.util.concurrent.atomic.AtomicBoolean
 import java.util.concurrent.atomic.AtomicInteger
 
+import javax.sound.sampled.AudioSystem;
+import javax.sound.sampled.Clip;
+import javax.sound.sampled.Line;
+import javax.sound.sampled.LineEvent;
+import javax.sound.sampled.LineListener;
+
 import static com.carrotsearch.ant.tasks.junit4.FormattingUtils.*
 import static com.carrotsearch.gradle.junit4.TestLoggingConfiguration.OutputMode
 
@@ -102,9 +109,36 @@ class TestReportLogger extends TestsSummaryEventListener implements AggregatedEv
                 formatTime(e.getCurrentTime()) + ", stalled for " +
                 formatDurationInSeconds(e.getNoEventDuration()) + " at: " +
                 (e.getDescription() == null ? "<unknown>" : formatDescription(e.getDescription())))
+        try {
+            playBeat();
+        } catch (Exception nosound) { /* handling exceptions with style */ }
         slowTestsFound = true
     }
 
+    void playBeat() throws Exception {
+        Clip clip = (Clip)AudioSystem.getLine(new Line.Info(Clip.class));
+        final AtomicBoolean stop = new AtomicBoolean();
+        clip.addLineListener(new LineListener() {
+            @Override
+            public void update(LineEvent event) {
+                if (event.getType() == LineEvent.Type.STOP) {
+                    stop.set(true);
+                }
+            }
+        });
+        InputStream stream = getClass().getResourceAsStream("/beat.wav");
+        try {
+            clip.open(AudioSystem.getAudioInputStream(stream));
+            clip.start();
+            while (!stop.get()) {
+                Thread.sleep(20);
+            }
+            clip.close();
+        } finally {
+            stream.close();
+        }
+    }
+
     @Subscribe
     void onQuit(AggregatedQuitEvent e) throws IOException {
         if (config.showNumFailuresAtEnd > 0 && !failedTests.isEmpty()) {
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
index eea2041..0d936ab 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
@@ -65,7 +65,6 @@ public class PluginBuildPlugin extends BuildPlugin {
             // with a full elasticsearch server that includes optional deps
             provided "com.spatial4j:spatial4j:${project.versions.spatial4j}"
             provided "com.vividsolutions:jts:${project.versions.jts}"
-            provided "com.github.spullara.mustache.java:compiler:${project.versions.mustache}"
             provided "log4j:log4j:${project.versions.log4j}"
             provided "log4j:apache-log4j-extras:${project.versions.log4j}"
             provided "org.slf4j:slf4j-api:${project.versions.slf4j}"
diff --git a/buildSrc/src/main/resources/beat.wav b/buildSrc/src/main/resources/beat.wav
new file mode 100644
index 0000000..4083a4c
Binary files /dev/null and b/buildSrc/src/main/resources/beat.wav differ
diff --git a/buildSrc/src/main/resources/forbidden/all-signatures.txt b/buildSrc/src/main/resources/forbidden/all-signatures.txt
index 5a91807..4bc24c9 100644
--- a/buildSrc/src/main/resources/forbidden/all-signatures.txt
+++ b/buildSrc/src/main/resources/forbidden/all-signatures.txt
@@ -92,6 +92,13 @@ java.net.InetAddress#getCanonicalHostName()
 java.net.InetSocketAddress#getHostName() @ Use getHostString() instead, which avoids a DNS lookup
 
 @defaultMessage Do not violate java's access system
+java.lang.Class#getDeclaredClasses() @ Do not violate java's access system: Use getClasses() instead
+java.lang.Class#getDeclaredConstructor(java.lang.Class[]) @ Do not violate java's access system: Use getConstructor() instead
+java.lang.Class#getDeclaredConstructors() @ Do not violate java's access system: Use getConstructors() instead
+java.lang.Class#getDeclaredField(java.lang.String) @ Do not violate java's access system: Use getField() instead
+java.lang.Class#getDeclaredFields() @ Do not violate java's access system: Use getFields() instead
+java.lang.Class#getDeclaredMethod(java.lang.String, java.lang.Class[]) @ Do not violate java's access system: Use getMethod() instead
+java.lang.Class#getDeclaredMethods() @ Do not violate java's access system: Use getMethods() instead
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
 java.lang.reflect.AccessibleObject#setAccessible(java.lang.reflect.AccessibleObject[], boolean)
 
diff --git a/buildSrc/version.properties b/buildSrc/version.properties
index 1a982d3..e33383a 100644
--- a/buildSrc/version.properties
+++ b/buildSrc/version.properties
@@ -1,10 +1,9 @@
 elasticsearch     = 3.0.0-SNAPSHOT
-lucene            = 5.4.0-snapshot-1715952
+lucene            = 5.5.0-snapshot-1719088
 
 # optional dependencies
 spatial4j         = 0.5
 jts               = 1.13
-mustache          = 0.9.1
 jackson           = 2.6.2
 log4j             = 1.2.17
 slf4j             = 1.6.2
@@ -12,7 +11,7 @@ jna               = 4.1.0
 
 
 # test dependencies
-randomizedrunner  = 2.2.0
+randomizedrunner  = 2.3.2
 junit             = 4.11
 httpclient        = 4.3.6
 httpcore          = 4.3.3
diff --git a/core/build.gradle b/core/build.gradle
index 3db5097..fd8a0c1 100644
--- a/core/build.gradle
+++ b/core/build.gradle
@@ -74,9 +74,6 @@ dependencies {
   compile "com.spatial4j:spatial4j:${versions.spatial4j}", optional
   compile "com.vividsolutions:jts:${versions.jts}", optional
 
-  // templating
-  compile "com.github.spullara.mustache.java:compiler:${versions.mustache}", optional
-
   // logging
   compile "log4j:log4j:${versions.log4j}", optional
   compile "log4j:apache-log4j-extras:${versions.log4j}", optional
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 4e4ec3b..a5e2e38 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -276,7 +276,7 @@ public class Version {
     public static final int V_2_2_0_ID = 2020099;
     public static final Version V_2_2_0 = new Version(V_2_2_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
     public static final int V_3_0_0_ID = 3000099;
-    public static final Version V_3_0_0 = new Version(V_3_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
+    public static final Version V_3_0_0 = new Version(V_3_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_5_0);
     public static final Version CURRENT = V_3_0_0;
 
     static {
diff --git a/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java b/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java
deleted file mode 100644
index f4152ac..0000000
--- a/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java
+++ /dev/null
@@ -1,306 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.bootstrap.Elasticsearch;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-import java.util.Collections;
-
-/**
- * Base class for write action responses.
- */
-public class ActionWriteResponse extends ActionResponse {
-
-    public final static ActionWriteResponse.ShardInfo.Failure[] EMPTY = new ActionWriteResponse.ShardInfo.Failure[0];
-
-    private ShardInfo shardInfo;
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        shardInfo = ActionWriteResponse.ShardInfo.readShardInfo(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        shardInfo.writeTo(out);
-    }
-
-    public ShardInfo getShardInfo() {
-        return shardInfo;
-    }
-
-    public void setShardInfo(ShardInfo shardInfo) {
-        this.shardInfo = shardInfo;
-    }
-
-    public static class ShardInfo implements Streamable, ToXContent {
-
-        private int total;
-        private int successful;
-        private Failure[] failures = EMPTY;
-
-        public ShardInfo() {
-        }
-
-        public ShardInfo(int total, int successful, Failure... failures) {
-            assert total >= 0 && successful >= 0;
-            this.total = total;
-            this.successful = successful;
-            this.failures = failures;
-        }
-
-        /**
-         * @return the total number of shards the write should go to (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
-         */
-        public int getTotal() {
-            return total;
-        }
-
-        /**
-         * @return the total number of shards the write succeeded on (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
-         */
-        public int getSuccessful() {
-            return successful;
-        }
-
-        /**
-         * @return The total number of replication failures.
-         */
-        public int getFailed() {
-            return failures.length;
-        }
-
-        /**
-         * @return The replication failures that have been captured in the case writes have failed on replica shards.
-         */
-        public Failure[] getFailures() {
-            return failures;
-        }
-
-        public RestStatus status() {
-            RestStatus status = RestStatus.OK;
-            for (Failure failure : failures) {
-                if (failure.primary() && failure.status().getStatus() > status.getStatus()) {
-                    status = failure.status();
-                }
-            }
-            return status;
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            total = in.readVInt();
-            successful = in.readVInt();
-            int size = in.readVInt();
-            failures = new Failure[size];
-            for (int i = 0; i < size; i++) {
-                Failure failure = new Failure();
-                failure.readFrom(in);
-                failures[i] = failure;
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(total);
-            out.writeVInt(successful);
-            out.writeVInt(failures.length);
-            for (Failure failure : failures) {
-                failure.writeTo(out);
-            }
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(Fields._SHARDS);
-            builder.field(Fields.TOTAL, total);
-            builder.field(Fields.SUCCESSFUL, successful);
-            builder.field(Fields.FAILED, getFailed());
-            if (failures.length > 0) {
-                builder.startArray(Fields.FAILURES);
-                for (Failure failure : failures) {
-                    failure.toXContent(builder, params);
-                }
-                builder.endArray();
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public String toString() {
-            return Strings.toString(this);
-        }
-
-        public static ShardInfo readShardInfo(StreamInput in) throws IOException {
-            ShardInfo shardInfo = new ShardInfo();
-            shardInfo.readFrom(in);
-            return shardInfo;
-        }
-
-        public static class Failure implements ShardOperationFailedException, ToXContent {
-
-            private String index;
-            private int shardId;
-            private String nodeId;
-            private Throwable cause;
-            private RestStatus status;
-            private boolean primary;
-
-            public Failure(String index, int shardId, @Nullable String nodeId, Throwable cause, RestStatus status, boolean primary) {
-                this.index = index;
-                this.shardId = shardId;
-                this.nodeId = nodeId;
-                this.cause = cause;
-                this.status = status;
-                this.primary = primary;
-            }
-
-            Failure() {
-            }
-
-            /**
-             * @return On what index the failure occurred.
-             */
-            @Override
-            public String index() {
-                return index;
-            }
-
-            /**
-             * @return On what shard id the failure occurred.
-             */
-            @Override
-            public int shardId() {
-                return shardId;
-            }
-
-            /**
-             * @return On what node the failure occurred.
-             */
-            @Nullable
-            public String nodeId() {
-                return nodeId;
-            }
-
-            /**
-             * @return A text description of the failure
-             */
-            @Override
-            public String reason() {
-                return ExceptionsHelper.detailedMessage(cause);
-            }
-
-            /**
-             * @return The status to report if this failure was a primary failure.
-             */
-            @Override
-            public RestStatus status() {
-                return status;
-            }
-
-            @Override
-            public Throwable getCause() {
-                return cause;
-            }
-
-            /**
-             * @return Whether this failure occurred on a primary shard.
-             * (this only reports true for delete by query)
-             */
-            public boolean primary() {
-                return primary;
-            }
-
-            @Override
-            public void readFrom(StreamInput in) throws IOException {
-                index = in.readString();
-                shardId = in.readVInt();
-                nodeId = in.readOptionalString();
-                cause = in.readThrowable();
-                status = RestStatus.readFrom(in);
-                primary = in.readBoolean();
-            }
-
-            @Override
-            public void writeTo(StreamOutput out) throws IOException {
-                out.writeString(index);
-                out.writeVInt(shardId);
-                out.writeOptionalString(nodeId);
-                out.writeThrowable(cause);
-                RestStatus.writeTo(out, status);
-                out.writeBoolean(primary);
-            }
-
-            @Override
-            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-                builder.startObject();
-                builder.field(Fields._INDEX, index);
-                builder.field(Fields._SHARD, shardId);
-                builder.field(Fields._NODE, nodeId);
-                builder.field(Fields.REASON);
-                builder.startObject();
-                ElasticsearchException.toXContent(builder, params, cause);
-                builder.endObject();
-                builder.field(Fields.STATUS, status);
-                builder.field(Fields.PRIMARY, primary);
-                builder.endObject();
-                return builder;
-            }
-
-            private static class Fields {
-
-                private static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-                private static final XContentBuilderString _SHARD = new XContentBuilderString("_shard");
-                private static final XContentBuilderString _NODE = new XContentBuilderString("_node");
-                private static final XContentBuilderString REASON = new XContentBuilderString("reason");
-                private static final XContentBuilderString STATUS = new XContentBuilderString("status");
-                private static final XContentBuilderString PRIMARY = new XContentBuilderString("primary");
-
-            }
-        }
-
-        private static class Fields {
-
-            private static final XContentBuilderString _SHARDS = new XContentBuilderString("_shards");
-            private static final XContentBuilderString TOTAL = new XContentBuilderString("total");
-            private static final XContentBuilderString SUCCESSFUL = new XContentBuilderString("successful");
-            private static final XContentBuilderString PENDING = new XContentBuilderString("pending");
-            private static final XContentBuilderString FAILED = new XContentBuilderString("failed");
-            private static final XContentBuilderString FAILURES = new XContentBuilderString("failures");
-
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java b/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java
new file mode 100644
index 0000000..009d3fc
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action;
+
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
+
+import java.io.IOException;
+
+/**
+ * A base class for the response of a write operation that involves a single doc
+ */
+public abstract class DocWriteResponse extends ReplicationResponse implements StatusToXContent {
+
+    private ShardId shardId;
+    private String id;
+    private String type;
+    private long version;
+
+    public DocWriteResponse(ShardId shardId, String type, String id, long version) {
+        this.shardId = shardId;
+        this.type = type;
+        this.id = id;
+        this.version = version;
+    }
+
+    // needed for deserialization
+    protected DocWriteResponse() {
+    }
+
+    /**
+     * The index the document was changed in.
+     */
+    public String getIndex() {
+        return this.shardId.getIndex();
+    }
+
+
+    /**
+     * The exact shard the document was changed in.
+     */
+    public ShardId getShardId() {
+        return this.shardId;
+    }
+
+    /**
+     * The type of the document changed.
+     */
+    public String getType() {
+        return this.type;
+    }
+
+    /**
+     * The id of the document changed.
+     */
+    public String getId() {
+        return this.id;
+    }
+
+    /**
+     * Returns the current version of the doc.
+     */
+    public long getVersion() {
+        return this.version;
+    }
+
+    /** returns the rest status for this response (based on {@link ShardInfo#status()} */
+    public RestStatus status() {
+        return getShardInfo().status();
+    }
+
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        shardId = ShardId.readShardId(in);
+        type = in.readString();
+        id = in.readString();
+        version = in.readZLong();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        shardId.writeTo(out);
+        out.writeString(type);
+        out.writeString(id);
+        out.writeZLong(version);
+    }
+
+    static final class Fields {
+        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
+        static final XContentBuilderString _ID = new XContentBuilderString("_id");
+        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        ReplicationResponse.ShardInfo shardInfo = getShardInfo();
+        builder.field(Fields._INDEX, shardId.getIndex())
+            .field(Fields._TYPE, type)
+            .field(Fields._ID, id)
+            .field(Fields._VERSION, version);
+        shardInfo.toXContent(builder, params);
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java b/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java
new file mode 100644
index 0000000..4e358c8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java
@@ -0,0 +1,303 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.rest.RestStatus;
+
+import java.io.IOException;
+
+/**
+ * Base class for write action responses.
+ */
+public class ReplicationResponse extends ActionResponse {
+
+    public final static ReplicationResponse.ShardInfo.Failure[] EMPTY = new ReplicationResponse.ShardInfo.Failure[0];
+
+    private ShardInfo shardInfo;
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        shardInfo = ReplicationResponse.ShardInfo.readShardInfo(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        shardInfo.writeTo(out);
+    }
+
+    public ShardInfo getShardInfo() {
+        return shardInfo;
+    }
+
+    public void setShardInfo(ShardInfo shardInfo) {
+        this.shardInfo = shardInfo;
+    }
+
+    public static class ShardInfo implements Streamable, ToXContent {
+
+        private int total;
+        private int successful;
+        private Failure[] failures = EMPTY;
+
+        public ShardInfo() {
+        }
+
+        public ShardInfo(int total, int successful, Failure... failures) {
+            assert total >= 0 && successful >= 0;
+            this.total = total;
+            this.successful = successful;
+            this.failures = failures;
+        }
+
+        /**
+         * @return the total number of shards the write should go to (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
+         */
+        public int getTotal() {
+            return total;
+        }
+
+        /**
+         * @return the total number of shards the write succeeded on (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
+         */
+        public int getSuccessful() {
+            return successful;
+        }
+
+        /**
+         * @return The total number of replication failures.
+         */
+        public int getFailed() {
+            return failures.length;
+        }
+
+        /**
+         * @return The replication failures that have been captured in the case writes have failed on replica shards.
+         */
+        public Failure[] getFailures() {
+            return failures;
+        }
+
+        public RestStatus status() {
+            RestStatus status = RestStatus.OK;
+            for (Failure failure : failures) {
+                if (failure.primary() && failure.status().getStatus() > status.getStatus()) {
+                    status = failure.status();
+                }
+            }
+            return status;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            total = in.readVInt();
+            successful = in.readVInt();
+            int size = in.readVInt();
+            failures = new Failure[size];
+            for (int i = 0; i < size; i++) {
+                Failure failure = new Failure();
+                failure.readFrom(in);
+                failures[i] = failure;
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeVInt(total);
+            out.writeVInt(successful);
+            out.writeVInt(failures.length);
+            for (Failure failure : failures) {
+                failure.writeTo(out);
+            }
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject(Fields._SHARDS);
+            builder.field(Fields.TOTAL, total);
+            builder.field(Fields.SUCCESSFUL, successful);
+            builder.field(Fields.FAILED, getFailed());
+            if (failures.length > 0) {
+                builder.startArray(Fields.FAILURES);
+                for (Failure failure : failures) {
+                    failure.toXContent(builder, params);
+                }
+                builder.endArray();
+            }
+            builder.endObject();
+            return builder;
+        }
+
+        @Override
+        public String toString() {
+            return Strings.toString(this);
+        }
+
+        public static ShardInfo readShardInfo(StreamInput in) throws IOException {
+            ShardInfo shardInfo = new ShardInfo();
+            shardInfo.readFrom(in);
+            return shardInfo;
+        }
+
+        public static class Failure implements ShardOperationFailedException, ToXContent {
+
+            private String index;
+            private int shardId;
+            private String nodeId;
+            private Throwable cause;
+            private RestStatus status;
+            private boolean primary;
+
+            public Failure(String index, int shardId, @Nullable String nodeId, Throwable cause, RestStatus status, boolean primary) {
+                this.index = index;
+                this.shardId = shardId;
+                this.nodeId = nodeId;
+                this.cause = cause;
+                this.status = status;
+                this.primary = primary;
+            }
+
+            Failure() {
+            }
+
+            /**
+             * @return On what index the failure occurred.
+             */
+            @Override
+            public String index() {
+                return index;
+            }
+
+            /**
+             * @return On what shard id the failure occurred.
+             */
+            @Override
+            public int shardId() {
+                return shardId;
+            }
+
+            /**
+             * @return On what node the failure occurred.
+             */
+            @Nullable
+            public String nodeId() {
+                return nodeId;
+            }
+
+            /**
+             * @return A text description of the failure
+             */
+            @Override
+            public String reason() {
+                return ExceptionsHelper.detailedMessage(cause);
+            }
+
+            /**
+             * @return The status to report if this failure was a primary failure.
+             */
+            @Override
+            public RestStatus status() {
+                return status;
+            }
+
+            @Override
+            public Throwable getCause() {
+                return cause;
+            }
+
+            /**
+             * @return Whether this failure occurred on a primary shard.
+             * (this only reports true for delete by query)
+             */
+            public boolean primary() {
+                return primary;
+            }
+
+            @Override
+            public void readFrom(StreamInput in) throws IOException {
+                index = in.readString();
+                shardId = in.readVInt();
+                nodeId = in.readOptionalString();
+                cause = in.readThrowable();
+                status = RestStatus.readFrom(in);
+                primary = in.readBoolean();
+            }
+
+            @Override
+            public void writeTo(StreamOutput out) throws IOException {
+                out.writeString(index);
+                out.writeVInt(shardId);
+                out.writeOptionalString(nodeId);
+                out.writeThrowable(cause);
+                RestStatus.writeTo(out, status);
+                out.writeBoolean(primary);
+            }
+
+            @Override
+            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+                builder.startObject();
+                builder.field(Fields._INDEX, index);
+                builder.field(Fields._SHARD, shardId);
+                builder.field(Fields._NODE, nodeId);
+                builder.field(Fields.REASON);
+                builder.startObject();
+                ElasticsearchException.toXContent(builder, params, cause);
+                builder.endObject();
+                builder.field(Fields.STATUS, status);
+                builder.field(Fields.PRIMARY, primary);
+                builder.endObject();
+                return builder;
+            }
+
+            private static class Fields {
+
+                private static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+                private static final XContentBuilderString _SHARD = new XContentBuilderString("_shard");
+                private static final XContentBuilderString _NODE = new XContentBuilderString("_node");
+                private static final XContentBuilderString REASON = new XContentBuilderString("reason");
+                private static final XContentBuilderString STATUS = new XContentBuilderString("status");
+                private static final XContentBuilderString PRIMARY = new XContentBuilderString("primary");
+
+            }
+        }
+
+        private static class Fields {
+
+            private static final XContentBuilderString _SHARDS = new XContentBuilderString("_shards");
+            private static final XContentBuilderString TOTAL = new XContentBuilderString("total");
+            private static final XContentBuilderString SUCCESSFUL = new XContentBuilderString("successful");
+            private static final XContentBuilderString PENDING = new XContentBuilderString("pending");
+            private static final XContentBuilderString FAILED = new XContentBuilderString("failed");
+            private static final XContentBuilderString FAILURES = new XContentBuilderString("failures");
+
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
index d09c73e..39d9cac 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
@@ -31,6 +31,7 @@ import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
 import org.elasticsearch.cluster.metadata.RepositoryMetaData;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.repositories.RepositoryMissingException;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -38,7 +39,9 @@ import org.elasticsearch.transport.TransportService;
 
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.LinkedHashSet;
 import java.util.List;
+import java.util.Set;
 
 /**
  * Transport action for get repositories operation
@@ -78,8 +81,20 @@ public class TransportGetRepositoriesAction extends TransportMasterNodeReadActio
             }
         } else {
             if (repositories != null) {
+                Set<String> repositoriesToGet = new LinkedHashSet<>(); // to keep insertion order
+                for (String repositoryOrPattern : request.repositories()) {
+                    if (Regex.isSimpleMatchPattern(repositoryOrPattern) == false) {
+                        repositoriesToGet.add(repositoryOrPattern);
+                    } else {
+                        for (RepositoryMetaData repository : repositories.repositories()) {
+                            if (Regex.simpleMatch(repositoryOrPattern, repository.name())) {
+                                repositoriesToGet.add(repository.name());
+                            }
+                        }
+                    }
+                }
                 List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>();
-                for (String repository : request.repositories()) {
+                for (String repository : repositoriesToGet) {
                     RepositoryMetaData repositoryMetaData = repositories.repository(repository);
                     if (repositoryMetaData == null) {
                         listener.onFailure(new RepositoryMissingException(repository));
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
index 7bfb0e8..478146d 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
@@ -29,6 +29,7 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.snapshots.Snapshot;
 import org.elasticsearch.snapshots.SnapshotInfo;
@@ -38,7 +39,9 @@ import org.elasticsearch.transport.TransportService;
 
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.LinkedHashSet;
 import java.util.List;
+import java.util.Set;
 
 /**
  * Transport Action for get snapshots operation
@@ -84,8 +87,24 @@ public class TransportGetSnapshotsAction extends TransportMasterNodeAction<GetSn
                     snapshotInfoBuilder.add(new SnapshotInfo(snapshot));
                 }
             } else {
-                for (int i = 0; i < request.snapshots().length; i++) {
-                    SnapshotId snapshotId = new SnapshotId(request.repository(), request.snapshots()[i]);
+                Set<String> snapshotsToGet = new LinkedHashSet<>(); // to keep insertion order
+                List<Snapshot> snapshots = null;
+                for (String snapshotOrPattern : request.snapshots()) {
+                    if (Regex.isSimpleMatchPattern(snapshotOrPattern) == false) {
+                        snapshotsToGet.add(snapshotOrPattern);
+                    } else {
+                        if (snapshots == null) { // lazily load snapshots
+                            snapshots = snapshotsService.snapshots(request.repository(), request.ignoreUnavailable());
+                        }
+                        for (Snapshot snapshot : snapshots) {
+                            if (Regex.simpleMatch(snapshotOrPattern, snapshot.name())) {
+                                snapshotsToGet.add(snapshot.name());
+                            }
+                        }
+                    }
+                }
+                for (String snapshot : snapshotsToGet) {
+                    SnapshotId snapshotId = new SnapshotId(request.repository(), snapshot);
                     snapshotInfoBuilder.add(new SnapshotInfo(snapshotsService.snapshot(snapshotId)));
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
index 13b7ee9..1da2662 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
@@ -78,19 +78,19 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             indices(indices);
             aliases(aliases);
         }
-        
+
         public AliasActions(AliasAction.Type type, String index, String alias) {
             aliasAction = new AliasAction(type);
             indices(index);
             aliases(alias);
         }
-        
+
         AliasActions(AliasAction.Type type, String[] index, String alias) {
             aliasAction = new AliasAction(type);
             indices(index);
             aliases(alias);
         }
-        
+
         public AliasActions(AliasAction action) {
             this.aliasAction = action;
             indices(action.index());
@@ -110,7 +110,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             aliasAction.filter(filter);
             return this;
         }
-        
+
         public AliasActions filter(QueryBuilder filter) {
             aliasAction.filter(filter);
             return this;
@@ -197,7 +197,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             aliasAction = readAliasAction(in);
             return this;
         }
-        
+
         public void writeTo(StreamOutput out) throws IOException {
             out.writeStringArray(indices);
             out.writeStringArray(aliases);
@@ -225,7 +225,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(action));
         return this;
     }
-    
+
     /**
      * Adds an alias to the index.
      * @param alias  The alias
@@ -247,8 +247,8 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(AliasAction.Type.ADD, indices, alias).filter(filterBuilder));
         return this;
     }
-    
-    
+
+
     /**
      * Removes an alias to the index.
      *
@@ -259,7 +259,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(AliasAction.Type.REMOVE, indices, aliases));
         return this;
     }
-    
+
     /**
      * Removes an alias to the index.
      *
@@ -286,25 +286,14 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             return addValidationError("Must specify at least one alias action", validationException);
         }
         for (AliasActions aliasAction : allAliasActions) {
-            if (aliasAction.actionType() == AliasAction.Type.ADD) {
-                if (aliasAction.aliases.length != 1) {
-                    validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "] requires exactly one [alias] to be set", validationException);
-                }
-                if (!Strings.hasText(aliasAction.aliases[0])) {
-                    validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "] requires an [alias] to be set", validationException);
-                }
-            } else {
-                if (aliasAction.aliases.length == 0) {
+            if (aliasAction.aliases.length == 0) {
+                validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
+                        + "]: aliases may not be empty", validationException);
+            }
+            for (String alias : aliasAction.aliases) {
+                if (!Strings.hasText(alias)) {
                     validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "]: aliases may not be empty", validationException);
-                }
-                for (String alias : aliasAction.aliases) {
-                    if (!Strings.hasText(alias)) {
-                        validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                                + "]: [alias] may not be empty string", validationException);
-                    }
+                            + "]: [alias] may not be empty string", validationException);
                 }
             }
             if (CollectionUtils.isEmpty(aliasAction.indices)) {
@@ -345,7 +334,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
     public IndicesOptions indicesOptions() {
         return INDICES_OPTIONS;
     }
-    
+
     private static AliasActions readAliasActions(StreamInput in) throws IOException {
         AliasActions actions = new AliasActions();
         return actions.readFrom(in);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
index 6482e34..db1a03e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.admin.indices.analyze;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.support.single.shard.SingleShardRequest;
 import org.elasticsearch.common.Strings;
@@ -46,6 +47,10 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
 
     private String field;
 
+    private boolean explain = false;
+
+    private String[] attributes = Strings.EMPTY_ARRAY;
+
     public AnalyzeRequest() {
     }
 
@@ -86,6 +91,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
     }
 
     public AnalyzeRequest tokenFilters(String... tokenFilters) {
+        if (tokenFilters == null) {
+            throw new IllegalArgumentException("token filters must not be null");
+        }
         this.tokenFilters = tokenFilters;
         return this;
     }
@@ -95,6 +103,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
     }
 
     public AnalyzeRequest charFilters(String... charFilters) {
+        if (charFilters == null) {
+            throw new IllegalArgumentException("char filters must not be null");
+        }
         this.charFilters = charFilters;
         return this;
     }
@@ -112,18 +123,33 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         return this.field;
     }
 
+    public AnalyzeRequest explain(boolean explain) {
+        this.explain = explain;
+        return this;
+    }
+
+    public boolean explain() {
+        return this.explain;
+    }
+
+    public AnalyzeRequest attributes(String... attributes) {
+        if (attributes == null) {
+            throw new IllegalArgumentException("attributes must not be null");
+        }
+        this.attributes = attributes;
+        return this;
+    }
+
+    public String[] attributes() {
+        return this.attributes;
+    }
+
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
         if (text == null || text.length == 0) {
             validationException = addValidationError("text is missing", validationException);
         }
-        if (tokenFilters == null) {
-            validationException = addValidationError("token filters must not be null", validationException);
-        }
-        if (charFilters == null) {
-            validationException = addValidationError("char filters must not be null", validationException);
-        }
         return validationException;
     }
 
@@ -136,6 +162,10 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         tokenFilters = in.readStringArray();
         charFilters = in.readStringArray();
         field = in.readOptionalString();
+        if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+            explain = in.readBoolean();
+            attributes = in.readStringArray();
+        }
     }
 
     @Override
@@ -147,5 +177,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         out.writeStringArray(tokenFilters);
         out.writeStringArray(charFilters);
         out.writeOptionalString(field);
+        if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+            out.writeBoolean(explain);
+            out.writeStringArray(attributes);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
index 9ed02e6..23c1739 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
@@ -79,6 +79,22 @@ public class AnalyzeRequestBuilder extends SingleShardOperationRequestBuilder<An
     }
 
     /**
+     * Sets explain
+     */
+    public AnalyzeRequestBuilder setExplain(boolean explain) {
+        request.explain(explain);
+        return this;
+    }
+
+    /**
+     * Sets attributes that will include results
+     */
+    public AnalyzeRequestBuilder setAttributes(String attributes){
+        request.attributes(attributes);
+        return this;
+    }
+
+    /**
      * Sets texts to analyze
      */
     public AnalyzeRequestBuilder setText(String... texts) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
index 720d19c..f867c2e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.admin.indices.analyze;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -30,28 +31,32 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
 /**
  *
  */
 public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeResponse.AnalyzeToken>, ToXContent {
 
-    public static class AnalyzeToken implements Streamable {
+    public static class AnalyzeToken implements Streamable, ToXContent {
         private String term;
         private int startOffset;
         private int endOffset;
         private int position;
+        private Map<String, Object> attributes;
         private String type;
 
         AnalyzeToken() {
         }
 
-        public AnalyzeToken(String term, int position, int startOffset, int endOffset, String type) {
+        public AnalyzeToken(String term, int position, int startOffset, int endOffset, String type,
+                            Map<String, Object> attributes) {
             this.term = term;
             this.position = position;
             this.startOffset = startOffset;
             this.endOffset = endOffset;
             this.type = type;
+            this.attributes = attributes;
         }
 
         public String getTerm() {
@@ -74,6 +79,27 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             return this.type;
         }
 
+        public Map<String, Object> getAttributes(){
+            return this.attributes;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.TOKEN, term);
+            builder.field(Fields.START_OFFSET, startOffset);
+            builder.field(Fields.END_OFFSET, endOffset);
+            builder.field(Fields.TYPE, type);
+            builder.field(Fields.POSITION, position);
+            if (attributes != null && !attributes.isEmpty()) {
+                for (Map.Entry<String, Object> entity : attributes.entrySet()) {
+                    builder.field(entity.getKey(), entity.getValue());
+                }
+            }
+            builder.endObject();
+            return builder;
+        }
+
         public static AnalyzeToken readAnalyzeToken(StreamInput in) throws IOException {
             AnalyzeToken analyzeToken = new AnalyzeToken();
             analyzeToken.readFrom(in);
@@ -87,6 +113,9 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             endOffset = in.readInt();
             position = in.readVInt();
             type = in.readOptionalString();
+            if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+                attributes = (Map<String, Object>) in.readGenericValue();
+            }
         }
 
         @Override
@@ -96,22 +125,32 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             out.writeInt(endOffset);
             out.writeVInt(position);
             out.writeOptionalString(type);
+            if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+                out.writeGenericValue(attributes);
+            }
         }
     }
 
+    private DetailAnalyzeResponse detail;
+
     private List<AnalyzeToken> tokens;
 
     AnalyzeResponse() {
     }
 
-    public AnalyzeResponse(List<AnalyzeToken> tokens) {
+    public AnalyzeResponse(List<AnalyzeToken> tokens, DetailAnalyzeResponse detail) {
         this.tokens = tokens;
+        this.detail = detail;
     }
 
     public List<AnalyzeToken> getTokens() {
         return this.tokens;
     }
 
+    public DetailAnalyzeResponse detail() {
+        return this.detail;
+    }
+
     @Override
     public Iterator<AnalyzeToken> iterator() {
         return tokens.iterator();
@@ -119,17 +158,19 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(Fields.TOKENS);
-        for (AnalyzeToken token : tokens) {
-            builder.startObject();
-            builder.field(Fields.TOKEN, token.getTerm());
-            builder.field(Fields.START_OFFSET, token.getStartOffset());
-            builder.field(Fields.END_OFFSET, token.getEndOffset());
-            builder.field(Fields.TYPE, token.getType());
-            builder.field(Fields.POSITION, token.getPosition());
+        if (tokens != null) {
+            builder.startArray(Fields.TOKENS);
+            for (AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+
+        if (detail != null) {
+            builder.startObject(Fields.DETAIL);
+            detail.toXContent(builder, params);
             builder.endObject();
         }
-        builder.endArray();
         return builder;
     }
 
@@ -141,14 +182,24 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
         for (int i = 0; i < size; i++) {
             tokens.add(AnalyzeToken.readAnalyzeToken(in));
         }
+        if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+            detail = in.readOptionalStreamable(DetailAnalyzeResponse::new);
+        }
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeVInt(tokens.size());
-        for (AnalyzeToken token : tokens) {
-            token.writeTo(out);
+        if (tokens != null) {
+            out.writeVInt(tokens.size());
+            for (AnalyzeToken token : tokens) {
+                token.writeTo(out);
+            }
+        } else {
+            out.writeVInt(0);
+        }
+        if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+            out.writeOptionalStreamable(detail);
         }
     }
 
@@ -159,5 +210,6 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
         static final XContentBuilderString END_OFFSET = new XContentBuilderString("end_offset");
         static final XContentBuilderString TYPE = new XContentBuilderString("type");
         static final XContentBuilderString POSITION = new XContentBuilderString("position");
+        static final XContentBuilderString DETAIL = new XContentBuilderString("detail");
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java
new file mode 100644
index 0000000..08d18ff
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java
@@ -0,0 +1,319 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.indices.analyze;
+
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+
+import java.io.IOException;
+
+public class DetailAnalyzeResponse implements Streamable, ToXContent {
+
+    DetailAnalyzeResponse() {
+    }
+
+    private boolean customAnalyzer = false;
+    private AnalyzeTokenList analyzer;
+    private CharFilteredText[] charfilters;
+    private AnalyzeTokenList tokenizer;
+    private AnalyzeTokenList[] tokenfilters;
+
+    public DetailAnalyzeResponse(AnalyzeTokenList analyzer) {
+        this(false, analyzer, null, null, null);
+    }
+
+    public DetailAnalyzeResponse(CharFilteredText[] charfilters, AnalyzeTokenList tokenizer, AnalyzeTokenList[] tokenfilters) {
+        this(true, null, charfilters, tokenizer, tokenfilters);
+    }
+
+    public DetailAnalyzeResponse(boolean customAnalyzer,
+                                 AnalyzeTokenList analyzer,
+                                 CharFilteredText[] charfilters,
+                                 AnalyzeTokenList tokenizer,
+                                 AnalyzeTokenList[] tokenfilters) {
+        this.customAnalyzer = customAnalyzer;
+        this.analyzer = analyzer;
+        this.charfilters = charfilters;
+        this.tokenizer = tokenizer;
+        this.tokenfilters = tokenfilters;
+    }
+
+    public AnalyzeTokenList analyzer() {
+        return this.analyzer;
+    }
+
+    public DetailAnalyzeResponse analyzer(AnalyzeTokenList analyzer) {
+        this.analyzer = analyzer;
+        return this;
+    }
+
+    public CharFilteredText[] charfilters() {
+        return this.charfilters;
+    }
+
+    public DetailAnalyzeResponse charfilters(CharFilteredText[] charfilters) {
+        this.charfilters = charfilters;
+        return this;
+    }
+
+    public AnalyzeTokenList tokenizer() {
+        return tokenizer;
+    }
+
+    public DetailAnalyzeResponse tokenizer(AnalyzeTokenList tokenizer) {
+        this.tokenizer = tokenizer;
+        return this;
+    }
+
+    public AnalyzeTokenList[] tokenfilters() {
+        return tokenfilters;
+    }
+
+    public DetailAnalyzeResponse tokenfilters(AnalyzeTokenList[] tokenfilters) {
+        this.tokenfilters = tokenfilters;
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.field(Fields.CUSTOM_ANALYZER, customAnalyzer);
+
+        if (analyzer != null) {
+            builder.startObject(Fields.ANALYZER);
+            analyzer.toXContentWithoutObject(builder, params);
+            builder.endObject();
+        }
+
+        if (charfilters != null) {
+            builder.startArray(Fields.CHARFILTERS);
+            for (CharFilteredText charfilter : charfilters) {
+                charfilter.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+
+        if (tokenizer != null) {
+            builder.startObject(Fields.TOKENIZER);
+            tokenizer.toXContentWithoutObject(builder, params);
+            builder.endObject();
+        }
+
+        if (tokenfilters != null) {
+            builder.startArray(Fields.TOKENFILTERS);
+            for (AnalyzeTokenList tokenfilter : tokenfilters) {
+                tokenfilter.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString NAME = new XContentBuilderString("name");
+        static final XContentBuilderString FILTERED_TEXT = new XContentBuilderString("filtered_text");
+        static final XContentBuilderString CUSTOM_ANALYZER = new XContentBuilderString("custom_analyzer");
+        static final XContentBuilderString ANALYZER = new XContentBuilderString("analyzer");
+        static final XContentBuilderString CHARFILTERS = new XContentBuilderString("charfilters");
+        static final XContentBuilderString TOKENIZER = new XContentBuilderString("tokenizer");
+        static final XContentBuilderString TOKENFILTERS = new XContentBuilderString("tokenfilters");
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        this.customAnalyzer = in.readBoolean();
+        if (customAnalyzer) {
+            tokenizer = AnalyzeTokenList.readAnalyzeTokenList(in);
+            int size = in.readVInt();
+            if (size > 0) {
+                charfilters = new CharFilteredText[size];
+                for (int i = 0; i < size; i++) {
+                    charfilters[i] = CharFilteredText.readCharFilteredText(in);
+                }
+            }
+            size = in.readVInt();
+            if (size > 0) {
+                tokenfilters = new AnalyzeTokenList[size];
+                for (int i = 0; i < size; i++) {
+                    tokenfilters[i] = AnalyzeTokenList.readAnalyzeTokenList(in);
+                }
+            }
+        } else {
+            analyzer = AnalyzeTokenList.readAnalyzeTokenList(in);
+        }
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeBoolean(customAnalyzer);
+        if (customAnalyzer) {
+            tokenizer.writeTo(out);
+            if (charfilters != null) {
+                out.writeVInt(charfilters.length);
+                for (CharFilteredText charfilter : charfilters) {
+                    charfilter.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+            if (tokenfilters != null) {
+                out.writeVInt(tokenfilters.length);
+                for (AnalyzeTokenList tokenfilter : tokenfilters) {
+                    tokenfilter.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+        } else {
+            analyzer.writeTo(out);
+        }
+    }
+
+    public static class AnalyzeTokenList implements Streamable, ToXContent {
+        private String name;
+        private AnalyzeResponse.AnalyzeToken[] tokens;
+
+        AnalyzeTokenList() {
+        }
+
+        public AnalyzeTokenList(String name, AnalyzeResponse.AnalyzeToken[] tokens) {
+            this.name = name;
+            this.tokens = tokens;
+        }
+
+        public String getName() {
+            return name;
+        }
+
+        public AnalyzeResponse.AnalyzeToken[] getTokens() {
+            return tokens;
+        }
+
+        public static AnalyzeTokenList readAnalyzeTokenList(StreamInput in) throws IOException {
+            AnalyzeTokenList list = new AnalyzeTokenList();
+            list.readFrom(in);
+            return list;
+        }
+
+        public XContentBuilder toXContentWithoutObject(XContentBuilder builder, Params params) throws IOException {
+            builder.field(Fields.NAME, this.name);
+            builder.startArray(AnalyzeResponse.Fields.TOKENS);
+            for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+            return builder;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.NAME, this.name);
+            builder.startArray(AnalyzeResponse.Fields.TOKENS);
+            for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+            builder.endObject();
+            return builder;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            name = in.readString();
+            int size = in.readVInt();
+            if (size > 0) {
+                tokens = new AnalyzeResponse.AnalyzeToken[size];
+                for (int i = 0; i < size; i++) {
+                    tokens[i] = AnalyzeResponse.AnalyzeToken.readAnalyzeToken(in);
+                }
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(name);
+            if (tokens != null) {
+                out.writeVInt(tokens.length);
+                for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                    token.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+        }
+    }
+
+    public static class CharFilteredText implements Streamable, ToXContent {
+        private String name;
+        private String[] texts;
+        CharFilteredText() {
+        }
+
+        public CharFilteredText(String name, String[] texts) {
+            this.name = name;
+            if (texts != null) {
+                this.texts = texts;
+            } else {
+                this.texts = Strings.EMPTY_ARRAY;
+            }
+        }
+
+        public String getName() {
+            return name;
+        }
+
+        public String[] getTexts() {
+            return texts;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.NAME, name);
+            builder.field(Fields.FILTERED_TEXT, texts);
+            builder.endObject();
+            return builder;
+        }
+
+        public static CharFilteredText readCharFilteredText(StreamInput in) throws IOException {
+            CharFilteredText text = new CharFilteredText();
+            text.readFrom(in);
+            return text;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            name = in.readString();
+            texts = in.readStringArray();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(name);
+            out.writeStringArray(texts);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
index ba49c33..ecdf977 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
@@ -20,10 +20,15 @@ package org.elasticsearch.action.admin.indices.analyze;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeReflector;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.single.shard.TransportSingleShardAction;
@@ -33,6 +38,7 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.routing.ShardsIterator;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.io.FastStringReader;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexService;
@@ -46,8 +52,8 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
+import java.io.Reader;
+import java.util.*;
 
 /**
  * Transport action used to execute analyze requests
@@ -222,6 +228,23 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
             throw new IllegalArgumentException("failed to find analyzer");
         }
 
+        List<AnalyzeResponse.AnalyzeToken> tokens = null;
+        DetailAnalyzeResponse detail = null;
+
+        if (request.explain()) {
+            detail = detailAnalyze(request, analyzer, field);
+        } else {
+            tokens = simpleAnalyze(request, analyzer, field);
+        }
+
+        if (closeAnalyzer) {
+            analyzer.close();
+        }
+
+        return new AnalyzeResponse(tokens, detail);
+    }
+
+    private static List<AnalyzeResponse.AnalyzeToken> simpleAnalyze(AnalyzeRequest request, Analyzer analyzer, String field) {
         List<AnalyzeResponse.AnalyzeToken> tokens = new ArrayList<>();
         int lastPosition = -1;
         int lastOffset = 0;
@@ -238,7 +261,7 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
                     if (increment > 0) {
                         lastPosition = lastPosition + increment;
                     }
-                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(), lastOffset + offset.endOffset(), type.type()));
+                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(), lastOffset + offset.endOffset(), type.type(), null));
 
                 }
                 stream.end();
@@ -251,11 +274,211 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
                 throw new ElasticsearchException("failed to analyze", e);
             }
         }
+        return tokens;
+    }
 
-        if (closeAnalyzer) {
-            analyzer.close();
+    private static DetailAnalyzeResponse detailAnalyze(AnalyzeRequest request, Analyzer analyzer, String field) {
+        DetailAnalyzeResponse detailResponse;
+        final Set<String> includeAttributes = new HashSet<>();
+        if (request.attributes() != null) {
+            for (String attribute : request.attributes()) {
+                includeAttributes.add(attribute.toLowerCase(Locale.ROOT));
+            }
+        }
+
+        CustomAnalyzer customAnalyzer = null;
+        if (analyzer instanceof CustomAnalyzer) {
+            customAnalyzer = (CustomAnalyzer) analyzer;
+        } else if (analyzer instanceof NamedAnalyzer && ((NamedAnalyzer) analyzer).analyzer() instanceof CustomAnalyzer) {
+            customAnalyzer = (CustomAnalyzer) ((NamedAnalyzer) analyzer).analyzer();
+        }
+
+        if (customAnalyzer != null) {
+            // customAnalyzer = divide charfilter, tokenizer tokenfilters
+            CharFilterFactory[] charFilterFactories = customAnalyzer.charFilters();
+            TokenizerFactory tokenizerFactory = customAnalyzer.tokenizerFactory();
+            TokenFilterFactory[] tokenFilterFactories = customAnalyzer.tokenFilters();
+
+            String[][] charFiltersTexts = new String[charFilterFactories != null ? charFilterFactories.length : 0][request.text().length];
+            TokenListCreator[] tokenFiltersTokenListCreator = new TokenListCreator[tokenFilterFactories != null ? tokenFilterFactories.length : 0];
+
+            TokenListCreator tokenizerTokenListCreator = new TokenListCreator();
+
+            for (int textIndex = 0; textIndex < request.text().length; textIndex++) {
+                String charFilteredSource = request.text()[textIndex];
+
+                Reader reader = new FastStringReader(charFilteredSource);
+                if (charFilterFactories != null) {
+
+                    for (int charFilterIndex = 0; charFilterIndex < charFilterFactories.length; charFilterIndex++) {
+                        reader = charFilterFactories[charFilterIndex].create(reader);
+                        Reader readerForWriteOut = new FastStringReader(charFilteredSource);
+                        readerForWriteOut = charFilterFactories[charFilterIndex].create(readerForWriteOut);
+                        charFilteredSource = writeCharStream(readerForWriteOut);
+                        charFiltersTexts[charFilterIndex][textIndex] = charFilteredSource;
+                    }
+                }
+
+                // analyzing only tokenizer
+                Tokenizer tokenizer = tokenizerFactory.create();
+                tokenizer.setReader(reader);
+                tokenizerTokenListCreator.analyze(tokenizer, customAnalyzer, field, includeAttributes);
+
+                // analyzing each tokenfilter
+                if (tokenFilterFactories != null) {
+                    for (int tokenFilterIndex = 0; tokenFilterIndex < tokenFilterFactories.length; tokenFilterIndex++) {
+                        if (tokenFiltersTokenListCreator[tokenFilterIndex] == null) {
+                            tokenFiltersTokenListCreator[tokenFilterIndex] = new TokenListCreator();
+                        }
+                        TokenStream stream = createStackedTokenStream(request.text()[textIndex],
+                            charFilterFactories, tokenizerFactory, tokenFilterFactories, tokenFilterIndex + 1);
+                        tokenFiltersTokenListCreator[tokenFilterIndex].analyze(stream, customAnalyzer, field, includeAttributes);
+                    }
+                }
+            }
+
+            DetailAnalyzeResponse.CharFilteredText[] charFilteredLists = new DetailAnalyzeResponse.CharFilteredText[charFiltersTexts.length];
+            if (charFilterFactories != null) {
+                for (int charFilterIndex = 0; charFilterIndex < charFiltersTexts.length; charFilterIndex++) {
+                    charFilteredLists[charFilterIndex] = new DetailAnalyzeResponse.CharFilteredText(
+                        charFilterFactories[charFilterIndex].name(), charFiltersTexts[charFilterIndex]);
+                }
+            }
+            DetailAnalyzeResponse.AnalyzeTokenList[] tokenFilterLists = new DetailAnalyzeResponse.AnalyzeTokenList[tokenFiltersTokenListCreator.length];
+            if (tokenFilterFactories != null) {
+                for (int tokenFilterIndex = 0; tokenFilterIndex < tokenFiltersTokenListCreator.length; tokenFilterIndex++) {
+                    tokenFilterLists[tokenFilterIndex] = new DetailAnalyzeResponse.AnalyzeTokenList(
+                        tokenFilterFactories[tokenFilterIndex].name(), tokenFiltersTokenListCreator[tokenFilterIndex].getArrayTokens());
+                }
+            }
+            detailResponse = new DetailAnalyzeResponse(charFilteredLists, new DetailAnalyzeResponse.AnalyzeTokenList(tokenizerFactory.name(), tokenizerTokenListCreator.getArrayTokens()), tokenFilterLists);
+        } else {
+            String name;
+            if (analyzer instanceof NamedAnalyzer) {
+                name = ((NamedAnalyzer) analyzer).name();
+            } else {
+                name = analyzer.getClass().getName();
+            }
+
+            TokenListCreator tokenListCreator = new TokenListCreator();
+            for (String text : request.text()) {
+                tokenListCreator.analyze(analyzer.tokenStream(field, text), analyzer, field,
+                        includeAttributes);
+            }
+            detailResponse = new DetailAnalyzeResponse(new DetailAnalyzeResponse.AnalyzeTokenList(name, tokenListCreator.getArrayTokens()));
         }
+        return detailResponse;
+    }
+
+    private static TokenStream createStackedTokenStream(String source, CharFilterFactory[] charFilterFactories, TokenizerFactory tokenizerFactory, TokenFilterFactory[] tokenFilterFactories, int current) {
+        Reader reader = new FastStringReader(source);
+        for (CharFilterFactory charFilterFactory : charFilterFactories) {
+            reader = charFilterFactory.create(reader);
+        }
+        Tokenizer tokenizer = tokenizerFactory.create();
+        tokenizer.setReader(reader);
+        TokenStream tokenStream = tokenizer;
+        for (int i = 0; i < current; i++) {
+            tokenStream = tokenFilterFactories[i].create(tokenStream);
+        }
+        return tokenStream;
+    }
+
+    private static String writeCharStream(Reader input) {
+        final int BUFFER_SIZE = 1024;
+        char[] buf = new char[BUFFER_SIZE];
+        int len;
+        StringBuilder sb = new StringBuilder();
+        do {
+            try {
+                len = input.read(buf, 0, BUFFER_SIZE);
+            } catch (IOException e) {
+                throw new ElasticsearchException("failed to analyze (charFiltering)", e);
+            }
+            if (len > 0)
+                sb.append(buf, 0, len);
+        } while (len == BUFFER_SIZE);
+        return sb.toString();
+    }
+
+    private static class TokenListCreator {
+        int lastPosition = -1;
+        int lastOffset = 0;
+        List<AnalyzeResponse.AnalyzeToken> tokens;
+
+        TokenListCreator() {
+            tokens = new ArrayList<>();
+        }
+
+        private void analyze(TokenStream stream, Analyzer analyzer, String field, Set<String> includeAttributes) {
+            try {
+                stream.reset();
+                CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
+                PositionIncrementAttribute posIncr = stream.addAttribute(PositionIncrementAttribute.class);
+                OffsetAttribute offset = stream.addAttribute(OffsetAttribute.class);
+                TypeAttribute type = stream.addAttribute(TypeAttribute.class);
+
+                while (stream.incrementToken()) {
+                    int increment = posIncr.getPositionIncrement();
+                    if (increment > 0) {
+                        lastPosition = lastPosition + increment;
+                    }
+                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(),
+                        lastOffset +offset.endOffset(), type.type(), extractExtendedAttributes(stream, includeAttributes)));
+
+                }
+                stream.end();
+                lastOffset += offset.endOffset();
+                lastPosition += posIncr.getPositionIncrement();
+
+                lastPosition += analyzer.getPositionIncrementGap(field);
+                lastOffset += analyzer.getOffsetGap(field);
+
+            } catch (IOException e) {
+                throw new ElasticsearchException("failed to analyze", e);
+            } finally {
+                IOUtils.closeWhileHandlingException(stream);
+            }
+        }
+
+        private AnalyzeResponse.AnalyzeToken[] getArrayTokens() {
+            return tokens.toArray(new AnalyzeResponse.AnalyzeToken[tokens.size()]);
+        }
+
+    }
+
+    /**
+     * other attribute extract object.
+     * Extracted object group by AttributeClassName
+     *
+     * @param stream current TokenStream
+     * @param includeAttributes filtering attributes
+     * @return Map&lt;key value&gt;
+     */
+    private static Map<String, Object> extractExtendedAttributes(TokenStream stream, final Set<String> includeAttributes) {
+        final Map<String, Object> extendedAttributes = new TreeMap<>();
+
+        stream.reflectWith(new AttributeReflector() {
+            @Override
+            public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
+                if (CharTermAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (PositionIncrementAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (OffsetAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (TypeAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (includeAttributes == null || includeAttributes.isEmpty() || includeAttributes.contains(key.toLowerCase(Locale.ROOT))) {
+                    if (value instanceof BytesRef) {
+                        final BytesRef p = (BytesRef) value;
+                        value = p.toString();
+                    }
+                    extendedAttributes.put(key, value);
+                }
+            }
+        });
 
-        return new AnalyzeResponse(tokens);
+        return extendedAttributes;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
index 10db46c..ccf06be 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
@@ -22,6 +22,7 @@ package org.elasticsearch.action.admin.indices.flush;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
 
@@ -29,8 +30,8 @@ public class ShardFlushRequest extends ReplicationRequest<ShardFlushRequest> {
 
     private FlushRequest request = new FlushRequest();
 
-    public ShardFlushRequest(FlushRequest request) {
-        super(request);
+    public ShardFlushRequest(FlushRequest request, ShardId shardId) {
+        super(request, shardId);
         this.request = request;
     }
 
@@ -53,5 +54,8 @@ public class ShardFlushRequest extends ReplicationRequest<ShardFlushRequest> {
         request.writeTo(out);
     }
 
-
+    @Override
+    public String toString() {
+        return "flush {" + super.toString() + "}";
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
index ac15962..d2a8f1a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.flush;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction;
@@ -36,7 +36,7 @@ import java.util.List;
 /**
  * Flush Action.
  */
-public class TransportFlushAction extends TransportBroadcastReplicationAction<FlushRequest, FlushResponse, ShardFlushRequest, ActionWriteResponse> {
+public class TransportFlushAction extends TransportBroadcastReplicationAction<FlushRequest, FlushResponse, ShardFlushRequest, ReplicationResponse> {
 
     @Inject
     public TransportFlushAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
@@ -47,13 +47,13 @@ public class TransportFlushAction extends TransportBroadcastReplicationAction<Fl
     }
 
     @Override
-    protected ActionWriteResponse newShardResponse() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newShardResponse() {
+        return new ReplicationResponse();
     }
 
     @Override
     protected ShardFlushRequest newShardRequest(FlushRequest request, ShardId shardId) {
-        return new ShardFlushRequest(request).setShardId(shardId);
+        return new ShardFlushRequest(request, shardId);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
index f768cfe..8f7fce8 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
@@ -19,22 +19,19 @@
 
 package org.elasticsearch.action.admin.indices.flush;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
-import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -42,7 +39,7 @@ import org.elasticsearch.transport.TransportService;
 /**
  *
  */
-public class TransportShardFlushAction extends TransportReplicationAction<ShardFlushRequest, ShardFlushRequest, ActionWriteResponse> {
+public class TransportShardFlushAction extends TransportReplicationAction<ShardFlushRequest, ShardFlushRequest, ReplicationResponse> {
 
     public static final String NAME = FlushAction.NAME + "[s]";
 
@@ -56,20 +53,20 @@ public class TransportShardFlushAction extends TransportReplicationAction<ShardF
     }
 
     @Override
-    protected ActionWriteResponse newResponseInstance() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newResponseInstance() {
+        return new ReplicationResponse();
     }
 
     @Override
-    protected Tuple<ActionWriteResponse, ShardFlushRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
-        indexShard.flush(shardRequest.request.getRequest());
+    protected Tuple<ReplicationResponse, ShardFlushRequest> shardOperationOnPrimary(MetaData metaData, ShardFlushRequest shardRequest) throws Throwable {
+        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId().getIndex()).getShard(shardRequest.shardId().id());
+        indexShard.flush(shardRequest.getRequest());
         logger.trace("{} flush request executed on primary", indexShard.shardId());
-        return new Tuple<>(new ActionWriteResponse(), shardRequest.request);
+        return new Tuple<>(new ReplicationResponse(), shardRequest);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, ShardFlushRequest request) {
+    protected void shardOperationOnReplica(ShardFlushRequest request) {
         IndexShard indexShard = indicesService.indexServiceSafe(request.shardId().getIndex()).getShard(request.shardId().id());
         indexShard.flush(request.getRequest());
         logger.trace("{} flush request executed on replica", indexShard.shardId());
@@ -81,18 +78,13 @@ public class TransportShardFlushAction extends TransportReplicationAction<ShardF
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.getRoutingTable().indicesRouting().get(request.concreteIndex()).getShards().get(request.request().shardId().getId()).shardsIt();
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-    @Override
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_WRITE, new String[]{request.concreteIndex()});
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
index e2d978d..a76b714 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.refresh;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
@@ -37,7 +37,7 @@ import java.util.List;
 /**
  * Refresh action.
  */
-public class TransportRefreshAction extends TransportBroadcastReplicationAction<RefreshRequest, RefreshResponse, ReplicationRequest, ActionWriteResponse> {
+public class TransportRefreshAction extends TransportBroadcastReplicationAction<RefreshRequest, RefreshResponse, ReplicationRequest, ReplicationResponse> {
 
     @Inject
     public TransportRefreshAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
@@ -48,13 +48,13 @@ public class TransportRefreshAction extends TransportBroadcastReplicationAction<
     }
 
     @Override
-    protected ActionWriteResponse newShardResponse() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newShardResponse() {
+        return new ReplicationResponse();
     }
 
     @Override
     protected ReplicationRequest newShardRequest(RefreshRequest request, ShardId shardId) {
-        return new ReplicationRequest(request).setShardId(shardId);
+        return new ReplicationRequest(request, shardId);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
index a06483a..c78977f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
@@ -19,18 +19,16 @@
 
 package org.elasticsearch.action.admin.indices.refresh;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
 import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
-import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -43,7 +41,7 @@ import org.elasticsearch.transport.TransportService;
 /**
  *
  */
-public class TransportShardRefreshAction extends TransportReplicationAction<ReplicationRequest, ReplicationRequest, ActionWriteResponse> {
+public class TransportShardRefreshAction extends TransportReplicationAction<ReplicationRequest, ReplicationRequest, ReplicationResponse> {
 
     public static final String NAME = RefreshAction.NAME + "[s]";
 
@@ -57,20 +55,21 @@ public class TransportShardRefreshAction extends TransportReplicationAction<Repl
     }
 
     @Override
-    protected ActionWriteResponse newResponseInstance() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newResponseInstance() {
+        return new ReplicationResponse();
     }
 
     @Override
-    protected Tuple<ActionWriteResponse, ReplicationRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
+    protected Tuple<ReplicationResponse, ReplicationRequest> shardOperationOnPrimary(MetaData metaData, ReplicationRequest shardRequest) throws Throwable {
+        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId().getIndex()).getShard(shardRequest.shardId().id());
         indexShard.refresh("api");
         logger.trace("{} refresh request executed on primary", indexShard.shardId());
-        return new Tuple<>(new ActionWriteResponse(), shardRequest.request);
+        return new Tuple<>(new ReplicationResponse(), shardRequest);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, ReplicationRequest request) {
+    protected void shardOperationOnReplica(ReplicationRequest request) {
+        final ShardId shardId = request.shardId();
         IndexShard indexShard = indicesService.indexServiceSafe(shardId.getIndex()).getShard(shardId.id());
         indexShard.refresh("api");
         logger.trace("{} refresh request executed on replica", indexShard.shardId());
@@ -82,18 +81,13 @@ public class TransportShardRefreshAction extends TransportReplicationAction<Repl
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.getRoutingTable().indicesRouting().get(request.concreteIndex()).getShards().get(request.request().shardId().getId()).shardsIt();
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-    @Override
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_WRITE, new String[]{request.concreteIndex()});
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
index 80e86ea..9827000 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
@@ -19,14 +19,18 @@
 
 package org.elasticsearch.action.bulk;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.action.delete.DeleteResponse;
 import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -35,7 +39,39 @@ import java.io.IOException;
  * Represents a single item response for an action executed as part of the bulk API. Holds the index/type/id
  * of the relevant action, and if it has failed or not (with the failure message incase it failed).
  */
-public class BulkItemResponse implements Streamable {
+public class BulkItemResponse implements Streamable, StatusToXContent {
+
+    @Override
+    public RestStatus status() {
+        return failure == null ? response.status() : failure.getStatus();
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(opType);
+        if (failure == null) {
+            response.toXContent(builder, params);
+            builder.field(Fields.STATUS, response.status());
+        } else {
+            builder.field(Fields._INDEX, failure.getIndex());
+            builder.field(Fields._TYPE, failure.getType());
+            builder.field(Fields._ID, failure.getId());
+            builder.field(Fields.STATUS, failure.getStatus());
+            builder.startObject(Fields.ERROR);
+            ElasticsearchException.toXContent(builder, params, failure.getCause());
+            builder.endObject();
+        }
+        builder.endObject();
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
+        static final XContentBuilderString _ID = new XContentBuilderString("_id");
+        static final XContentBuilderString STATUS = new XContentBuilderString("status");
+        static final XContentBuilderString ERROR = new XContentBuilderString("error");
+    }
 
     /**
      * Represents a failure.
@@ -99,7 +135,7 @@ public class BulkItemResponse implements Streamable {
 
     private String opType;
 
-    private ActionWriteResponse response;
+    private DocWriteResponse response;
 
     private Failure failure;
 
@@ -107,7 +143,7 @@ public class BulkItemResponse implements Streamable {
 
     }
 
-    public BulkItemResponse(int id, String opType, ActionWriteResponse response) {
+    public BulkItemResponse(int id, String opType, DocWriteResponse response) {
         this.id = id;
         this.opType = opType;
         this.response = response;
@@ -140,14 +176,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getIndex();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getIndex();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getIndex();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getIndex();
-        }
-        return null;
+        return response.getIndex();
     }
 
     /**
@@ -157,14 +186,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getType();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getType();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getType();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getType();
-        }
-        return null;
+        return response.getType();
     }
 
     /**
@@ -174,14 +196,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getId();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getId();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getId();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getId();
-        }
-        return null;
+        return response.getId();
     }
 
     /**
@@ -191,21 +206,14 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return -1;
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getVersion();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getVersion();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getVersion();
-        }
-        return -1;
+        return response.getVersion();
     }
 
     /**
      * The actual response ({@link IndexResponse} or {@link DeleteResponse}). <tt>null</tt> in
      * case of failure.
      */
-    public <T extends ActionWriteResponse> T getResponse() {
+    public <T extends DocWriteResponse> T getResponse() {
         return (T) response;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
index ec15038..1edba16 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
@@ -40,10 +40,8 @@ public class BulkShardRequest extends ReplicationRequest<BulkShardRequest> {
     public BulkShardRequest() {
     }
 
-    BulkShardRequest(BulkRequest bulkRequest, String index, int shardId, boolean refresh, BulkItemRequest[] items) {
-        super(bulkRequest);
-        this.index = index;
-        this.setShardId(new ShardId(index, shardId));
+    BulkShardRequest(BulkRequest bulkRequest, ShardId shardId, boolean refresh, BulkItemRequest[] items) {
+        super(bulkRequest, shardId);
         this.items = items;
         this.refresh = refresh;
     }
@@ -93,4 +91,9 @@ public class BulkShardRequest extends ReplicationRequest<BulkShardRequest> {
         }
         refresh = in.readBoolean();
     }
+
+    @Override
+    public String toString() {
+        return "shard bulk {" + super.toString() + "}";
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
index 6b08627..76c80a9 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.bulk;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.index.shard.ShardId;
@@ -29,7 +29,7 @@ import java.io.IOException;
 /**
  *
  */
-public class BulkShardResponse extends ActionWriteResponse {
+public class BulkShardResponse extends ReplicationResponse {
 
     private ShardId shardId;
     private BulkItemResponse[] responses;
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
index 51d32e3..9b18d03 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
@@ -275,7 +275,7 @@ public class TransportBulkAction extends HandledTransportAction<BulkRequest, Bul
                         list.add(new BulkItemRequest(i, new DeleteRequest(deleteRequest)));
                     }
                 } else {
-                    ShardId shardId = clusterService.operationRouting().deleteShards(clusterState, concreteIndex, deleteRequest.type(), deleteRequest.id(), deleteRequest.routing()).shardId();
+                    ShardId shardId = clusterService.operationRouting().indexShards(clusterState, concreteIndex, deleteRequest.type(), deleteRequest.id(), deleteRequest.routing()).shardId();
                     List<BulkItemRequest> list = requestsByShard.get(shardId);
                     if (list == null) {
                         list = new ArrayList<>();
@@ -312,7 +312,7 @@ public class TransportBulkAction extends HandledTransportAction<BulkRequest, Bul
         for (Map.Entry<ShardId, List<BulkItemRequest>> entry : requestsByShard.entrySet()) {
             final ShardId shardId = entry.getKey();
             final List<BulkItemRequest> requests = entry.getValue();
-            BulkShardRequest bulkShardRequest = new BulkShardRequest(bulkRequest, shardId.index().name(), shardId.id(), bulkRequest.refresh(), requests.toArray(new BulkItemRequest[requests.size()]));
+            BulkShardRequest bulkShardRequest = new BulkShardRequest(bulkRequest, shardId, bulkRequest.refresh(), requests.toArray(new BulkItemRequest[requests.size()]));
             bulkShardRequest.consistencyLevel(bulkRequest.consistencyLevel());
             bulkShardRequest.timeout(bulkRequest.timeout());
             shardBulkAction.execute(bulkShardRequest, new ActionListener<BulkShardResponse>() {
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index e51a1b9..2597695 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -35,12 +35,11 @@ import org.elasticsearch.action.update.UpdateHelper;
 import org.elasticsearch.action.update.UpdateRequest;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
@@ -88,11 +87,6 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected TransportRequestOptions transportOptions() {
         return BulkAction.INSTANCE.transportOptions(settings);
     }
@@ -108,15 +102,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.routingTable().index(request.concreteIndex()).shard(request.request().shardId().id()).shardsIt();
-    }
-
-    @Override
-    protected Tuple<BulkShardResponse, BulkShardRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) {
-        final BulkShardRequest request = shardRequest.request;
+    protected Tuple<BulkShardResponse, BulkShardRequest> shardOperationOnPrimary(MetaData metaData, BulkShardRequest request) {
         final IndexService indexService = indicesService.indexServiceSafe(request.index());
-        final IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
+        final IndexShard indexShard = indexService.getShard(request.shardId().id());
 
         long[] preVersions = new long[request.items().length];
         VersionType[] preVersionTypes = new VersionType[request.items().length];
@@ -128,7 +116,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 preVersions[requestIndex] = indexRequest.version();
                 preVersionTypes[requestIndex] = indexRequest.versionType();
                 try {
-                    WriteResult<IndexResponse> result = shardIndexOperation(request, indexRequest, clusterState, indexShard, true);
+                    WriteResult<IndexResponse> result = shardIndexOperation(request, indexRequest, metaData, indexShard, true);
                     location = locationToSync(location, result.location);
                     // add the response
                     IndexResponse indexResponse = result.response();
@@ -143,9 +131,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                         throw (ElasticsearchException) e;
                     }
                     if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
-                        logger.trace("{} failed to execute bulk item (index) {}", e, shardRequest.shardId, indexRequest);
+                        logger.trace("{} failed to execute bulk item (index) {}", e, request.shardId(), indexRequest);
                     } else {
-                        logger.debug("{} failed to execute bulk item (index) {}", e, shardRequest.shardId, indexRequest);
+                        logger.debug("{} failed to execute bulk item (index) {}", e, request.shardId(), indexRequest);
                     }
                     // if its a conflict failure, and we already executed the request on a primary (and we execute it
                     // again, due to primary relocation and only processing up to N bulk items when the shard gets closed)
@@ -178,9 +166,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                         throw (ElasticsearchException) e;
                     }
                     if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
-                        logger.trace("{} failed to execute bulk item (delete) {}", e, shardRequest.shardId, deleteRequest);
+                        logger.trace("{} failed to execute bulk item (delete) {}", e, request.shardId(), deleteRequest);
                     } else {
-                        logger.debug("{} failed to execute bulk item (delete) {}", e, shardRequest.shardId, deleteRequest);
+                        logger.debug("{} failed to execute bulk item (delete) {}", e, request.shardId(), deleteRequest);
                     }
                     // if its a conflict failure, and we already executed the request on a primary (and we execute it
                     // again, due to primary relocation and only processing up to N bulk items when the shard gets closed)
@@ -200,7 +188,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 for (int updateAttemptsCount = 0; updateAttemptsCount <= updateRequest.retryOnConflict(); updateAttemptsCount++) {
                     UpdateResult updateResult;
                     try {
-                        updateResult = shardUpdateOperation(clusterState, request, updateRequest, indexShard);
+                        updateResult = shardUpdateOperation(metaData, request, updateRequest, indexShard);
                     } catch (Throwable t) {
                         updateResult = new UpdateResult(null, null, false, t, null);
                     }
@@ -216,10 +204,10 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                 BytesReference indexSourceAsBytes = indexRequest.source();
                                 // add the response
                                 IndexResponse indexResponse = result.response();
-                                UpdateResponse updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getIndex(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.isCreated());
+                                UpdateResponse updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.isCreated());
                                 if (updateRequest.fields() != null && updateRequest.fields().length > 0) {
                                     Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true);
-                                    updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, shardRequest.request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes));
+                                    updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes));
                                 }
                                 item = request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), indexRequest);
                                 setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, updateResponse));
@@ -228,8 +216,8 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                 WriteResult<DeleteResponse> writeResult = updateResult.writeResult;
                                 DeleteResponse response = writeResult.response();
                                 DeleteRequest deleteRequest = updateResult.request();
-                                updateResponse = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), false);
-                                updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, shardRequest.request.index(), response.getVersion(), updateResult.result.updatedSourceAsMap(), updateResult.result.updateSourceContentType(), null));
+                                updateResponse = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), false);
+                                updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), response.getVersion(), updateResult.result.updatedSourceAsMap(), updateResult.result.updateSourceContentType(), null));
                                 // Replace the update request to the translated delete request to execute on the replica.
                                 item = request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), deleteRequest);
                                 setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, updateResponse));
@@ -264,16 +252,16 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                             if (item.getPrimaryResponse() != null && isConflictException(t)) {
                                 setResponse(item, item.getPrimaryResponse());
                             } else if (updateResult.result == null) {
-                                setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, new BulkItemResponse.Failure(shardRequest.request.index(), updateRequest.type(), updateRequest.id(), t)));
+                                setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, new BulkItemResponse.Failure(request.index(), updateRequest.type(), updateRequest.id(), t)));
                             } else {
                                 switch (updateResult.result.operation()) {
                                     case UPSERT:
                                     case INDEX:
                                         IndexRequest indexRequest = updateResult.request();
                                         if (ExceptionsHelper.status(t) == RestStatus.CONFLICT) {
-                                            logger.trace("{} failed to execute bulk item (index) {}", t, shardRequest.shardId, indexRequest);
+                                            logger.trace("{} failed to execute bulk item (index) {}", t, request.shardId(), indexRequest);
                                         } else {
-                                            logger.debug("{} failed to execute bulk item (index) {}", t, shardRequest.shardId, indexRequest);
+                                            logger.debug("{} failed to execute bulk item (index) {}", t, request.shardId(), indexRequest);
                                         }
                                         setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE,
                                                 new BulkItemResponse.Failure(request.index(), indexRequest.type(), indexRequest.id(), t)));
@@ -281,9 +269,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                     case DELETE:
                                         DeleteRequest deleteRequest = updateResult.request();
                                         if (ExceptionsHelper.status(t) == RestStatus.CONFLICT) {
-                                            logger.trace("{} failed to execute bulk item (delete) {}", t, shardRequest.shardId, deleteRequest);
+                                            logger.trace("{} failed to execute bulk item (delete) {}", t, request.shardId(), deleteRequest);
                                         } else {
-                                            logger.debug("{} failed to execute bulk item (delete) {}", t, shardRequest.shardId, deleteRequest);
+                                            logger.debug("{} failed to execute bulk item (delete) {}", t, request.shardId(), deleteRequest);
                                         }
                                         setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_DELETE,
                                                 new BulkItemResponse.Failure(request.index(), deleteRequest.type(), deleteRequest.id(), t)));
@@ -310,7 +298,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         for (int i = 0; i < items.length; i++) {
             responses[i] = items[i].getPrimaryResponse();
         }
-        return new Tuple<>(new BulkShardResponse(shardRequest.shardId, responses), shardRequest.request);
+        return new Tuple<>(new BulkShardResponse(request.shardId(), responses), request);
     }
 
     private void setResponse(BulkItemRequest request, BulkItemResponse response) {
@@ -320,11 +308,11 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         }
     }
 
-    private WriteResult<IndexResponse> shardIndexOperation(BulkShardRequest request, IndexRequest indexRequest, ClusterState clusterState,
+    private WriteResult shardIndexOperation(BulkShardRequest request, IndexRequest indexRequest, MetaData metaData,
                                             IndexShard indexShard, boolean processed) throws Throwable {
 
         // validate, if routing is required, that we got routing
-        MappingMetaData mappingMd = clusterState.metaData().index(request.index()).mappingOrDefault(indexRequest.type());
+        MappingMetaData mappingMd = metaData.index(request.index()).mappingOrDefault(indexRequest.type());
         if (mappingMd != null && mappingMd.routing().required()) {
             if (indexRequest.routing() == null) {
                 throw new RoutingMissingException(request.index(), indexRequest.type(), indexRequest.id());
@@ -332,7 +320,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         }
 
         if (!processed) {
-            indexRequest.process(clusterState.metaData(), mappingMd, allowIdGeneration, request.index());
+            indexRequest.process(metaData, mappingMd, allowIdGeneration, request.index());
         }
         return TransportIndexAction.executeIndexRequestOnPrimary(indexRequest, indexShard, mappingUpdatedAction);
     }
@@ -390,14 +378,14 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
 
     }
 
-    private UpdateResult shardUpdateOperation(ClusterState clusterState, BulkShardRequest bulkShardRequest, UpdateRequest updateRequest, IndexShard indexShard) {
+    private UpdateResult shardUpdateOperation(MetaData metaData, BulkShardRequest bulkShardRequest, UpdateRequest updateRequest, IndexShard indexShard) {
         UpdateHelper.Result translate = updateHelper.prepare(updateRequest, indexShard);
         switch (translate.operation()) {
             case UPSERT:
             case INDEX:
                 IndexRequest indexRequest = translate.action();
                 try {
-                    WriteResult result = shardIndexOperation(bulkShardRequest, indexRequest, clusterState, indexShard, false);
+                    WriteResult result = shardIndexOperation(bulkShardRequest, indexRequest, metaData, indexShard, false);
                     return new UpdateResult(translate, indexRequest, result);
                 } catch (Throwable t) {
                     t = ExceptionsHelper.unwrapCause(t);
@@ -431,7 +419,8 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
 
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, BulkShardRequest request) {
+    protected void shardOperationOnReplica(BulkShardRequest request) {
+        final ShardId shardId = request.shardId();
         IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());
         IndexShard indexShard = indexService.getShard(shardId.id());
         Translog.Location location = null;
diff --git a/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java b/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
index 3767267..5778154 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
@@ -19,72 +19,35 @@
 
 package org.elasticsearch.action.delete;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
-import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
-
 /**
  * The response of the delete action.
  *
  * @see org.elasticsearch.action.delete.DeleteRequest
  * @see org.elasticsearch.client.Client#delete(DeleteRequest)
  */
-public class DeleteResponse extends ActionWriteResponse implements StatusToXContent {
+public class DeleteResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean found;
 
     public DeleteResponse() {
 
     }
 
-    public DeleteResponse(String index, String type, String id, long version, boolean found) {
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
+    public DeleteResponse(ShardId shardId, String type, String id, long version, boolean found) {
+        super(shardId, type, id, version);
         this.found = found;
     }
 
-    /**
-     * The index the document was deleted from.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document deleted.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document deleted.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * The version of the delete operation.
-     */
-    public long getVersion() {
-        return this.version;
-    }
 
     /**
      * Returns <tt>true</tt> if a doc was found to delete.
@@ -96,49 +59,44 @@ public class DeleteResponse extends ActionWriteResponse implements StatusToXCont
     @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         found = in.readBoolean();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(found);
     }
 
     @Override
     public RestStatus status() {
-        RestStatus status = getShardInfo().status();
-        if (isFound() == false) {
-            status = NOT_FOUND;
+        if (found == false) {
+            return RestStatus.NOT_FOUND;
         }
-        return status;
+        return super.status();
+    }
+
+    static final class Fields {
+        static final XContentBuilderString FOUND = new XContentBuilderString("found");
     }
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        ActionWriteResponse.ShardInfo shardInfo = getShardInfo();
-        builder.field(Fields.FOUND, found)
-            .field(Fields._INDEX, index)
-            .field(Fields._TYPE, type)
-            .field(Fields._ID, id)
-            .field(Fields._VERSION, version)
-            .value(shardInfo);
+        builder.field(Fields.FOUND, isFound());
+        super.toXContent(builder, params);
         return builder;
     }
 
-    static final class Fields {
-        static final XContentBuilderString FOUND = new XContentBuilderString("found");
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
+    @Override
+    public String toString() {
+        StringBuilder builder = new StringBuilder();
+        builder.append("DeleteResponse[");
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
+        builder.append(",found=").append(found);
+        builder.append(",shards=").append(getShardInfo());
+        return builder.append("]").toString();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
index dc81992..ca66b28 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
@@ -34,7 +34,7 @@ import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -94,22 +94,24 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
     }
 
     @Override
-    protected void resolveRequest(final ClusterState state, final InternalRequest request, final ActionListener<DeleteResponse> listener) {
-        request.request().routing(state.metaData().resolveIndexRouting(request.request().routing(), request.request().index()));
-        if (state.metaData().hasIndex(request.concreteIndex())) {
+    protected void resolveRequest(final MetaData metaData, String concreteIndex, DeleteRequest request) {
+        request.routing(metaData.resolveIndexRouting(request.routing(), request.index()));
+        if (metaData.hasIndex(concreteIndex)) {
             // check if routing is required, if so, do a broadcast delete
-            MappingMetaData mappingMd = state.metaData().index(request.concreteIndex()).mappingOrDefault(request.request().type());
+            MappingMetaData mappingMd = metaData.index(concreteIndex).mappingOrDefault(request.type());
             if (mappingMd != null && mappingMd.routing().required()) {
-                if (request.request().routing() == null) {
-                    if (request.request().versionType() != VersionType.INTERNAL) {
+                if (request.routing() == null) {
+                    if (request.versionType() != VersionType.INTERNAL) {
                         // TODO: implement this feature
-                        throw new IllegalArgumentException("routing value is required for deleting documents of type [" + request.request().type()
-                                + "] while using version_type [" + request.request().versionType() + "]");
+                        throw new IllegalArgumentException("routing value is required for deleting documents of type [" + request.type()
+                                + "] while using version_type [" + request.versionType() + "]");
                     }
-                    throw new RoutingMissingException(request.concreteIndex(), request.request().type(), request.request().id());
+                    throw new RoutingMissingException(concreteIndex, request.type(), request.id());
                 }
             }
         }
+        ShardId shardId = clusterService.operationRouting().shardId(clusterService.state(), concreteIndex, request.id(), request.routing());
+        request.setShardId(shardId);
     }
 
     private void innerExecute(final DeleteRequest request, final ActionListener<DeleteResponse> listener) {
@@ -117,22 +119,16 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected DeleteResponse newResponseInstance() {
         return new DeleteResponse();
     }
 
     @Override
-    protected Tuple<DeleteResponse, DeleteRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) {
-        DeleteRequest request = shardRequest.request;
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
+    protected Tuple<DeleteResponse, DeleteRequest> shardOperationOnPrimary(MetaData metaData, DeleteRequest request) {
+        IndexShard indexShard = indicesService.indexServiceSafe(request.shardId().getIndex()).getShard(request.shardId().id());
         final WriteResult<DeleteResponse> result = executeDeleteRequestOnPrimary(request, indexShard);
         processAfterWrite(request.refresh(), indexShard, result.location);
-        return new Tuple<>(result.response, shardRequest.request);
+        return new Tuple<>(result.response, request);
     }
 
     public static WriteResult<DeleteResponse> executeDeleteRequestOnPrimary(DeleteRequest request, IndexShard indexShard) {
@@ -144,7 +140,7 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
 
         assert request.versionType().validateVersionForWrites(request.version());
         return new WriteResult<>(
-            new DeleteResponse(indexShard.shardId().getIndex(), request.type(), request.id(), delete.version(), delete.found()),
+            new DeleteResponse(indexShard.shardId(), request.type(), request.id(), delete.version(), delete.found()),
             delete.getTranslogLocation());
     }
 
@@ -154,17 +150,12 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
         return delete;
     }
 
-
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, DeleteRequest request) {
+    protected void shardOperationOnReplica(DeleteRequest request) {
+        final ShardId shardId = request.shardId();
         IndexShard indexShard = indicesService.indexServiceSafe(shardId.getIndex()).getShard(shardId.id());
         Engine.Delete delete = executeDeleteRequestOnReplica(request, indexShard);
         processAfterWrite(request.refresh(), indexShard, delete.getTranslogLocation());
     }
 
-    @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterService.operationRouting()
-                .deleteShards(clusterService.state(), request.concreteIndex(), request.request().type(), request.request().id(), request.request().routing());
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java b/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
index 8f43d43..665327a 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
@@ -19,74 +19,36 @@
 
 package org.elasticsearch.action.index;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
-import static org.elasticsearch.rest.RestStatus.CREATED;
-
 /**
  * A response of an index operation,
  *
  * @see org.elasticsearch.action.index.IndexRequest
  * @see org.elasticsearch.client.Client#index(IndexRequest)
  */
-public class IndexResponse extends ActionWriteResponse implements StatusToXContent {
+public class IndexResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean created;
 
     public IndexResponse() {
 
     }
 
-    public IndexResponse(String index, String type, String id, long version, boolean created) {
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
+    public IndexResponse(ShardId shardId, String type, String id, long version, boolean created) {
+        super(shardId, type, id, version);
         this.created = created;
     }
 
     /**
-     * The index the document was indexed into.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document indexed.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document indexed.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * Returns the current version of the doc indexed.
-     */
-    public long getVersion() {
-        return this.version;
-    }
-
-    /**
      * Returns true if the document was created, false if updated.
      */
     public boolean isCreated() {
@@ -94,64 +56,46 @@ public class IndexResponse extends ActionWriteResponse implements StatusToXConte
     }
 
     @Override
+    public RestStatus status() {
+        if (created) {
+            return RestStatus.CREATED;
+        }
+        return super.status();
+    }
+
+    @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         created = in.readBoolean();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(created);
     }
 
     @Override
-    public RestStatus status() {
-        RestStatus status = getShardInfo().status();
-        if (created) {
-            status = CREATED;
-        }
-        return status;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        ActionWriteResponse.ShardInfo shardInfo = getShardInfo();
-        builder.field(Fields._INDEX, index)
-            .field(Fields._TYPE, type)
-            .field(Fields._ID, id)
-            .field(Fields._VERSION, version);
-        shardInfo.toXContent(builder, params);
-        builder.field(Fields.CREATED, created);
-        return builder;
-    }
-
-    @Override
     public String toString() {
         StringBuilder builder = new StringBuilder();
         builder.append("IndexResponse[");
-        builder.append("index=").append(index);
-        builder.append(",type=").append(type);
-        builder.append(",id=").append(id);
-        builder.append(",version=").append(version);
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
         builder.append(",created=").append(created);
         builder.append(",shards=").append(getShardInfo());
         return builder.append("]").toString();
     }
 
     static final class Fields {
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
         static final XContentBuilderString CREATED = new XContentBuilderString("created");
     }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        super.toXContent(builder, params);
+        builder.field(Fields.CREATED, isCreated());
+        return builder;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 6fb7ab6..620056d 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -36,7 +36,6 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -120,14 +119,14 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
     }
 
     @Override
-    protected void resolveRequest(ClusterState state, InternalRequest request, ActionListener<IndexResponse> indexResponseActionListener) {
-        MetaData metaData = clusterService.state().metaData();
-
+    protected void resolveRequest(MetaData metaData, String concreteIndex, IndexRequest request) {
         MappingMetaData mappingMd = null;
-        if (metaData.hasIndex(request.concreteIndex())) {
-            mappingMd = metaData.index(request.concreteIndex()).mappingOrDefault(request.request().type());
+        if (metaData.hasIndex(concreteIndex)) {
+            mappingMd = metaData.index(concreteIndex).mappingOrDefault(request.type());
         }
-        request.request().process(metaData, mappingMd, allowIdGeneration, request.concreteIndex());
+        request.process(metaData, mappingMd, allowIdGeneration, concreteIndex);
+        ShardId shardId = clusterService.operationRouting().shardId(clusterService.state(), concreteIndex, request.id(), request.routing());
+        request.setShardId(shardId);
     }
 
     private void innerExecute(final IndexRequest request, final ActionListener<IndexResponse> listener) {
@@ -135,47 +134,36 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected IndexResponse newResponseInstance() {
         return new IndexResponse();
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterService.operationRouting()
-                .indexShards(clusterService.state(), request.concreteIndex(), request.request().type(), request.request().id(), request.request().routing());
-    }
-
-    @Override
-    protected Tuple<IndexResponse, IndexRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        final IndexRequest request = shardRequest.request;
+    protected Tuple<IndexResponse, IndexRequest> shardOperationOnPrimary(MetaData metaData, IndexRequest request) throws Throwable {
 
         // validate, if routing is required, that we got routing
-        IndexMetaData indexMetaData = clusterState.metaData().index(shardRequest.shardId.getIndex());
+        IndexMetaData indexMetaData = metaData.index(request.shardId().getIndex());
         MappingMetaData mappingMd = indexMetaData.mappingOrDefault(request.type());
         if (mappingMd != null && mappingMd.routing().required()) {
             if (request.routing() == null) {
-                throw new RoutingMissingException(shardRequest.shardId.getIndex(), request.type(), request.id());
+                throw new RoutingMissingException(request.shardId().getIndex(), request.type(), request.id());
             }
         }
 
-        IndexService indexService = indicesService.indexServiceSafe(shardRequest.shardId.getIndex());
-        IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
+        IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex());
+        IndexShard indexShard = indexService.getShard(request.shardId().id());
 
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(request, indexShard, mappingUpdatedAction);
 
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
         processAfterWrite(request.refresh(), indexShard, location);
-        return new Tuple<>(response, shardRequest.request);
+        return new Tuple<>(response, request);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, IndexRequest request) {
+    protected void shardOperationOnReplica(IndexRequest request) {
+        final ShardId shardId = request.shardId();
         IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());
         IndexShard indexShard = indexService.getShard(shardId.id());
         final Engine.Index operation = executeIndexRequestOnReplica(request, indexShard);
@@ -234,7 +222,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
 
         assert request.versionType().validateVersionForWrites(request.version());
 
-        return new WriteResult<>(new IndexResponse(shardId.getIndex(), request.type(), request.id(), request.version(), created), operation.getTranslogLocation());
+        return new WriteResult<>(new IndexResponse(shardId, request.type(), request.id(), request.version(), created), operation.getTranslogLocation());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
index 967d5a0..79f51db 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
@@ -238,17 +238,7 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            XContentType contentType = XContentFactory.xContentType(doc);
-            if (contentType == builder.contentType()) {
-                builder.rawField("doc", doc);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(doc)) {
-                    parser.nextToken();
-                    builder.field("doc");
-                    builder.copyCurrentStructure(parser);
-                }
-            }
-            return builder;
+            return builder.rawField("doc", doc);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
index c629a70..adbe199 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
@@ -42,7 +42,12 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
 
     public static final TimeValue DEFAULT_TIMEOUT = new TimeValue(1, TimeUnit.MINUTES);
 
-    ShardId internalShardId;
+    /**
+     * Target shard the request should execute on. In case of index and delete requests,
+     * shard id gets resolved by the transport action before performing request operation
+     * and at request creation time for shard-level bulk, refresh and flush requests.
+     */
+    protected ShardId shardId;
 
     protected TimeValue timeout = DEFAULT_TIMEOUT;
     protected String index;
@@ -61,6 +66,15 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     }
 
     /**
+     * Creates a new request with resolved shard id
+     */
+    public ReplicationRequest(ActionRequest request, ShardId shardId) {
+        super(request);
+        this.index = shardId.getIndex();
+        this.shardId = shardId;
+    }
+
+    /**
      * Copy constructor that creates a new request that is a copy of the one provided as an argument.
      */
     protected ReplicationRequest(T request) {
@@ -124,12 +138,12 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
 
     /**
      * @return the shardId of the shard where this operation should be executed on.
-     * can be null in case the shardId is determined by a single document (index, type, id) for example for index or delete request.
+     * can be null if the shardID has not yet been resolved
      */
     public
     @Nullable
     ShardId shardId() {
-        return internalShardId;
+        return shardId;
     }
 
     /**
@@ -154,9 +168,9 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         if (in.readBoolean()) {
-            internalShardId = ShardId.readShardId(in);
+            shardId = ShardId.readShardId(in);
         } else {
-            internalShardId = null;
+            shardId = null;
         }
         consistencyLevel = WriteConsistencyLevel.fromId(in.readByte());
         timeout = TimeValue.readTimeValue(in);
@@ -166,9 +180,9 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        if (internalShardId != null) {
+        if (shardId != null) {
             out.writeBoolean(true);
-            internalShardId.writeTo(out);
+            shardId.writeTo(out);
         } else {
             out.writeBoolean(false);
         }
@@ -177,9 +191,21 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
         out.writeString(index);
     }
 
+    /**
+     * Sets the target shard id for the request. The shard id is set when a
+     * index/delete request is resolved by the transport action
+     */
     public T setShardId(ShardId shardId) {
-        this.internalShardId = shardId;
-        this.index = shardId.getIndex();
+        this.shardId = shardId;
         return (T) this;
     }
+
+    @Override
+    public String toString() {
+        if (shardId != null) {
+            return shardId.toString();
+        } else {
+            return index;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
index ddd4d42..33a9d34 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
@@ -22,9 +22,8 @@ package org.elasticsearch.action.support.replication;
 import com.carrotsearch.hppc.cursors.IntObjectCursor;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
-import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.DefaultShardOperationFailedException;
 import org.elasticsearch.action.support.HandledTransportAction;
@@ -53,7 +52,7 @@ import java.util.function.Supplier;
  * Base class for requests that should be executed on all shards of an index or several indices.
  * This action sends shard requests to all primary shards of the indices and they are then replicated like write requests
  */
-public abstract class TransportBroadcastReplicationAction<Request extends BroadcastRequest, Response extends BroadcastResponse, ShardRequest extends ReplicationRequest, ShardResponse extends ActionWriteResponse> extends HandledTransportAction<Request, Response> {
+public abstract class TransportBroadcastReplicationAction<Request extends BroadcastRequest, Response extends BroadcastResponse, ShardRequest extends ReplicationRequest, ShardResponse extends ReplicationResponse> extends HandledTransportAction<Request, Response> {
 
     private final TransportReplicationAction replicatedBroadcastShardAction;
     private final ClusterService clusterService;
@@ -91,15 +90,15 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
                     logger.trace("{}: got failure from {}", actionName, shardId);
                     int totalNumCopies = clusterState.getMetaData().index(shardId.index().getName()).getNumberOfReplicas() + 1;
                     ShardResponse shardResponse = newShardResponse();
-                    ActionWriteResponse.ShardInfo.Failure[] failures;
+                    ReplicationResponse.ShardInfo.Failure[] failures;
                     if (TransportActions.isShardNotAvailableException(e)) {
-                        failures = new ActionWriteResponse.ShardInfo.Failure[0];
+                        failures = new ReplicationResponse.ShardInfo.Failure[0];
                     } else {
-                        ActionWriteResponse.ShardInfo.Failure failure = new ActionWriteResponse.ShardInfo.Failure(shardId.index().name(), shardId.id(), null, e, ExceptionsHelper.status(e), true);
-                        failures = new ActionWriteResponse.ShardInfo.Failure[totalNumCopies];
+                        ReplicationResponse.ShardInfo.Failure failure = new ReplicationResponse.ShardInfo.Failure(shardId.index().name(), shardId.id(), null, e, ExceptionsHelper.status(e), true);
+                        failures = new ReplicationResponse.ShardInfo.Failure[totalNumCopies];
                         Arrays.fill(failures, failure);
                     }
-                    shardResponse.setShardInfo(new ActionWriteResponse.ShardInfo(totalNumCopies, 0, failures));
+                    shardResponse.setShardInfo(new ReplicationResponse.ShardInfo(totalNumCopies, 0, failures));
                     shardsResponses.add(shardResponse);
                     if (responsesCountDown.countDown()) {
                         finishAndNotifyListener(listener, shardsResponses);
@@ -142,7 +141,7 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
         int totalNumCopies = 0;
         List<ShardOperationFailedException> shardFailures = null;
         for (int i = 0; i < shardsResponses.size(); i++) {
-            ActionWriteResponse shardResponse = shardsResponses.get(i);
+            ReplicationResponse shardResponse = shardsResponses.get(i);
             if (shardResponse == null) {
                 // non active shard, ignore
             } else {
@@ -152,7 +151,7 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
                 if (shardFailures == null) {
                     shardFailures = new ArrayList<>();
                 }
-                for (ActionWriteResponse.ShardInfo.Failure failure : shardResponse.getShardInfo().getFailures()) {
+                for (ReplicationResponse.ShardInfo.Failure failure : shardResponse.getShardInfo().getFailures()) {
                     shardFailures.add(new DefaultShardOperationFailedException(new BroadcastShardOperationFailedException(new ShardId(failure.index(), failure.shardId()), failure.getCause())));
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index 3b4d860..26c439c 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -22,7 +22,7 @@ package org.elasticsearch.action.support.replication;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.support.ActionFilters;
@@ -37,11 +37,10 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.ShardIterator;
-import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -63,6 +62,8 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
 import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -70,8 +71,14 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
 
 /**
+ * Base class for requests that should be executed on a primary copy followed by replica copies.
+ * Subclasses can resolve the target shard and provide implementation for primary and replica operations.
+ *
+ * The action samples cluster state on the receiving node to reroute to node with primary copy and on the
+ * primary node to validate request before primary operation followed by sampling state again for resolving
+ * nodes with replica copies to perform replication.
  */
-public abstract class TransportReplicationAction<Request extends ReplicationRequest, ReplicaRequest extends ReplicationRequest, Response extends ActionWriteResponse> extends TransportAction<Request, Response> {
+public abstract class TransportReplicationAction<Request extends ReplicationRequest, ReplicaRequest extends ReplicationRequest, Response extends ReplicationResponse> extends TransportAction<Request, Response> {
 
     public static final String SHARD_FAILURE_TIMEOUT = "action.support.replication.shard.failure_timeout";
 
@@ -85,6 +92,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
     private final TimeValue shardFailedTimeout;
 
     final String transportReplicaAction;
+    final String transportPrimaryAction;
     final String executor;
     final boolean checkWriteConsistency;
 
@@ -101,11 +109,12 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         this.shardStateAction = shardStateAction;
         this.mappingUpdatedAction = mappingUpdatedAction;
 
+        this.transportPrimaryAction = actionName + "[p]";
         this.transportReplicaAction = actionName + "[r]";
         this.executor = executor;
         this.checkWriteConsistency = checkWriteConsistency();
-
         transportService.registerRequestHandler(actionName, request, ThreadPool.Names.SAME, new OperationTransportHandler());
+        transportService.registerRequestHandler(transportPrimaryAction, request, executor, new PrimaryOperationTransportHandler());
         // we must never reject on because of thread pool capacity on replicas
         transportService.registerRequestHandler(transportReplicaAction, replicaRequest, executor, true, new ReplicaOperationTransportHandler());
 
@@ -118,40 +127,57 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     @Override
     protected void doExecute(Request request, ActionListener<Response> listener) {
-        new PrimaryPhase(request, listener).run();
+        new ReroutePhase(request, listener).run();
     }
 
     protected abstract Response newResponseInstance();
 
     /**
+     * Resolves the target shard id of the incoming request.
+     * Additional processing or validation of the request should be done here.
+     */
+    protected void resolveRequest(MetaData metaData, String concreteIndex, Request request) {
+        // implementation should be provided if request shardID is not already resolved at request construction
+    }
+
+    /**
+     * Primary operation on node with primary copy, the provided metadata should be used for request validation if needed
      * @return A tuple containing not null values, as first value the result of the primary operation and as second value
      * the request to be executed on the replica shards.
      */
-    protected abstract Tuple<Response, ReplicaRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable;
-
-    protected abstract void shardOperationOnReplica(ShardId shardId, ReplicaRequest shardRequest);
-
-    protected abstract ShardIterator shards(ClusterState clusterState, InternalRequest request);
+    protected abstract Tuple<Response, ReplicaRequest> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable;
 
-    protected abstract boolean checkWriteConsistency();
+    /**
+     * Replica operation on nodes with replica copies
+     */
+    protected abstract void shardOperationOnReplica(ReplicaRequest shardRequest);
 
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.WRITE);
+    /**
+     * True if write consistency should be checked for an implementation
+     */
+    protected boolean checkWriteConsistency() {
+        return true;
     }
 
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indexBlockedException(ClusterBlockLevel.WRITE, request.concreteIndex());
+    /**
+     * Cluster level block to check before request execution
+     */
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.WRITE;
     }
 
-    protected boolean resolveIndex() {
-        return true;
+    /**
+     * Index level block to check before request execution
+     */
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.WRITE;
     }
 
     /**
-     * Resolves the request, by default doing nothing. Can be subclassed to do
-     * additional processing or validation depending on the incoming request
+     * True if provided index should be resolved when resolving request
      */
-    protected void resolveRequest(ClusterState state, InternalRequest request, ActionListener<Response> listener) {
+    protected boolean resolveIndex() {
+        return true;
     }
 
     protected TransportRequestOptions transportOptions() {
@@ -188,7 +214,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return false;
     }
 
-    protected static class WriteResult<T extends ActionWriteResponse> {
+    protected static class WriteResult<T extends ReplicationResponse> {
 
         public final T response;
         public final Translog.Location location;
@@ -199,10 +225,10 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
 
         @SuppressWarnings("unchecked")
-        public <T extends ActionWriteResponse> T response() {
+        public <T extends ReplicationResponse> T response() {
             // this sets total, pending and failed to 0 and this is ok, because we will embed this into the replica
             // request and not use it
-            response.setShardInfo(new ActionWriteResponse.ShardInfo());
+            response.setShardInfo(new ReplicationResponse.ShardInfo());
             return (T) response;
         }
 
@@ -233,6 +259,13 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
     }
 
+    class PrimaryOperationTransportHandler implements TransportRequestHandler<Request> {
+        @Override
+        public void messageReceived(final Request request, final TransportChannel channel) throws Exception {
+            new PrimaryPhase(request, channel).run();
+        }
+    }
+
     class ReplicaOperationTransportHandler implements TransportRequestHandler<ReplicaRequest> {
         @Override
         public void messageReceived(final ReplicaRequest request, final TransportChannel channel) throws Exception {
@@ -259,7 +292,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         // something we want to avoid at all costs
         private final ClusterStateObserver observer = new ClusterStateObserver(clusterService, null, logger);
 
-
         AsyncReplicaAction(ReplicaRequest request, TransportChannel channel) {
             this.request = request;
             this.channel = channel;
@@ -287,14 +319,32 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                 });
             } else {
                 try {
-                    failReplicaIfNeeded(request.internalShardId.getIndex(), request.internalShardId.id(), t, request);
+                    failReplicaIfNeeded(t);
                 } catch (Throwable unexpected) {
-                    logger.error("{} unexpected error while failing replica", request.internalShardId.id(), unexpected);
+                    logger.error("{} unexpected error while failing replica", unexpected, request.shardId().id());
                 } finally {
                     responseWithFailure(t);
                 }
             }
         }
+        private void failReplicaIfNeeded(Throwable t) {
+            String index = request.shardId().getIndex();
+            int shardId = request.shardId().id();
+            logger.trace("failure on replica [{}][{}], action [{}], request [{}]", t, index, shardId, actionName, request);
+            if (ignoreReplicaException(t) == false) {
+                IndexService indexService = indicesService.indexService(index);
+                if (indexService == null) {
+                    logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
+                    return;
+                }
+                IndexShard indexShard = indexService.getShardOrNull(shardId);
+                if (indexShard == null) {
+                    logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
+                    return;
+                }
+                indexShard.failShard(actionName + " failed on replica", t);
+            }
+        }
 
         protected void responseWithFailure(Throwable t) {
             try {
@@ -307,23 +357,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         protected void doRun() throws Exception {
-            try (Releasable shardReference = getIndexShardOperationsCounter(request.internalShardId)) {
-                shardOperationOnReplica(request.internalShardId, request);
+            assert request.shardId() != null : "request shardId must be set";
+            try (Releasable ignored = getIndexShardOperationsCounter(request.shardId())) {
+                shardOperationOnReplica(request);
+                if (logger.isTraceEnabled()) {
+                    logger.trace("action [{}] completed on shard [{}] for request [{}]", transportReplicaAction, request.shardId(), request);
+                }
             }
             channel.sendResponse(TransportResponse.Empty.INSTANCE);
         }
     }
 
-    protected class PrimaryOperationRequest {
-        public final ShardId shardId;
-        public final Request request;
-
-        public PrimaryOperationRequest(int shardId, String index, Request request) {
-            this.shardId = new ShardId(index, shardId);
-            this.request = request;
-        }
-    }
-
     public static class RetryOnPrimaryException extends ElasticsearchException {
         public RetryOnPrimaryException(ShardId shardId, String msg) {
             super(msg);
@@ -336,22 +380,22 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
     }
 
     /**
-     * Responsible for performing all operations up to the point we start starting sending requests to replica shards.
-     * Including forwarding the request to another node if the primary is not assigned locally.
-     * <p>
-     * Note that as soon as we start sending request to replicas, state responsibility is transferred to {@link ReplicationPhase}
+     * Responsible for routing and retrying failed operations on the primary.
+     * The actual primary operation is done in {@link PrimaryPhase} on the
+     * node with primary copy.
+     *
+     * Resolves index and shard id for the request before routing it to target node
      */
-    final class PrimaryPhase extends AbstractRunnable {
+    final class ReroutePhase extends AbstractRunnable {
         private final ActionListener<Response> listener;
-        private final InternalRequest internalRequest;
+        private final Request request;
         private final ClusterStateObserver observer;
-        private final AtomicBoolean finished = new AtomicBoolean(false);
-        private volatile Releasable indexShardReference;
+        private final AtomicBoolean finished = new AtomicBoolean();
 
-        PrimaryPhase(Request request, ActionListener<Response> listener) {
-            this.internalRequest = new InternalRequest(request);
+        ReroutePhase(Request request, ActionListener<Response> listener) {
+            this.request = request;
             this.listener = listener;
-            this.observer = new ClusterStateObserver(clusterService, internalRequest.request().timeout(), logger);
+            this.observer = new ClusterStateObserver(clusterService, request.timeout(), logger);
         }
 
         @Override
@@ -361,135 +405,91 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         protected void doRun() {
-            if (checkBlocks() == false) {
+            final ClusterState state = observer.observedState();
+            ClusterBlockException blockException = state.blocks().globalBlockedException(globalBlockLevel());
+            if (blockException != null) {
+                handleBlockException(blockException);
                 return;
             }
-            final ShardIterator shardIt = shards(observer.observedState(), internalRequest);
-            final ShardRouting primary = resolvePrimary(shardIt);
-            if (primary == null) {
-                retryBecauseUnavailable(shardIt.shardId(), "No active shards.");
+            final String concreteIndex = resolveIndex() ? indexNameExpressionResolver.concreteSingleIndex(state, request) : request.index();
+            blockException = state.blocks().indexBlockedException(indexBlockLevel(), concreteIndex);
+            if (blockException != null) {
+                handleBlockException(blockException);
                 return;
             }
-            if (primary.active() == false) {
-                logger.trace("primary shard [{}] is not yet active, scheduling a retry. action [{}], request [{}]", primary.shardId(), actionName, internalRequest.request);
-                retryBecauseUnavailable(shardIt.shardId(), "Primary shard is not active or isn't assigned to a known node.");
+            // request does not have a shardId yet, we need to pass the concrete index to resolve shardId
+            resolveRequest(state.metaData(), concreteIndex, request);
+            assert request.shardId() != null : "request shardId must be set in resolveRequest";
+
+            IndexShardRoutingTable indexShard = state.getRoutingTable().shardRoutingTable(request.shardId().getIndex(), request.shardId().id());
+            final ShardRouting primary = indexShard.primaryShard();
+            if (primary == null || primary.active() == false) {
+                logger.trace("primary shard [{}] is not yet active, scheduling a retry: action [{}], request [{}], cluster state version [{}]", request.shardId(), actionName, request, state.version());
+                retryBecauseUnavailable(request.shardId(), "primary shard is not active");
                 return;
             }
-            if (observer.observedState().nodes().nodeExists(primary.currentNodeId()) == false) {
-                logger.trace("primary shard [{}] is assigned to anode we do not know the node, scheduling a retry.", primary.shardId(), primary.currentNodeId());
-                retryBecauseUnavailable(shardIt.shardId(), "Primary shard is not active or isn't assigned to a known node.");
+            if (state.nodes().nodeExists(primary.currentNodeId()) == false) {
+                logger.trace("primary shard [{}] is assigned to an unknown node [{}], scheduling a retry: action [{}], request [{}], cluster state version [{}]", request.shardId(), primary.currentNodeId(), actionName, request, state.version());
+                retryBecauseUnavailable(request.shardId(), "primary shard isn't assigned to a known node.");
                 return;
             }
-            routeRequestOrPerformLocally(primary, shardIt);
-        }
-
-        /**
-         * checks for any cluster state blocks. Returns true if operation is OK to proceeded.
-         * if false is return, no further action is needed. The method takes care of any continuation, by either
-         * responding to the listener or scheduling a retry
-         */
-        protected boolean checkBlocks() {
-            ClusterBlockException blockException = checkGlobalBlock(observer.observedState());
-            if (blockException != null) {
-                if (blockException.retryable()) {
-                    logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
-                    retry(blockException);
-                } else {
-                    finishAsFailed(blockException);
+            final DiscoveryNode node = state.nodes().get(primary.currentNodeId());
+            if (primary.currentNodeId().equals(state.nodes().localNodeId())) {
+                if (logger.isTraceEnabled()) {
+                    logger.trace("send action [{}] on primary [{}] for request [{}] with cluster state version [{}] to [{}] ", transportPrimaryAction, request.shardId(), request, state.version(), primary.currentNodeId());
                 }
-                return false;
-            }
-            if (resolveIndex()) {
-                internalRequest.concreteIndex(indexNameExpressionResolver.concreteSingleIndex(observer.observedState(), internalRequest.request()));
+                performAction(node, transportPrimaryAction, true);
             } else {
-                internalRequest.concreteIndex(internalRequest.request().index());
-            }
-
-            resolveRequest(observer.observedState(), internalRequest, listener);
-
-            blockException = checkRequestBlock(observer.observedState(), internalRequest);
-            if (blockException != null) {
-                if (blockException.retryable()) {
-                    logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
-                    retry(blockException);
-                } else {
-                    finishAsFailed(blockException);
+                if (logger.isTraceEnabled()) {
+                    logger.trace("send action [{}] on primary [{}] for request [{}] with cluster state version [{}] to [{}]", actionName, request.shardId(), request, state.version(), primary.currentNodeId());
                 }
-                return false;
+                performAction(node, actionName, false);
             }
-            return true;
         }
 
-        protected ShardRouting resolvePrimary(ShardIterator shardIt) {
-            // no shardIt, might be in the case between index gateway recovery and shardIt initialization
-            ShardRouting shard;
-            while ((shard = shardIt.nextOrNull()) != null) {
-                // we only deal with primary shardIt here...
-                if (shard.primary()) {
-                    return shard;
-                }
+        private void handleBlockException(ClusterBlockException blockException) {
+            if (blockException.retryable()) {
+                logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
+                retry(blockException);
+            } else {
+                finishAsFailed(blockException);
             }
-            return null;
         }
 
-        /**
-         * send the request to the node holding the primary or execute if local
-         */
-        protected void routeRequestOrPerformLocally(final ShardRouting primary, final ShardIterator shardsIt) {
-            if (primary.currentNodeId().equals(observer.observedState().nodes().localNodeId())) {
-                try {
-                    threadPool.executor(executor).execute(new AbstractRunnable() {
-                        @Override
-                        public void onFailure(Throwable t) {
-                            finishAsFailed(t);
-                        }
+        private void performAction(final DiscoveryNode node, final String action, final boolean isPrimaryAction) {
+            transportService.sendRequest(node, action, request, transportOptions, new BaseTransportResponseHandler<Response>() {
 
-                        @Override
-                        protected void doRun() throws Exception {
-                            performOnPrimary(primary, shardsIt);
-                        }
-                    });
-                } catch (Throwable t) {
-                    finishAsFailed(t);
+                @Override
+                public Response newInstance() {
+                    return newResponseInstance();
                 }
-            } else {
-                DiscoveryNode node = observer.observedState().nodes().get(primary.currentNodeId());
-                transportService.sendRequest(node, actionName, internalRequest.request(), transportOptions, new BaseTransportResponseHandler<Response>() {
 
-                    @Override
-                    public Response newInstance() {
-                        return newResponseInstance();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.SAME;
-                    }
+                @Override
+                public String executor() {
+                    return ThreadPool.Names.SAME;
+                }
 
-                    @Override
-                    public void handleResponse(Response response) {
-                        finishOnRemoteSuccess(response);
-                    }
+                @Override
+                public void handleResponse(Response response) {
+                    finishOnSuccess(response);
+                }
 
-                    @Override
-                    public void handleException(TransportException exp) {
-                        try {
-                            // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
-                            if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
-                                    retryPrimaryException(exp)) {
-                                // we already marked it as started when we executed it (removed the listener) so pass false
-                                // to re-add to the cluster listener
-                                logger.trace("received an error from node the primary was assigned to ({}), scheduling a retry", exp.getMessage());
-                                retry(exp);
-                            } else {
-                                finishAsFailed(exp);
-                            }
-                        } catch (Throwable t) {
-                            finishWithUnexpectedFailure(t);
+                @Override
+                public void handleException(TransportException exp) {
+                    try {
+                        // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
+                        if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
+                                (isPrimaryAction && retryPrimaryException(exp.unwrapCause()))) {
+                            logger.trace("received an error from node [{}] for request [{}], scheduling a retry", exp, node.id(), request);
+                            retry(exp);
+                        } else {
+                            finishAsFailed(exp);
                         }
+                    } catch (Throwable t) {
+                        finishWithUnexpectedFailure(t);
                     }
-                });
-            }
+                }
+            });
         }
 
         void retry(Throwable failure) {
@@ -518,22 +518,9 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
             });
         }
 
-        /**
-         * upon success, finish the first phase and transfer responsibility to the {@link ReplicationPhase}
-         */
-        void finishAndMoveToReplication(ReplicationPhase replicationPhase) {
-            if (finished.compareAndSet(false, true)) {
-                replicationPhase.run();
-            } else {
-                assert false : "finishAndMoveToReplication called but operation is already finished";
-            }
-        }
-
-
         void finishAsFailed(Throwable failure) {
             if (finished.compareAndSet(false, true)) {
-                Releasables.close(indexShardReference);
-                logger.trace("operation failed. action [{}], request [{}]", failure, actionName, internalRequest.request);
+                logger.trace("operation failed. action [{}], request [{}]", failure, actionName, request);
                 listener.onFailure(failure);
             } else {
                 assert false : "finishAsFailed called but operation is already finished";
@@ -541,64 +528,79 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
 
         void finishWithUnexpectedFailure(Throwable failure) {
-            logger.warn("unexpected error during the primary phase for action [{}], request [{}]", failure, actionName, internalRequest.request);
+            logger.warn("unexpected error during the primary phase for action [{}], request [{}]", failure, actionName, request);
             if (finished.compareAndSet(false, true)) {
-                Releasables.close(indexShardReference);
                 listener.onFailure(failure);
             } else {
                 assert false : "finishWithUnexpectedFailure called but operation is already finished";
             }
         }
 
-        void finishOnRemoteSuccess(Response response) {
+        void finishOnSuccess(Response response) {
             if (finished.compareAndSet(false, true)) {
                 if (logger.isTraceEnabled()) {
-                    logger.trace("operation succeeded. action [{}],request [{}]", actionName, internalRequest.request);
+                    logger.trace("operation succeeded. action [{}],request [{}]", actionName, request);
                 }
                 listener.onResponse(response);
             } else {
-                assert false : "finishOnRemoteSuccess called but operation is already finished";
+                assert false : "finishOnSuccess called but operation is already finished";
             }
         }
 
-        /**
-         * perform the operation on the node holding the primary
-         */
-        void performOnPrimary(final ShardRouting primary, final ShardIterator shardsIt) {
-            final String writeConsistencyFailure = checkWriteConsistency(primary);
+        void retryBecauseUnavailable(ShardId shardId, String message) {
+            retry(new UnavailableShardsException(shardId, "{} Timeout: [{}], request: [{}]", message, request.timeout(), request));
+        }
+    }
+
+    /**
+     * Responsible for performing primary operation locally and delegating to replication action once successful
+     * <p>
+     * Note that as soon as we move to replication action, state responsibility is transferred to {@link ReplicationPhase}.
+     */
+    final class PrimaryPhase extends AbstractRunnable {
+        private final Request request;
+        private final TransportChannel channel;
+        private final ClusterState state;
+        private final AtomicBoolean finished = new AtomicBoolean();
+        private Releasable indexShardReference;
+
+        PrimaryPhase(Request request, TransportChannel channel) {
+            this.state = clusterService.state();
+            this.request = request;
+            this.channel = channel;
+        }
+
+        @Override
+        public void onFailure(Throwable e) {
+            finishAsFailed(e);
+        }
+
+        @Override
+        protected void doRun() throws Exception {
+            // request shardID was set in ReroutePhase
+            assert request.shardId() != null : "request shardID must be set prior to primary phase";
+            final ShardId shardId = request.shardId();
+            final String writeConsistencyFailure = checkWriteConsistency(shardId);
             if (writeConsistencyFailure != null) {
-                retryBecauseUnavailable(primary.shardId(), writeConsistencyFailure);
+                finishBecauseUnavailable(shardId, writeConsistencyFailure);
                 return;
             }
             final ReplicationPhase replicationPhase;
             try {
-                indexShardReference = getIndexShardOperationsCounter(primary.shardId());
-                PrimaryOperationRequest por = new PrimaryOperationRequest(primary.id(), internalRequest.concreteIndex(), internalRequest.request());
-                Tuple<Response, ReplicaRequest> primaryResponse = shardOperationOnPrimary(observer.observedState(), por);
+                indexShardReference = getIndexShardOperationsCounter(shardId);
+                Tuple<Response, ReplicaRequest> primaryResponse = shardOperationOnPrimary(state.metaData(), request);
                 if (logger.isTraceEnabled()) {
-                    logger.trace("operation completed on primary [{}], action [{}], request [{}], cluster state version [{}]", primary, actionName, por.request, observer.observedState().version());
+                    logger.trace("action [{}] completed on shard [{}] for request [{}] with cluster state version [{}]", transportPrimaryAction, shardId, request, state.version());
                 }
-                replicationPhase = new ReplicationPhase(shardsIt, primaryResponse.v2(), primaryResponse.v1(), observer, primary, internalRequest, listener, indexShardReference, shardFailedTimeout);
+                replicationPhase = new ReplicationPhase(primaryResponse.v2(), primaryResponse.v1(), shardId, channel, indexShardReference, shardFailedTimeout);
             } catch (Throwable e) {
-                // shard has not been allocated yet, retry it here
-                if (retryPrimaryException(e)) {
-                    logger.trace("had an error while performing operation on primary ({}, action [{}], request [{}]), scheduling a retry.", e, primary, actionName, internalRequest.request);
-                    // We have to close here because when we retry we will increment get a new reference on index shard again and we do not want to
-                    // increment twice.
-                    Releasables.close(indexShardReference);
-                    // We have to reset to null here because whe we retry it might be that we never get to the point where we assign a new reference
-                    // (for example, in case the operation was rejected because queue is full). In this case we would release again once one of the finish methods is called.
-                    indexShardReference = null;
-                    retry(e);
-                    return;
-                }
                 if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
                     if (logger.isTraceEnabled()) {
-                        logger.trace(primary.shortSummary() + ": Failed to execute [" + internalRequest.request() + "]", e);
+                        logger.trace("failed to execute [{}] on [{}]", e, request, shardId);
                     }
                 } else {
                     if (logger.isDebugEnabled()) {
-                        logger.debug(primary.shortSummary() + ": Failed to execute [" + internalRequest.request() + "]", e);
+                        logger.debug("failed to execute [{}] on [{}]", e, request, shardId);
                     }
                 }
                 finishAsFailed(e);
@@ -611,22 +613,22 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
          * checks whether we can perform a write based on the write consistency setting
          * returns **null* if OK to proceed, or a string describing the reason to stop
          */
-        String checkWriteConsistency(ShardRouting shard) {
+        String checkWriteConsistency(ShardId shardId) {
             if (checkWriteConsistency == false) {
                 return null;
             }
 
             final WriteConsistencyLevel consistencyLevel;
-            if (internalRequest.request().consistencyLevel() != WriteConsistencyLevel.DEFAULT) {
-                consistencyLevel = internalRequest.request().consistencyLevel();
+            if (request.consistencyLevel() != WriteConsistencyLevel.DEFAULT) {
+                consistencyLevel = request.consistencyLevel();
             } else {
                 consistencyLevel = defaultWriteConsistencyLevel;
             }
             final int sizeActive;
             final int requiredNumber;
-            IndexRoutingTable indexRoutingTable = observer.observedState().getRoutingTable().index(shard.index());
+            IndexRoutingTable indexRoutingTable = state.getRoutingTable().index(shardId.getIndex());
             if (indexRoutingTable != null) {
-                IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(shard.getId());
+                IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(shardId.getId());
                 if (shardRoutingTable != null) {
                     sizeActive = shardRoutingTable.activeShards().size();
                     if (consistencyLevel == WriteConsistencyLevel.QUORUM && shardRoutingTable.getSize() > 2) {
@@ -648,17 +650,44 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
             if (sizeActive < requiredNumber) {
                 logger.trace("not enough active copies of shard [{}] to meet write consistency of [{}] (have {}, needed {}), scheduling a retry. action [{}], request [{}]",
-                        shard.shardId(), consistencyLevel, sizeActive, requiredNumber, actionName, internalRequest.request);
+                        shardId, consistencyLevel, sizeActive, requiredNumber, transportPrimaryAction, request);
                 return "Not enough active copies to meet write consistency of [" + consistencyLevel + "] (have " + sizeActive + ", needed " + requiredNumber + ").";
             } else {
                 return null;
             }
         }
 
-        void retryBecauseUnavailable(ShardId shardId, String message) {
-            retry(new UnavailableShardsException(shardId, message + " Timeout: [" + internalRequest.request().timeout() + "], request: " + internalRequest.request().toString()));
+        /**
+         * upon success, finish the first phase and transfer responsibility to the {@link ReplicationPhase}
+         */
+        void finishAndMoveToReplication(ReplicationPhase replicationPhase) {
+            if (finished.compareAndSet(false, true)) {
+                replicationPhase.run();
+            } else {
+                assert false : "finishAndMoveToReplication called but operation is already finished";
+            }
+        }
+
+        /**
+         * upon failure, send failure back to the {@link ReroutePhase} for retrying if appropriate
+         */
+        void finishAsFailed(Throwable failure) {
+            if (finished.compareAndSet(false, true)) {
+                Releasables.close(indexShardReference);
+                logger.trace("operation failed", failure);
+                try {
+                    channel.sendResponse(failure);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [{}]", responseException, transportPrimaryAction);
+                }
+            } else {
+                assert false : "finishAsFailed called but operation is already finished";
+            }
         }
 
+        void finishBecauseUnavailable(ShardId shardId, String message) {
+            finishAsFailed(new UnavailableShardsException(shardId, "{} Timeout: [{}], request: [{}]", message, request.timeout(), request));
+        }
     }
 
     protected Releasable getIndexShardOperationsCounter(ShardId shardId) {
@@ -667,135 +696,78 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return new IndexShardReference(indexShard);
     }
 
-    private void failReplicaIfNeeded(String index, int shardId, Throwable t, ReplicaRequest request) {
-        logger.trace("failure on replica [{}][{}], action [{}], request [{}]", t, index, shardId, actionName, request);
-        if (ignoreReplicaException(t) == false) {
-            IndexService indexService = indicesService.indexService(index);
-            if (indexService == null) {
-                logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
-                return;
-            }
-            IndexShard indexShard = indexService.getShardOrNull(shardId);
-            if (indexShard == null) {
-                logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
-                return;
-            }
-            indexShard.failShard(actionName + " failed on replica", t);
-        }
-    }
-
     /**
-     * inner class is responsible for send the requests to all replica shards and manage the responses
+     * Responsible for sending replica requests (see {@link AsyncReplicaAction}) to nodes with replica copy, including
+     * relocating copies
      */
     final class ReplicationPhase extends AbstractRunnable {
 
         private final ReplicaRequest replicaRequest;
         private final Response finalResponse;
-        private final ShardIterator shardIt;
-        private final ActionListener<Response> listener;
-        private final AtomicBoolean finished = new AtomicBoolean(false);
+        private final TransportChannel channel;
+        private final ShardId shardId;
+        private final List<ShardRouting> shards;
+        private final DiscoveryNodes nodes;
+        private final boolean executeOnReplica;
+        private final String indexUUID;
+        private final AtomicBoolean finished = new AtomicBoolean();
         private final AtomicInteger success = new AtomicInteger(1); // We already wrote into the primary shard
         private final ConcurrentMap<String, Throwable> shardReplicaFailures = ConcurrentCollections.newConcurrentMap();
-        private final IndexMetaData indexMetaData;
-        private final ShardRouting originalPrimaryShard;
         private final AtomicInteger pending;
         private final int totalShards;
-        private final ClusterStateObserver observer;
         private final Releasable indexShardReference;
         private final TimeValue shardFailedTimeout;
 
-        /**
-         * the constructor doesn't take any action, just calculates state. Call {@link #run()} to start
-         * replicating.
-         */
-        public ReplicationPhase(ShardIterator originalShardIt, ReplicaRequest replicaRequest, Response finalResponse,
-                                ClusterStateObserver observer, ShardRouting originalPrimaryShard,
-                                InternalRequest internalRequest, ActionListener<Response> listener, Releasable indexShardReference,
-                                TimeValue shardFailedTimeout) {
+        public ReplicationPhase(ReplicaRequest replicaRequest, Response finalResponse, ShardId shardId,
+                                TransportChannel channel, Releasable indexShardReference, TimeValue shardFailedTimeout) {
             this.replicaRequest = replicaRequest;
-            this.listener = listener;
+            this.channel = channel;
             this.finalResponse = finalResponse;
-            this.originalPrimaryShard = originalPrimaryShard;
-            this.observer = observer;
-            indexMetaData = observer.observedState().metaData().index(internalRequest.concreteIndex());
             this.indexShardReference = indexShardReference;
             this.shardFailedTimeout = shardFailedTimeout;
-
-            ShardRouting shard;
-            // we double check on the state, if it got changed we need to make sure we take the latest one cause
-            // maybe a replica shard started its recovery process and we need to apply it there...
-
-            // we also need to make sure if the new state has a new primary shard (that we indexed to before) started
-            // and assigned to another node (while the indexing happened). In that case, we want to apply it on the
-            // new primary shard as well...
-            ClusterState newState = clusterService.state();
-
-            int numberOfUnassignedOrIgnoredReplicas = 0;
+            this.shardId = shardId;
+
+            // we have to get a new state after successfully indexing into the primary in order to honour recovery semantics.
+            // we have to make sure that every operation indexed into the primary after recovery start will also be replicated
+            // to the recovery target. If we use an old cluster state, we may miss a relocation that has started since then.
+            // If the index gets deleted after primary operation, we skip replication
+            final ClusterState state = clusterService.state();
+            final IndexRoutingTable index = state.getRoutingTable().index(shardId.getIndex());
+            final IndexShardRoutingTable shardRoutingTable = (index != null) ? index.shard(shardId.id()) : null;
+            final IndexMetaData indexMetaData = state.getMetaData().index(shardId.getIndex());
+            this.shards = (shardRoutingTable != null) ? shardRoutingTable.shards() : Collections.emptyList();
+            this.executeOnReplica = (indexMetaData == null) || shouldExecuteReplication(indexMetaData.getSettings());
+            this.indexUUID = (indexMetaData != null) ? indexMetaData.getIndexUUID() : null;
+            this.nodes = state.getNodes();
+
+            if (shards.isEmpty()) {
+                logger.debug("replication phase for request [{}] on [{}] is skipped due to index deletion after primary operation", replicaRequest, shardId);
+            }
+
+            // we calculate number of target nodes to send replication operations, including nodes with relocating shards
+            int numberOfIgnoredShardInstances = 0;
             int numberOfPendingShardInstances = 0;
-            if (observer.observedState() != newState) {
-                observer.reset(newState);
-                shardIt = shards(newState, internalRequest);
-                while ((shard = shardIt.nextOrNull()) != null) {
-                    if (shard.primary()) {
-                        if (originalPrimaryShard.currentNodeId().equals(shard.currentNodeId()) == false) {
-                            // there is a new primary, we'll have to replicate to it.
-                            numberOfPendingShardInstances++;
-                        }
-                        if (shard.relocating()) {
-                            numberOfPendingShardInstances++;
-                        }
-                    } else if (shouldExecuteReplication(indexMetaData.getSettings()) == false) {
-                        // If the replicas use shadow replicas, there is no reason to
-                        // perform the action on the replica, so skip it and
-                        // immediately return
-
-                        // this delays mapping updates on replicas because they have
-                        // to wait until they get the new mapping through the cluster
-                        // state, which is why we recommend pre-defined mappings for
-                        // indices using shadow replicas
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.unassigned()) {
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.relocating()) {
-                        // we need to send to two copies
-                        numberOfPendingShardInstances += 2;
-                    } else {
+            for (ShardRouting shard : shards) {
+                if (shard.primary() == false && executeOnReplica == false) {
+                    numberOfIgnoredShardInstances++;
+                } else if (shard.unassigned()) {
+                    numberOfIgnoredShardInstances++;
+                } else {
+                    if (shard.currentNodeId().equals(nodes.localNodeId()) == false) {
                         numberOfPendingShardInstances++;
                     }
-                }
-            } else {
-                shardIt = originalShardIt;
-                shardIt.reset();
-                while ((shard = shardIt.nextOrNull()) != null) {
-                    if (shard.unassigned()) {
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.primary()) {
-                        if (shard.relocating()) {
-                            // we have to replicate to the other copy
-                            numberOfPendingShardInstances += 1;
-                        }
-                    } else if (shouldExecuteReplication(indexMetaData.getSettings()) == false) {
-                        // If the replicas use shadow replicas, there is no reason to
-                        // perform the action on the replica, so skip it and
-                        // immediately return
-
-                        // this delays mapping updates on replicas because they have
-                        // to wait until they get the new mapping through the cluster
-                        // state, which is why we recommend pre-defined mappings for
-                        // indices using shadow replicas
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.relocating()) {
-                        // we need to send to two copies
-                        numberOfPendingShardInstances += 2;
-                    } else {
+                    if (shard.relocating()) {
                         numberOfPendingShardInstances++;
                     }
                 }
             }
-
-            // one for the primary already done
-            this.totalShards = 1 + numberOfPendingShardInstances + numberOfUnassignedOrIgnoredReplicas;
+            // one for the local primary copy
+            this.totalShards = 1 + numberOfPendingShardInstances + numberOfIgnoredShardInstances;
             this.pending = new AtomicInteger(numberOfPendingShardInstances);
+            if (logger.isTraceEnabled()) {
+                logger.trace("replication phase started. pending [{}], action [{}], request [{}], cluster state version used [{}]", pending.get(),
+                    transportReplicaAction, replicaRequest, state.version());
+            }
         }
 
         /**
@@ -821,114 +793,84 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         public void onFailure(Throwable t) {
-            logger.error("unexpected error while replicating for action [{}]. shard [{}]. ", t, actionName, shardIt.shardId());
+            logger.error("unexpected error while replicating for action [{}]. shard [{}]. ", t, actionName, shardId);
             forceFinishAsFailed(t);
         }
 
         /**
-         * start sending current requests to replicas
+         * start sending replica requests to target nodes
          */
         @Override
         protected void doRun() {
-            if (logger.isTraceEnabled()) {
-                logger.trace("replication phase started. pending [{}], action [{}], request [{}], cluster state version used [{}]", pending.get(),
-                        actionName, replicaRequest, observer.observedState().version());
-            }
             if (pending.get() == 0) {
                 doFinish();
                 return;
             }
-            ShardRouting shard;
-            shardIt.reset(); // reset the iterator
-            while ((shard = shardIt.nextOrNull()) != null) {
-                // if its unassigned, nothing to do here...
+            for (ShardRouting shard : shards) {
+                if (shard.primary() == false && executeOnReplica == false) {
+                    // If the replicas use shadow replicas, there is no reason to
+                    // perform the action on the replica, so skip it and
+                    // immediately return
+
+                    // this delays mapping updates on replicas because they have
+                    // to wait until they get the new mapping through the cluster
+                    // state, which is why we recommend pre-defined mappings for
+                    // indices using shadow replicas
+                    continue;
+                }
                 if (shard.unassigned()) {
                     continue;
                 }
-
                 // we index on a replica that is initializing as well since we might not have got the event
                 // yet that it was started. We will get an exception IllegalShardState exception if its not started
                 // and that's fine, we will ignore it
-                if (shard.primary()) {
-                    if (originalPrimaryShard.currentNodeId().equals(shard.currentNodeId()) == false) {
-                        // there is a new primary, we'll have to replicate to it.
-                        performOnReplica(shard, shard.currentNodeId());
-                    }
-                    if (shard.relocating()) {
-                        performOnReplica(shard, shard.relocatingNodeId());
-                    }
-                } else if (shouldExecuteReplication(indexMetaData.getSettings())) {
+
+                // we never execute replication operation locally as primary operation has already completed locally
+                // hence, we ignore any local shard for replication
+                if (nodes.localNodeId().equals(shard.currentNodeId()) == false) {
                     performOnReplica(shard, shard.currentNodeId());
-                    if (shard.relocating()) {
-                        performOnReplica(shard, shard.relocatingNodeId());
-                    }
+                }
+                // send operation to relocating shard
+                if (shard.relocating()) {
+                    performOnReplica(shard, shard.relocatingNodeId());
                 }
             }
         }
 
         /**
-         * send operation to the given node or perform it if local
+         * send replica operation to target node
          */
         void performOnReplica(final ShardRouting shard, final String nodeId) {
             // if we don't have that node, it means that it might have failed and will be created again, in
             // this case, we don't have to do the operation, and just let it failover
-            if (!observer.observedState().nodes().nodeExists(nodeId)) {
+            if (!nodes.nodeExists(nodeId)) {
+                logger.trace("failed to send action [{}] on replica [{}] for request [{}] due to unknown node [{}]", transportReplicaAction, shard.shardId(), replicaRequest, nodeId);
                 onReplicaFailure(nodeId, null);
                 return;
             }
+            if (logger.isTraceEnabled()) {
+                logger.trace("send action [{}] on replica [{}] for request [{}] to [{}]", transportReplicaAction, shard.shardId(), replicaRequest, nodeId);
+            }
 
-            replicaRequest.internalShardId = shardIt.shardId();
-
-            if (!nodeId.equals(observer.observedState().nodes().localNodeId())) {
-                final DiscoveryNode node = observer.observedState().nodes().get(nodeId);
-                transportService.sendRequest(node, transportReplicaAction, replicaRequest,
-                        transportOptions, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
-                            @Override
-                            public void handleResponse(TransportResponse.Empty vResponse) {
-                                onReplicaSuccess();
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, actionName);
-                                if (ignoreReplicaException(exp)) {
-                                    onReplicaFailure(nodeId, exp);
-                                } else {
-                                    logger.warn("{} failed to perform {} on node {}", exp, shardIt.shardId(), actionName, node);
-                                    shardStateAction.shardFailed(shard, indexMetaData.getIndexUUID(), "failed to perform " + actionName + " on replica on node " + node, exp, shardFailedTimeout, new ReplicationFailedShardStateListener(nodeId, exp));
-                                }
-                            }
-                        });
-            } else {
-                try {
-                    threadPool.executor(executor).execute(new AbstractRunnable() {
-                        @Override
-                        protected void doRun() {
-                            try {
-                                shardOperationOnReplica(shard.shardId(), replicaRequest);
-                                onReplicaSuccess();
-                            } catch (Throwable e) {
-                                onReplicaFailure(nodeId, e);
-                                failReplicaIfNeeded(shard.index(), shard.id(), e, replicaRequest);
-                            }
-                        }
-
-                        // we must never reject on because of thread pool capacity on replicas
+            final DiscoveryNode node = nodes.get(nodeId);
+            transportService.sendRequest(node, transportReplicaAction, replicaRequest, transportOptions, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
                         @Override
-                        public boolean isForceExecution() {
-                            return true;
+                        public void handleResponse(TransportResponse.Empty vResponse) {
+                            onReplicaSuccess();
                         }
 
                         @Override
-                        public void onFailure(Throwable t) {
-                            onReplicaFailure(nodeId, t);
+                        public void handleException(TransportException exp) {
+                            logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction);
+                            if (ignoreReplicaException(exp)) {
+                                onReplicaFailure(nodeId, exp);
+                            } else {
+                                logger.warn("{} failed to perform {} on node {}", exp, shardId, transportReplicaAction, node);
+                                shardStateAction.shardFailed(shard, indexUUID, "failed to perform " + transportReplicaAction + " on replica on node " + node, exp, shardFailedTimeout, new ReplicationFailedShardStateListener(nodeId, exp));
+                            }
                         }
-                    });
-                } catch (Throwable e) {
-                    failReplicaIfNeeded(shard.index(), shard.id(), e, replicaRequest);
-                    onReplicaFailure(nodeId, e);
-                }
-            }
+                    }
+            );
         }
 
 
@@ -954,35 +896,46 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         private void forceFinishAsFailed(Throwable t) {
             if (finished.compareAndSet(false, true)) {
                 Releasables.close(indexShardReference);
-                listener.onFailure(t);
+                try {
+                    channel.sendResponse(t);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [{}]", responseException, transportReplicaAction);
+                    logger.warn("actual Exception", t);
+                }
             }
         }
 
         private void doFinish() {
             if (finished.compareAndSet(false, true)) {
                 Releasables.close(indexShardReference);
-                final ShardId shardId = shardIt.shardId();
-                final ActionWriteResponse.ShardInfo.Failure[] failuresArray;
+                final ReplicationResponse.ShardInfo.Failure[] failuresArray;
                 if (!shardReplicaFailures.isEmpty()) {
                     int slot = 0;
-                    failuresArray = new ActionWriteResponse.ShardInfo.Failure[shardReplicaFailures.size()];
+                    failuresArray = new ReplicationResponse.ShardInfo.Failure[shardReplicaFailures.size()];
                     for (Map.Entry<String, Throwable> entry : shardReplicaFailures.entrySet()) {
                         RestStatus restStatus = ExceptionsHelper.status(entry.getValue());
-                        failuresArray[slot++] = new ActionWriteResponse.ShardInfo.Failure(
+                        failuresArray[slot++] = new ReplicationResponse.ShardInfo.Failure(
                                 shardId.getIndex(), shardId.getId(), entry.getKey(), entry.getValue(), restStatus, false
                         );
                     }
                 } else {
-                    failuresArray = ActionWriteResponse.EMPTY;
+                    failuresArray = ReplicationResponse.EMPTY;
                 }
-                finalResponse.setShardInfo(new ActionWriteResponse.ShardInfo(
+                finalResponse.setShardInfo(new ReplicationResponse.ShardInfo(
                                 totalShards,
                                 success.get(),
                                 failuresArray
 
                         )
                 );
-                listener.onResponse(finalResponse);
+                try {
+                    channel.sendResponse(finalResponse);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [" + transportReplicaAction + "]", responseException);
+                }
+                if (logger.isTraceEnabled()) {
+                    logger.trace("action [{}] completed on all replicas [{}] for request [{}]", transportReplicaAction, shardId, replicaRequest);
+                }
             }
         }
 
@@ -1023,34 +976,10 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return IndexMetaData.isIndexUsingShadowReplicas(settings) == false;
     }
 
-    /**
-     * Internal request class that gets built on each node. Holds the original request plus additional info.
-     */
-    protected class InternalRequest {
-        final Request request;
-        String concreteIndex;
-
-        InternalRequest(Request request) {
-            this.request = request;
-        }
-
-        public Request request() {
-            return request;
-        }
-
-        void concreteIndex(String concreteIndex) {
-            this.concreteIndex = concreteIndex;
-        }
-
-        public String concreteIndex() {
-            return concreteIndex;
-        }
-    }
-
     static class IndexShardReference implements Releasable {
 
         final private IndexShard counter;
-        private final AtomicBoolean closed = new AtomicBoolean(false);
+        private final AtomicBoolean closed = new AtomicBoolean();
 
         IndexShardReference(IndexShard counter) {
             counter.incrementOperationCounter();
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java b/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
index e654eda..dd78d7a 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.termvectors;
 
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.DocumentRequest;
 import org.elasticsearch.action.support.ActionFilters;
@@ -79,8 +78,8 @@ public class TransportMultiTermVectorsAction extends HandledTransportAction<Mult
                         new IllegalArgumentException("routing is required for [" + concreteSingleIndex + "]/[" + termVectorsRequest.type() + "]/[" + termVectorsRequest.id() + "]"))));
                 continue;
             }
-            ShardId shardId = clusterService.operationRouting().getShards(clusterState, concreteSingleIndex,
-                    termVectorsRequest.type(), termVectorsRequest.id(), termVectorsRequest.routing(), null).shardId();
+            ShardId shardId = clusterService.operationRouting().shardId(clusterState, concreteSingleIndex,
+                    termVectorsRequest.id(), termVectorsRequest.routing());
             MultiTermVectorsShardRequest shardRequest = shardRequests.get(shardId);
             if (shardRequest == null) {
                 shardRequest = new MultiTermVectorsShardRequest(request, shardId.index().name(), shardId.id());
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index b2d24fe..e5edc1a 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -175,7 +175,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 indexAction.execute(upsertRequest, new ActionListener<IndexResponse>() {
                     @Override
                     public void onResponse(IndexResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
                         if (request.fields() != null && request.fields().length > 0) {
                             Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(upsertSourceBytes, true);
                             update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), upsertSourceBytes));
@@ -212,7 +212,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 indexAction.execute(indexRequest, new ActionListener<IndexResponse>() {
                     @Override
                     public void onResponse(IndexResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
                         update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), result.updatedSourceAsMap(), result.updateSourceContentType(), indexSourceBytes));
                         listener.onResponse(update);
                     }
@@ -240,7 +240,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 deleteAction.execute(deleteRequest, new ActionListener<DeleteResponse>() {
                     @Override
                     public void onResponse(DeleteResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), false);
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), false);
                         update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), result.updatedSourceAsMap(), result.updateSourceContentType(), null));
                         listener.onResponse(update);
                     }
diff --git a/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java b/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
index 4bdcd43..9f8b2a2 100644
--- a/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
+++ b/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
@@ -83,9 +83,10 @@ public class UpdateHelper extends AbstractComponent {
     @SuppressWarnings("unchecked")
     protected Result prepare(UpdateRequest request, final GetResult getResult) {
         long getDateNS = System.nanoTime();
+        final ShardId shardId = new ShardId(getResult.getIndex(), request.shardId());
         if (!getResult.isExists()) {
             if (request.upsertRequest() == null && !request.docAsUpsert()) {
-                throw new DocumentMissingException(new ShardId(request.index(), request.shardId()), request.type(), request.id());
+                throw new DocumentMissingException(shardId, request.type(), request.id());
             }
             IndexRequest indexRequest = request.docAsUpsert() ? request.doc() : request.upsertRequest();
             TimeValue ttl = indexRequest.ttl();
@@ -113,7 +114,7 @@ public class UpdateHelper extends AbstractComponent {
                         logger.warn("Used upsert operation [{}] for script [{}], doing nothing...", scriptOpChoice,
                                 request.script.getScript());
                     }
-                    UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(),
+                    UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(),
                             getResult.getVersion(), false);
                     update.setGetResult(getResult);
                     return new Result(update, Operation.NONE, upsertDoc, XContentType.JSON);
@@ -145,7 +146,7 @@ public class UpdateHelper extends AbstractComponent {
 
         if (getResult.internalSourceRef() == null) {
             // no source, we can't do nothing, through a failure...
-            throw new DocumentSourceMissingException(new ShardId(request.index(), request.shardId()), request.type(), request.id());
+            throw new DocumentSourceMissingException(shardId, request.type(), request.id());
         }
 
         Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(getResult.internalSourceRef(), true);
@@ -231,12 +232,12 @@ public class UpdateHelper extends AbstractComponent {
                     .consistencyLevel(request.consistencyLevel());
             return new Result(deleteRequest, Operation.DELETE, updatedSourceAsMap, updateSourceContentType);
         } else if ("none".equals(operation)) {
-            UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(), getResult.getVersion(), false);
+            UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(), getResult.getVersion(), false);
             update.setGetResult(extractGetResult(request, request.index(), getResult.getVersion(), updatedSourceAsMap, updateSourceContentType, getResult.internalSourceRef()));
             return new Result(update, Operation.NONE, updatedSourceAsMap, updateSourceContentType);
         } else {
             logger.warn("Used update operation [{}] for script [{}], doing nothing...", operation, request.script.getScript());
-            UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(), getResult.getVersion(), false);
+            UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(), getResult.getVersion(), false);
             return new Result(update, Operation.NONE, updatedSourceAsMap, updateSourceContentType);
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java b/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
index af64380..2f3146b 100644
--- a/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
@@ -19,21 +19,21 @@
 
 package org.elasticsearch.action.update;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
 /**
  */
-public class UpdateResponse extends ActionWriteResponse {
+public class UpdateResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean created;
     private GetResult getResult;
 
@@ -44,47 +44,16 @@ public class UpdateResponse extends ActionWriteResponse {
      * Constructor to be used when a update didn't translate in a write.
      * For example: update script with operation set to none
      */
-    public UpdateResponse(String index, String type, String id, long version, boolean created) {
-        this(new ShardInfo(0, 0), index, type, id, version, created);
+    public UpdateResponse(ShardId shardId, String type, String id, long version, boolean created) {
+        this(new ShardInfo(0, 0), shardId, type, id, version, created);
     }
 
-    public UpdateResponse(ShardInfo shardInfo, String index, String type, String id, long version, boolean created) {
+    public UpdateResponse(ShardInfo shardInfo, ShardId shardId, String type, String id, long version, boolean created) {
+        super(shardId, type, id, version);
         setShardInfo(shardInfo);
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
         this.created = created;
     }
 
-    /**
-     * The index the document was indexed into.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document indexed.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document indexed.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * Returns the current version of the doc indexed.
-     */
-    public long getVersion() {
-        return this.version;
-    }
-
     public void setGetResult(GetResult getResult) {
         this.getResult = getResult;
     }
@@ -102,12 +71,16 @@ public class UpdateResponse extends ActionWriteResponse {
     }
 
     @Override
+    public RestStatus status() {
+        if (created) {
+            return RestStatus.CREATED;
+        }
+        return super.status();
+    }
+
+    @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         created = in.readBoolean();
         if (in.readBoolean()) {
             getResult = GetResult.readGetResult(in);
@@ -117,10 +90,6 @@ public class UpdateResponse extends ActionWriteResponse {
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(created);
         if (getResult == null) {
             out.writeBoolean(false);
@@ -129,4 +98,34 @@ public class UpdateResponse extends ActionWriteResponse {
             getResult.writeTo(out);
         }
     }
+
+
+    static final class Fields {
+        static final XContentBuilderString GET = new XContentBuilderString("get");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        super.toXContent(builder, params);
+        if (getGetResult() != null) {
+            builder.startObject(Fields.GET);
+            getGetResult().toXContentEmbedded(builder, params);
+            builder.endObject();
+        }
+        return builder;
+    }
+
+    @Override
+    public String toString() {
+        StringBuilder builder = new StringBuilder();
+        builder.append("UpdateResponse[");
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
+        builder.append(",created=").append(created);
+        builder.append(",shards=").append(getShardInfo());
+        return builder.append("]").toString();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 3a0ddf1..053f4ae 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -37,7 +37,6 @@ import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.monitor.os.OsProbe;
 import org.elasticsearch.monitor.process.ProcessProbe;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 
 import java.io.ByteArrayOutputStream;
@@ -80,11 +79,11 @@ final class Bootstrap {
             }
         });
     }
-    
+
     /** initialize native resources */
     public static void initializeNatives(Path tmpFile, boolean mlockAll, boolean seccomp, boolean ctrlHandler) {
         final ESLogger logger = Loggers.getLogger(Bootstrap.class);
-        
+
         // check if the user is running as root, and bail
         if (Natives.definitelyRunningAsRoot()) {
             if (Boolean.parseBoolean(System.getProperty("es.insecure.allow.root"))) {
@@ -93,12 +92,12 @@ final class Bootstrap {
                 throw new RuntimeException("don't run elasticsearch as root.");
             }
         }
-        
+
         // enable secure computing mode
         if (seccomp) {
             Natives.trySeccomp(tmpFile);
         }
-        
+
         // mlockall if requested
         if (mlockAll) {
             if (Constants.WINDOWS) {
@@ -175,11 +174,10 @@ final class Bootstrap {
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
                 .build();
 
-        NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder().settings(nodeSettings);
-        node = nodeBuilder.build();
+        node = new Node(nodeSettings);
     }
-    
-    /** 
+
+    /**
      * option for elasticsearch.yml etc to turn off our security manager completely,
      * for example if you want to have your own configuration or just disable.
      */
@@ -322,7 +320,7 @@ final class Bootstrap {
             if (foreground) {
                 Loggers.enableConsoleLogging();
             }
-            
+
             throw e;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
index 44542b5..267dae8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
@@ -25,7 +25,6 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.math.MathUtils;
@@ -55,19 +54,16 @@ public class OperationRouting extends AbstractComponent {
     }
 
     public ShardIterator indexShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        return shards(clusterState, index, type, id, routing).shardsIt();
-    }
-
-    public ShardIterator deleteShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        return shards(clusterState, index, type, id, routing).shardsIt();
+        return shards(clusterState, index, id, routing).shardsIt();
     }
 
     public ShardIterator getShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing, @Nullable String preference) {
-        return preferenceActiveShardIterator(shards(clusterState, index, type, id, routing), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
+        return preferenceActiveShardIterator(shards(clusterState, index, id, routing), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
     }
 
     public ShardIterator getShards(ClusterState clusterState, String index, int shardId, @Nullable String preference) {
-        return preferenceActiveShardIterator(shards(clusterState, index, shardId), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
+        final IndexShardRoutingTable indexShard = clusterState.getRoutingTable().shardRoutingTable(index, shardId);
+        return preferenceActiveShardIterator(indexShard, clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
     }
 
     public GroupShardsIterator broadcastDeleteShards(ClusterState clusterState, String index) {
@@ -102,7 +98,7 @@ public class OperationRouting extends AbstractComponent {
             final Set<String> effectiveRouting = routing.get(index);
             if (effectiveRouting != null) {
                 for (String r : effectiveRouting) {
-                    int shardId = shardId(clusterState, index, null, null, r);
+                    int shardId = generateShardId(clusterState, index, null, r);
                     IndexShardRoutingTable indexShard = indexRouting.shard(shardId);
                     if (indexShard == null) {
                         throw new ShardNotFoundException(new ShardId(index, shardId));
@@ -200,14 +196,6 @@ public class OperationRouting extends AbstractComponent {
         }
     }
 
-    public IndexMetaData indexMetaData(ClusterState clusterState, String index) {
-        IndexMetaData indexMetaData = clusterState.metaData().index(index);
-        if (indexMetaData == null) {
-            throw new IndexNotFoundException(index);
-        }
-        return indexMetaData;
-    }
-
     protected IndexRoutingTable indexRoutingTable(ClusterState clusterState, String index) {
         IndexRoutingTable indexRouting = clusterState.routingTable().index(index);
         if (indexRouting == null) {
@@ -216,25 +204,20 @@ public class OperationRouting extends AbstractComponent {
         return indexRouting;
     }
 
-
-    // either routing is set, or type/id are set
-
-    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, String type, String id, String routing) {
-        int shardId = shardId(clusterState, index, type, id, routing);
-        return shards(clusterState, index, shardId);
+    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, String id, String routing) {
+        int shardId = generateShardId(clusterState, index, id, routing);
+        return clusterState.getRoutingTable().shardRoutingTable(index, shardId);
     }
 
-    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, int shardId) {
-        IndexShardRoutingTable indexShard = indexRoutingTable(clusterState, index).shard(shardId);
-        if (indexShard == null) {
-            throw new ShardNotFoundException(new ShardId(index, shardId));
-        }
-        return indexShard;
+    public ShardId shardId(ClusterState clusterState, String index, String id, @Nullable String routing) {
+        return new ShardId(index, generateShardId(clusterState, index, id, routing));
     }
 
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    private int shardId(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        final IndexMetaData indexMetaData = indexMetaData(clusterState, index);
+    private int generateShardId(ClusterState clusterState, String index, String id, @Nullable String routing) {
+        IndexMetaData indexMetaData = clusterState.metaData().index(index);
+        if (indexMetaData == null) {
+            throw new IndexNotFoundException(index);
+        }
         final int hash;
         if (routing == null) {
             hash = Murmur3HashFunction.hash(id);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index c210539..fbabacd 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -33,6 +33,8 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.IndexNotFoundException;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -95,6 +97,24 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return indicesRouting();
     }
 
+    /**
+     * All shards for the provided index and shard id
+     * @return All the shard routing entries for the given index and shard id
+     * @throws IndexNotFoundException if provided index does not exist
+     * @throws ShardNotFoundException if provided shard id is unknown
+     */
+    public IndexShardRoutingTable shardRoutingTable(String index, int shardId) {
+        IndexRoutingTable indexRouting = index(index);
+        if (indexRouting == null) {
+            throw new IndexNotFoundException(index);
+        }
+        IndexShardRoutingTable shard = indexRouting.shard(shardId);
+        if (shard == null) {
+            throw new ShardNotFoundException(new ShardId(index, shardId));
+        }
+        return shard;
+    }
+
     public RoutingTable validateRaiseException(MetaData metaData) throws RoutingValidationException {
         RoutingTableValidation validation = validate(metaData);
         if (!validation.valid()) {
diff --git a/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java b/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java
deleted file mode 100644
index 18535d1..0000000
--- a/core/src/main/java/org/elasticsearch/common/SearchScrollIterator.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common;
-
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchScrollRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.SearchHit;
-
-import java.util.Collections;
-import java.util.Iterator;
-
-/**
- * An iterator that easily helps to consume all hits from a scroll search.
- */
-public final class SearchScrollIterator implements Iterator<SearchHit> {
-
-    /**
-     * Creates an iterator that returns all matching hits of a scroll search via an iterator.
-     * The iterator will return all hits per scroll search and execute additional scroll searches
-     * to get more hits until all hits have been returned by the scroll search on the ES side.
-     */
-    public static Iterable<SearchHit> createIterator(Client client, TimeValue scrollTimeout, SearchRequest searchRequest) {
-        searchRequest.scroll(scrollTimeout);
-        SearchResponse searchResponse = client.search(searchRequest).actionGet(scrollTimeout);
-        if (searchResponse.getHits().getTotalHits() == 0) {
-            return Collections.emptyList();
-        } else {
-            return () -> new SearchScrollIterator(client, scrollTimeout, searchResponse);
-        }
-    }
-
-    private final Client client;
-    private final TimeValue scrollTimeout;
-
-    private int currentIndex;
-    private SearchHit[] currentHits;
-    private SearchResponse searchResponse;
-
-    private SearchScrollIterator(Client client, TimeValue scrollTimeout, SearchResponse searchResponse) {
-        this.client = client;
-        this.scrollTimeout = scrollTimeout;
-        this.searchResponse = searchResponse;
-        this.currentHits = searchResponse.getHits().getHits();
-    }
-
-    @Override
-    public boolean hasNext() {
-        if (currentIndex < currentHits.length) {
-            return true;
-        } else {
-            if (searchResponse == null) {
-                return false;
-            }
-
-            SearchScrollRequest request = new SearchScrollRequest(searchResponse.getScrollId());
-            request.scroll(scrollTimeout);
-            searchResponse = client.searchScroll(request).actionGet(scrollTimeout);
-            if (searchResponse.getHits().getHits().length == 0) {
-                searchResponse = null;
-                return false;
-            } else {
-                currentHits = searchResponse.getHits().getHits();
-                currentIndex = 0;
-                return true;
-            }
-        }
-    }
-
-    @Override
-    public SearchHit next() {
-        return currentHits[currentIndex++];
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Key.java b/core/src/main/java/org/elasticsearch/common/inject/Key.java
index 3af3b4e..7344dfe 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Key.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Key.java
@@ -333,7 +333,7 @@ public class Key<T> {
      * Returns {@code true} if the given annotation type has no attributes.
      */
     static boolean isMarker(Class<? extends Annotation> annotationType) {
-        return annotationType.getDeclaredMethods().length == 0;
+        return annotationType.getMethods().length == 0;
     }
 
     /**
@@ -345,7 +345,7 @@ public class Key<T> {
         ensureRetainedAtRuntime(annotationType);
         ensureIsBindingAnnotation(annotationType);
 
-        if (annotationType.getDeclaredMethods().length == 0) {
+        if (annotationType.getMethods().length == 0) {
             return new AnnotationTypeStrategy(annotationType, annotation);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
index 22c542b..667466f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
@@ -41,7 +41,7 @@ class Reflection {
     @SuppressWarnings("unchecked")
     static <T> Constructor<T> invalidConstructor() {
         try {
-            return (Constructor<T>) InvalidConstructor.class.getDeclaredConstructor();
+            return (Constructor<T>) InvalidConstructor.class.getConstructor();
         } catch (NoSuchMethodException e) {
             throw new AssertionError(e);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
index 3837de8..0def65b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
@@ -212,7 +212,7 @@ public class FactoryProvider<F> implements Provider<F>, HasDependencies {
             TypeLiteral<?> factoryType, TypeLiteral<?> implementationType) {
         List<AssistedConstructor<?>> constructors = new ArrayList<>();
 
-        for (Constructor<?> constructor : implementationType.getRawType().getDeclaredConstructors()) {
+        for (Constructor<?> constructor : implementationType.getRawType().getConstructors()) {
             if (constructor.getAnnotation(AssistedInject.class) != null) {
                 @SuppressWarnings("unchecked") // the constructor type and implementation type agree
                         AssistedConstructor assistedConstructor = new AssistedConstructor(
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
index 1671b4a..5e45b49 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
@@ -83,7 +83,7 @@ public final class ProviderMethodsModule implements Module {
     public List<ProviderMethod<?>> getProviderMethods(Binder binder) {
         List<ProviderMethod<?>> result = new ArrayList<>();
         for (Class<?> c = delegate.getClass(); c != Object.class; c = c.getSuperclass()) {
-            for (Method method : c.getDeclaredMethods()) {
+            for (Method method : c.getMethods()) {
                 if (method.getAnnotation(Provides.class) != null) {
                     result.add(createProviderMethod(binder, method));
                 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
index 17497d5..286635b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
@@ -188,7 +188,7 @@ public final class InjectionPoint {
         Errors errors = new Errors(rawType);
 
         Constructor<?> injectableConstructor = null;
-        for (Constructor<?> constructor : rawType.getDeclaredConstructors()) {
+        for (Constructor<?> constructor : rawType.getConstructors()) {
             Inject inject = constructor.getAnnotation(Inject.class);
             if (inject != null) {
                 if (inject.optional()) {
@@ -212,7 +212,7 @@ public final class InjectionPoint {
 
         // If no annotated constructor is found, look for a no-arg constructor instead.
         try {
-            Constructor<?> noArgConstructor = rawType.getDeclaredConstructor();
+            Constructor<?> noArgConstructor = rawType.getConstructor();
 
             // Disallow private constructors on non-private classes (unless they have @Inject)
             if (Modifier.isPrivate(noArgConstructor.getModifiers())
@@ -334,7 +334,7 @@ public final class InjectionPoint {
         // name. In Scala, fields always get accessor methods (that we need to ignore). See bug 242.
         if (member instanceof Method) {
             try {
-                if (member.getDeclaringClass().getDeclaredField(member.getName()) != null) {
+                if (member.getDeclaringClass().getField(member.getName()) != null) {
                     return;
                 }
             } catch (NoSuchFieldException ignore) {
@@ -390,7 +390,7 @@ public final class InjectionPoint {
         Factory<Field> FIELDS = new Factory<Field>() {
             @Override
             public Field[] getMembers(Class<?> type) {
-                return type.getDeclaredFields();
+                return type.getFields();
             }
 
             @Override
@@ -402,7 +402,7 @@ public final class InjectionPoint {
         Factory<Method> METHODS = new Factory<Method>() {
             @Override
             public Method[] getMembers(Class<?> type) {
-                return type.getDeclaredMethods();
+                return type.getMethods();
             }
 
             @Override
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 05eaac1..835a35d 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -137,8 +137,7 @@ public class NetworkService extends AbstractComponent {
      * Resolves {@code publishHosts} to a single publish address. The fact that it returns
      * only one address is just a current limitation.
      * <p>
-     * If {@code publishHosts} resolves to more than one address, <b>then one is selected with magic</b>,
-     * and the user is warned (they can always just be more specific).
+     * If {@code publishHosts} resolves to more than one address, <b>then one is selected with magic</b>
      * @param publishHosts list of hosts to publish as. this may contain special pseudo-hostnames
      *                     such as _local_ (see the documentation). if it is null, it will be populated
      *                     based on global default settings.
@@ -186,13 +185,12 @@ public class NetworkService extends AbstractComponent {
             }
         }
         
-        // 3. warn user if we end out with multiple publish addresses
+        // 3. if we end out with multiple publish addresses, select by preference.
+        // don't warn the user, or they will get confused by bind_host vs publish_host etc.
         if (addresses.length > 1) {
             List<InetAddress> sorted = new ArrayList<>(Arrays.asList(addresses));
             NetworkUtils.sortAddresses(sorted);
             addresses = new InetAddress[] { sorted.get(0) };
-            logger.warn("publish host: {} resolves to multiple addresses, auto-selecting {{}} as single publish address", 
-                    Arrays.toString(publishHosts), NetworkAddress.format(addresses[0]));
         }
         return addresses[0];
     }
diff --git a/core/src/main/java/org/elasticsearch/common/regex/Regex.java b/core/src/main/java/org/elasticsearch/common/regex/Regex.java
index f5c3094..061ad6c 100644
--- a/core/src/main/java/org/elasticsearch/common/regex/Regex.java
+++ b/core/src/main/java/org/elasticsearch/common/regex/Regex.java
@@ -150,7 +150,7 @@ public class Regex {
                 pFlags |= Pattern.LITERAL;
             } else if ("COMMENTS".equals(s)) {
                 pFlags |= Pattern.COMMENTS;
-            } else if ("UNICODE_CHAR_CLASS".equals(s)) {
+            } else if (("UNICODE_CHAR_CLASS".equals(s)) || ("UNICODE_CHARACTER_CLASS".equals(s))) {
                 pFlags |= UNICODE_CHARACTER_CLASS;
             } else {
                 throw new IllegalArgumentException("Unknown regex flag [" + s + "]");
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
index 101098d..50c0493 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
@@ -46,11 +46,6 @@ public interface XContent {
     XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException;
 
     /**
-     * Creates a new generator using the provided writer.
-     */
-    XContentGenerator createGenerator(Writer writer) throws IOException;
-
-    /**
      * Creates a parser over the provided string content.
      */
     XContentParser createParser(String content) throws IOException;
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
index 2bd4b9e..af8e753 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
@@ -920,23 +920,18 @@ public final class XContentBuilder implements BytesStream, Releasable {
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, byte[] content) throws IOException {
-        generator.writeRawField(fieldName, content, bos);
+    public XContentBuilder rawField(String fieldName, InputStream content) throws IOException {
+        generator.writeRawField(fieldName, content);
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, byte[] content, int offset, int length) throws IOException {
-        generator.writeRawField(fieldName, content, offset, length, bos);
-        return this;
-    }
-
-    public XContentBuilder rawField(String fieldName, InputStream content, XContentType contentType) throws IOException {
-        generator.writeRawField(fieldName, content, bos, contentType);
+    public XContentBuilder rawField(String fieldName, BytesReference content) throws IOException {
+        generator.writeRawField(fieldName, content);
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, BytesReference content) throws IOException {
-        generator.writeRawField(fieldName, content, bos);
+    public XContentBuilder rawValue(BytesReference content) throws IOException {
+        generator.writeRawValue(content);
         return this;
     }
 
@@ -1202,10 +1197,6 @@ public final class XContentBuilder implements BytesStream, Releasable {
         return this.generator;
     }
 
-    public OutputStream stream() {
-        return this.bos;
-    }
-
     @Override
     public BytesReference bytes() {
         close();
@@ -1213,14 +1204,6 @@ public final class XContentBuilder implements BytesStream, Releasable {
     }
 
     /**
-     * Returns the actual stream used.
-     */
-    public BytesStream bytesStream() throws IOException {
-        close();
-        return (BytesStream) bos;
-    }
-
-    /**
      * Returns a string representation of the builder (only applicable for text based xcontent).
      */
     public String string() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
index 9843a19..11a42e3 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 
 /**
  *
@@ -112,13 +111,11 @@ public interface XContentGenerator extends Closeable {
 
     void writeObjectFieldStart(XContentString fieldName) throws IOException;
 
-    void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException;
+    void writeRawField(String fieldName, InputStream content) throws IOException;
 
-    void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException;
+    void writeRawField(String fieldName, BytesReference content) throws IOException;
 
-    void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException;
-
-    void writeRawField(String fieldName, BytesReference content, OutputStream bos) throws IOException;
+    void writeRawValue(BytesReference content) throws IOException;
 
     void copyCurrentStructure(XContentParser parser) throws IOException;
 
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
index f16332f..4466d29 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.compress.Compressor;
 import org.elasticsearch.common.compress.CompressorFactory;
-import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.xcontent.ToXContent.Params;
 
 import java.io.BufferedInputStream;
@@ -102,9 +101,7 @@ public class XContentHelper {
             BytesArray bytesArray = bytes.toBytesArray();
             return new String(bytesArray.array(), bytesArray.arrayOffset(), bytesArray.length(), StandardCharsets.UTF_8);
         }
-        XContentParser parser = null;
-        try {
-            parser = XContentFactory.xContent(xContentType).createParser(bytes.streamInput());
+        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(bytes.streamInput())) {
             parser.nextToken();
             XContentBuilder builder = XContentFactory.jsonBuilder();
             if (prettyPrint) {
@@ -112,10 +109,6 @@ public class XContentHelper {
             }
             builder.copyCurrentStructure(parser);
             return builder.string();
-        } finally {
-            if (parser != null) {
-                parser.close();
-            }
         }
     }
 
@@ -128,9 +121,7 @@ public class XContentHelper {
         if (xContentType == XContentType.JSON && !reformatJson) {
             return new String(data, offset, length, StandardCharsets.UTF_8);
         }
-        XContentParser parser = null;
-        try {
-            parser = XContentFactory.xContent(xContentType).createParser(data, offset, length);
+        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(data, offset, length)) {
             parser.nextToken();
             XContentBuilder builder = XContentFactory.jsonBuilder();
             if (prettyPrint) {
@@ -138,10 +129,6 @@ public class XContentHelper {
             }
             builder.copyCurrentStructure(parser);
             return builder.string();
-        } finally {
-            if (parser != null) {
-                parser.close();
-            }
         }
     }
 
@@ -379,38 +366,6 @@ public class XContentHelper {
     }
 
     /**
-     * Directly writes the source to the output builder
-     */
-    public static void writeDirect(BytesReference source, XContentBuilder rawBuilder, ToXContent.Params params) throws IOException {
-        Compressor compressor = CompressorFactory.compressor(source);
-        if (compressor != null) {
-            InputStream compressedStreamInput = compressor.streamInput(source.streamInput());
-            if (compressedStreamInput.markSupported() == false) {
-                compressedStreamInput = new BufferedInputStream(compressedStreamInput);
-            }
-            XContentType contentType = XContentFactory.xContentType(compressedStreamInput);
-            if (contentType == rawBuilder.contentType()) {
-                Streams.copy(compressedStreamInput, rawBuilder.stream());
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(compressedStreamInput)) {
-                    parser.nextToken();
-                    rawBuilder.copyCurrentStructure(parser);
-                }
-            }
-        } else {
-            XContentType contentType = XContentFactory.xContentType(source);
-            if (contentType == rawBuilder.contentType()) {
-                source.writeTo(rawBuilder.stream());
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(source)) {
-                    parser.nextToken();
-                    rawBuilder.copyCurrentStructure(parser);
-                }
-            }
-        }
-    }
-
-    /**
      * Writes a "raw" (bytes) field, handling cases where the bytes are compressed, and tries to optimize writing using
      * {@link XContentBuilder#rawField(String, org.elasticsearch.common.bytes.BytesReference)}.
      */
@@ -418,30 +373,9 @@ public class XContentHelper {
         Compressor compressor = CompressorFactory.compressor(source);
         if (compressor != null) {
             InputStream compressedStreamInput = compressor.streamInput(source.streamInput());
-            if (compressedStreamInput.markSupported() == false) {
-                compressedStreamInput = new BufferedInputStream(compressedStreamInput);
-            }
-            XContentType contentType = XContentFactory.xContentType(compressedStreamInput);
-            if (contentType == builder.contentType()) {
-                builder.rawField(field, compressedStreamInput, contentType);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(compressedStreamInput)) {
-                    parser.nextToken();
-                    builder.field(field);
-                    builder.copyCurrentStructure(parser);
-                }
-            }
+            builder.rawField(field, compressedStreamInput);
         } else {
-            XContentType contentType = XContentFactory.xContentType(source);
-            if (contentType == builder.contentType()) {
-                builder.rawField(field, source);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(source)) {
-                    parser.nextToken();
-                    builder.field(field);
-                    builder.copyCurrentStructure(parser);
-                }
-            }
+            builder.rawField(field, source);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
index b68d3e1..d647c5f 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
@@ -178,12 +178,6 @@ public interface XContentParser extends Releasable {
 
     NumberType numberType() throws IOException;
 
-    /**
-     * Is the number type estimated or not (i.e. an int might actually be a long, its just low enough
-     * to be an int).
-     */
-    boolean estimatedNumberType();
-
     short shortValue(boolean coerce) throws IOException;
 
     int intValue(boolean coerce) throws IOException;
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
index 5417675..5f8dddc 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
@@ -61,17 +61,12 @@ public class CborXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(writer));
+        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
index a3ca669..517266b 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.cbor;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.cbor.CBORParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class CborXContentGenerator extends JsonXContentGenerator {
 
-    public CborXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public CborXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class CborXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        CBORParser parser;
-        if (content.hasArray()) {
-            parser = CborXContent.cborFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = CborXContent.cborFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
index 671dee0..86f5bc2 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
@@ -65,17 +65,12 @@ public class JsonXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(writer));
+        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
index 7f3cc63..0854f7a 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
@@ -26,13 +26,13 @@ import com.fasterxml.jackson.core.filter.FilteringGeneratorDelegate;
 import com.fasterxml.jackson.core.io.SerializedString;
 import com.fasterxml.jackson.core.util.DefaultIndenter;
 import com.fasterxml.jackson.core.util.DefaultPrettyPrinter;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.common.xcontent.support.filtering.FilterPathBasedFilter;
 
+import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -58,11 +58,14 @@ public class JsonXContentGenerator implements XContentGenerator {
      */
     private final FilteringGeneratorDelegate filter;
 
+    private final OutputStream os;
+
     private boolean writeLineFeedAtEnd;
     private static final SerializedString LF = new SerializedString("\n");
     private static final DefaultPrettyPrinter.Indenter INDENTER = new DefaultIndenter("  ", LF.getValue());
+    private boolean prettyPrint = false;
 
-    public JsonXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
+    public JsonXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
         if (jsonGenerator instanceof GeneratorBase) {
             this.base = (GeneratorBase) jsonGenerator;
         } else {
@@ -76,6 +79,8 @@ public class JsonXContentGenerator implements XContentGenerator {
             this.filter = new FilteringGeneratorDelegate(jsonGenerator, new FilterPathBasedFilter(filters), true, true);
             this.generator = this.filter;
         }
+
+        this.os = os;
     }
 
     @Override
@@ -86,6 +91,7 @@ public class JsonXContentGenerator implements XContentGenerator {
     @Override
     public final void usePrettyPrint() {
         generator.setPrettyPrinter(new DefaultPrettyPrinter().withObjectIndenter(INDENTER));
+        prettyPrint = true;
     }
 
     @Override
@@ -323,22 +329,16 @@ public class JsonXContentGenerator implements XContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeRawField(fieldName, new BytesArray(content), bos);
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeRawField(fieldName, new BytesArray(content, offset, length), bos);
-    }
-
-    @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        if (isFiltered() || (contentType != contentType())) {
-            // When the current generator is filtered (ie filter != null)
-            // or the content is in a different format than the current generator,
-            // we need to copy the whole structure so that it will be correctly
-            // filtered or converted
+    public void writeRawField(String fieldName, InputStream content) throws IOException {
+        if (content.markSupported() == false) {
+            // needed for the XContentFactory.xContentType call
+            content = new BufferedInputStream(content);
+        }
+        XContentType contentType = XContentFactory.xContentType(content);
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
             try (XContentParser parser = XContentFactory.xContent(contentType).createParser(content)) {
                 parser.nextToken();
                 writeFieldName(fieldName);
@@ -347,49 +347,59 @@ public class JsonXContentGenerator implements XContentGenerator {
         } else {
             writeStartRaw(fieldName);
             flush();
-            Streams.copy(content, bos);
+            Streams.copy(content, os);
             writeEndRaw();
         }
     }
 
     @Override
-    public final void writeRawField(String fieldName, BytesReference content, OutputStream bos) throws IOException {
+    public final void writeRawField(String fieldName, BytesReference content) throws IOException {
         XContentType contentType = XContentFactory.xContentType(content);
-        if (contentType != null) {
-            if (isFiltered() || (contentType != contentType())) {
-                // When the current generator is filtered (ie filter != null)
-                // or the content is in a different format than the current generator,
-                // we need to copy the whole structure so that it will be correctly
-                // filtered or converted
-                copyRawField(fieldName, content, contentType.xContent());
-            } else {
-                // Otherwise, the generator is not filtered and has the same type: we can potentially optimize the write
-                writeObjectRaw(fieldName, content, bos);
-            }
-        } else {
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
             writeFieldName(fieldName);
-            // we could potentially optimize this to not rely on exception logic...
-            String sValue = content.toUtf8();
-            try {
-                writeNumber(Long.parseLong(sValue));
-            } catch (NumberFormatException e) {
-                try {
-                    writeNumber(Double.parseDouble(sValue));
-                } catch (NumberFormatException e1) {
-                    writeString(sValue);
-                }
-            }
+            copyRawValue(content, contentType.xContent());
+        } else {
+            writeStartRaw(fieldName);
+            flush();
+            content.writeTo(os);
+            writeEndRaw();
+        }
+    }
+
+    public final void writeRawValue(BytesReference content) throws IOException {
+        XContentType contentType = XContentFactory.xContentType(content);
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
+            copyRawValue(content, contentType.xContent());
+        } else {
+            flush();
+            content.writeTo(os);
+            writeEndRaw();
         }
     }
 
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeStartRaw(fieldName);
-        flush();
-        content.writeTo(bos);
-        writeEndRaw();
+    private boolean mayWriteRawData(XContentType contentType) {
+        // When the current generator is filtered (ie filter != null)
+        // or the content is in a different format than the current generator,
+        // we need to copy the whole structure so that it will be correctly
+        // filtered or converted
+        return supportsRawWrites()
+                && isFiltered() == false
+                && contentType == contentType()
+                && prettyPrint == false;
     }
 
-    protected void copyRawField(String fieldName, BytesReference content, XContent xContent) throws IOException {
+    /** Whether this generator supports writing raw data directly */
+    protected boolean supportsRawWrites() {
+        return true;
+    }
+
+    protected void copyRawValue(BytesReference content, XContent xContent) throws IOException {
         XContentParser parser = null;
         try {
             if (content.hasArray()) {
@@ -397,9 +407,6 @@ public class JsonXContentGenerator implements XContentGenerator {
             } else {
                 parser = xContent.createParser(content.streamInput());
             }
-            if (fieldName != null) {
-                writeFieldName(fieldName);
-            }
             copyCurrentStructure(parser);
         } finally {
             if (parser != null) {
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
index 787c283..c3aca76 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
@@ -69,11 +69,6 @@ public class JsonXContentParser extends AbstractXContentParser {
     }
 
     @Override
-    public boolean estimatedNumberType() {
-        return true;
-    }
-
-    @Override
     public String currentName() throws IOException {
         return parser.getCurrentName();
     }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
index cb1bdd3..51b8590 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
@@ -62,17 +62,12 @@ public class SmileXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(writer));
+        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
index 5002cfa..451abab 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.smile;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.smile.SmileParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class SmileXContentGenerator extends JsonXContentGenerator {
 
-    public SmileXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public SmileXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class SmileXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        SmileParser parser;
-        if (content.hasArray()) {
-            parser = SmileXContent.smileFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = SmileXContent.smileFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
index d79a6a8..c24ddb7 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
@@ -60,17 +60,12 @@ public class YamlXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(writer));
+        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
index e1a71c1..dcb2155 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.yaml;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.yaml.YAMLParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class YamlXContentGenerator extends JsonXContentGenerator {
 
-    public YamlXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public YamlXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class YamlXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        YAMLParser parser;
-        if (content.hasArray()) {
-            parser = YamlXContent.yamlFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = YamlXContent.yamlFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
index ce2cbd4..b0ad972 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
@@ -560,44 +560,13 @@ class DocumentParser implements Closeable {
             return builder;
         } else if (token == XContentParser.Token.VALUE_NUMBER) {
             XContentParser.NumberType numberType = context.parser().numberType();
-            if (numberType == XContentParser.NumberType.INT) {
-                if (context.parser().estimatedNumberType()) {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "long");
-                    if (builder == null) {
-                        builder = MapperBuilders.longField(currentFieldName);
-                    }
-                    return builder;
-                } else {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "integer");
-                    if (builder == null) {
-                        builder = MapperBuilders.integerField(currentFieldName);
-                    }
-                    return builder;
-                }
-            } else if (numberType == XContentParser.NumberType.LONG) {
+            if (numberType == XContentParser.NumberType.INT || numberType == XContentParser.NumberType.LONG) {
                 Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "long");
                 if (builder == null) {
                     builder = MapperBuilders.longField(currentFieldName);
                 }
                 return builder;
-            } else if (numberType == XContentParser.NumberType.FLOAT) {
-                if (context.parser().estimatedNumberType()) {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "double");
-                    if (builder == null) {
-                        // no templates are defined, we use float by default instead of double
-                        // since this is much more space-efficient and should be enough most of
-                        // the time
-                        builder = MapperBuilders.floatField(currentFieldName);
-                    }
-                    return builder;
-                } else {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "float");
-                    if (builder == null) {
-                        builder = MapperBuilders.floatField(currentFieldName);
-                    }
-                    return builder;
-                }
-            } else if (numberType == XContentParser.NumberType.DOUBLE) {
+            } else if (numberType == XContentParser.NumberType.FLOAT || numberType == XContentParser.NumberType.DOUBLE) {
                 Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "double");
                 if (builder == null) {
                     // no templates are defined, we use float by default instead of double
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 3c72adf..7f64eb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,25 +18,15 @@
  */
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.*;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.elasticsearch.common.Strings.hasLength;
-
 /**
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index d7e4294..c0bf924 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -506,9 +506,6 @@ public class IndexShard extends AbstractIndexShardComponent {
     }
 
     public Engine.Delete prepareDeleteOnReplica(String type, String id, long version, VersionType versionType) {
-        if (shardRouting.primary() && shardRouting.isRelocationTarget() == false) {
-            throw new IllegalIndexShardStateException(shardId, state, "shard is not a replica");
-        }
         final DocumentMapper documentMapper = docMapper(type).getDocumentMapper();
         return prepareDelete(type, id, documentMapper.uidMapper().term(Uid.createUid(type, id)), version, versionType, Engine.Operation.Origin.REPLICA);
     }
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 3caff62..f4bc34a 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -108,8 +108,6 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 /**
  * A node represent a node within a cluster (<tt>cluster.name</tt>). The {@link #client()} can be used
  * in order to use a {@link Client} to perform actions/operations against the cluster.
- * <p>In order to create a node, the {@link NodeBuilder} can be used. When done with it, make sure to
- * call {@link #close()} on it.
  */
 public class Node implements Releasable {
 
diff --git a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java b/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
deleted file mode 100644
index 377c409..0000000
--- a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.node;
-
-import org.elasticsearch.common.settings.Settings;
-
-/**
- * A node builder is used to construct a {@link Node} instance.
- * <p>
- * Settings will be loaded relative to the ES home (with or without <tt>config/</tt> prefix) and if not found,
- * within the classpath (with or without <tt>config/</tt> prefix). The settings file loaded can either be named
- * <tt>elasticsearch.yml</tt> or <tt>elasticsearch.json</tt>).
- * <p>
- * Explicit settings can be passed by using the {@link #settings(org.elasticsearch.common.settings.Settings)} method.
- * <p>
- * In any case, settings will be resolved from system properties as well that are either prefixed with <tt>es.</tt>
- * or <tt>elasticsearch.</tt>.
- * <p>
- * An example for creating a simple node with optional settings loaded from the classpath:
- * <pre>
- * Node node = NodeBuilder.nodeBuilder().node();
- * </pre>
- * <p>
- * An example for creating a node with explicit settings (in this case, a node in the cluster that does not hold
- * data):
- * <pre>
- * Node node = NodeBuilder.nodeBuilder()
- *                      .settings(Settings.settingsBuilder().put("node.data", false)
- *                      .node();
- * </pre>
- * <p>
- * When done with the node, make sure you call {@link Node#close()} on it.
- *
- *
- */
-public class NodeBuilder {
-
-    private final Settings.Builder settings = Settings.settingsBuilder();
-
-    /**
-     * A convenient factory method to create a {@link NodeBuilder}.
-     */
-    public static NodeBuilder nodeBuilder() {
-        return new NodeBuilder();
-    }
-
-    /**
-     * Set addition settings simply by working directly against the settings builder.
-     */
-    public Settings.Builder settings() {
-        return settings;
-    }
-
-    /**
-     * Set addition settings simply by working directly against the settings builder.
-     */
-    public Settings.Builder getSettings() {
-        return settings;
-    }
-
-    /**
-     * Explicit node settings to set.
-     */
-    public NodeBuilder settings(Settings.Builder settings) {
-        return settings(settings.build());
-    }
-
-    /**
-     * Explicit node settings to set.
-     */
-    public NodeBuilder settings(Settings settings) {
-        this.settings.put(settings);
-        return this;
-    }
-
-    /**
-     * Is the node going to be a client node which means it will hold no data (<tt>node.data</tt> is
-     * set to <tt>false</tt>) and other optimizations by different modules.
-     *
-     * @param client Should the node be just a client node or not.
-     */
-    public NodeBuilder client(boolean client) {
-        settings.put("node.client", client);
-        return this;
-    }
-
-    /**
-     * Is the node going to be allowed to allocate data (shards) to it or not. This setting map to
-     * the <tt>node.data</tt> setting. Note, when setting {@link #client(boolean)}, the node will
-     * not hold any data by default.
-     *
-     * @param data Should the node be allocated data to or not.
-     */
-    public NodeBuilder data(boolean data) {
-        settings.put("node.data", data);
-        return this;
-    }
-
-    /**
-     * Is the node a local node. A local node is a node that uses a local (JVM level) discovery and
-     * transport. Other (local) nodes started within the same JVM (actually, class-loader) will be
-     * discovered and communicated with. Nodes outside of the JVM will not be discovered.
-     *
-     * @param local Should the node be local or not
-     */
-    public NodeBuilder local(boolean local) {
-        settings.put("node.local", local);
-        return this;
-    }
-
-    /**
-     * The cluster name this node is part of (maps to the <tt>cluster.name</tt> setting). Defaults
-     * to <tt>elasticsearch</tt>.
-     *
-     * @param clusterName The cluster name this node is part of.
-     */
-    public NodeBuilder clusterName(String clusterName) {
-        settings.put("cluster.name", clusterName);
-        return this;
-    }
-
-    /**
-     * Builds the node without starting it.
-     */
-    public Node build() {
-        return new Node(settings.build());
-    }
-
-    /**
-     * {@link #build()}s and starts the node.
-     */
-    public Node node() {
-        return build().start();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/node/package-info.java b/core/src/main/java/org/elasticsearch/node/package-info.java
index fa503a9..02538cd 100644
--- a/core/src/main/java/org/elasticsearch/node/package-info.java
+++ b/core/src/main/java/org/elasticsearch/node/package-info.java
@@ -18,7 +18,7 @@
  */
 
 /**
- * Allow to build a {@link org.elasticsearch.node.Node} using {@link org.elasticsearch.node.NodeBuilder} which is a
+ * Allow to build a {@link org.elasticsearch.node.Node} which is a
  * node within the cluster.
  */
-package org.elasticsearch.node;
\ No newline at end of file
+package org.elasticsearch.node;
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 5993958..6600bf7 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -83,6 +83,7 @@ public class PluginManager {
             "discovery-gce",
             "discovery-multicast",
             "lang-javascript",
+            "lang-plan-a",
             "lang-python",
             "mapper-attachments",
             "mapper-murmur3",
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
index 4841500..5648abc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.rest.action.admin.indices.alias;
 
 import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest;
+import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest.AliasActions;
 import org.elasticsearch.action.admin.indices.alias.IndicesAliasesResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.metadata.AliasAction;
@@ -30,9 +31,10 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
 
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Map;
 
-import static org.elasticsearch.cluster.metadata.AliasAction.newAddAliasAction;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 
 /**
@@ -75,8 +77,8 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                             } else {
                                 throw new IllegalArgumentException("Alias action [" + action + "] not supported");
                             }
-                            String index = null;
-                            String alias = null;
+                            String[] indices = null;
+                            String[] aliases = null;
                             Map<String, Object> filter = null;
                             String routing = null;
                             boolean routingSet = false;
@@ -90,9 +92,9 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                                     currentFieldName = parser.currentName();
                                 } else if (token.isValue()) {
                                     if ("index".equals(currentFieldName)) {
-                                        index = parser.text();
+                                        indices = new String[] { parser.text() };
                                     } else if ("alias".equals(currentFieldName)) {
-                                        alias = parser.text();
+                                        aliases = new String[] { parser.text() };
                                     } else if ("routing".equals(currentFieldName)) {
                                         routing = parser.textOrNull();
                                         routingSet = true;
@@ -103,6 +105,23 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                                         searchRouting = parser.textOrNull();
                                         searchRoutingSet = true;
                                     }
+                                } else if (token == XContentParser.Token.START_ARRAY) {
+                                    if ("indices".equals(currentFieldName)) {
+                                        List<String> indexNames = new ArrayList<>();
+                                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                                            String index = parser.text();
+                                            indexNames.add(index);
+                                        }
+                                        indices = indexNames.toArray(new String[indexNames.size()]);
+                                    }
+                                    if ("aliases".equals(currentFieldName)) {
+                                        List<String> aliasNames = new ArrayList<>();
+                                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                                            String alias = parser.text();
+                                            aliasNames.add(alias);
+                                        }
+                                        aliases = aliasNames.toArray(new String[aliasNames.size()]);
+                                    }
                                 } else if (token == XContentParser.Token.START_OBJECT) {
                                     if ("filter".equals(currentFieldName)) {
                                         filter = parser.mapOrdered();
@@ -111,19 +130,19 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                             }
 
                             if (type == AliasAction.Type.ADD) {
-                                AliasAction aliasAction = newAddAliasAction(index, alias).filter(filter);
+                                AliasActions aliasActions = new AliasActions(type, indices, aliases);
                                 if (routingSet) {
-                                    aliasAction.routing(routing);
+                                    aliasActions.routing(routing);
                                 }
                                 if (indexRoutingSet) {
-                                    aliasAction.indexRouting(indexRouting);
+                                    aliasActions.indexRouting(indexRouting);
                                 }
                                 if (searchRoutingSet) {
-                                    aliasAction.searchRouting(searchRouting);
+                                    aliasActions.searchRouting(searchRouting);
                                 }
-                                indicesAliasesRequest.addAliasAction(aliasAction);
+                                indicesAliasesRequest.addAliasAction(aliasActions);
                             } else if (type == AliasAction.Type.REMOVE) {
-                                indicesAliasesRequest.removeAlias(index, alias);
+                                indicesAliasesRequest.removeAlias(indices, aliases);
                             }
                         }
                     }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
index 57ceb21..3a86911 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
@@ -21,7 +21,8 @@ package org.elasticsearch.rest.action.admin.indices.analyze;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -47,6 +48,17 @@ import static org.elasticsearch.rest.RestRequest.Method.POST;
  */
 public class RestAnalyzeAction extends BaseRestHandler {
 
+    public static class Fields {
+        public static final ParseField ANALYZER = new ParseField("analyzer");
+        public static final ParseField TEXT = new ParseField("text");
+        public static final ParseField FIELD = new ParseField("field");
+        public static final ParseField TOKENIZER = new ParseField("tokenizer");
+        public static final ParseField TOKEN_FILTERS = new ParseField("token_filters", "filters");
+        public static final ParseField CHAR_FILTERS = new ParseField("char_filters");
+        public static final ParseField EXPLAIN = new ParseField("explain");
+        public static final ParseField ATTRIBUTES = new ParseField("attributes");
+    }
+
     @Inject
     public RestAnalyzeAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
@@ -68,6 +80,8 @@ public class RestAnalyzeAction extends BaseRestHandler {
         analyzeRequest.tokenizer(request.param("tokenizer"));
         analyzeRequest.tokenFilters(request.paramAsStringArray("token_filters", request.paramAsStringArray("filters", analyzeRequest.tokenFilters())));
         analyzeRequest.charFilters(request.paramAsStringArray("char_filters", analyzeRequest.charFilters()));
+        analyzeRequest.explain(request.paramAsBoolean("explain", false));
+        analyzeRequest.attributes(request.paramAsStringArray("attributes", analyzeRequest.attributes()));
 
         if (RestActions.hasBodyContent(request)) {
             XContentType type = RestActions.guessBodyContentType(request);
@@ -78,14 +92,14 @@ public class RestAnalyzeAction extends BaseRestHandler {
                 }
             } else {
                 // NOTE: if rest request with xcontent body has request parameters, the parameters does not override xcontent values
-                buildFromContent(RestActions.getRestContent(request), analyzeRequest);
+                buildFromContent(RestActions.getRestContent(request), analyzeRequest, parseFieldMatcher);
             }
         }
 
         client.admin().indices().analyze(analyzeRequest, new RestToXContentListener<AnalyzeResponse>(channel));
     }
 
-    public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest) {
+    public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest, ParseFieldMatcher parseFieldMatcher) {
         try (XContentParser parser = XContentHelper.createParser(content)) {
             if (parser.nextToken() != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Malforrmed content, must start with an object");
@@ -95,9 +109,9 @@ public class RestAnalyzeAction extends BaseRestHandler {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
                         currentFieldName = parser.currentName();
-                    } else if ("text".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TEXT) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.text(parser.text());
-                    } else if ("text".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TEXT) && token == XContentParser.Token.START_ARRAY) {
                         List<String> texts = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -105,14 +119,14 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             texts.add(parser.text());
                         }
-                        analyzeRequest.text(texts.toArray(Strings.EMPTY_ARRAY));
-                    } else if ("analyzer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                        analyzeRequest.text(texts.toArray(new String[texts.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.ANALYZER) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.analyzer(parser.text());
-                    } else if ("field".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.FIELD) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.field(parser.text());
-                    } else if ("tokenizer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TOKENIZER) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.tokenizer(parser.text());
-                    } else if (("token_filters".equals(currentFieldName) || "filters".equals(currentFieldName)) && token == XContentParser.Token.START_ARRAY) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TOKEN_FILTERS) && token == XContentParser.Token.START_ARRAY) {
                         List<String> filters = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -120,8 +134,8 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             filters.add(parser.text());
                         }
-                        analyzeRequest.tokenFilters(filters.toArray(Strings.EMPTY_ARRAY));
-                    } else if ("char_filters".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) {
+                        analyzeRequest.tokenFilters(filters.toArray(new String[filters.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.CHAR_FILTERS) && token == XContentParser.Token.START_ARRAY) {
                         List<String> charFilters = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -129,7 +143,18 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             charFilters.add(parser.text());
                         }
-                        analyzeRequest.tokenFilters(charFilters.toArray(Strings.EMPTY_ARRAY));
+                        analyzeRequest.charFilters(charFilters.toArray(new String[charFilters.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.EXPLAIN) && token == XContentParser.Token.VALUE_BOOLEAN) {
+                        analyzeRequest.explain(parser.booleanValue());
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.ATTRIBUTES) && token == XContentParser.Token.START_ARRAY){
+                        List<String> attributes = new ArrayList<>();
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                            if (token.isValue() == false) {
+                                throw new IllegalArgumentException(currentFieldName + " array element should only contain attribute name");
+                            }
+                            attributes.add(parser.text());
+                        }
+                        analyzeRequest.attributes(attributes.toArray(new String[attributes.size()]));
                     } else {
                         throw new IllegalArgumentException("Unknown parameter [" + currentFieldName + "] in request body or parameter is of the wrong type[" + token + "] ");
                     }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
index a25754d..5ebec71 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
@@ -41,7 +41,6 @@ import org.elasticsearch.rest.action.support.RestBuilderListener;
 import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.util.Map;
 
@@ -89,7 +88,7 @@ public class RestRenderSearchTemplateAction extends BaseRestHandler {
                     throw new ElasticsearchParseException("failed to parse request. unknown field [{}] of type [{}]", currentFieldName, token);
                 }
             }
-            template = new Template(templateId, ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params);
+            template = new Template(templateId, ScriptType.INDEXED, Template.DEFAULT_LANG, null, params);
         }
         renderSearchTemplateRequest = new RenderSearchTemplateRequest();
         renderSearchTemplateRequest.template(template);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
index 9018435..536b73b 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
@@ -19,16 +19,11 @@
 
 package org.elasticsearch.rest.action.bulk;
 
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.bulk.BulkShardRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Strings;
@@ -96,52 +91,7 @@ public class RestBulkAction extends BaseRestHandler {
                 builder.startArray(Fields.ITEMS);
                 for (BulkItemResponse itemResponse : response) {
                     builder.startObject();
-                    builder.startObject(itemResponse.getOpType());
-                    builder.field(Fields._INDEX, itemResponse.getIndex());
-                    builder.field(Fields._TYPE, itemResponse.getType());
-                    builder.field(Fields._ID, itemResponse.getId());
-                    long version = itemResponse.getVersion();
-                    if (version != -1) {
-                        builder.field(Fields._VERSION, itemResponse.getVersion());
-                    }
-                    if (itemResponse.isFailed()) {
-                        builder.field(Fields.STATUS, itemResponse.getFailure().getStatus().getStatus());
-                        builder.startObject(Fields.ERROR);
-                        ElasticsearchException.toXContent(builder, request, itemResponse.getFailure().getCause());
-                        builder.endObject();
-                    } else {
-                        ActionWriteResponse.ShardInfo shardInfo = itemResponse.getResponse().getShardInfo();
-                        shardInfo.toXContent(builder, request);
-                        if (itemResponse.getResponse() instanceof DeleteResponse) {
-                            DeleteResponse deleteResponse = itemResponse.getResponse();
-                            if (deleteResponse.isFound()) {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, RestStatus.NOT_FOUND.getStatus());
-                            }
-                            builder.field(Fields.FOUND, deleteResponse.isFound());
-                        } else if (itemResponse.getResponse() instanceof IndexResponse) {
-                            IndexResponse indexResponse = itemResponse.getResponse();
-                            if (indexResponse.isCreated()) {
-                                builder.field(Fields.STATUS, RestStatus.CREATED.getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            }
-                        } else if (itemResponse.getResponse() instanceof UpdateResponse) {
-                            UpdateResponse updateResponse = itemResponse.getResponse();
-                            if (updateResponse.isCreated()) {
-                                builder.field(Fields.STATUS, RestStatus.CREATED.getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            }
-                            if (updateResponse.getGetResult() != null) {
-                                builder.startObject(Fields.GET);
-                                updateResponse.getGetResult().toXContentEmbedded(builder, request);
-                                builder.endObject();
-                            }
-                        }
-                    }
-                    builder.endObject();
+                    itemResponse.toXContent(builder, request);
                     builder.endObject();
                 }
                 builder.endArray();
@@ -155,15 +105,7 @@ public class RestBulkAction extends BaseRestHandler {
     static final class Fields {
         static final XContentBuilderString ITEMS = new XContentBuilderString("items");
         static final XContentBuilderString ERRORS = new XContentBuilderString("errors");
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString STATUS = new XContentBuilderString("status");
-        static final XContentBuilderString ERROR = new XContentBuilderString("error");
         static final XContentBuilderString TOOK = new XContentBuilderString("took");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
-        static final XContentBuilderString FOUND = new XContentBuilderString("found");
-        static final XContentBuilderString GET = new XContentBuilderString("get");
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java b/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
index 4a5182f..e583ed3 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
@@ -21,15 +21,15 @@ package org.elasticsearch.rest.action.delete;
 
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.delete.DeleteRequest;
+import org.elasticsearch.action.delete.DeleteResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
+import org.elasticsearch.rest.action.support.RestBuilderListener;
 import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
@@ -62,5 +62,4 @@ public class RestDeleteAction extends BaseRestHandler {
 
         client.delete(deleteRequest, new RestStatusToXContentListener<>(channel));
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java b/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
index 1fe0715..c0e45fc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
@@ -26,7 +26,6 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
@@ -77,7 +76,7 @@ public class RestGetSourceAction extends BaseRestHandler {
                 if (!response.isExists()) {
                     return new BytesRestResponse(NOT_FOUND, builder);
                 } else {
-                    XContentHelper.writeDirect(response.getSourceInternal(), builder, request);
+                    builder.rawValue(response.getSourceInternal());
                     return new BytesRestResponse(OK, builder);
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
index 3714f83..310ce0a 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.index;
 
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.index.IndexResponse;
@@ -27,7 +26,6 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
@@ -102,5 +100,4 @@ public class RestIndexAction extends BaseRestHandler {
         }
         client.index(indexRequest, new RestStatusToXContentListener<>(channel));
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
index 9b205a8..3d0daf3 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.script.RestDeleteIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
 
@@ -38,6 +38,6 @@ public class RestDeleteSearchTemplateAction extends RestDeleteIndexedScriptActio
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
index 39be6a5..0e8aa35 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
@@ -25,7 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.script.RestGetIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
@@ -42,7 +42,7 @@ public class RestGetSearchTemplateAction extends RestGetIndexedScriptAction {
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
index a734ce3..0d23645 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
@@ -23,7 +23,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.script.RestPutIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
@@ -59,6 +59,6 @@ public class RestPutSearchTemplateAction extends RestPutIndexedScriptAction {
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java b/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
index 76e96ab..f59c329 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.update;
 
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.update.UpdateRequest;
@@ -29,7 +28,6 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.BytesRestResponse;
@@ -40,6 +38,7 @@ import org.elasticsearch.rest.RestResponse;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
+import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
@@ -48,7 +47,6 @@ import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.rest.RestRequest.Method.POST;
-import static org.elasticsearch.rest.RestStatus.CREATED;
 
 /**
  */
@@ -123,38 +121,6 @@ public class RestUpdateAction extends BaseRestHandler {
             }
         }
 
-        client.update(updateRequest, new RestBuilderListener<UpdateResponse>(channel) {
-            @Override
-            public RestResponse buildResponse(UpdateResponse response, XContentBuilder builder) throws Exception {
-                builder.startObject();
-                ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo();
-                builder.field(Fields._INDEX, response.getIndex())
-                        .field(Fields._TYPE, response.getType())
-                        .field(Fields._ID, response.getId())
-                        .field(Fields._VERSION, response.getVersion());
-
-                shardInfo.toXContent(builder, request);
-                if (response.getGetResult() != null) {
-                    builder.startObject(Fields.GET);
-                    response.getGetResult().toXContentEmbedded(builder, request);
-                    builder.endObject();
-                }
-
-                builder.endObject();
-                RestStatus status = shardInfo.status();
-                if (response.isCreated()) {
-                    status = CREATED;
-                }
-                return new BytesRestResponse(status, builder);
-            }
-        });
-    }
-
-    static final class Fields {
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
-        static final XContentBuilderString GET = new XContentBuilderString("get");
+        client.update(updateRequest, new RestStatusToXContentListener<>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModule.java b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
index 3c19826..f3bdad6 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModule.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
@@ -22,9 +22,7 @@ package org.elasticsearch.script;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -75,13 +73,6 @@ public class ScriptModule extends AbstractModule {
 
         Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
         multibinder.addBinding().to(NativeScriptEngineService.class);
-        
-        try {
-            Class.forName("com.github.mustachejava.Mustache");
-            multibinder.addBinding().to(MustacheScriptEngineService.class).asEagerSingleton();
-        } catch (Throwable t) {
-            Loggers.getLogger(ScriptService.class, settings).debug("failed to load mustache", t);
-        }
 
         for (Class<? extends ScriptEngineService> scriptEngine : scriptEngines) {
             multibinder.addBinding().to(scriptEngine).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/script/Template.java b/core/src/main/java/org/elasticsearch/script/Template.java
index 4419d6f..c9bb908 100644
--- a/core/src/main/java/org/elasticsearch/script/Template.java
+++ b/core/src/main/java/org/elasticsearch/script/Template.java
@@ -29,13 +29,15 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Map;
 
 public class Template extends Script {
+    
+    /** Default templating language */
+    public static final String DEFAULT_LANG = "mustache";
 
     private XContentType contentType;
 
@@ -51,7 +53,7 @@ public class Template extends Script {
      *            The inline template.
      */
     public Template(String template) {
-        super(template, MustacheScriptEngineService.NAME);
+        super(template, DEFAULT_LANG);
     }
 
     /**
@@ -73,7 +75,7 @@ public class Template extends Script {
      */
     public Template(String template, ScriptType type, @Nullable String lang, @Nullable XContentType xContentType,
             @Nullable Map<String, Object> params) {
-        super(template, type, lang == null ? MustacheScriptEngineService.NAME : lang, params);
+        super(template, type, lang == null ? DEFAULT_LANG : lang, params);
         this.contentType = xContentType;
     }
 
@@ -120,16 +122,16 @@ public class Template extends Script {
     }
 
     public static Script parse(Map<String, Object> config, boolean removeMatchedEntries, ParseFieldMatcher parseFieldMatcher) {
-        return new TemplateParser(Collections.emptyMap(), MustacheScriptEngineService.NAME).parse(config, removeMatchedEntries, parseFieldMatcher);
+        return new TemplateParser(Collections.emptyMap(), DEFAULT_LANG).parse(config, removeMatchedEntries, parseFieldMatcher);
     }
 
     public static Template parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return new TemplateParser(Collections.emptyMap(), MustacheScriptEngineService.NAME).parse(parser, parseFieldMatcher);
+        return new TemplateParser(Collections.emptyMap(), DEFAULT_LANG).parse(parser, parseFieldMatcher);
     }
 
     @Deprecated
     public static Template parse(XContentParser parser, Map<String, ScriptType> additionalTemplateFieldNames, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return new TemplateParser(additionalTemplateFieldNames, MustacheScriptEngineService.NAME).parse(parser, parseFieldMatcher);
+        return new TemplateParser(additionalTemplateFieldNames, DEFAULT_LANG).parse(parser, parseFieldMatcher);
     }
 
     @Deprecated
@@ -172,7 +174,7 @@ public class Template extends Script {
 
         @Override
         protected Template createSimpleScript(XContentParser parser) throws IOException {
-            return new Template(String.valueOf(parser.objectText()), ScriptType.INLINE, MustacheScriptEngineService.NAME, contentType, null);
+            return new Template(String.valueOf(parser.objectText()), ScriptType.INLINE, DEFAULT_LANG, contentType, null);
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java b/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
deleted file mode 100644
index 7734d03..0000000
--- a/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.fasterxml.jackson.core.io.JsonStringEncoder;
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.MustacheException;
-
-import java.io.IOException;
-import java.io.Writer;
-
-/**
- * A MustacheFactory that does simple JSON escaping.
- */
-public final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
-    
-    @Override
-    public void encode(String value, Writer writer) {
-        try {
-            JsonStringEncoder utils = new JsonStringEncoder();
-            writer.write(utils.quoteAsString(value));;
-        } catch (IOException e) {
-            throw new MustacheException("Failed to encode value: " + value);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
deleted file mode 100644
index 3affd0c..0000000
--- a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
+++ /dev/null
@@ -1,166 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.github.mustachejava.Mustache;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.FastStringReader;
-import org.elasticsearch.common.io.UTF8StreamWriter;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.lang.ref.SoftReference;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * Main entry point handling template registration, compilation and
- * execution.
- *
- * Template handling is based on Mustache. Template handling is a two step
- * process: First compile the string representing the template, the resulting
- * {@link Mustache} object can then be re-used for subsequent executions.
- */
-public class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
-
-    public static final String NAME = "mustache";
-
-    /** Thread local UTF8StreamWriter to store template execution results in, thread local to save object creation.*/
-    private static ThreadLocal<SoftReference<UTF8StreamWriter>> utf8StreamWriter = new ThreadLocal<>();
-
-    /** If exists, reset and return, otherwise create, reset and return a writer.*/
-    private static UTF8StreamWriter utf8StreamWriter() {
-        SoftReference<UTF8StreamWriter> ref = utf8StreamWriter.get();
-        UTF8StreamWriter writer = (ref == null) ? null : ref.get();
-        if (writer == null) {
-            writer = new UTF8StreamWriter(1024 * 4);
-            utf8StreamWriter.set(new SoftReference<>(writer));
-        }
-        writer.reset();
-        return writer;
-    }
-
-    /**
-     * @param settings automatically wired by Guice.
-     * */
-    @Inject
-    public MustacheScriptEngineService(Settings settings) {
-        super(settings);
-    }
-
-    /**
-     * Compile a template string to (in this case) a Mustache object than can
-     * later be re-used for execution to fill in missing parameter values.
-     *
-     * @param template
-     *            a string representing the template to compile.
-     * @return a compiled template object for later execution.
-     * */
-    @Override
-    public Object compile(String template) {
-        /** Factory to generate Mustache objects from. */
-        return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
-    }
-
-    @Override
-    public String[] types() {
-        return new String[] {NAME};
-    }
-
-    @Override
-    public String[] extensions() {
-        return new String[] {NAME};
-    }
-
-    @Override
-    public boolean sandboxed() {
-        return true;
-    }
-
-    @Override
-    public ExecutableScript executable(CompiledScript compiledScript,
-            @Nullable Map<String, Object> vars) {
-        return new MustacheExecutableScript(compiledScript, vars);
-    }
-
-    @Override
-    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup,
-            @Nullable Map<String, Object> vars) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void close() {
-        // Nothing to do here
-    }
-
-    @Override
-    public void scriptRemoved(CompiledScript script) {
-        // Nothing to do here
-    }
-
-    /**
-     * Used at query execution time by script service in order to execute a query template.
-     * */
-    private class MustacheExecutableScript implements ExecutableScript {
-        /** Compiled template object wrapper. */
-        private CompiledScript template;
-        /** Parameters to fill above object with. */
-        private Map<String, Object> vars;
-
-        /**
-         * @param template the compiled template object wrapper
-         * @param vars the parameters to fill above object with
-         **/
-        public MustacheExecutableScript(CompiledScript template, Map<String, Object> vars) {
-            this.template = template;
-            this.vars = vars == null ? Collections.<String, Object>emptyMap() : vars;
-        }
-
-        @Override
-        public void setNextVar(String name, Object value) {
-            this.vars.put(name, value);
-        }
-
-        @Override
-        public Object run() {
-            BytesStreamOutput result = new BytesStreamOutput();
-            try (UTF8StreamWriter writer = utf8StreamWriter().setOutput(result)) {
-                ((Mustache) template.compiled()).execute(writer, vars);
-            } catch (Exception e) {
-                logger.error("Error running " + template, e);
-                throw new ScriptException("Error running " + template, e);
-            }
-            return result.bytes();
-        }
-
-        @Override
-        public Object unwrap(Object value) {
-            return value;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
index e181805..d30144f 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
@@ -125,7 +125,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * Set the fragment size in characters, defaults to {@link HighlighterParseElement#DEFAULT_FRAGMENT_CHAR_SIZE}
+     * Set the fragment size in characters, defaults to {@link HighlightBuilder#DEFAULT_FRAGMENT_CHAR_SIZE}
      */
     @SuppressWarnings("unchecked")
     public HB fragmentSize(Integer fragmentSize) {
@@ -141,7 +141,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * Set the number of fragments, defaults to {@link HighlighterParseElement#DEFAULT_NUMBER_OF_FRAGMENTS}
+     * Set the number of fragments, defaults to {@link HighlightBuilder#DEFAULT_NUMBER_OF_FRAGMENTS}
      */
     @SuppressWarnings("unchecked")
     public HB numOfFragments(Integer numOfFragments) {
@@ -428,7 +428,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * internal hashCode calculation to overwrite for the implementing classes.
+     * fields only present in subclass should contribute to hashCode in the implementation
      */
     protected abstract int doHashCode();
 
@@ -462,7 +462,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * internal equals to overwrite for the implementing classes.
+     * fields only present in subclass should be checked for equality in the implementation
      */
     protected abstract boolean doEquals(HB other);
 
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index 3233673..e45303c 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.search.highlight;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.vectorhighlight.SimpleBoundaryScanner;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -28,13 +30,20 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
-
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions.Builder;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Objects;
+import java.util.Set;
 
 /**
  * A builder for search highlighting. Settings can control how large fields
@@ -48,6 +57,51 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
 
     public static final String HIGHLIGHT_ELEMENT_NAME = "highlight";
 
+    /** default for whether to highlight fields based on the source even if stored separately */
+    public static final boolean DEFAULT_FORCE_SOURCE = false;
+    /** default for whether a field should be highlighted only if a query matches that field */
+    public static final boolean DEFAULT_REQUIRE_FIELD_MATCH = true;
+    /** default for whether <tt>fvh</tt> should provide highlighting on filter clauses */
+    public static final boolean DEFAULT_HIGHLIGHT_FILTER = false;
+    /** default for highlight fragments being ordered by score */
+    public static final boolean DEFAULT_SCORE_ORDERED = false;
+    /** the default encoder setting */
+    public static final String DEFAULT_ENCODER = "default";
+    /** default for the maximum number of phrases the fvh will consider */
+    public static final int DEFAULT_PHRASE_LIMIT = 256;
+    /** default for fragment size when there are no matches */
+    public static final int DEFAULT_NO_MATCH_SIZE = 0;
+    /** the default number of fragments for highlighting */
+    public static final int DEFAULT_NUMBER_OF_FRAGMENTS = 5;
+    /** the default number of fragments size in characters */
+    public static final int DEFAULT_FRAGMENT_CHAR_SIZE = 100;
+    /** the default opening tag  */
+    public static final String[] DEFAULT_PRE_TAGS = new String[]{"<em>"};
+    /** the default closing tag  */
+    public static final String[] DEFAULT_POST_TAGS = new String[]{"</em>"};
+
+    /** the default opening tags when <tt>tag_schema = "styled"</tt>  */
+    public static final String[] DEFAULT_STYLED_PRE_TAG = {
+            "<em class=\"hlt1\">", "<em class=\"hlt2\">", "<em class=\"hlt3\">",
+            "<em class=\"hlt4\">", "<em class=\"hlt5\">", "<em class=\"hlt6\">",
+            "<em class=\"hlt7\">", "<em class=\"hlt8\">", "<em class=\"hlt9\">",
+            "<em class=\"hlt10\">"
+    };
+    /** the default closing tags when <tt>tag_schema = "styled"</tt>  */
+    public static final String[] DEFAULT_STYLED_POST_TAGS = {"</em>"};
+
+    /**
+     * a {@link FieldOptions.Builder} with default settings
+     */
+    public final static Builder defaultFieldOptions() {
+        return new SearchContextHighlight.FieldOptions.Builder()
+                .preTags(DEFAULT_PRE_TAGS).postTags(DEFAULT_POST_TAGS).scoreOrdered(DEFAULT_SCORE_ORDERED).highlightFilter(DEFAULT_HIGHLIGHT_FILTER)
+                .requireFieldMatch(DEFAULT_REQUIRE_FIELD_MATCH).forceSource(DEFAULT_FORCE_SOURCE).fragmentCharSize(DEFAULT_FRAGMENT_CHAR_SIZE).numberOfFragments(DEFAULT_NUMBER_OF_FRAGMENTS)
+                .encoder(DEFAULT_ENCODER).boundaryMaxScan(SimpleBoundaryScanner.DEFAULT_MAX_SCAN)
+                .boundaryChars(SimpleBoundaryScanner.DEFAULT_BOUNDARY_CHARS)
+                .noMatchSize(DEFAULT_NO_MATCH_SIZE).phraseLimit(DEFAULT_PHRASE_LIMIT);
+    }
+
     private final List<Field> fields = new ArrayList<>();
 
     private String encoder;
@@ -120,12 +174,12 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
     public HighlightBuilder tagsSchema(String schemaName) {
         switch (schemaName) {
         case "default":
-            preTags(HighlighterParseElement.DEFAULT_PRE_TAGS);
-            postTags(HighlighterParseElement.DEFAULT_POST_TAGS);
+            preTags(DEFAULT_PRE_TAGS);
+            postTags(DEFAULT_POST_TAGS);
             break;
         case "styled":
-            preTags(HighlighterParseElement.STYLED_PRE_TAG);
-            postTags(HighlighterParseElement.STYLED_POST_TAGS);
+            preTags(DEFAULT_STYLED_PRE_TAG);
+            postTags(DEFAULT_STYLED_POST_TAGS);
             break;
         default:
             throw new IllegalArgumentException("Unknown tag schema ["+ schemaName +"]");
@@ -289,7 +343,87 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
         return highlightBuilder;
     }
 
+    public SearchContextHighlight build(QueryShardContext context) throws IOException {
+        // create template global options that are later merged with any partial field options
+        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder();
+        globalOptionsBuilder.encoder(this.encoder);
+        transferOptions(this, globalOptionsBuilder, context);
+
+        // overwrite unset global options by default values
+        globalOptionsBuilder.merge(defaultFieldOptions().build());
+
+        // create field options
+        Collection<org.elasticsearch.search.highlight.SearchContextHighlight.Field> fieldOptions = new ArrayList<>();
+        for (Field field : this.fields) {
+            final SearchContextHighlight.FieldOptions.Builder fieldOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder();
+            fieldOptionsBuilder.fragmentOffset(field.fragmentOffset);
+            if (field.matchedFields != null) {
+                Set<String> matchedFields = new HashSet<String>(field.matchedFields.length);
+                Collections.addAll(matchedFields, field.matchedFields);
+                fieldOptionsBuilder.matchedFields(matchedFields);
+            }
+            transferOptions(field, fieldOptionsBuilder, context);
+            fieldOptions.add(new SearchContextHighlight.Field(field.name(), fieldOptionsBuilder.merge(globalOptionsBuilder.build()).build()));
+        }
+        return new SearchContextHighlight(fieldOptions);
+    }
 
+    /**
+     * Transfers field options present in the input {@link AbstractHighlighterBuilder} to the receiving
+     * {@link FieldOptions.Builder}, effectively overwriting existing settings
+     * @param targetOptionsBuilder the receiving options builder
+     * @param highlighterBuilder highlight builder with the input options
+     * @param context needed to convert {@link QueryBuilder} to {@link Query}
+     * @throws IOException on errors parsing any optional nested highlight query
+     */
+    @SuppressWarnings({ "rawtypes", "unchecked" })
+    private static void transferOptions(AbstractHighlighterBuilder highlighterBuilder, SearchContextHighlight.FieldOptions.Builder targetOptionsBuilder, QueryShardContext context) throws IOException {
+        targetOptionsBuilder.preTags(highlighterBuilder.preTags);
+        targetOptionsBuilder.postTags(highlighterBuilder.postTags);
+        targetOptionsBuilder.scoreOrdered("score".equals(highlighterBuilder.order));
+        if (highlighterBuilder.highlightFilter != null) {
+            targetOptionsBuilder.highlightFilter(highlighterBuilder.highlightFilter);
+        }
+        if (highlighterBuilder.fragmentSize != null) {
+            targetOptionsBuilder.fragmentCharSize(highlighterBuilder.fragmentSize);
+        }
+        if (highlighterBuilder.numOfFragments != null) {
+            targetOptionsBuilder.numberOfFragments(highlighterBuilder.numOfFragments);
+        }
+        if (highlighterBuilder.requireFieldMatch != null) {
+            targetOptionsBuilder.requireFieldMatch(highlighterBuilder.requireFieldMatch);
+        }
+        if (highlighterBuilder.boundaryMaxScan != null) {
+            targetOptionsBuilder.boundaryMaxScan(highlighterBuilder.boundaryMaxScan);
+        }
+        targetOptionsBuilder.boundaryChars(convertCharArray(highlighterBuilder.boundaryChars));
+        targetOptionsBuilder.highlighterType(highlighterBuilder.highlighterType);
+        targetOptionsBuilder.fragmenter(highlighterBuilder.fragmenter);
+        if (highlighterBuilder.noMatchSize != null) {
+            targetOptionsBuilder.noMatchSize(highlighterBuilder.noMatchSize);
+        }
+        if (highlighterBuilder.forceSource != null) {
+            targetOptionsBuilder.forceSource(highlighterBuilder.forceSource);
+        }
+        if (highlighterBuilder.phraseLimit != null) {
+            targetOptionsBuilder.phraseLimit(highlighterBuilder.phraseLimit);
+        }
+        targetOptionsBuilder.options(highlighterBuilder.options);
+        if (highlighterBuilder.highlightQuery != null) {
+            targetOptionsBuilder.highlightQuery(highlighterBuilder.highlightQuery.toQuery(context));
+        }
+    }
+
+    private static Character[] convertCharArray(char[] array) {
+        if (array == null) {
+            return null;
+        }
+        Character[] charArray = new Character[array.length];
+        for (int i = 0; i < array.length; i++) {
+            charArray[i] = array[i];
+        }
+        return charArray;
+    }
 
     public void innerXContent(XContentBuilder builder) throws IOException {
         // first write common options
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
index 298a667..38534ba 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.highlight;
 
-import org.apache.lucene.search.vectorhighlight.SimpleBoundaryScanner;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardContext;
@@ -52,39 +51,6 @@ import java.util.Set;
  */
 public class HighlighterParseElement implements SearchParseElement {
 
-    /** default for whether to highlight fields based on the source even if stored separately */
-    public static final boolean DEFAULT_FORCE_SOURCE = false;
-    /** default for whether a field should be highlighted only if a query matches that field */
-    public static final boolean DEFAULT_REQUIRE_FIELD_MATCH = true;
-    /** default for whether <tt>fvh</tt> should provide highlighting on filter clauses */
-    public static final boolean DEFAULT_HIGHLIGHT_FILTER = false;
-    /** default for highlight fragments being ordered by score */
-    public static final boolean DEFAULT_SCORE_ORDERED = false;
-    /** the default encoder setting */
-    public static final String DEFAULT_ENCODER = "default";
-    /** default for the maximum number of phrases the fvh will consider */
-    public static final int DEFAULT_PHRASE_LIMIT = 256;
-    /** default for fragment size when there are no matches */
-    public static final int DEFAULT_NO_MATCH_SIZE = 0;
-    /** the default number of fragments for highlighting */
-    public static final int DEFAULT_NUMBER_OF_FRAGMENTS = 5;
-    /** the default number of fragments size in characters */
-    public static final int DEFAULT_FRAGMENT_CHAR_SIZE = 100;
-    /** the default opening tag  */
-    public static final String[] DEFAULT_PRE_TAGS = new String[]{"<em>"};
-    /** the default closing tag  */
-    public static final String[] DEFAULT_POST_TAGS = new String[]{"</em>"};
-
-    /** the default opening tags when <tt>tag_schema = "styled"</tt>  */
-    public static final String[] STYLED_PRE_TAG = {
-            "<em class=\"hlt1\">", "<em class=\"hlt2\">", "<em class=\"hlt3\">",
-            "<em class=\"hlt4\">", "<em class=\"hlt5\">", "<em class=\"hlt6\">",
-            "<em class=\"hlt7\">", "<em class=\"hlt8\">", "<em class=\"hlt9\">",
-            "<em class=\"hlt10\">"
-    };
-    /** the default closing tags when <tt>tag_schema = "styled"</tt>  */
-    public static final String[] STYLED_POST_TAGS = {"</em>"};
-
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         try {
@@ -99,12 +65,7 @@ public class HighlighterParseElement implements SearchParseElement {
         String topLevelFieldName = null;
         final List<Tuple<String, SearchContextHighlight.FieldOptions.Builder>> fieldsOptions = new ArrayList<>();
 
-        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder()
-                .preTags(DEFAULT_PRE_TAGS).postTags(DEFAULT_POST_TAGS).scoreOrdered(DEFAULT_SCORE_ORDERED).highlightFilter(DEFAULT_HIGHLIGHT_FILTER)
-                .requireFieldMatch(DEFAULT_REQUIRE_FIELD_MATCH).forceSource(DEFAULT_FORCE_SOURCE).fragmentCharSize(DEFAULT_FRAGMENT_CHAR_SIZE).numberOfFragments(DEFAULT_NUMBER_OF_FRAGMENTS)
-                .encoder(DEFAULT_ENCODER).boundaryMaxScan(SimpleBoundaryScanner.DEFAULT_MAX_SCAN)
-                .boundaryChars(SimpleBoundaryScanner.DEFAULT_BOUNDARY_CHARS)
-                .noMatchSize(DEFAULT_NO_MATCH_SIZE).phraseLimit(DEFAULT_PHRASE_LIMIT);
+        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = HighlightBuilder.defaultFieldOptions();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -147,8 +108,8 @@ public class HighlighterParseElement implements SearchParseElement {
                 } else if ("tags_schema".equals(topLevelFieldName) || "tagsSchema".equals(topLevelFieldName)) {
                     String schema = parser.text();
                     if ("styled".equals(schema)) {
-                        globalOptionsBuilder.preTags(STYLED_PRE_TAG);
-                        globalOptionsBuilder.postTags(STYLED_POST_TAGS);
+                        globalOptionsBuilder.preTags(HighlightBuilder.DEFAULT_STYLED_PRE_TAG);
+                        globalOptionsBuilder.postTags(HighlightBuilder.DEFAULT_STYLED_POST_TAGS);
                     }
                 } else if ("highlight_filter".equals(topLevelFieldName) || "highlightFilter".equals(topLevelFieldName)) {
                     globalOptionsBuilder.highlightFilter(parser.booleanValue());
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java b/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
index 38a8147..293143f 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
@@ -53,6 +53,10 @@ public class SearchContextHighlight {
         this.globalForceSource = globalForceSource;
     }
 
+    boolean globalForceSource() {
+        return this.globalForceSource;
+    }
+
     public boolean forceSource(Field field) {
         if (globalForceSource) {
             return true;
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index db17d4d..f577415 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.tribe;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
 import org.elasticsearch.cluster.ClusterChangedEvent;
@@ -46,8 +45,6 @@ import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.rest.RestStatus;
 
 import java.util.EnumSet;
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index 7e060cd..2678501 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -31,10 +31,12 @@ grant codeBase "${codebase.securesm-1.0.jar}" {
 //// Very special jar permissions:
 //// These are dangerous permissions that we don't want to grant to everything.
 
-grant codeBase "${codebase.lucene-core-5.4.0-snapshot-1715952.jar}" {
+grant codeBase "${codebase.lucene-core-5.5.0-snapshot-1719088.jar}" {
   // needed to allow MMapDirectory's "unmap hack"
   permission java.lang.RuntimePermission "accessClassInPackage.sun.misc";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
+  // NOTE: also needed for RAMUsageEstimator size calculations
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
 };
 
 //// Everything else:
@@ -97,9 +99,6 @@ grant {
   // (TODO: clean this up?)
   permission java.lang.RuntimePermission "getProtectionDomain";
   
-  // likely not low hanging fruit...
-  permission java.lang.RuntimePermission "accessDeclaredMembers";
-
   // needed by HotThreads and potentially more
   // otherwise can be provided only to test libraries
   permission java.lang.RuntimePermission "getStackTrace";
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy b/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
index cde6795..b5f9c24 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
@@ -21,30 +21,38 @@
 //// These are mock objects and test management that we allow test framework libs
 //// to provide on our behalf. But tests themselves cannot do this stuff!
 
-grant codeBase "${codebase.securemock-1.1.jar}" {
+grant codeBase "${codebase.securemock-1.2.jar}" {
   // needed to access ReflectionFactory (see below)
   permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
   // needed to support creation of mocks
   permission java.lang.RuntimePermission "reflectionFactoryAccess";
   // needed for spy interception, etc
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${codebase.lucene-test-framework-5.4.0-snapshot-1715952.jar}" {
+grant codeBase "${codebase.lucene-test-framework-5.5.0-snapshot-1719088.jar}" {
   // needed by RamUsageTester
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${codebase.randomizedtesting-runner-2.2.0.jar}" {
+grant codeBase "${codebase.randomizedtesting-runner-2.3.2.jar}" {
   // optionally needed for access to private test methods (e.g. beforeClass)
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
   // needed to fail tests on uncaught exceptions from other threads
   permission java.lang.RuntimePermission "setDefaultUncaughtExceptionHandler";
   // needed for top threads handling
   permission org.elasticsearch.ThreadPermission "modifyArbitraryThreadGroup";
+  // needed for TestClass creation
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
 };
 
-grant codeBase "${codebase.junit4-ant-2.2.0.jar}" {
+grant codeBase "${codebase.junit4-ant-2.3.2.jar}" {
   // needed for stream redirection
   permission java.lang.RuntimePermission "setIO";
 };
+
+grant codeBase "${codebase.junit-4.11.jar}" {
+  // needed for TestClass creation
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+};
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index e6622e2..2a4e6a6 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -44,6 +44,7 @@ OFFICIAL PLUGINS
     - discovery-gce
     - discovery-multicast
     - lang-javascript
+    - lang-plan-a
     - lang-python
     - mapper-attachments
     - mapper-murmur3
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 21ee6de..46cdea3 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -136,7 +136,7 @@ public class ExceptionSerializationTests extends ESTestCase {
                                 } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz)) {
                                     registered.add(clazz);
                                     try {
-                                        if (clazz.getDeclaredMethod("writeTo", StreamOutput.class) != null) {
+                                        if (clazz.getMethod("writeTo", StreamOutput.class) != null) {
                                             hasDedicatedWrite.add(clazz);
                                         }
                                     } catch (Exception e) {
diff --git a/core/src/test/java/org/elasticsearch/VersionTests.java b/core/src/test/java/org/elasticsearch/VersionTests.java
index d79d1ee..52508f8 100644
--- a/core/src/test/java/org/elasticsearch/VersionTests.java
+++ b/core/src/test/java/org/elasticsearch/VersionTests.java
@@ -191,7 +191,7 @@ public class VersionTests extends ESTestCase {
 
     public void testAllVersionsMatchId() throws Exception {
         Map<String, Version> maxBranchVersions = new HashMap<>();
-        for (java.lang.reflect.Field field : Version.class.getDeclaredFields()) {
+        for (java.lang.reflect.Field field : Version.class.getFields()) {
             if (field.getName().endsWith("_ID")) {
                 assertTrue(field.getName() + " should be static", Modifier.isStatic(field.getModifiers()));
                 assertTrue(field.getName() + " should be final", Modifier.isFinal(field.getModifiers()));
diff --git a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
index ded2abb..237f3a2 100644
--- a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
+++ b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
@@ -105,7 +105,7 @@ public class BulkProcessorIT extends ESIntegTestCase {
     public void testBulkProcessorConcurrentRequests() throws Exception {
         int bulkActions = randomIntBetween(10, 100);
         int numDocs = randomIntBetween(bulkActions, bulkActions + 100);
-        int concurrentRequests = randomIntBetween(0, 10);
+        int concurrentRequests = randomIntBetween(0, 7);
 
         int expectedBulkActions = numDocs / bulkActions;
 
@@ -141,7 +141,7 @@ public class BulkProcessorIT extends ESIntegTestCase {
 
         Set<String> ids = new HashSet<>();
         for (BulkItemResponse bulkItemResponse : listener.bulkItems) {
-            assertThat(bulkItemResponse.isFailed(), equalTo(false));
+            assertThat(bulkItemResponse.getFailureMessage(), bulkItemResponse.isFailed(), equalTo(false));
             assertThat(bulkItemResponse.getIndex(), equalTo("test"));
             assertThat(bulkItemResponse.getType(), equalTo("test"));
             //with concurrent requests > 1 we can't rely on the order of the bulk requests
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
index d31a024..4d17155 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
@@ -20,7 +20,7 @@ package org.elasticsearch.action.support.replication;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.NoShardAvailableActionException;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.admin.indices.flush.FlushRequest;
@@ -101,7 +101,7 @@ public class BroadcastReplicationTests extends ESTestCase {
                 randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.UNASSIGNED, ShardRoutingState.UNASSIGNED));
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
             if (randomBoolean()) {
                 shardRequests.v2().onFailure(new NoShardAvailableActionException(shardRequests.v1()));
             } else {
@@ -120,10 +120,10 @@ public class BroadcastReplicationTests extends ESTestCase {
                 ShardRoutingState.STARTED));
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
-            ActionWriteResponse actionWriteResponse = new ActionWriteResponse();
-            actionWriteResponse.setShardInfo(new ActionWriteResponse.ShardInfo(1, 1, new ActionWriteResponse.ShardInfo.Failure[0]));
-            shardRequests.v2().onResponse(actionWriteResponse);
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+            ReplicationResponse replicationResponse = new ReplicationResponse();
+            replicationResponse.setShardInfo(new ReplicationResponse.ShardInfo(1, 1, new ReplicationResponse.ShardInfo.Failure[0]));
+            shardRequests.v2().onResponse(replicationResponse);
         }
         logger.info("total shards: {}, ", response.get().getTotalShards());
         assertBroadcastResponse(1, 1, 0, response.get(), null);
@@ -137,20 +137,20 @@ public class BroadcastReplicationTests extends ESTestCase {
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
         int succeeded = 0;
         int failed = 0;
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
             if (randomBoolean()) {
-                ActionWriteResponse.ShardInfo.Failure[] failures = new ActionWriteResponse.ShardInfo.Failure[0];
+                ReplicationResponse.ShardInfo.Failure[] failures = new ReplicationResponse.ShardInfo.Failure[0];
                 int shardsSucceeded = randomInt(1) + 1;
                 succeeded += shardsSucceeded;
-                ActionWriteResponse actionWriteResponse = new ActionWriteResponse();
+                ReplicationResponse replicationResponse = new ReplicationResponse();
                 if (shardsSucceeded == 1 && randomBoolean()) {
                     //sometimes add failure (no failure means shard unavailable)
-                    failures = new ActionWriteResponse.ShardInfo.Failure[1];
-                    failures[0] = new ActionWriteResponse.ShardInfo.Failure(index, shardRequests.v1().id(), null, new Exception("pretend shard failed"), RestStatus.GATEWAY_TIMEOUT, false);
+                    failures = new ReplicationResponse.ShardInfo.Failure[1];
+                    failures[0] = new ReplicationResponse.ShardInfo.Failure(index, shardRequests.v1().id(), null, new Exception("pretend shard failed"), RestStatus.GATEWAY_TIMEOUT, false);
                     failed++;
                 }
-                actionWriteResponse.setShardInfo(new ActionWriteResponse.ShardInfo(2, shardsSucceeded, failures));
-                shardRequests.v2().onResponse(actionWriteResponse);
+                replicationResponse.setShardInfo(new ReplicationResponse.ShardInfo(2, shardsSucceeded, failures));
+                shardRequests.v2().onResponse(replicationResponse);
             } else {
                 // sometimes fail
                 failed += 2;
@@ -179,16 +179,16 @@ public class BroadcastReplicationTests extends ESTestCase {
         assertThat(shards.get(0), equalTo(shardId));
     }
 
-    private class TestBroadcastReplicationAction extends TransportBroadcastReplicationAction<BroadcastRequest, BroadcastResponse, ReplicationRequest, ActionWriteResponse> {
-        protected final Set<Tuple<ShardId, ActionListener<ActionWriteResponse>>> capturedShardRequests = ConcurrentCollections.newConcurrentSet();
+    private class TestBroadcastReplicationAction extends TransportBroadcastReplicationAction<BroadcastRequest, BroadcastResponse, ReplicationRequest, ReplicationResponse> {
+        protected final Set<Tuple<ShardId, ActionListener<ReplicationResponse>>> capturedShardRequests = ConcurrentCollections.newConcurrentSet();
 
         public TestBroadcastReplicationAction(Settings settings, ThreadPool threadPool, ClusterService clusterService, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, TransportReplicationAction replicatedBroadcastShardAction) {
             super("test-broadcast-replication-action", BroadcastRequest::new, settings, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver, replicatedBroadcastShardAction);
         }
 
         @Override
-        protected ActionWriteResponse newShardResponse() {
-            return new ActionWriteResponse();
+        protected ReplicationResponse newShardResponse() {
+            return new ReplicationResponse();
         }
 
         @Override
@@ -202,7 +202,7 @@ public class BroadcastReplicationTests extends ESTestCase {
         }
 
         @Override
-        protected void shardExecute(BroadcastRequest request, ShardId shardId, ActionListener<ActionWriteResponse> shardActionListener) {
+        protected void shardExecute(BroadcastRequest request, ShardId shardId, ActionListener<ReplicationResponse> shardActionListener) {
             capturedShardRequests.add(new Tuple<>(shardId, shardActionListener));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
index 79f3853..5834b26 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
@@ -20,7 +20,7 @@ package org.elasticsearch.action.support.replication;
 
 import org.apache.lucene.index.CorruptIndexException;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.support.ActionFilter;
@@ -28,7 +28,6 @@ import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.PlainActionFuture;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateObserver;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockException;
@@ -46,18 +45,17 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.shard.IndexShardNotStartedException;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.cluster.TestClusterService;
 import org.elasticsearch.test.transport.CapturingTransport;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseOptions;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -132,22 +130,22 @@ public class TransportReplicationActionTests extends ESTestCase {
         ClusterBlocks.Builder block = ClusterBlocks.builder()
                 .addGlobalBlock(new ClusterBlock(1, "non retryable", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
         clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        assertFalse("primary phase should stop execution", primaryPhase.checkBlocks());
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertListenerThrows("primary phase should fail operation", listener, ClusterBlockException.class);
 
         block = ClusterBlocks.builder()
                 .addGlobalBlock(new ClusterBlock(1, "retryable", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
         clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(new Request().timeout("5ms"), listener);
-        assertFalse("primary phase should stop execution on retryable block", primaryPhase.checkBlocks());
+        reroutePhase = action.new ReroutePhase(new Request().timeout("5ms"), listener);
+        reroutePhase.run();
         assertListenerThrows("failed to timeout on retryable block", listener, ClusterBlockException.class);
 
 
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(new Request(), listener);
-        assertFalse("primary phase should stop execution on retryable block", primaryPhase.checkBlocks());
+        reroutePhase = action.new ReroutePhase(new Request(), listener);
+        reroutePhase.run();
         assertFalse("primary phase should wait on retryable block", listener.isDone());
 
         block = ClusterBlocks.builder()
@@ -172,25 +170,47 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         Request request = new Request(shardId).timeout("1ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        primaryPhase.run();
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertListenerThrows("unassigned primary didn't cause a timeout", listener, UnavailableShardsException.class);
 
         request = new Request(shardId);
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(request, listener);
-        primaryPhase.run();
+        reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertFalse("unassigned primary didn't cause a retry", listener.isDone());
 
         clusterService.setState(state(index, true, ShardRoutingState.STARTED));
         logger.debug("--> primary assigned state:\n{}", clusterService.state().prettyPrint());
 
-        listener.get();
-        assertTrue("request wasn't processed on primary, despite of it being assigned", request.processedOnPrimary.get());
+        final IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());
+        final String primaryNodeId = shardRoutingTable.primaryShard().currentNodeId();
+        final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
+        assertThat(capturedRequests, notNullValue());
+        assertThat(capturedRequests.size(), equalTo(1));
+        assertThat(capturedRequests.get(0).action, equalTo("testAction[p]"));
         assertIndexShardCounter(1);
     }
 
-    public void testRoutingToPrimary() {
+    public void testUnknownIndexOrShardOnReroute() throws InterruptedException {
+        final String index = "test";
+        // no replicas in oder to skip the replication part
+        clusterService.setState(state(index, true,
+                randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.UNASSIGNED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        Request request = new Request(new ShardId("unknown_index", 0)).timeout("1ms");
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertListenerThrows("must throw index not found exception", listener, IndexNotFoundException.class);
+        request = new Request(new ShardId(index, 10)).timeout("1ms");
+        listener = new PlainActionFuture<>();
+        reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertListenerThrows("must throw shard not found exception", listener, ShardNotFoundException.class);
+    }
+
+    public void testRoutePhaseExecutesRequest() {
         final String index = "test";
         final ShardId shardId = new ShardId(index, 0);
 
@@ -203,25 +223,126 @@ public class TransportReplicationActionTests extends ESTestCase {
         Request request = new Request(shardId);
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
 
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        assertTrue(primaryPhase.checkBlocks());
-        primaryPhase.routeRequestOrPerformLocally(shardRoutingTable.primaryShard(), shardRoutingTable.shardsIt());
-        if (primaryNodeId.equals(clusterService.localNode().id())) {
-            logger.info("--> primary is assigned locally, testing for execution");
-            assertTrue("request failed to be processed on a local primary", request.processedOnPrimary.get());
-            if (transport.capturedRequests().length > 0) {
-                assertIndexShardCounter(2);
-            } else {
-                assertIndexShardCounter(1);
-            }
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertThat(request.shardId(), equalTo(shardId));
+        logger.info("--> primary is assigned to [{}], checking request forwarded", primaryNodeId);
+        final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
+        assertThat(capturedRequests, notNullValue());
+        assertThat(capturedRequests.size(), equalTo(1));
+        if (clusterService.state().nodes().localNodeId().equals(primaryNodeId)) {
+            assertThat(capturedRequests.get(0).action, equalTo("testAction[p]"));
         } else {
-            logger.info("--> primary is assigned to [{}], checking request forwarded", primaryNodeId);
-            final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
-            assertThat(capturedRequests, notNullValue());
-            assertThat(capturedRequests.size(), equalTo(1));
             assertThat(capturedRequests.get(0).action, equalTo("testAction"));
-            assertIndexShardUninitialized();
         }
+        assertIndexShardUninitialized();
+    }
+
+    public void testPrimaryPhaseExecutesRequest() throws InterruptedException, ExecutionException {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        Request request = new Request(shardId).timeout("1ms");
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        final String replicaNodeId = clusterService.state().getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards().get(0).currentNodeId();
+        final List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(replicaNodeId);
+        assertThat(requests, notNullValue());
+        assertThat(requests.size(), equalTo(1));
+        assertThat("replica request was not sent", requests.get(0).action, equalTo("testAction[r]"));
+    }
+
+    public void testAddedReplicaAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        // start with no replicas
+        clusterService.setState(stateWithStartedPrimary(index, true, 0));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithAddedReplicas = state(index, true, ShardRoutingState.STARTED, randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.STARTED);
+
+        final Action actionWithAddedReplicaAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // add replicas after primary operation
+                ((TestClusterService) clusterService).setState(stateWithAddedReplicas);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithAddedReplicaAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        for (ShardRouting replica : stateWithAddedReplicas.getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards()) {
+            List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(replica.currentNodeId());
+            assertThat(requests, notNullValue());
+            assertThat(requests.size(), equalTo(1));
+            assertThat("replica request was not sent", requests.get(0).action, equalTo("testAction[r]"));
+        }
+    }
+
+    public void testRelocatingReplicaAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        // start with a replica
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED,  randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.STARTED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithRelocatingReplica = state(index, true, ShardRoutingState.STARTED, ShardRoutingState.RELOCATING);
+
+        final Action actionWithRelocatingReplicasAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // set replica to relocating
+                ((TestClusterService) clusterService).setState(stateWithRelocatingReplica);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithRelocatingReplicasAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        ShardRouting relocatingReplicaShard = stateWithRelocatingReplica.getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards().get(0);
+        for (String node : new String[] {relocatingReplicaShard.currentNodeId(), relocatingReplicaShard.relocatingNodeId()}) {
+            List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(node);
+            assertThat(requests, notNullValue());
+            assertThat(requests.size(), equalTo(1));
+            assertThat("replica request was not sent to replica", requests.get(0).action, equalTo("testAction[r]"));
+        }
+    }
+
+    public void testIndexDeletedAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithDeletedIndex = state(index + "_new", true, ShardRoutingState.STARTED, ShardRoutingState.RELOCATING);
+
+        final Action actionWithDeletedIndexAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // delete index after primary op
+                ((TestClusterService) clusterService).setState(stateWithDeletedIndex);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithDeletedIndexAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        assertThat("replication phase should be skipped if index gets deleted after primary operation", transport.capturedRequestsByTargetNode().size(), equalTo(0));
     }
 
     public void testWriteConsistency() throws ExecutionException, InterruptedException {
@@ -266,10 +387,9 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         final IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         if (passesWriteConsistency) {
-            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard()), nullValue());
+            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard().shardId()), nullValue());
             primaryPhase.run();
             assertTrue("operations should have been perform, consistency level is met", request.processedOnPrimary.get());
             if (assignedReplicas > 0) {
@@ -278,14 +398,18 @@ public class TransportReplicationActionTests extends ESTestCase {
                 assertIndexShardCounter(1);
             }
         } else {
-            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard()), notNullValue());
+            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard().shardId()), notNullValue());
             primaryPhase.run();
             assertFalse("operations should not have been perform, consistency level is *NOT* met", request.processedOnPrimary.get());
+            assertListenerThrows("should throw exception to trigger retry", listener, UnavailableShardsException.class);
             assertIndexShardUninitialized();
             for (int i = 0; i < replicaStates.length; i++) {
                 replicaStates[i] = ShardRoutingState.STARTED;
             }
             clusterService.setState(state(index, true, ShardRoutingState.STARTED, replicaStates));
+            listener = new PlainActionFuture<>();
+            primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
+            primaryPhase.run();
             assertTrue("once the consistency level met, operation should continue", request.processedOnPrimary.get());
             assertIndexShardCounter(2);
         }
@@ -340,23 +464,19 @@ public class TransportReplicationActionTests extends ESTestCase {
 
 
     protected void runReplicateTest(IndexShardRoutingTable shardRoutingTable, int assignedReplicas, int totalShards) throws InterruptedException, ExecutionException {
-        final ShardRouting primaryShard = shardRoutingTable.primaryShard();
         final ShardIterator shardIt = shardRoutingTable.shardsIt();
         final ShardId shardId = shardIt.shardId();
-        final Request request = new Request();
-        PlainActionFuture<Response> listener = new PlainActionFuture<>();
-
+        final Request request = new Request(shardId);
+        final PlainActionFuture<Response> listener = new PlainActionFuture<>();
         logger.debug("expecting [{}] assigned replicas, [{}] total shards. using state: \n{}", assignedReplicas, totalShards, clusterService.state().prettyPrint());
 
-        final TransportReplicationAction<Request, Request, Response>.InternalRequest internalRequest = action.new InternalRequest(request);
-        internalRequest.concreteIndex(shardId.index().name());
         Releasable reference = getOrCreateIndexShardOperationsCounter();
         assertIndexShardCounter(2);
         // TODO: set a default timeout
         TransportReplicationAction<Request, Request, Response>.ReplicationPhase replicationPhase =
-                action.new ReplicationPhase(shardIt, request,
-                        new Response(), new ClusterStateObserver(clusterService, logger),
-                        primaryShard, internalRequest, listener, reference, null);
+                action.new ReplicationPhase(request,
+                        new Response(),
+                        request.shardId(), createTransportChannel(listener), reference, null);
 
         assertThat(replicationPhase.totalShards(), equalTo(totalShards));
         assertThat(replicationPhase.pending(), equalTo(assignedReplicas));
@@ -401,7 +521,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
         assertThat(listener.isDone(), equalTo(true));
         Response response = listener.get();
-        final ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo();
+        final ReplicationResponse.ShardInfo shardInfo = response.getShardInfo();
         assertThat(shardInfo.getFailed(), equalTo(criticalFailures));
         assertThat(shardInfo.getFailures(), arrayWithSize(criticalFailures));
         assertThat(shardInfo.getSuccessful(), equalTo(successful));
@@ -433,7 +553,7 @@ public class TransportReplicationActionTests extends ESTestCase {
          * However, this failure would only become apparent once listener.get is called. Seems a little implicit.
          * */
         action = new ActionWithDelay(Settings.EMPTY, "testActionWithExceptions", transportService, clusterService, threadPool);
-        final TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        final TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         Thread t = new Thread() {
             @Override
             public void run() {
@@ -464,7 +584,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Request request = new Request(shardId).timeout("100ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         assertIndexShardCounter(2);
         assertThat(transport.capturedRequests().length, equalTo(1));
@@ -473,7 +593,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         assertIndexShardCounter(1);
         transport.clear();
         request = new Request(shardId).timeout("100ms");
-        primaryPhase = action.new PrimaryPhase(request, listener);
+        primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         assertIndexShardCounter(2);
         CapturingTransport.CapturedRequest[] replicationRequests = transport.capturedRequests();
@@ -498,7 +618,7 @@ public class TransportReplicationActionTests extends ESTestCase {
             @Override
             public void run() {
                 try {
-                    replicaOperationTransportHandler.messageReceived(new Request(), createTransportChannel());
+                    replicaOperationTransportHandler.messageReceived(new Request(), createTransportChannel(new PlainActionFuture<>()));
                 } catch (Exception e) {
                 }
             }
@@ -515,7 +635,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         action = new ActionWithExceptions(Settings.EMPTY, "testActionWithExceptions", transportService, clusterService, threadPool);
         final Action.ReplicaOperationTransportHandler replicaOperationTransportHandlerForException = action.new ReplicaOperationTransportHandler();
         try {
-            replicaOperationTransportHandlerForException.messageReceived(new Request(shardId), createTransportChannel());
+            replicaOperationTransportHandlerForException.messageReceived(new Request(shardId), createTransportChannel(new PlainActionFuture<>()));
             fail();
         } catch (Throwable t2) {
         }
@@ -531,7 +651,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Request request = new Request(shardId).timeout("100ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         // no replica request should have been sent yet
         assertThat(transport.capturedRequests().length, equalTo(0));
@@ -559,7 +679,6 @@ public class TransportReplicationActionTests extends ESTestCase {
     }
 
     public static class Request extends ReplicationRequest<Request> {
-        int shardId;
         public AtomicBoolean processedOnPrimary = new AtomicBoolean();
         public AtomicInteger processedOnReplicas = new AtomicInteger();
 
@@ -568,25 +687,23 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         Request(ShardId shardId) {
             this();
-            this.shardId = shardId.id();
-            this.index(shardId.index().name());
+            this.shardId = shardId;
+            this.index = shardId.getIndex();
             // keep things simple
         }
 
         @Override
         public void writeTo(StreamOutput out) throws IOException {
             super.writeTo(out);
-            out.writeVInt(shardId);
         }
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
             super.readFrom(in);
-            shardId = in.readVInt();
         }
     }
 
-    static class Response extends ActionWriteResponse {
+    static class Response extends ReplicationResponse {
     }
 
     class Action extends TransportReplicationAction<Request, Request, Response> {
@@ -605,23 +722,18 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-            boolean executedBefore = shardRequest.request.processedOnPrimary.getAndSet(true);
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+            boolean executedBefore = shardRequest.processedOnPrimary.getAndSet(true);
             assert executedBefore == false : "request has already been executed on the primary";
-            return new Tuple<>(new Response(), shardRequest.request);
+            return new Tuple<>(new Response(), shardRequest);
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request request) {
+        protected void shardOperationOnReplica(Request request) {
             request.processedOnReplicas.incrementAndGet();
         }
 
         @Override
-        protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-            return clusterState.getRoutingTable().index(request.concreteIndex()).shard(request.request().shardId).shardsIt();
-        }
-
-        @Override
         protected boolean checkWriteConsistency() {
             return false;
         }
@@ -659,8 +771,8 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-            return throwException(shardRequest.shardId);
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+            return throwException(shardRequest.shardId());
         }
 
         private Tuple<Response, Request> throwException(ShardId shardId) {
@@ -681,8 +793,8 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request shardRequest) {
-            throwException(shardRequest.internalShardId);
+        protected void shardOperationOnReplica(Request shardRequest) {
+            throwException(shardRequest.shardId());
         }
     }
 
@@ -697,9 +809,9 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
             awaitLatch();
-            return new Tuple<>(new Response(), shardRequest.request);
+            return new Tuple<>(new Response(), shardRequest);
         }
 
         private void awaitLatch() throws InterruptedException {
@@ -708,7 +820,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request shardRequest) {
+        protected void shardOperationOnReplica(Request shardRequest) {
             try {
                 awaitLatch();
             } catch (InterruptedException e) {
@@ -720,7 +832,7 @@ public class TransportReplicationActionTests extends ESTestCase {
     /*
     * Transport channel that is needed for replica operation testing.
     * */
-    public TransportChannel createTransportChannel() {
+    public TransportChannel createTransportChannel(final PlainActionFuture<Response> listener) {
         return new TransportChannel() {
 
             @Override
@@ -735,14 +847,17 @@ public class TransportReplicationActionTests extends ESTestCase {
 
             @Override
             public void sendResponse(TransportResponse response) throws IOException {
+                listener.onResponse(((Response) response));
             }
 
             @Override
             public void sendResponse(TransportResponse response, TransportResponseOptions options) throws IOException {
+                listener.onResponse(((Response) response));
             }
 
             @Override
             public void sendResponse(Throwable error) throws IOException {
+                listener.onFailure(error);
             }
         };
     }
diff --git a/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java b/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
index 28d4b0f..0eb7c07 100644
--- a/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
+++ b/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
@@ -141,8 +141,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, null);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"terms\":{\"brown\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"doc_freq\":15,\"ttf\":30,\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
@@ -198,8 +197,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, null);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"field_statistics\":{\"sum_doc_freq\":120,\"doc_count\":15,\"sum_ttf\":135},\"terms\":{\"brown\":{\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
@@ -258,8 +256,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, ToXContent.EMPTY_PARAMS);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"field_statistics\":{\"sum_doc_freq\":120,\"doc_count\":15,\"sum_ttf\":135},\"terms\":{\"brown\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"doc_freq\":15,\"ttf\":30,\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 81b09c8..3a3876b 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -759,7 +759,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
             admin().indices().prepareAliases().addAliasAction(AliasAction.newAddAliasAction("index1", null)).get();
             fail("Expected ActionRequestValidationException");
         } catch (ActionRequestValidationException e) {
-            assertThat(e.getMessage(), containsString("requires an [alias] to be set"));
+            assertThat(e.getMessage(), containsString("[alias] may not be empty string"));
         }
     }
 
@@ -768,7 +768,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
             admin().indices().prepareAliases().addAliasAction(AliasAction.newAddAliasAction("index1", "")).get();
             fail("Expected ActionRequestValidationException");
         } catch (ActionRequestValidationException e) {
-            assertThat(e.getMessage(), containsString("requires an [alias] to be set"));
+            assertThat(e.getMessage(), containsString("[alias] may not be empty string"));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java
deleted file mode 100644
index 7b5d489..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.aliases;
-
-import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequestBuilder;
-import org.elasticsearch.action.admin.indices.alias.get.GetAliasesResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.AliasMetaData;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.monitor.jvm.JvmStats;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.io.IOException;
-import java.util.List;
-
-/**
- */
-public class AliasesBenchmark {
-
-    private final static String INDEX_NAME = "my-index";
-
-    public static void main(String[] args) throws IOException {
-        int NUM_ADDITIONAL_NODES = 1;
-        int BASE_ALIAS_COUNT = 100000;
-        int NUM_ADD_ALIAS_REQUEST = 1000;
-
-        Settings settings = Settings.settingsBuilder()
-                .put("node.master", false).build();
-        Node node1 = NodeBuilder.nodeBuilder().settings(
-                Settings.settingsBuilder().put(settings).put("node.master", true)
-        ).node();
-
-        Node[] otherNodes = new Node[NUM_ADDITIONAL_NODES];
-        for (int i = 0; i < otherNodes.length; i++) {
-            otherNodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-
-        Client client = node1.client();
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME).execute().actionGet();
-        } catch (IndexAlreadyExistsException e) {}
-        client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
-        int numberOfAliases = countAliases(client);
-        System.out.println("Number of aliases: " + numberOfAliases);
-
-        if (numberOfAliases < BASE_ALIAS_COUNT) {
-            int diff = BASE_ALIAS_COUNT - numberOfAliases;
-            System.out.println("Adding " + diff + " more aliases to get to the start amount of " + BASE_ALIAS_COUNT + " aliases");
-            IndicesAliasesRequestBuilder builder = client.admin().indices().prepareAliases();
-            for (int i = 1; i <= diff; i++) {
-                builder.addAlias(INDEX_NAME, Strings.randomBase64UUID());
-                if (i % 1000 == 0) {
-                    builder.execute().actionGet();
-                    builder = client.admin().indices().prepareAliases();
-                }
-            }
-            if (!builder.request().getAliasActions().isEmpty()) {
-                builder.execute().actionGet();
-            }
-        } else if (numberOfAliases > BASE_ALIAS_COUNT) {
-            IndicesAliasesRequestBuilder builder = client.admin().indices().prepareAliases();
-            int diff = numberOfAliases - BASE_ALIAS_COUNT;
-            System.out.println("Removing " + diff + " aliases to get to the start amount of " + BASE_ALIAS_COUNT + " aliases");
-            List<AliasMetaData> aliases= client.admin().indices().prepareGetAliases("*")
-                    .addIndices(INDEX_NAME)
-                    .execute().actionGet().getAliases().get(INDEX_NAME);
-            for (int i = 0; i <= diff; i++) {
-                builder.removeAlias(INDEX_NAME, aliases.get(i).alias());
-                if (i % 1000 == 0) {
-                    builder.execute().actionGet();
-                    builder = client.admin().indices().prepareAliases();
-                }
-            }
-            if (!builder.request().getAliasActions().isEmpty()) {
-                builder.execute().actionGet();
-            }
-        }
-
-        numberOfAliases = countAliases(client);
-        System.out.println("Number of aliases: " + numberOfAliases);
-
-        long totalTime = 0;
-        int max = numberOfAliases + NUM_ADD_ALIAS_REQUEST;
-        for (int i = numberOfAliases; i <= max; i++) {
-            if (i != numberOfAliases && i % 100 == 0) {
-                long avgTime = totalTime / 100;
-                System.out.println("Added [" + (i - numberOfAliases) + "] aliases. Avg create time: "  + avgTime + " ms");
-                System.out.println("Heap used [" + JvmStats.jvmStats().getMem().getHeapUsed() + "]");
-                totalTime = 0;
-            }
-
-            long time = System.currentTimeMillis();
-//            String filter = termFilter("field" + i, "value" + i).toXContent(XContentFactory.jsonBuilder(), null).string();
-            client.admin().indices().prepareAliases().addAlias(INDEX_NAME, Strings.randomBase64UUID()/*, filter*/)
-                    .execute().actionGet();
-            totalTime += System.currentTimeMillis() - time;
-        }
-        System.gc();
-        System.out.println("Final heap used [" + JvmStats.jvmStats().getMem().getHeapUsed() + "]");
-        System.out.println("Number of aliases: " + countAliases(client));
-
-        client.close();
-        node1.close();
-        for (Node otherNode : otherNodes) {
-            otherNode.close();
-        }
-    }
-
-    private static int countAliases(Client client) {
-        GetAliasesResponse response = client.admin().indices().prepareGetAliases("*")
-                .addIndices(INDEX_NAME)
-                .execute().actionGet();
-        if (response.getAliases().isEmpty()) {
-            return 0;
-        } else {
-            return response.getAliases().get(INDEX_NAME).size();
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java b/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java
deleted file mode 100644
index 15745fc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.bloom;
-
-import org.apache.lucene.codecs.bloom.FuzzySet;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.util.BloomFilter;
-
-import java.security.SecureRandom;
-
-/**
- */
-public class BloomBench {
-
-    public static void main(String[] args) throws Exception {
-        SecureRandom random = new SecureRandom();
-        final int ELEMENTS = (int) SizeValue.parseSizeValue("1m").singles();
-        final double fpp = 0.01;
-        BloomFilter gFilter = BloomFilter.create(ELEMENTS, fpp);
-        System.out.println("G SIZE: " + new ByteSizeValue(gFilter.getSizeInBytes()));
-
-        FuzzySet lFilter = FuzzySet.createSetBasedOnMaxMemory((int) gFilter.getSizeInBytes());
-        //FuzzySet lFilter = FuzzySet.createSetBasedOnQuality(ELEMENTS, 0.97f);
-
-        for (int i = 0; i < ELEMENTS; i++) {
-            BytesRef bytesRef = new BytesRef(Strings.randomBase64UUID(random));
-            gFilter.put(bytesRef);
-            lFilter.addValue(bytesRef);
-        }
-
-        int lFalse = 0;
-        int gFalse = 0;
-        for (int i = 0; i < ELEMENTS; i++) {
-            BytesRef bytesRef = new BytesRef(Strings.randomBase64UUID(random));
-            if (gFilter.mightContain(bytesRef)) {
-                gFalse++;
-            }
-            if (lFilter.contains(bytesRef) == FuzzySet.ContainsResult.MAYBE) {
-                lFalse++;
-            }
-        }
-        System.out.println("Failed positives, g[" + gFalse + "], l[" + lFalse + "]");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java
deleted file mode 100644
index f6b0497..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.breaker;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.breaker.CircuitBreaker;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-
-import java.util.UUID;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static junit.framework.Assert.assertNotNull;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-
-/**
- * Benchmarks for different implementations of the circuit breaker
- */
-public class CircuitBreakerBenchmark {
-
-    private static final String INDEX = UUID.randomUUID().toString();
-    private static final int QUERIES = 100;
-    private static final int BULK_SIZE = 100;
-    private static final int NUM_DOCS = 2_000_000;
-    private static final int AGG_SIZE = 25;
-
-    private static void switchToNoop(Client client) {
-        Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, CircuitBreaker.Type.NOOP)
-                .build();
-        client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings).execute().actionGet();
-    }
-
-    private static void switchToMemory(Client client) {
-        Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, CircuitBreaker.Type.MEMORY)
-                .build();
-        client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings).execute().actionGet();
-    }
-
-    private static void runSingleThreadedQueries(Client client) {
-        long totalTime = 0;
-        for (int i = 0; i < QUERIES; i++) {
-            if (i % 10 == 0) {
-                System.out.println("--> query #" + i);
-            }
-            SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                    .addAggregation(
-                            terms("myterms")
-                                    .size(AGG_SIZE)
-                                    .field("num")
-                    ).setSize(0).get();
-            Terms terms = resp.getAggregations().get("myterms");
-            assertNotNull("term aggs were calculated", terms);
-            totalTime += resp.getTookInMillis();
-        }
-
-        System.out.println("--> single threaded average time: " + (totalTime / QUERIES) + "ms");
-    }
-
-    private static void runMultiThreadedQueries(final Client client) throws Exception {
-        final AtomicLong totalThreadedTime = new AtomicLong(0);
-        int THREADS = 10;
-        Thread threads[] = new Thread[THREADS];
-        for (int i = 0; i < THREADS; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    long tid = Thread.currentThread().getId();
-                    for (int i = 0; i < QUERIES; i++) {
-                        if (i % 30 == 0) {
-                            System.out.println("--> [" + tid + "] query # "+ i);
-                        }
-                        SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                                .addAggregation(
-                                        terms("myterms")
-                                                .size(AGG_SIZE)
-                                                .field("num")
-                                ).setSize(0).get();
-                        Terms terms = resp.getAggregations().get("myterms");
-                        assertNotNull("term aggs were calculated", terms);
-                        totalThreadedTime.addAndGet(resp.getTookInMillis());
-                    }
-                }
-            });
-        }
-
-        System.out.println("--> starting " + THREADS + " threads for parallel aggregating");
-        for (Thread t : threads) {
-            t.start();
-        }
-
-        for (Thread t : threads) {
-            t.join();
-        }
-
-        System.out.println("--> threaded average time: " + (totalThreadedTime.get() / (THREADS * QUERIES)) + "ms");
-    }
-
-    public static void main(String args[]) throws Exception {
-        Node node = NodeBuilder.nodeBuilder().settings(Settings.settingsBuilder()).node();
-        final Client client = node.client();
-        try {
-            try {
-                client.admin().indices().prepareDelete(INDEX).get();
-            } catch (Exception e) {
-                // Ignore
-            }
-            try {
-                client.admin().indices().prepareCreate(INDEX).setSettings(
-                        settingsBuilder().put("number_of_shards", 2).put("number_of_replicas", 0)).get();
-            } catch (IndexAlreadyExistsException e) {}
-            client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
-
-
-            System.out.println("--> indexing: " + NUM_DOCS + " documents...");
-            BulkRequestBuilder bulkBuilder = client.prepareBulk();
-            for (int i = 0; i < NUM_DOCS; i++) {
-                bulkBuilder.add(client.prepareIndex(INDEX, "doc").setSource("num", i));
-                if (i % BULK_SIZE == 0) {
-                    // Send off bulk request
-                    bulkBuilder.get();
-                    // Create a new holder
-                    bulkBuilder = client.prepareBulk();
-                }
-            }
-            bulkBuilder.get();
-            client.admin().indices().prepareRefresh(INDEX).get();
-            SearchResponse countResp = client.prepareSearch(INDEX).setQuery(matchAllQuery()).setSize(0).get();
-            assert countResp.getHits().getTotalHits() == NUM_DOCS : "all docs should be indexed";
-
-            final int warmupCount = 100;
-            for (int i = 0; i < warmupCount; i++) {
-                if (i % 15 == 0) {
-                    System.out.println("--> warmup #" + i);
-                }
-                SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                        .addAggregation(
-                                terms("myterms")
-                                        .size(AGG_SIZE)
-                                        .field("num")
-                        ).setSize(0).get();
-                Terms terms = resp.getAggregations().get("myterms");
-                assertNotNull("term aggs were calculated", terms);
-            }
-
-            System.out.println("--> running single-threaded tests");
-            runSingleThreadedQueries(client);
-            System.out.println("--> switching to NOOP breaker");
-            switchToNoop(client);
-            runSingleThreadedQueries(client);
-            switchToMemory(client);
-
-            System.out.println("--> running multi-threaded tests");
-            runMultiThreadedQueries(client);
-            System.out.println("--> switching to NOOP breaker");
-            switchToNoop(client);
-            runMultiThreadedQueries(client);
-        } finally {
-            client.close();
-            node.close();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java
deleted file mode 100644
index 660d042..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.checksum;
-
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-
-import java.security.MessageDigest;
-import java.util.zip.Adler32;
-import java.util.zip.CRC32;
-
-/**
- *
- */
-public class ChecksumBenchmark {
-
-    public static final int BATCH_SIZE = 16 * 1024;
-
-    public static void main(String[] args) throws Exception {
-        System.out.println("Warning up");
-        long warmSize = ByteSizeValue.parseBytesSizeValue("1g", null).bytes();
-        crc(warmSize);
-        adler(warmSize);
-        md5(warmSize);
-
-        long dataSize = ByteSizeValue.parseBytesSizeValue("10g", null).bytes();
-        System.out.println("Running size: " + dataSize);
-        crc(dataSize);
-        adler(dataSize);
-        md5(dataSize);
-    }
-
-    private static void crc(long dataSize) {
-        long start = System.currentTimeMillis();
-        CRC32 crc = new CRC32();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        for (long i = 0; i < iter; i++) {
-            crc.update(data);
-        }
-        crc.getValue();
-        System.out.println("CRC took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-
-    private static void adler(long dataSize) {
-        long start = System.currentTimeMillis();
-        Adler32 crc = new Adler32();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        for (long i = 0; i < iter; i++) {
-            crc.update(data);
-        }
-        crc.getValue();
-        System.out.println("Adler took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-
-    private static void md5(long dataSize) throws Exception {
-        long start = System.currentTimeMillis();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        MessageDigest digest = MessageDigest.getInstance("MD5");
-        for (long i = 0; i < iter; i++) {
-            digest.update(data);
-        }
-        digest.digest();
-        System.out.println("md5 took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
deleted file mode 100644
index c50574d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.cluster;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.allocation.AllocationService;
-import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.test.ESAllocationTestCase;
-
-import java.util.Random;
-
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-
-public class ClusterAllocationRerouteBenchmark {
-
-    private static final ESLogger logger = Loggers.getLogger(ClusterAllocationRerouteBenchmark.class);
-
-    public static void main(String[] args) {
-        final int numberOfRuns = 1;
-        final int numIndices = 5 * 365; // five years
-        final int numShards = 6;
-        final int numReplicas = 2;
-        final int numberOfNodes = 30;
-        final int numberOfTags = 2;
-        AllocationService strategy = ESAllocationTestCase.createAllocationService(Settings.builder()
-            .put("cluster.routing.allocation.awareness.attributes", "tag")
-            .build(), new Random(1));
-
-        MetaData.Builder mb = MetaData.builder();
-        for (int i = 1; i <= numIndices; i++) {
-            mb.put(IndexMetaData.builder("test_" + i).numberOfShards(numShards).numberOfReplicas(numReplicas));
-        }
-        MetaData metaData = mb.build();
-        RoutingTable.Builder rb = RoutingTable.builder();
-        for (int i = 1; i <= numIndices; i++) {
-            rb.addAsNew(metaData.index("test_" + i));
-        }
-        RoutingTable routingTable = rb.build();
-        DiscoveryNodes.Builder nb = DiscoveryNodes.builder();
-        for (int i = 1; i <= numberOfNodes; i++) {
-            nb.put(ESAllocationTestCase.newNode("node" + i, singletonMap("tag", "tag_" + (i % numberOfTags))));
-        }
-        ClusterState initialClusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).nodes(nb).build();
-
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < numberOfRuns; i++) {
-            logger.info("[{}] starting... ", i);
-            long runStart = System.currentTimeMillis();
-            ClusterState clusterState = initialClusterState;
-            while (clusterState.getRoutingNodes().hasUnassignedShards()) {
-                logger.info("[{}] remaining unassigned {}", i, clusterState.getRoutingNodes().unassigned().size());
-                RoutingAllocation.Result result = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();
-                result = strategy.reroute(clusterState, "reroute");
-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();
-            }
-            logger.info("[{}] took {}", i, TimeValue.timeValueMillis(System.currentTimeMillis() - runStart));
-        }
-        long took = System.currentTimeMillis() - start;
-        logger.info("total took {}, AVG {}", TimeValue.timeValueMillis(took), TimeValue.timeValueMillis(took / numberOfRuns));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java
deleted file mode 100644
index fe548b9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.common.lucene.uidscan;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.*;
-import org.apache.lucene.store.FSDirectory;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.unit.SizeValue;
-
-import java.nio.file.Paths;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- *
- */
-public class LuceneUidScanBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        FSDirectory dir = FSDirectory.open(PathUtils.get("work/test"));
-        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Lucene.STANDARD_ANALYZER));
-
-        final int NUMBER_OF_THREADS = 2;
-        final long INDEX_COUNT = SizeValue.parseSizeValue("1m").singles();
-        final long SCAN_COUNT = SizeValue.parseSizeValue("100k").singles();
-        final long startUid = 1000000;
-
-        long LIMIT = startUid + INDEX_COUNT;
-        StopWatch watch = new StopWatch().start();
-        System.out.println("Indexing " + INDEX_COUNT + " docs...");
-        for (long i = startUid; i < LIMIT; i++) {
-            Document doc = new Document();
-            doc.add(new StringField("_uid", Long.toString(i), Store.NO));
-            doc.add(new NumericDocValuesField("_version", i));
-            writer.addDocument(doc);
-        }
-        System.out.println("Done indexing, took " + watch.stop().lastTaskTime());
-
-        final IndexReader reader = DirectoryReader.open(writer, true);
-
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        for (long i = 0; i < SCAN_COUNT; i++) {
-                            long id = startUid + (Math.abs(ThreadLocalRandom.current().nextInt()) % INDEX_COUNT);
-                            final long version = Versions.loadVersion(reader, new Term("_uid", Long.toString(id)));
-                            if (version != id) {
-                                System.err.println("wrong id...");
-                                break;
-                            }
-                        }
-                    } catch (Exception e) {
-                        e.printStackTrace();
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-
-        watch = new StopWatch().start();
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        latch.await();
-        watch.stop();
-        System.out.println("Scanned in " + watch.totalTime() + " TP Seconds " + ((SCAN_COUNT * NUMBER_OF_THREADS) / watch.totalTime().secondsFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
deleted file mode 100644
index 9710605..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.common.recycler;
-
-import org.elasticsearch.common.recycler.AbstractRecyclerC;
-import org.elasticsearch.common.recycler.Recycler;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.common.recycler.Recyclers.concurrent;
-import static org.elasticsearch.common.recycler.Recyclers.concurrentDeque;
-import static org.elasticsearch.common.recycler.Recyclers.deque;
-import static org.elasticsearch.common.recycler.Recyclers.dequeFactory;
-import static org.elasticsearch.common.recycler.Recyclers.locked;
-import static org.elasticsearch.common.recycler.Recyclers.none;
-
-/** Benchmark that tries to measure the overhead of object recycling depending on concurrent access. */
-public class RecyclerBenchmark {
-
-    private static final long NUM_RECYCLES = 5000000L;
-    private static final Random RANDOM = new Random(0);
-
-    private static long bench(final Recycler<?> recycler, long numRecycles, int numThreads) throws InterruptedException {
-        final AtomicLong recycles = new AtomicLong(numRecycles);
-        final CountDownLatch latch = new CountDownLatch(1);
-        final Thread[] threads = new Thread[numThreads];
-        for (int i = 0; i < numThreads; ++i){
-            // Thread ids happen to be generated sequentially, so we also generate random threads so that distribution of IDs
-            // is not perfect for the concurrent recycler
-            for (int j = RANDOM.nextInt(5); j >= 0; --j) {
-                new Thread();
-            }
-
-            threads[i] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        latch.await();
-                    } catch (InterruptedException e) {
-                        return;
-                    }
-                    while (recycles.getAndDecrement() > 0) {
-                        final Recycler.V<?> v = recycler.obtain();
-                        v.close();
-                    }
-                }
-            };
-        }
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        final long start = System.nanoTime();
-        latch.countDown();
-        for (Thread thread : threads) {
-            thread.join();
-        }
-        return System.nanoTime() - start;
-    }
-
-    public static void main(String[] args) throws InterruptedException {
-        final int limit = 100;
-        final Recycler.C<Object> c = new AbstractRecyclerC<Object>() {
-
-            @Override
-            public Object newInstance(int sizing) {
-                return new Object();
-            }
-
-            @Override
-            public void recycle(Object value) {
-                // do nothing
-            }
-        };
-
-        Map<String, Recycler<Object>> recyclers = new HashMap<>();
-        recyclers.put("none", none(c));
-        recyclers.put("concurrent-queue", concurrentDeque(c, limit));
-        recyclers.put("locked", locked(deque(c, limit)));
-        recyclers.put("concurrent", concurrent(dequeFactory(c, limit), Runtime.getRuntime().availableProcessors()));
-
-        // warmup
-        final long start = System.nanoTime();
-        while (System.nanoTime() - start < TimeUnit.SECONDS.toNanos(10)) {
-            for (Recycler<?> recycler : recyclers.values()) {
-                bench(recycler, NUM_RECYCLES, 2);
-            }
-        }
-
-        // run
-        for (int numThreads = 1; numThreads <= 4 * Runtime.getRuntime().availableProcessors(); numThreads *= 2) {
-            System.out.println("## " + numThreads + " threads\n");
-            System.gc();
-            Thread.sleep(1000);
-            for (Recycler<?> recycler : recyclers.values()) {
-                bench(recycler, NUM_RECYCLES, numThreads);
-            }
-            for (int i = 0; i < 5; ++i) {
-                for (Map.Entry<String, Recycler<Object>> entry : recyclers.entrySet()) {
-                    System.out.println(entry.getKey() + "\t" + TimeUnit.NANOSECONDS.toMillis(bench(entry.getValue(), NUM_RECYCLES, numThreads)));
-                }
-                System.out.println();
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java
deleted file mode 100644
index ea1e589..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.counter;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- *
- */
-public class SimpleCounterBenchmark {
-
-    private static long NUMBER_OF_ITERATIONS = 10000000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        final AtomicLong counter = new AtomicLong();
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            counter.incrementAndGet();
-        }
-        System.out.println("Took " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Running using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        counter.incrementAndGet();
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Took " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java
deleted file mode 100644
index 06fc39d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.fs;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.unit.ByteSizeValue;
-
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.file.Path;
-import java.nio.file.Paths;
-import java.nio.file.StandardOpenOption;
-import java.util.Random;
-
-/**
- *
- */
-public class FsAppendBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Path path = PathUtils.get("work/test.log");
-        IOUtils.deleteFilesIgnoringExceptions(path);
-
-        int CHUNK = (int) ByteSizeValue.parseBytesSizeValue("1k", "CHUNK").bytes();
-        long DATA = ByteSizeValue.parseBytesSizeValue("10gb", "DATA").bytes();
-
-        byte[] data = new byte[CHUNK];
-        new Random().nextBytes(data);
-
-        StopWatch watch = new StopWatch().start("write");
-        try (FileChannel channel = FileChannel.open(path, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW)) {
-            long position = 0;
-            while (position < DATA) {
-                channel.write(ByteBuffer.wrap(data), position);
-                position += data.length;
-            }
-            watch.stop().start("flush");
-            channel.force(true);
-        }
-        watch.stop();
-        System.out.println("Wrote [" + (new ByteSizeValue(DATA)) + "], chunk [" + (new ByteSizeValue(CHUNK)) + "], in " + watch);
-    }
-
-    private static final ByteBuffer fill = ByteBuffer.allocateDirect(1);
-
-//    public static long padLogFile(long position, long currentSize, long preAllocSize) throws IOException {
-//        if (position + 4096 >= currentSize) {
-//            currentSize = currentSize + preAllocSize;
-//            fill.position(0);
-//            f.getChannel().write(fill, currentSize - fill.remaining());
-//        }
-//        return currentSize;
-//    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java
deleted file mode 100644
index d78df7f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.get;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-// simple test for embedded / single remote lookup
-public class SimpleGetActionBenchmark {
-
-    public static void main(String[] args) {
-        long OPERATIONS = SizeValue.parseSizeValue("300k").singles();
-
-        Node node = NodeBuilder.nodeBuilder().node();
-
-        Client client;
-        if (false) {
-            client = NodeBuilder.nodeBuilder().client(true).node().client();
-        } else {
-            client = node.client();
-        }
-
-        client.prepareIndex("test", "type1", "1").setSource("field1", "value1").execute().actionGet();
-
-        StopWatch stopWatch = new StopWatch().start();
-        for (long i = 0; i < OPERATIONS; i++) {
-            client.prepareGet("test", "type1", "1").execute().actionGet();
-        }
-        stopWatch.stop();
-
-        System.out.println("Ran in " + stopWatch.totalTime() + ", per second: " + (((double) OPERATIONS) / stopWatch.totalTime().secondsFrac()));
-
-        node.close();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java
deleted file mode 100644
index e51ba31..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java
+++ /dev/null
@@ -1,262 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.hppc;
-
-import com.carrotsearch.hppc.IntIntHashMap;
-import com.carrotsearch.hppc.IntObjectHashMap;
-import com.carrotsearch.hppc.ObjectIntHashMap;
-import com.carrotsearch.hppc.ObjectObjectHashMap;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.unit.SizeValue;
-
-import java.util.HashMap;
-import java.util.IdentityHashMap;
-import java.util.concurrent.ThreadLocalRandom;
-
-// TODO: these benchmarks aren't too good and may be easily skewed by jit doing 
-// escape analysis/ side-effects/ local
-// optimisations. Proper benchmarks with JMH (bulk ops, single-shot mode) 
-// should be better here.
-// https://github.com/carrotsearch/hppc/blob/master/hppc-benchmarks/src/main/java/com/carrotsearch/hppc/benchmarks/B003_HashSet_Contains.java
-
-public class StringMapAdjustOrPutBenchmark {
-
-    public static void main(String[] args) {
-
-        int NUMBER_OF_KEYS = (int) SizeValue.parseSizeValue("20").singles();
-        int STRING_SIZE = 5;
-        long PUT_OPERATIONS = SizeValue.parseSizeValue("5m").singles();
-        long ITERATIONS = 10;
-        boolean REUSE = true;
-
-
-        String[] values = new String[NUMBER_OF_KEYS];
-        for (int i = 0; i < values.length; i++) {
-            values[i] = RandomStrings.randomAsciiOfLength(ThreadLocalRandom.current(), STRING_SIZE);
-        }
-
-        StopWatch stopWatch;
-
-        stopWatch = new StopWatch().start();
-        ObjectIntHashMap<String> map = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                map.clear();
-            } else {
-                map = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                map.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        map.clear();
-        map = null;
-
-        stopWatch.stop();
-        System.out.println("TObjectIntHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        stopWatch = new StopWatch().start();
-//        TObjectIntCustomHashMap<String> iMap = new TObjectIntCustomHashMap<String>(new StringIdentityHashingStrategy());
-        ObjectIntHashMap<String> iMap = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                iMap.clear();
-            } else {
-                iMap = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                iMap.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TObjectIntCustomHashMap(StringIdentity): " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-        iMap.clear();
-        iMap = null;
-
-        stopWatch = new StopWatch().start();
-        iMap = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                iMap.clear();
-            } else {
-                iMap = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                iMap.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TObjectIntCustomHashMap(PureIdentity): " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-        iMap.clear();
-        iMap = null;
-
-        // now test with THashMap
-        stopWatch = new StopWatch().start();
-        ObjectObjectHashMap<String, StringEntry> tMap = new ObjectObjectHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                tMap.clear();
-            } else {
-                tMap = new ObjectObjectHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = tMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    tMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-
-        tMap.clear();
-        tMap = null;
-
-        stopWatch.stop();
-        System.out.println("THashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        stopWatch = new StopWatch().start();
-        HashMap<String, StringEntry> hMap = new HashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                hMap.clear();
-            } else {
-                hMap = new HashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = hMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    hMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-
-        hMap.clear();
-        hMap = null;
-
-        stopWatch.stop();
-        System.out.println("HashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-
-        stopWatch = new StopWatch().start();
-        IdentityHashMap<String, StringEntry> ihMap = new IdentityHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                ihMap.clear();
-            } else {
-                hMap = new HashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = ihMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    ihMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-        stopWatch.stop();
-        System.out.println("IdentityHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        ihMap.clear();
-        ihMap = null;
-
-        int[] iValues = new int[NUMBER_OF_KEYS];
-        for (int i = 0; i < values.length; i++) {
-            iValues[i] = ThreadLocalRandom.current().nextInt();
-        }
-
-        stopWatch = new StopWatch().start();
-        IntIntHashMap intMap = new IntIntHashMap();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                intMap.clear();
-            } else {
-                intMap = new IntIntHashMap();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                int key = iValues[(int) (i % NUMBER_OF_KEYS)];
-                intMap.addTo(key, 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TIntIntHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        intMap.clear();
-        intMap = null;
-
-        // now test with THashMap
-        stopWatch = new StopWatch().start();
-        IntObjectHashMap<IntEntry> tIntMap = new IntObjectHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                tIntMap.clear();
-            } else {
-                tIntMap = new IntObjectHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                int key = iValues[(int) (i % NUMBER_OF_KEYS)];
-                IntEntry intEntry = tIntMap.get(key);
-                if (intEntry == null) {
-                    intEntry = new IntEntry(key, 1);
-                    tIntMap.put(key, intEntry);
-                } else {
-                    intEntry.counter++;
-                }
-            }
-        }
-
-        tIntMap.clear();
-        tIntMap = null;
-
-        stopWatch.stop();
-        System.out.println("TIntObjectHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-    }
-
-
-    static class StringEntry {
-        String key;
-        int counter;
-
-        StringEntry(String key, int counter) {
-            this.key = key;
-            this.counter = counter;
-        }
-    }
-
-    static class IntEntry {
-        int key;
-        int counter;
-
-        IntEntry(int key, int counter) {
-            this.key = key;
-            this.counter = counter;
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
deleted file mode 100644
index ff7d084..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.mapping;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class ManyMappingsBenchmark {
-
-    private static final String MAPPING = "{\n" +
-            "        \"dynamic_templates\": [\n" +
-            "          {\n" +
-            "            \"t1\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"norms\": {\n" +
-            "                  \"enabled\": false\n" +
-            "                },\n" +
-            "                \"type\": \"string\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_ss\"\n" +
-            "            }\n" +
-            "          },\n" +
-            "          {\n" +
-            "            \"t2\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"type\": \"date\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_dt\"\n" +
-            "            }\n" +
-            "          },\n" +
-            "          {\n" +
-            "            \"t3\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"type\": \"integer\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_i\"\n" +
-            "            }\n" +
-            "          }\n" +
-            "        ],\n" +
-            "        \"_source\": {\n" +
-            "          \"enabled\": false\n" +
-            "        },\n" +
-            "        \"properties\": {}\n" +
-            "      }";
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-    private static final int FIELD_COUNT = 100000;
-    private static final int DOC_COUNT = 10000000;
-    private static final boolean TWO_NODES = true;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-        Settings settings = settingsBuilder()
-                .put("")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ManyMappingsBenchmark.class.getSimpleName();
-        Node node = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-        if (TWO_NODES) {
-            Node node2 = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings))
-                    .node();
-        }
-
-        Client client = node.client();
-
-        client.admin().indices().prepareDelete(INDEX_NAME)
-                .setIndicesOptions(IndicesOptions.lenientExpandOpen())
-                .get();
-        client.admin().indices().prepareCreate(INDEX_NAME)
-                .addMapping(TYPE_NAME, MAPPING)
-                .get();
-
-        BulkRequestBuilder builder = client.prepareBulk();
-        int fieldCount = 0;
-        long time = System.currentTimeMillis();
-        final int PRINT = 1000;
-        for (int i = 0; i < DOC_COUNT; i++) {
-            XContentBuilder sourceBuilder = jsonBuilder().startObject();
-            sourceBuilder.field(++fieldCount + "_ss", "xyz");
-            sourceBuilder.field(++fieldCount + "_dt", System.currentTimeMillis());
-            sourceBuilder.field(++fieldCount + "_i", i % 100);
-            sourceBuilder.endObject();
-
-            if (fieldCount >= FIELD_COUNT) {
-                fieldCount = 0;
-                System.out.println("dynamic fields rolled up");
-            }
-
-            builder.add(
-                    client.prepareIndex(INDEX_NAME, TYPE_NAME, String.valueOf(i))
-                            .setSource(sourceBuilder)
-            );
-
-            if (builder.numberOfActions() >= 1000) {
-                builder.get();
-                builder = client.prepareBulk();
-            }
-
-            if (i % PRINT == 0) {
-                long took = System.currentTimeMillis() - time;
-                time = System.currentTimeMillis();
-                System.out.println("Indexed " + i +  " docs, in " + TimeValue.timeValueMillis(took));
-            }
-        }
-        if (builder.numberOfActions() > 0) {
-            builder.get();
-        }
-
-
-
-    }
-
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java
deleted file mode 100644
index 6788490..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.monitor.os;
-
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.monitor.os.OsProbe;
-
-@SuppressForbidden(reason = "not really source code or a test")
-public class OsProbeBenchmark {
-
-    private static final int ITERATIONS = 100_000;
-
-    public static void main(String[] args) {
-        System.setProperty("es.logger.prefix", "");
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        logger.info("--> loading OS probe");
-        OsProbe probe = OsProbe.getInstance();
-
-        logger.info("--> warming up...");
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalPhysicalMemorySize();
-            probe.getFreePhysicalMemorySize();
-            probe.getTotalSwapSpaceSize();
-            probe.getFreeSwapSpaceSize();
-            probe.getSystemLoadAverage();
-            probe.getSystemCpuPercent();
-        }
-        logger.info("--> warmed up");
-
-        logger.info("--> testing 'getTotalPhysicalMemorySize' method...");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalPhysicalMemorySize();
-        }
-        long elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getFreePhysicalMemorySize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getFreePhysicalMemorySize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getTotalSwapSpaceSize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalSwapSpaceSize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getFreeSwapSpaceSize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getFreeSwapSpaceSize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getSystemLoadAverage' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getSystemLoadAverage();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getSystemCpuPercent' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getSystemCpuPercent();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java
deleted file mode 100644
index b91b516..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.monitor.process;
-
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.monitor.process.ProcessProbe;
-
-import java.lang.management.ManagementFactory;
-import java.lang.management.ThreadMXBean;
-
-@SuppressForbidden(reason = "use of om.sun.management.ThreadMXBean to compare performance")
-public class ProcessProbeBenchmark {
-
-    private static final int ITERATIONS = 100_000;
-
-    public static void main(String[] args) {
-        System.setProperty("es.logger.prefix", "");
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        logger.info("--> loading process probe");
-        ProcessProbe probe = ProcessProbe.getInstance();
-
-        logger.info("--> warming up...");
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getOpenFileDescriptorCount();
-            probe.getMaxFileDescriptorCount();
-            probe.getTotalVirtualMemorySize();
-            probe.getProcessCpuPercent();
-            probe.getProcessCpuTotalTime();
-        }
-        logger.info("--> warmed up");
-
-
-
-
-        logger.info("--> testing 'getOpenFileDescriptorCount' method...");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getOpenFileDescriptorCount();
-        }
-        long elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getMaxFileDescriptorCount' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getMaxFileDescriptorCount();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getTotalVirtualMemorySize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalVirtualMemorySize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getProcessCpuPercent' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getProcessCpuPercent();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getProcessCpuTotalTime' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getProcessCpuTotalTime();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-
-
-
-        logger.info("--> calculating process CPU user time with 'getAllThreadIds + getThreadUserTime' methods...");
-        final ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
-        final long[] threadIds = threadMxBean.getAllThreadIds();
-        long sum = 0;
-
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            for (long threadId : threadIds) {
-                sum += threadMxBean.getThreadUserTime(threadId);
-            }
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> execution time [total: {} ms, avg: {} ms] for {} iterations with average result of {}",
-                elapsed, (elapsed / (double)ITERATIONS), ITERATIONS, (sum / (double)ITERATIONS));
-
-        if (threadMxBean instanceof com.sun.management.ThreadMXBean) {
-            logger.info("--> calculating process CPU user time with 'getAllThreadIds + getThreadUserTime(long[])' methods...");
-            final com.sun.management.ThreadMXBean threadMxBean2 = (com.sun.management.ThreadMXBean)threadMxBean;
-            sum = 0;
-
-            start = System.currentTimeMillis();
-            for (int i = 0; i < ITERATIONS; i++) {
-                long[] user = threadMxBean2.getThreadUserTime(threadIds);
-                for (int n = 0 ; n != threadIds.length; ++n) {
-                    sum += user[n];
-                }
-            }
-            elapsed = System.currentTimeMillis() - start;
-            logger.info("--> execution time [total: {} ms, avg: {} ms] for {} iterations with average result of {}",
-                    elapsed, (elapsed / (double)ITERATIONS), ITERATIONS, (sum / (double)ITERATIONS));
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java
deleted file mode 100644
index 81492eb..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.percolator;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
-import org.elasticsearch.action.percolate.PercolateResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.percolator.PercolatorService;
-
-import java.io.IOException;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class PercolatorStressBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 4)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        Node clientNode = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-        Client client = clientNode.client();
-
-        client.admin().indices().create(createIndexRequest("test")).actionGet();
-        ClusterHealthResponse healthResponse = client.admin().cluster().prepareHealth("test")
-                .setWaitForGreenStatus()
-                .execute().actionGet();
-        if (healthResponse.isTimedOut()) {
-            System.err.println("Quiting, because cluster health requested timed out...");
-            return;
-        } else if (healthResponse.getStatus() != ClusterHealthStatus.GREEN) {
-            System.err.println("Quiting, because cluster state isn't green...");
-            return;
-        }
-
-        int COUNT = 200000;
-        int QUERIES = 100;
-        int TERM_QUERIES = QUERIES / 2;
-        int RANGE_QUERIES = QUERIES - TERM_QUERIES;
-
-        client.prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject().field("numeric1", 1).endObject()).execute().actionGet();
-
-        // register queries
-        int i = 0;
-        for (; i < TERM_QUERIES; i++) {
-            client.prepareIndex("test", PercolatorService.TYPE_NAME, Integer.toString(i))
-                    .setSource(jsonBuilder().startObject()
-                            .field("query", termQuery("name", "value"))
-                            .endObject())
-                    .execute().actionGet();
-        }
-
-        int[] numbers = new int[RANGE_QUERIES];
-        for (; i < QUERIES; i++) {
-            client.prepareIndex("test", PercolatorService.TYPE_NAME, Integer.toString(i))
-                    .setSource(jsonBuilder().startObject()
-                            .field("query", rangeQuery("numeric1").from(i).to(i))
-                            .endObject())
-                    .execute().actionGet();
-            numbers[i - TERM_QUERIES] = i;
-        }
-
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Percolating [" + COUNT + "] ...");
-        for (i = 1; i <= COUNT; i++) {
-            XContentBuilder source;
-            int expectedMatches;
-            if (i % 2 == 0) {
-                source = source(Integer.toString(i), "value");
-                expectedMatches = TERM_QUERIES;
-            } else {
-                int number = numbers[i % RANGE_QUERIES];
-                source = source(Integer.toString(i), number);
-                expectedMatches = 1;
-            }
-            PercolateResponse percolate = client.preparePercolate()
-                    .setIndices("test").setDocumentType("type1")
-                    .setSource(source)
-                    .execute().actionGet();
-            if (percolate.getMatches().length != expectedMatches) {
-                System.err.println("No matching number of queries");
-            }
-
-            if ((i % 10000) == 0) {
-                System.out.println("Percolated " + i + " took " + stopWatch.stop().lastTaskTime());
-                stopWatch.start();
-            }
-        }
-        System.out.println("Percolation took " + stopWatch.totalTime() + ", TPS " + (((double) COUNT) / stopWatch.totalTime().secondsFrac()));
-
-        clientNode.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String id, String nameValue) throws IOException {
-        return jsonBuilder().startObject().startObject("doc")
-                .field("id", id)
-                .field("name", nameValue)
-                .endObject().endObject();
-    }
-
-    private static XContentBuilder source(String id, int number) throws IOException {
-        return jsonBuilder().startObject().startObject("doc")
-                .field("id", id)
-                .field("numeric1", number)
-                .field("numeric2", number)
-                .field("numeric3", number)
-                .field("numeric4", number)
-                .field("numeric5", number)
-                .field("numeric6", number)
-                .field("numeric7", number)
-                .field("numeric8", number)
-                .field("numeric9", number)
-                .field("numeric10", number)
-                .endObject().endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
deleted file mode 100644
index b857e4d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
+++ /dev/null
@@ -1,200 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.recovery;
-
-import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.indices.recovery.RecoveryState;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.test.BackgroundIndexer;
-import org.elasticsearch.transport.TransportModule;
-
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class ReplicaRecoveryBenchmark {
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-
-
-    static int DOC_COUNT = (int) SizeValue.parseSizeValue("40k").singles();
-    static int CONCURRENT_INDEXERS = 2;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-
-        Settings settings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, "false")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put(TransportModule.TRANSPORT_TYPE_KEY, "local")
-                .build();
-
-        String clusterName = ReplicaRecoveryBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        final Client client1 = node1.client();
-        client1.admin().cluster().prepareUpdateSettings().setPersistentSettings("logger.indices.recovery: TRACE").get();
-        final BackgroundIndexer indexer = new BackgroundIndexer(INDEX_NAME, TYPE_NAME, client1, 0, CONCURRENT_INDEXERS, false, new Random());
-        indexer.setMinFieldSize(10);
-        indexer.setMaxFieldSize(150);
-        try {
-            client1.admin().indices().prepareDelete(INDEX_NAME).get();
-        } catch (IndexNotFoundException e) {
-        }
-        client1.admin().indices().prepareCreate(INDEX_NAME).get();
-        indexer.start(DOC_COUNT / 2);
-        while (indexer.totalIndexedDocs() < DOC_COUNT / 2) {
-            Thread.sleep(5000);
-            logger.info("--> indexed {} of {}", indexer.totalIndexedDocs(), DOC_COUNT);
-        }
-        client1.admin().indices().prepareFlush().get();
-        indexer.continueIndexing(DOC_COUNT / 2);
-        while (indexer.totalIndexedDocs() < DOC_COUNT) {
-            Thread.sleep(5000);
-            logger.info("--> indexed {} of {}", indexer.totalIndexedDocs(), DOC_COUNT);
-        }
-
-
-        logger.info("--> starting another node and allocating a shard on it");
-
-        Node node2 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-
-        client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 1").get();
-
-        final AtomicBoolean end = new AtomicBoolean(false);
-
-        final Thread backgroundLogger = new Thread(new Runnable() {
-
-            long lastTime = System.currentTimeMillis();
-            long lastDocs = indexer.totalIndexedDocs();
-            long lastBytes = 0;
-            long lastTranslogOps = 0;
-
-            @Override
-            public void run() {
-                while (true) {
-                    try {
-                        Thread.sleep(5000);
-                    } catch (InterruptedException e) {
-
-                    }
-                    if (end.get()) {
-                        return;
-                    }
-                    long currentTime = System.currentTimeMillis();
-                    long currentDocs = indexer.totalIndexedDocs();
-                    RecoveryResponse recoveryResponse = client1.admin().indices().prepareRecoveries(INDEX_NAME).setActiveOnly(true).get();
-                    List<RecoveryState> indexRecoveries = recoveryResponse.shardRecoveryStates().get(INDEX_NAME);
-                    long translogOps;
-                    long bytes;
-                    if (indexRecoveries.size() > 0) {
-                        translogOps = indexRecoveries.get(0).getTranslog().recoveredOperations();
-                        bytes = recoveryResponse.shardRecoveryStates().get(INDEX_NAME).get(0).getIndex().recoveredBytes();
-                    } else {
-                        bytes = lastBytes = 0;
-                        translogOps = lastTranslogOps = 0;
-                    }
-                    float seconds = (currentTime - lastTime) / 1000.0F;
-                    logger.info("--> indexed [{}];[{}] doc/s, recovered [{}] MB/s , translog ops [{}]/s ",
-                            currentDocs, (currentDocs - lastDocs) / seconds,
-                            (bytes - lastBytes) / 1024.0F / 1024F / seconds, (translogOps - lastTranslogOps) / seconds);
-                    lastBytes = bytes;
-                    lastTranslogOps = translogOps;
-                    lastTime = currentTime;
-                    lastDocs = currentDocs;
-                }
-            }
-        });
-
-        backgroundLogger.start();
-
-        client1.admin().cluster().prepareHealth().setWaitForGreenStatus().get();
-
-        logger.info("--> green. starting relocation cycles");
-
-        long startDocIndexed = indexer.totalIndexedDocs();
-        indexer.continueIndexing(DOC_COUNT * 50);
-
-        long totalRecoveryTime = 0;
-        long startTime = System.currentTimeMillis();
-        long[] recoveryTimes = new long[3];
-        for (int iteration = 0; iteration < 3; iteration++) {
-            logger.info("--> removing replicas");
-            client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 0").get();
-            logger.info("--> adding replica again");
-            long recoveryStart = System.currentTimeMillis();
-            client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 1").get();
-            client1.admin().cluster().prepareHealth(INDEX_NAME).setWaitForGreenStatus().setTimeout("15m").get();
-            long recoveryTime = System.currentTimeMillis() - recoveryStart;
-            totalRecoveryTime += recoveryTime;
-            recoveryTimes[iteration] = recoveryTime;
-            logger.info("--> recovery done in [{}]", new TimeValue(recoveryTime));
-
-            // sleep some to let things clean up
-            Thread.sleep(10000);
-        }
-
-        long endDocIndexed = indexer.totalIndexedDocs();
-        long totalTime = System.currentTimeMillis() - startTime;
-        indexer.stop();
-
-        end.set(true);
-
-        backgroundLogger.interrupt();
-
-        backgroundLogger.join();
-
-        logger.info("average doc/s [{}], average relocation time [{}], taking [{}], [{}], [{}]", (endDocIndexed - startDocIndexed) * 1000.0 / totalTime, new TimeValue(totalRecoveryTime / 3),
-                TimeValue.timeValueMillis(recoveryTimes[0]), TimeValue.timeValueMillis(recoveryTimes[1]), TimeValue.timeValueMillis(recoveryTimes[2])
-        );
-
-        client1.close();
-        node1.close();
-        node2.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
deleted file mode 100644
index 6f666b3..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript1 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript1();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_1 = "native_1";
-
-    @Override
-    public Object run() {
-        return docFieldLongs("x").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
deleted file mode 100644
index 585d7a5..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript2 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript2();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_2 = "native_2";
-
-    @Override
-    public Object run() {
-        return docFieldLongs("x").getValue() + docFieldDoubles("y").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
deleted file mode 100644
index c2d50fe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript3 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript3();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_3 = "native_3";
-
-    @Override
-    public Object run() {
-        return 1.2 * docFieldLongs("x").getValue() / docFieldDoubles("y").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
deleted file mode 100644
index 2bda86e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript4 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript4();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_4 = "native_4";
-
-    @Override
-    public Object run() {
-        return Math.sqrt(Math.abs(docFieldDoubles("z").getValue())) + Math.log(Math.abs(docFieldLongs("x").getValue() * docFieldDoubles("y").getValue()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java
deleted file mode 100644
index 92f19a7..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.ScriptModule;
-
-public class NativeScriptPlugin extends Plugin {
-
-    @Override
-    public String name() {
-        return "native-benchmark-scripts";
-    }
-
-    @Override
-    public String description() {
-        return "Native benchmark script";
-    }
-
-    public void onModule(ScriptModule module) {
-        module.registerScript(NativeScript1.NATIVE_SCRIPT_1, NativeScript1.Factory.class);
-        module.registerScript(NativeScript2.NATIVE_SCRIPT_2, NativeScript2.Factory.class);
-        module.registerScript(NativeScript3.NATIVE_SCRIPT_3, NativeScript3.Factory.class);
-        module.registerScript(NativeScript4.NATIVE_SCRIPT_4, NativeScript4.Factory.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java
deleted file mode 100644
index ce4cbf1..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.IndicesAdminClient;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.sort.ScriptSortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.joda.time.PeriodType;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Random;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-public class ScriptComparisonBenchmark {
-
-    static final String clusterName = ScriptComparisonBenchmark.class.getSimpleName();
-    static final String indexName = "test";
-
-    static String[] langs = {
-        "expression",
-        "native",
-        "groovy"
-    };
-    static String[][] scripts = {
-        // the first value is the "reference" version (pure math)
-        {
-            "x",
-            "doc['x'].value",
-            NativeScript1.NATIVE_SCRIPT_1,
-            "doc['x'].value"
-        }, {
-            "x + y",
-            "doc['x'].value + doc['y'].value",
-            NativeScript2.NATIVE_SCRIPT_2,
-            "doc['x'].value + doc['y'].value",
-        }, {
-            "1.2 * x / y",
-            "1.2 * doc['x'].value / doc['y'].value",
-            NativeScript3.NATIVE_SCRIPT_3,
-            "1.2 * doc['x'].value / doc['y'].value",
-        }, {
-            "sqrt(abs(z)) + ln(abs(x * y))",
-            "sqrt(abs(doc['z'].value)) + ln(abs(doc['x'].value * doc['y'].value))",
-            NativeScript4.NATIVE_SCRIPT_4,
-            "sqrt(abs(doc['z'].value)) + log(abs(doc['x'].value * doc['y'].value))"
-        }
-    };
-
-    public static void main(String[] args) throws Exception {
-        int numDocs = 1000000;
-        int numQueries = 1000;
-        Client client = setupIndex();
-        indexDocs(client, numDocs);
-
-        for (int scriptNum = 0; scriptNum < scripts.length; ++scriptNum) {
-            runBenchmark(client, scriptNum, numQueries);
-        }
-    }
-
-    static void runBenchmark(Client client, int scriptNum, int numQueries) {
-        System.out.println("");
-        System.out.println("Script: " + scripts[scriptNum][0]);
-        System.out.println("--------------------------------");
-        for (int langNum = 0; langNum < langs.length; ++langNum) {
-            String lang = langs[langNum];
-            String script = scripts[scriptNum][langNum + 1];
-
-            timeQueries(client, lang, script, numQueries / 10); // warmup
-            TimeValue time = timeQueries(client, lang, script, numQueries);
-            printResults(lang, time, numQueries);
-        }
-    }
-
-    static Client setupIndex() throws Exception {
-        // create cluster
-        Settings settings = settingsBuilder().put("name", "node1")
-                                             .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        // delete the index, if it exists
-        try {
-            client.admin().indices().prepareDelete(indexName).execute().actionGet();
-        } catch (ElasticsearchException e) {
-            // ok if the index didn't exist
-        }
-
-        // create mappings
-        IndicesAdminClient admin = client.admin().indices();
-        admin.prepareCreate(indexName).addMapping("doc", "x", "type=long", "y", "type=double");
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        return client;
-    }
-
-    static void indexDocs(Client client, int numDocs) {
-        System.out.print("Indexing " + numDocs + " random docs...");
-        BulkRequestBuilder bulkRequest = client.prepareBulk();
-        Random r = new Random(1);
-        for (int i = 0; i < numDocs; i++) {
-            bulkRequest.add(client.prepareIndex("test", "doc", Integer.toString(i))
-                            .setSource("x", r.nextInt(), "y", r.nextDouble(), "z", r.nextDouble()));
-
-            if (i % 1000 == 0) {
-                bulkRequest.execute().actionGet();
-                bulkRequest = client.prepareBulk();
-            }
-        }
-        bulkRequest.execute().actionGet();
-        client.admin().indices().prepareRefresh("test").execute().actionGet();
-        client.admin().indices().prepareFlush("test").execute().actionGet();
-        System.out.println("done");
-    }
-
-    static TimeValue timeQueries(Client client, String lang, String script, int numQueries) {
-        ScriptSortBuilder sort = SortBuilders.scriptSort(new Script(script, ScriptType.INLINE, lang, null), "number");
-        SearchRequestBuilder req = client.prepareSearch(indexName)
-                .setQuery(QueryBuilders.matchAllQuery())
-                .addSort(sort);
-
-        StopWatch timer = new StopWatch();
-        timer.start();
-        for (int i = 0; i < numQueries; ++i) {
-            req.get();
-        }
-        timer.stop();
-        return timer.totalTime();
-    }
-
-    static void printResults(String lang, TimeValue time, int numQueries) {
-        long avgReq = time.millis() / numQueries;
-        System.out.println(lang + ": " + time.format(PeriodType.seconds()) + " (" + avgReq + " msec per req)");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java
deleted file mode 100644
index 81d4a78..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java
+++ /dev/null
@@ -1,335 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.io.BufferedWriter;
-import java.io.IOException;
-import java.math.BigInteger;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.security.SecureRandom;
-import java.util.AbstractMap;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-
-import static org.elasticsearch.client.Requests.searchRequest;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-
-public class BasicScriptBenchmark {
-
-    public static class RequestInfo {
-        public RequestInfo(SearchRequest source, int i) {
-            request = source;
-            numTerms = i;
-        }
-
-        SearchRequest request;
-        int numTerms;
-    }
-
-    public static class Results {
-        public static final String TIME_PER_DOCIN_MILLIS = "timePerDocinMillis";
-        public static final String NUM_TERMS = "numTerms";
-        public static final String NUM_DOCS = "numDocs";
-        public static final String TIME_PER_QUERY_IN_SEC = "timePerQueryInSec";
-        public static final String TOTAL_TIME_IN_SEC = "totalTimeInSec";
-        Double[] resultSeconds;
-        Double[] resultMSPerQuery;
-        Long[] numDocs;
-        Integer[] numTerms;
-        Double[] timePerDoc;
-        String label;
-        String description;
-        public String lineStyle;
-        public String color;
-
-        void init(int numVariations, String label, String description, String color, String lineStyle) {
-            resultSeconds = new Double[numVariations];
-            resultMSPerQuery = new Double[numVariations];
-            numDocs = new Long[numVariations];
-            numTerms = new Integer[numVariations];
-            timePerDoc = new Double[numVariations];
-            this.label = label;
-            this.description = description;
-            this.color = color;
-            this.lineStyle = lineStyle;
-        }
-
-        void set(SearchResponse searchResponse, StopWatch stopWatch, String message, int maxIter, int which, int numTerms) {
-            resultSeconds[which] = (double) ((double) stopWatch.lastTaskTime().getMillis() / (double) 1000);
-            resultMSPerQuery[which] = (double) ((double) stopWatch.lastTaskTime().secondsFrac() / (double) maxIter);
-            numDocs[which] = searchResponse.getHits().totalHits();
-            this.numTerms[which] = numTerms;
-            timePerDoc[which] = resultMSPerQuery[which] / numDocs[which];
-        }
-
-        public void printResults(BufferedWriter writer) throws IOException {
-            String comma = (writer == null) ? "" : ";";
-            String results = description + "\n" + Results.TOTAL_TIME_IN_SEC + " = " + getResultArray(resultSeconds) + comma + "\n"
-                    + Results.TIME_PER_QUERY_IN_SEC + " = " + getResultArray(resultMSPerQuery) + comma + "\n" + Results.NUM_DOCS + " = "
-                    + getResultArray(numDocs) + comma + "\n" + Results.NUM_TERMS + " = " + getResultArray(numTerms) + comma + "\n"
-                    + Results.TIME_PER_DOCIN_MILLIS + " = " + getResultArray(timePerDoc) + comma + "\n";
-            if (writer != null) {
-                writer.write(results);
-            } else {
-                System.out.println(results);
-            }
-
-        }
-
-        private String getResultArray(Object[] resultArray) {
-            String result = "[";
-            for (int i = 0; i < resultArray.length; i++) {
-                result += resultArray[i].toString();
-                if (i != resultArray.length - 1) {
-                    result += ",";
-                }
-            }
-            result += "]";
-            return result;
-        }
-    }
-
-    public BasicScriptBenchmark() {
-    }
-
-    static List<String> termsList = new ArrayList<>();
-
-    static void init(int numTerms) {
-        SecureRandom random = new SecureRandom();
-        random.setSeed(1);
-        termsList.clear();
-        for (int i = 0; i < numTerms; i++) {
-            String term = new BigInteger(512, random).toString(32);
-            termsList.add(term);
-        }
-
-    }
-
-    static String[] getTerms(int numTerms) {
-        String[] terms = new String[numTerms];
-        for (int i = 0; i < numTerms; i++) {
-            terms[i] = termsList.get(i);
-        }
-        return terms;
-    }
-
-    public static void writeHelperFunction() throws IOException {
-        try (BufferedWriter out = Files.newBufferedWriter(PathUtils.get("addToPlot.m"), StandardCharsets.UTF_8)) {
-            out.write("function handle = addToPlot(numTerms, perDoc, color, linestyle, linewidth)\n" + "handle = line(numTerms, perDoc);\n"
-                + "set(handle, 'color', color);\n" + "set(handle, 'linestyle',linestyle);\n" + "set(handle, 'LineWidth',linewidth);\n"
-                + "end\n");
-        }
-    }
-
-    public static void printOctaveScript(List<Results> allResults, String[] args) throws IOException {
-        if (args.length == 0) {
-            return;
-        }
-        try (BufferedWriter out = Files.newBufferedWriter(PathUtils.get(args[0]), StandardCharsets.UTF_8)) {
-            out.write("#! /usr/local/bin/octave -qf");
-            out.write("\n\n\n\n");
-            out.write("######################################\n");
-            out.write("# Octave script for plotting results\n");
-            String filename = "scriptScoreBenchmark" + new DateTime(DateTimeZone.UTC).toString();
-            out.write("#Call '" + args[0] + "' from the command line. The plot is then in " + filename + "\n\n");
-
-            out.write("handleArray = [];\n tagArray = [];\n plot([]);\n hold on;\n");
-            for (Results result : allResults) {
-                out.write("\n");
-                out.write("# " + result.description);
-                result.printResults(out);
-                out.write("handleArray = [handleArray, addToPlot(" + Results.NUM_TERMS + ", " + Results.TIME_PER_DOCIN_MILLIS + ", '"
-                        + result.color + "','" + result.lineStyle + "',5)];\n");
-                out.write("tagArray = [tagArray; '" + result.label + "'];\n");
-                out.write("\n");
-            }
-
-            out.write("xlabel(\'number of query terms');");
-            out.write("ylabel(\'query time per document');");
-
-            out.write("legend(handleArray,tagArray);\n");
-
-            out.write("saveas(gcf,'" + filename + ".png','png')\n");
-            out.write("hold off;\n\n");
-        } catch (IOException e) {
-            System.err.println("Error: " + e.getMessage());
-        }
-        writeHelperFunction();
-    }
-
-    static void printResult(SearchResponse searchResponse, StopWatch stopWatch, String queryInfo) {
-        System.out.println("--> Searching with " + queryInfo + " took " + stopWatch.lastTaskTime() + ", per query "
-                + (stopWatch.lastTaskTime().secondsFrac() / 100) + " for " + searchResponse.getHits().totalHits() + " docs");
-    }
-
-    static void indexData(long numDocs, Client client, boolean randomizeTerms) throws IOException {
-        try {
-            client.admin().indices().prepareDelete("test").execute().actionGet();
-        } catch (Throwable t) {
-            // index might exist already, in this case we do nothing TODO: make
-            // saver in general
-        }
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("text").field("type", "string").field("index_options", "offsets").field("analyzer", "payload_float")
-                .endObject().endObject().endObject().endObject();
-        client.admin()
-                .indices()
-                .prepareCreate("test")
-                .addMapping("type1", mapping)
-                .setSettings(
-                        Settings.settingsBuilder().put("index.analysis.analyzer.payload_float.tokenizer", "whitespace")
-                                .putArray("index.analysis.analyzer.payload_float.filter", "delimited_float")
-                                .put("index.analysis.filter.delimited_float.delimiter", "|")
-                                .put("index.analysis.filter.delimited_float.encoding", "float")
-                                .put("index.analysis.filter.delimited_float.type", "delimited_payload_filter")
-                                .put("index.number_of_replicas", 0).put("index.number_of_shards", 1)).execute().actionGet();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        BulkRequestBuilder bulkRequest = client.prepareBulk();
-        Random random = new Random(1);
-        for (int i = 0; i < numDocs; i++) {
-
-            bulkRequest.add(client.prepareIndex().setType("type1").setIndex("test")
-                    .setSource(jsonBuilder().startObject().field("text", randomText(random, randomizeTerms)).endObject()));
-            if (i % 1000 == 0) {
-                bulkRequest.execute().actionGet();
-                bulkRequest = client.prepareBulk();
-            }
-        }
-        bulkRequest.execute().actionGet();
-        client.admin().indices().prepareRefresh("test").execute().actionGet();
-        client.admin().indices().prepareFlush("test").execute().actionGet();
-        System.out.println("Done indexing " + numDocs + " documents");
-
-    }
-
-    private static String randomText(Random random, boolean randomizeTerms) {
-        String text = "";
-        for (int i = 0; i < termsList.size(); i++) {
-            if (random.nextInt(5) == 3 || !randomizeTerms) {
-                text = text + " " + termsList.get(i) + "|1";
-            }
-        }
-        return text;
-    }
-
-    static void printTimings(SearchResponse searchResponse, StopWatch stopWatch, String message, int maxIter) {
-        System.out.println(message);
-        System.out.println(stopWatch.lastTaskTime() + ", " + (stopWatch.lastTaskTime().secondsFrac() / maxIter) + ", "
-                + searchResponse.getHits().totalHits() + ", "
-                + (stopWatch.lastTaskTime().secondsFrac() / (maxIter + searchResponse.getHits().totalHits())));
-    }
-
-    static List<Entry<String, RequestInfo>> initTermQueries(int minTerms, int maxTerms) {
-        List<Entry<String, RequestInfo>> termSearchRequests = new ArrayList<>();
-        for (int nTerms = minTerms; nTerms < maxTerms; nTerms++) {
-            Map<String, Object> params = new HashMap<>();
-            String[] terms = getTerms(nTerms + 1);
-            params.put("text", terms);
-            SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                    searchSource().explain(false).size(0).query(QueryBuilders.termsQuery("text", terms)));
-            String infoString = "Results for term query with " + (nTerms + 1) + " terms:";
-            termSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, nTerms + 1)));
-        }
-        return termSearchRequests;
-    }
-
-    static List<Entry<String, RequestInfo>> initNativeSearchRequests(int minTerms, int maxTerms, String script, boolean langNative) {
-        List<Entry<String, RequestInfo>> nativeSearchRequests = new ArrayList<>();
-        for (int nTerms = minTerms; nTerms < maxTerms; nTerms++) {
-            Map<String, Object> params = new HashMap<>();
-            String[] terms = getTerms(nTerms + 1);
-            params.put("text", terms);
-            String infoString = "Results for native script with " + (nTerms + 1) + " terms:";
-            ScriptScoreFunctionBuilder scriptFunction = (langNative == true) ? scriptFunction(new Script(script, ScriptType.INLINE,
-                    "native", params)) : scriptFunction(new Script(script, ScriptType.INLINE, null, params));
-            SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                    searchSource()
-                            .explain(false)
-                            .size(0)
-                            .query(functionScoreQuery(QueryBuilders.termsQuery("text", terms), scriptFunction).boostMode(
-                                    CombineFunction.REPLACE)));
-            nativeSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, nTerms + 1)));
-        }
-        return nativeSearchRequests;
-    }
-
-    static List<Entry<String, RequestInfo>> initScriptMatchAllSearchRequests(String script, boolean langNative) {
-        List<Entry<String, RequestInfo>> nativeSearchRequests = new ArrayList<>();
-        String infoString = "Results for constant score script:";
-        ScriptScoreFunctionBuilder scriptFunction = (langNative == true) ? scriptFunction(new Script(script, ScriptType.INLINE, "native",
-                null)) : scriptFunction(new Script(script));
-        SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                searchSource().explain(false).size(0)
-                        .query(functionScoreQuery(QueryBuilders.matchAllQuery(), scriptFunction).boostMode(CombineFunction.REPLACE)));
-        nativeSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, 0)));
-
-        return nativeSearchRequests;
-    }
-
-    static void runBenchmark(Client client, int maxIter, Results results, List<Entry<String, RequestInfo>> nativeSearchRequests,
-            int minTerms, int warmerIter) throws IOException {
-        int counter = 0;
-        for (Entry<String, RequestInfo> entry : nativeSearchRequests) {
-            SearchResponse searchResponse = null;
-            // warm up
-            for (int i = 0; i < warmerIter; i++) {
-                searchResponse = client.search(entry.getValue().request).actionGet();
-            }
-            System.gc();
-            // run benchmark
-            StopWatch stopWatch = new StopWatch();
-            stopWatch.start();
-            for (int i = 0; i < maxIter; i++) {
-                searchResponse = client.search(entry.getValue().request).actionGet();
-            }
-            stopWatch.stop();
-            results.set(searchResponse, stopWatch, entry.getKey(), maxIter, counter, entry.getValue().numTerms);
-            counter++;
-        }
-        results.printResults(null);
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java
deleted file mode 100644
index 53baf78..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantForLoopScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsConstantScoreBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 49;
-        int maxTerms = 50;
-        int maxIter = 1000;
-        int warmerIter = 1000;
-
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-
-        String clusterName = ScriptsConstantScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-                                             .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, true);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-
-        results.init(maxTerms - minTerms, "native const script score (log(2) 10X)",
-                "Results for native const script score with score = log(2) 10X:", "black", "-.");
-        // init script searches
-        List<Entry<String, RequestInfo>> searchRequests = initScriptMatchAllSearchRequests(
-                NativeConstantForLoopScoreScript.NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        // init native script searches
-        results = new Results();
-        results.init(maxTerms - minTerms, "mvel const (log(2) 10X)", "Results for mvel const score = log(2) 10X:", "red", "-.");
-        searchRequests = initScriptMatchAllSearchRequests("score = 0; for (int i=0; i<10;i++) {score = score + log(2);} return score",
-                false);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        results.init(maxTerms - minTerms, "native const script score (2)", "Results for native const script score with score = 2:",
-                "black", ":");
-        // init native script searches
-        searchRequests = initScriptMatchAllSearchRequests(NativeConstantScoreScript.NATIVE_CONSTANT_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        results.init(maxTerms - minTerms, "mvel const (2)", "Results for mvel const score = 2:", "red", "--");
-        // init native script searches
-        searchRequests = initScriptMatchAllSearchRequests("2", false);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java
deleted file mode 100644
index 53c34a2..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativeNaiveTFIDFScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsScoreBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 1;
-        int maxTerms = 50;
-        int maxIter = 100;
-        int warmerIter = 10;
-
-        boolean runMVEL = false;
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-        String clusterName = ScriptsScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-            .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, false);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-        results.init(maxTerms - minTerms, "native tfidf script score dense posting list",
-                "Results for native script score with dense posting list:", "black", "--");
-        // init native script searches
-        List<Entry<String, RequestInfo>> searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-
-        results.init(maxTerms - minTerms, "term query dense posting list", "Results for term query with dense posting lists:", "green",
-                "--");
-        // init term queries
-        searchRequests = initTermQueries(minTerms, maxTerms);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        if (runMVEL) {
-
-            results = new Results();
-            results.init(maxTerms - minTerms, "mvel tfidf dense posting list", "Results for mvel score with dense posting list:", "red",
-                    "--");
-            // init native script searches
-            searchRequests = initNativeSearchRequests(
-                    minTerms,
-                    maxTerms,
-                    "score = 0.0; fi= _terminfo[\"text\"]; for(i=0; i<text.size(); i++){terminfo = fi[text.get(i)]; score = score + terminfo.tf()*fi.getDocCount()/terminfo.df();} return score;",
-                    false);
-            // run actual benchmark
-            runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-            allResults.add(results);
-        }
-
-        indexData(10000, client, true);
-        results = new Results();
-        results.init(maxTerms - minTerms, "native tfidf script score sparse posting list",
-                "Results for native script scorewith sparse posting list:", "black", "-.");
-        // init native script searches
-        searchRequests = initNativeSearchRequests(minTerms, maxTerms, NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-
-        results.init(maxTerms - minTerms, "term query sparse posting list", "Results for term query with sparse posting lists:", "green",
-                "-.");
-        // init term queries
-        searchRequests = initTermQueries(minTerms, maxTerms);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        if (runMVEL) {
-
-            results = new Results();
-            results.init(maxTerms - minTerms, "mvel tfidf sparse posting list", "Results for mvel score with sparse posting list:", "red",
-                    "-.");
-            // init native script searches
-            searchRequests = initNativeSearchRequests(
-                    minTerms,
-                    maxTerms,
-                    "score = 0.0; fi= _terminfo[\"text\"]; for(i=0; i<text.size(); i++){terminfo = fi[text.get(i)]; score = score + terminfo.tf()*fi.getDocCount()/terminfo.df();} return score;",
-                    false);
-            // run actual benchmark
-            runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-            allResults.add(results);
-        }
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java
deleted file mode 100644
index b809192..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumNoRecordScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsScorePayloadSumBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 1;
-        int maxTerms = 50;
-        int maxIter = 100;
-        int warmerIter = 10;
-
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-        String clusterName = ScriptsScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-            .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, false);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-        // init script searches
-        results.init(maxTerms - minTerms, "native payload sum script score", "Results for native script score:", "green", ":");
-        List<Entry<String, RequestInfo>> searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativePayloadSumScoreScript.NATIVE_PAYLOAD_SUM_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        // init script searches
-        results.init(maxTerms - minTerms, "native payload sum script score no record", "Results for native script score:", "black", ":");
-        searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativePayloadSumNoRecordScoreScript.NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java
deleted file mode 100644
index 2a25f8f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score.plugin;
-
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantForLoopScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeNaiveTFIDFScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumNoRecordScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumScoreScript;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.ScriptModule;
-
-public class NativeScriptExamplesPlugin extends Plugin {
-
-
-    @Override
-    public String name() {
-        return "native-script-example";
-    }
-
-    @Override
-    public String description() {
-        return "Native script examples";
-    }
-
-    public void onModule(ScriptModule module) {
-        module.registerScript(NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, NativeNaiveTFIDFScoreScript.Factory.class);
-        module.registerScript(NativeConstantForLoopScoreScript.NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE, NativeConstantForLoopScoreScript.Factory.class);
-        module.registerScript(NativeConstantScoreScript.NATIVE_CONSTANT_SCRIPT_SCORE, NativeConstantScoreScript.Factory.class);
-        module.registerScript(NativePayloadSumScoreScript.NATIVE_PAYLOAD_SUM_SCRIPT_SCORE, NativePayloadSumScoreScript.Factory.class);
-        module.registerScript(NativePayloadSumNoRecordScoreScript.NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE, NativePayloadSumNoRecordScoreScript.Factory.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
deleted file mode 100644
index fee0a7e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeConstantForLoopScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE = "native_constant_for_loop_script_score";
-    
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeConstantForLoopScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeConstantForLoopScoreScript(Map<String, Object> params) {
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        for (int i = 0; i < 10; i++) {
-            score += Math.log(2);
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
deleted file mode 100644
index 17220cd..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeConstantScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_CONSTANT_SCRIPT_SCORE = "native_constant_script_score";
-    
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeConstantScoreScript();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeConstantScoreScript() {
-    }
-
-    @Override
-    public Object run() {
-        return 2;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
deleted file mode 100644
index e96b35d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativeNaiveTFIDFScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_NAIVE_TFIDF_SCRIPT_SCORE = "native_naive_tfidf_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeNaiveTFIDFScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeNaiveTFIDFScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i]);
-            try {
-                if (indexFieldTerm.tf() != 0) {
-                    score += indexFieldTerm.tf() * indexField.docCount() / indexFieldTerm.df();
-                }
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
deleted file mode 100644
index 7570426..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-import org.elasticsearch.search.lookup.IndexLookup;
-import org.elasticsearch.search.lookup.TermPosition;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativePayloadSumNoRecordScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE = "native_payload_sum_no_record_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativePayloadSumNoRecordScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativePayloadSumNoRecordScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i], IndexLookup.FLAG_PAYLOADS);
-            for (TermPosition pos : indexFieldTerm) {
-                score += pos.payloadAsFloat(0);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
deleted file mode 100644
index 1522b3a..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-import org.elasticsearch.search.lookup.IndexLookup;
-import org.elasticsearch.search.lookup.TermPosition;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativePayloadSumScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_PAYLOAD_SUM_SCRIPT_SCORE = "native_payload_sum_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativePayloadSumScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativePayloadSumScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i], IndexLookup.FLAG_PAYLOADS | IndexLookup.FLAG_CACHE);
-            for (TermPosition pos : indexFieldTerm) {
-                score += pos.payloadAsFloat(0);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
deleted file mode 100644
index d78a478..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.suggest.Suggest.Suggestion.Entry.Option;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.SuggestBuilders;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class SuggestSearchBenchMark {
-
-    public static void main(String[] args) throws Exception {
-        int SEARCH_ITERS = 200;
-
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        Client client = nodes[0].client();
-        try {
-            client.admin().indices().prepareCreate("test").setSettings(settings).addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("_source").field("enabled", false).endObject()
-                    .startObject("_all").field("enabled", false).endObject()
-                    .startObject("_type").field("index", "no").endObject()
-                    .startObject("_id").field("index", "no").endObject()
-                    .startObject("properties")
-                    .startObject("field").field("type", "string").field("index", "not_analyzed").field("omit_norms", true).endObject()
-                    .endObject()
-                    .endObject().endObject()).execute().actionGet();
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-
-            StopWatch stopWatch = new StopWatch().start();
-            long COUNT = SizeValue.parseSizeValue("10m").singles();
-            int BATCH = 100;
-            System.out.println("Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            char character = 'a';
-            int idCounter = 0;
-            for (; i <= ITERS; i++) {
-                int termCounter = 0;
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(idCounter++)).source(source("prefix" + character + termCounter++)));
-                }
-                character++;
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("failures...");
-                }
-            }
-            System.out.println("Indexing took " + stopWatch.totalTime());
-
-            client.admin().indices().prepareRefresh().execute().actionGet();
-            System.out.println("Count: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-            client.admin().indices().prepareRefresh().execute().actionGet();
-            System.out.println("Count: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-        }
-
-
-        System.out.println("Warming up...");
-        char startChar = 'a';
-        for (int i = 0; i <= 20; i++) {
-            String term = "prefix" + startChar;
-            SearchResponse response = client.prepareSearch()
-                    .setQuery(prefixQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term)
-                                    .suggestMode("always")))
-                    .execute().actionGet();
-            if (response.getHits().totalHits() == 0) {
-                System.err.println("No hits");
-                continue;
-            }
-            startChar++;
-        }
-
-
-        System.out.println("Starting benchmarking suggestions.");
-        startChar = 'a';
-        long timeTaken = 0;
-        for (int i = 0; i <= SEARCH_ITERS; i++) {
-            String term = "prefix" + startChar;
-            SearchResponse response = client.prepareSearch()
-                    .setQuery(matchQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field")
-                                    .suggestMode("always")))
-                    .execute().actionGet();
-            timeTaken += response.getTookInMillis();
-            if (response.getSuggest() == null) {
-                System.err.println("No suggestions");
-                continue;
-            }
-            List<? extends Option> options = response.getSuggest().getSuggestion("field").getEntries().get(0).getOptions();
-            if (options == null || options.isEmpty()) {
-                System.err.println("No suggestions");
-            }
-            startChar++;
-        }
-
-        System.out.println("Avg time taken without filter " + (timeTaken / SEARCH_ITERS));
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String nameValue) throws IOException {
-        return jsonBuilder().startObject()
-                .field("field", nameValue)
-                .endObject();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java
deleted file mode 100644
index 40e2781..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;
-
-public class CardinalityAggregationSearchBenchmark {
-
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = CardinalityAggregationSearchBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int LOW_CARD = 1000;
-    private static final int HIGH_CARD = 1000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 5;
-    private static final int RUNS = 10;
-    private static final int ITERS = 5;
-
-    public static void main(String[] args) {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().create(createIndexRequest("index").settings(settings).mapping("type",
-                    jsonBuilder().startObject().startObject("type").startObject("properties")
-                        .startObject("low_card_str_value")
-                            .field("type", "multi_field")
-                            .startObject("fields")
-                                .startObject("low_card_str_value")
-                                    .field("type", "string")
-                                .endObject()
-                                .startObject("hash")
-                                    .field("type", "murmur3")
-                                .endObject()
-                            .endObject()
-                        .endObject()
-                        .startObject("high_card_str_value")
-                            .field("type", "multi_field")
-                            .startObject("fields")
-                                .startObject("high_card_str_value")
-                                    .field("type", "string")
-                                .endObject()
-                                .startObject("hash")
-                                    .field("type", "murmur3")
-                                .endObject()
-                            .endObject()
-                        .endObject()
-                        .startObject("low_card_num_value")
-                            .field("type", "long")
-                        .endObject()
-                        .startObject("high_card_num_value")
-                            .field("type", "long")
-                        .endObject()
-                    .endObject().endObject().endObject())).actionGet();
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    final int lowCard = RandomInts.randomInt(R, LOW_CARD);
-                    final int highCard = RandomInts.randomInt(R, HIGH_CARD);
-                    request.add(client.prepareIndex("index", "type", Integer.toString(i)).setSource("low_card_str_value", "str" + lowCard, "high_card_str_value", "str" + highCard, "low_card_num_value", lowCard , "high_card_num_value", highCard));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh("index").execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            if (i >= WARM) {
-                System.out.println("RUN " + (i - WARM));
-            }
-            for (String field : new String[] {"low_card_str_value", "low_card_str_value.hash", "high_card_str_value", "high_card_str_value.hash", "low_card_num_value", "high_card_num_value"}) {
-                long start = System.nanoTime();
-                SearchResponse resp = null;
-                for (int j = 0; j < ITERS; ++j) {
-                    resp = client.prepareSearch("index").setSize(0).addAggregation(cardinality("cardinality").field(field)).execute().actionGet();
-                }
-                long end = System.nanoTime();
-                final long cardinality = ((Cardinality) resp.getAggregations().get("cardinality")).getValue();
-                if (i >= WARM) {
-                    System.out.println(field + "\t" + new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS) + "\tcardinality=" + cardinality);
-                }
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
deleted file mode 100644
index ed94397..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
+++ /dev/null
@@ -1,248 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.IntIntHashMap;
-import com.carrotsearch.hppc.ObjectHashSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.benchmark.search.aggregations.TermsAggregationSearchBenchmark.StatsResult;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.transport.TransportModule;
-
-import java.util.*;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class GlobalOrdinalsBenchmark {
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-    private static final int QUERY_WARMUP = 25;
-    private static final int QUERY_COUNT = 100;
-    private static final int FIELD_START = 1;
-    private static final int FIELD_LIMIT = 1 << 22;
-    private static final boolean USE_DOC_VALUES = false;
-
-    static long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static Node node;
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put(TransportModule.TRANSPORT_TYPE_KEY, "local")
-                .build();
-
-        String clusterName = GlobalOrdinalsBenchmark.class.getSimpleName();
-        node = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings))
-                    .node();
-
-        client = node.client();
-
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME)
-                    .addMapping(TYPE_NAME, jsonBuilder().startObject().startObject(TYPE_NAME)
-                            .startArray("dynamic_templates")
-                                .startObject()
-                                    .startObject("default")
-                                        .field("match", "*")
-                                        .field("match_mapping_type", "string")
-                                        .startObject("mapping")
-                                            .field("type", "string")
-                                            .field("index", "not_analyzed")
-                                            .startObject("fields")
-                                                .startObject("doc_values")
-                                                    .field("type", "string")
-                                                    .field("index", "no")
-                                                    .startObject("fielddata")
-                                                        .field("format", "doc_values")
-                                                    .endObject()
-                                                .endObject()
-                                            .endObject()
-                                        .endObject()
-                                    .endObject()
-                                .endObject()
-                            .endArray()
-                    .endObject().endObject())
-                    .get();
-            ObjectHashSet<String> uniqueTerms = new ObjectHashSet<>();
-            for (int i = 0; i < FIELD_LIMIT; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, 16));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            BulkRequestBuilder builder = client.prepareBulk();
-            IntIntHashMap tracker = new IntIntHashMap();
-            for (int i = 0; i < COUNT; i++) {
-                Map<String, Object> fieldValues = new HashMap<>();
-                for (int fieldSuffix = 1; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-                    int index = tracker.putOrAdd(fieldSuffix, 0, 0);
-                    if (index >= fieldSuffix) {
-                        index = random.nextInt(fieldSuffix);
-                        fieldValues.put("field_" + fieldSuffix, sValues[index]);
-                    } else {
-                        fieldValues.put("field_" + fieldSuffix, sValues[index]);
-                        tracker.put(fieldSuffix, ++index);
-                    }
-                }
-                builder.add(
-                        client.prepareIndex(INDEX_NAME, TYPE_NAME, String.valueOf(i))
-                        .setSource(fieldValues)
-                );
-
-                if (builder.numberOfActions() >= 1000) {
-                    builder.get();
-                    builder = client.prepareBulk();
-                }
-            }
-            if (builder.numberOfActions() > 0) {
-                builder.get();
-            }
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-
-        client.admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put("logger.index.fielddata.ordinals", "DEBUG"))
-                .get();
-
-        client.admin().indices().prepareRefresh(INDEX_NAME).execute().actionGet();
-        COUNT = client.prepareSearch(INDEX_NAME).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        List<StatsResult> stats = new ArrayList<>();
-        for (int fieldSuffix = FIELD_START; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-            String fieldName = "field_" + fieldSuffix;
-            String name = "global_ordinals-" + fieldName;
-            if (USE_DOC_VALUES) {
-                fieldName = fieldName + ".doc_values";
-                name = name + "_doc_values"; // can't have . in agg name
-            }
-            stats.add(terms(name, fieldName, "global_ordinals_low_cardinality"));
-        }
-
-        for (int fieldSuffix = FIELD_START; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-            String fieldName = "field_" + fieldSuffix;
-            String name = "ordinals-" + fieldName;
-            if (USE_DOC_VALUES) {
-                fieldName = fieldName + ".doc_values";
-                name = name + "_doc_values"; // can't have . in agg name
-            }
-            stats.add(terms(name, fieldName, "ordinals"));
-        }
-
-        System.out.println("------------------ SUMMARY -----------------------------------------");
-        System.out.format(Locale.ENGLISH, "%30s%10s%10s%15s\n", "name", "took", "millis", "fieldata size");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%30s%10s%10d%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed);
-        }
-        System.out.println("------------------ SUMMARY -----------------------------------------");
-
-        client.close();
-        node.close();
-    }
-
-    private static StatsResult terms(String name, String field, String executionHint) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(INDEX_NAME)
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name).field(field).executionHint(executionHint))
-                    .get();
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(INDEX_NAME)
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name).field(field).executionHint(executionHint))
-                    .get();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String nodeId = node.injector().getInstance(Discovery.class).localNode().getId();
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeId).get();
-        System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java
deleted file mode 100644
index af0eee6..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeUnit;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.InternalHDRPercentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.percentiles;
-
-public class HDRPercentilesAggregationBenchmark {
-
-    private static final String TYPE_NAME = "type";
-    private static final String INDEX_NAME = "index";
-    private static final String HIGH_CARD_FIELD_NAME = "high_card";
-    private static final String LOW_CARD_FIELD_NAME = "low_card";
-    private static final String GAUSSIAN_FIELD_NAME = "gauss";
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = HDRPercentilesAggregationBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int LOW_CARD = 1000;
-    private static final int HIGH_CARD = 1000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 5;
-    private static final int RUNS = 10;
-    private static final int ITERS = 5;
-
-    public static void main(String[] args) {
-        long overallStartTime = System.currentTimeMillis();
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME);
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    final int lowCard = RandomInts.randomInt(R, LOW_CARD);
-                    final int highCard = RandomInts.randomInt(R, HIGH_CARD);
-                    int gauss = -1;
-                    while (gauss < 0) {
-                        gauss = (int) (R.nextGaussian() * 1000) + 5000; // mean: 5 sec, std deviation: 1 sec
-                    }
-                    request.add(client.prepareIndex(INDEX_NAME, TYPE_NAME, Integer.toString(i)).setSource(LOW_CARD_FIELD_NAME, lowCard,
-                            HIGH_CARD_FIELD_NAME, highCard, GAUSSIAN_FIELD_NAME, gauss));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh(INDEX_NAME).execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        System.out.println("Run\tField\tMethod\tAggregationTime\tEstimatedMemory");
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            for (String field : new String[] { LOW_CARD_FIELD_NAME, HIGH_CARD_FIELD_NAME, GAUSSIAN_FIELD_NAME }) {
-                for (PercentilesMethod method : new PercentilesMethod[] {PercentilesMethod.TDIGEST, PercentilesMethod.HDR}) {
-                    long start = System.nanoTime();
-                    SearchResponse resp = null;
-                    for (int j = 0; j < ITERS; ++j) {
-                        resp = client.prepareSearch(INDEX_NAME).setSize(0).addAggregation(percentiles("percentiles").field(field).method(method)).execute().actionGet();
-                    }
-                    long end = System.nanoTime();
-                    long memoryEstimate = 0;
-                    switch (method) {
-                    case TDIGEST:
-                        memoryEstimate = ((InternalTDigestPercentiles) resp.getAggregations().get("percentiles"))
-                                .getEstimatedMemoryFootprint();
-                        break;
-                    case HDR:
-                        memoryEstimate = ((InternalHDRPercentiles) resp.getAggregations().get("percentiles")).getEstimatedMemoryFootprint();
-                        break;
-                    }
-                    if (i >= WARM) {
-                        System.out.println((i - WARM) + "\t" + field + "\t" + method + "\t"
-                                + new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS).millis() + "\t"
-                                + new SizeValue(memoryEstimate, SizeUnit.SINGLE).singles());
-                    }
-                }
-            }
-        }
-        long overallEndTime = System.currentTimeMillis();
-        System.out.println("Benchmark completed in " + ((overallEndTime - overallStartTime) / 1000) + " seconds");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java
deleted file mode 100644
index d54c295..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-
-import java.util.Date;
-import java.util.Random;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.*;
-
-/**
- *
- */
-public class HistogramAggregationSearchBenchmark {
-
-    static final long COUNT = SizeValue.parseSizeValue("20m").singles();
-    static final int BATCH = 1000;
-    static final int QUERY_WARMUP = 5;
-    static final int QUERY_COUNT = 20;
-    static final int NUMBER_OF_TERMS = 1000;
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = HistogramAggregationSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1")).node();
-
-        //Node clientNode = nodeBuilder().clusterName(clusterName).settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = node1.client();
-
-        long[] lValues = new long[NUMBER_OF_TERMS];
-        for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-            lValues[i] = i;
-        }
-
-        Random r = new Random();
-        try {
-            client.admin().indices().prepareCreate("test")
-                    .setSettings(settingsBuilder().put(settings))
-                    .addMapping("type1", jsonBuilder()
-                        .startObject()
-                            .startObject("type1")
-                                .startObject("properties")
-                                    .startObject("l_value")
-                                        .field("type", "long")
-                                    .endObject()
-                                    .startObject("i_value")
-                                        .field("type", "integer")
-                                    .endObject()
-                                    .startObject("s_value")
-                                        .field("type", "short")
-                                    .endObject()
-                                    .startObject("b_value")
-                                        .field("type", "byte")
-                                    .endObject()
-                                .endObject()
-                            .endObject()
-                        .endObject())
-                    .execute().actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long iters = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= iters; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-                    final long value = lValues[r.nextInt(lValues.length)];
-                    XContentBuilder source = jsonBuilder().startObject()
-                            .field("id", Integer.valueOf(counter))
-                            .field("l_value", value)
-                            .field("i_value", (int) value)
-                            .field("s_value", (short) value)
-                            .field("b_value", (byte) value)
-                            .field("date", new Date())
-                            .endObject();
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(source));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            client.admin().indices().prepareFlush("test").execute().actionGet();
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        if (client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits() != COUNT) {
-            throw new Error();
-        }
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        System.out.println("--> Warmup...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(histogram("l_value").field("l_value").interval(4))
-                    .addAggregation(histogram("i_value").field("i_value").interval(4))
-                    .addAggregation(histogram("s_value").field("s_value").interval(4))
-                    .addAggregation(histogram("b_value").field("b_value").interval(4))
-                    .addAggregation(histogram("date").field("date").interval(1000))
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Warmup took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup DONE");
-
-        long totalQueryTime = 0;
-        for (String field : new String[] {"b_value", "s_value", "i_value", "l_value"}) {
-            totalQueryTime = 0;
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch()
-                        .setQuery(matchAllQuery())
-                        .addAggregation(histogram(field).field(field).interval(4))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != COUNT) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> Histogram Aggregation (" + field + ") " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-            totalQueryTime = 0;
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch()
-                        .setQuery(matchAllQuery())
-                        .addAggregation(histogram(field).field(field).subAggregation(stats(field).field(field)).interval(4))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != COUNT) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> Histogram Aggregation (" + field + "/" + field + ") " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(dateHistogram("date").field("date").interval(1000))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram Aggregation (date) " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(dateHistogram("date").field("date").interval(1000).subAggregation(stats("stats").field("l_value")))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram Aggregation (date/l_value) " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java
deleted file mode 100644
index 1bf8a33..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-
-public class IncludeExcludeAggregationSearchBenchmark {
-
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = IncludeExcludeAggregationSearchBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 3;
-    private static final int RUNS = 10;
-    private static final int ITERS = 3;
-
-    public static void main(String[] args) {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().create(createIndexRequest("index").settings(settings).mapping("type",
-                    jsonBuilder().startObject().startObject("type").startObject("properties")
-                        .startObject("str")
-                            .field("type", "string")
-                            .field("index", "not_analyzed")
-                        .endObject()
-                    .endObject().endObject().endObject())).actionGet();
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    request.add(client.prepareIndex("index", "type", Integer.toString(i)).setSource("str", TestUtil.randomSimpleString(R)));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh("index").execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            if (i >= WARM) {
-                System.out.println("RUN " + (i - WARM));
-            }
-            long start = System.nanoTime();
-            SearchResponse resp = null;
-            for (int j = 0; j < ITERS; ++j) {
-                resp = client.prepareSearch("index").setQuery(QueryBuilders.prefixQuery("str", "sf")).setSize(0).addAggregation(terms("t").field("str").include("s.*")).execute().actionGet();
-            }
-            long end = System.nanoTime();
-            if (i >= WARM) {
-                System.out.println(new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS));
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java
deleted file mode 100644
index 1d5bebe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
-import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-
-import java.util.Arrays;
-import java.util.LinkedHashMap;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.client.Requests.getRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.percentiles;
-
-public class PercentilesAggregationSearchBenchmark {
-
-    private static final int AMPLITUDE = 10000;
-    private static final int NUM_DOCS = (int) SizeValue.parseSizeValue("1m").singles();
-    private static final int BATCH = 100;
-    private static final String CLUSTER_NAME = PercentilesAggregationSearchBenchmark.class.getSimpleName();
-    private static final double[] PERCENTILES = new double[] { 0, 0.01, 0.1, 1, 10, 25, 50, 75, 90, 99, 99.9, 99.99, 100};
-    private static final int QUERY_WARMUP = 10;
-    private static final int QUERY_COUNT = 20;
-
-    private static Random R = new Random(0);
-
-    // we generate ints to not disadvantage qdigest which only works with integers
-    private enum Distribution {
-        UNIFORM {
-            @Override
-            int next() {
-                return (int) (R.nextDouble() * AMPLITUDE);
-            }
-        },
-        GAUSS {
-            @Override
-            int next() {
-                return (int) (R.nextDouble() * AMPLITUDE);
-            }
-        },
-        LOG_NORMAL {
-            @Override
-            int next() {
-                return (int) Math.exp(R.nextDouble() * Math.log(AMPLITUDE));
-            }
-        };
-        String indexName() {
-            return name().toLowerCase(Locale.ROOT);
-        }
-        abstract int next();
-    }
-
-    private static double accuratePercentile(double percentile, int[] sortedValues) {
-        final double index = percentile / 100 * (sortedValues.length - 1);
-        final int intIndex = (int) index;
-        final double delta = index - intIndex;
-        if (delta == 0) {
-            return sortedValues[intIndex];
-        } else {
-            return sortedValues[intIndex] * (1 - delta) + sortedValues[intIndex + 1] * delta;
-        }
-    }
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 100) // to also test performance and accuracy of the reduce phase
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        for (Distribution d : Distribution.values()) {
-            try {
-//                client.admin().indices().prepareDelete(d.indexName()).execute().actionGet();
-                client.admin().indices().create(createIndexRequest(d.indexName()).settings(settings)).actionGet();
-            } catch (Exception e) {
-                System.out.println("Index " + d.indexName() + " already exists, skipping index creation");
-                continue;
-            }
-
-            final int[] values = new int[NUM_DOCS];
-            for (int i = 0; i < NUM_DOCS; ++i) {
-                values[i] = d.next();
-            }
-            System.out.println("Indexing " + NUM_DOCS + " documents into " + d.indexName());
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    request.add(client.prepareIndex(d.indexName(), "values", Integer.toString(i)).setSource("v", values[i]));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            Arrays.sort(values);
-            XContentBuilder builder = JsonXContent.contentBuilder().startObject();
-            for (double percentile : PERCENTILES) {
-                builder.field(Double.toString(percentile), accuratePercentile(percentile, values));
-            }
-            client.prepareIndex(d.indexName(), "values", "percentiles").setSource(builder.endObject()).execute().actionGet();
-            client.admin().indices().prepareRefresh(d.indexName()).execute().actionGet();
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        System.out.println("## Precision");
-        for (Distribution d : Distribution.values()) {
-            System.out.println("#### " + d);
-            final long count = client.prepareSearch(d.indexName()).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-            if (count != NUM_DOCS + 1) {
-                throw new Error("Expected " + NUM_DOCS + " documents, got " + (count - 1));
-            }
-            Map<String, Object> percentilesUnsorted = client.get(getRequest(d.indexName()).type("values").id("percentiles")).actionGet().getSourceAsMap();
-            SortedMap<Double, Double> percentiles = new TreeMap<>();
-            for (Map.Entry<String, Object> entry : percentilesUnsorted.entrySet()) {
-                percentiles.put(Double.parseDouble(entry.getKey()), (Double) entry.getValue());
-            }
-            System.out.println("Expected percentiles: " + percentiles);
-            System.out.println();
-            SearchResponse resp = client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-            Percentiles pcts = resp.getAggregations().get("pcts");
-            Map<Double, Double> asMap = new LinkedHashMap<>();
-            double sumOfErrorSquares = 0;
-            for (Percentile percentile : pcts) {
-                asMap.put(percentile.getPercent(), percentile.getValue());
-                double error = percentile.getValue() - percentiles.get(percentile.getPercent());
-                sumOfErrorSquares += error * error;
-            }
-            System.out.println("Percentiles: " + asMap);
-            System.out.println("Sum of error squares: " + sumOfErrorSquares);
-            System.out.println();
-        }
-        
-        System.out.println("## Performance");
-        for (int i = 0; i < 3; ++i) {
-            for (Distribution d : Distribution.values()) {
-                System.out.println("#### " + d);
-                for (int j = 0; j < QUERY_WARMUP; ++j) {
-                    client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-                }
-                long start = System.nanoTime();
-                for (int j = 0; j < QUERY_COUNT; ++j) {
-                    client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-                }
-                System.out.println(new TimeValue((System.nanoTime() - start) / QUERY_COUNT, TimeUnit.NANOSECONDS));
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java
deleted file mode 100644
index 7dd0167..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-public class QueryFilterAggregationSearchBenchmark {
-
-    static final long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static final int BATCH = 1000;
-    static final int QUERY_COUNT = 200;
-    static final int NUMBER_OF_TERMS = 200;
-
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 2)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = QueryFilterAggregationSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1")).node();
-        client = node1.client();
-
-        long[] lValues = new long[NUMBER_OF_TERMS];
-        for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-            lValues[i] = ThreadLocalRandom.current().nextLong();
-        }
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test")).actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    builder.field("l_value", lValues[ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS)]);
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 100000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        if (client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits() != COUNT) {
-            throw new Error();
-        }
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        final long anyValue = ((Number) client.prepareSearch().execute().actionGet().getHits().hits()[0].sourceAsMap().get("l_value")).longValue();
-        
-        long totalQueryTime = 0;
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(termQuery("l_value", anyValue))
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("-->  Simple Query on first l_value " + totalQueryTime + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(termQuery("l_value", anyValue))
-                    .addAggregation(AggregationBuilders.filter("filter").filter(QueryBuilders.termQuery("l_value", anyValue)))
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("-->  Filter agg first l_value " + totalQueryTime + "ms");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
deleted file mode 100644
index e58787e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
+++ /dev/null
@@ -1,315 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class SubAggregationSearchCollectModeBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int QUERY_WARMUP = 10;
-    static int QUERY_COUNT = 100;
-    static int NUMBER_OF_TERMS = 200;
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Client client;
-    static Node[] nodes;
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = SubAggregationSearchCollectModeBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        client = clientNode.client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test").mapping("type1", jsonBuilder()
-              .startObject()
-                .startObject("type1")
-                  .startObject("properties")
-                    .startObject("s_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("sm_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("l_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("lm_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                  .endObject()
-                .endObject()
-              .endObject())).actionGet();
-
-            long[] lValues = new long[NUMBER_OF_TERMS];
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                lValues[i] = ThreadLocalRandom.current().nextLong();
-            }
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[ThreadLocalRandom.current().nextInt(sValues.length)];
-                    final long lValue = lValues[ThreadLocalRandom.current().nextInt(lValues.length)];
-                    builder.field("s_value", sValue);
-                    builder.field("l_value", lValue);
-                    builder.field("s_value_dv", sValue);
-                    builder.field("l_value_dv", lValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    for (String field : new String[] {"lm_value", "lm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(lValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(runTest("0000", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0001", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0010", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0011", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0100", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0101", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0110", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0111", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1000", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1001", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1010", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1011", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1100", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1101", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1110", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1111", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%35s%10s%10s%15s%15s\n", "name", "took", "millis", "fieldata size", "heap used");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%35s%10s%10d%15s%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed, stat.heapUsed);
-        }
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-
-        clientNode.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    public static class StatsResult {
-        final String name;
-        final long took;
-        final ByteSizeValue fieldDataMemoryUsed;
-        final ByteSizeValue heapUsed;
-
-        public StatsResult(String name, long took, ByteSizeValue fieldDataMemoryUsed, ByteSizeValue heapUsed) {
-            this.name = name;
-            this.took = took;
-            this.fieldDataMemoryUsed = fieldDataMemoryUsed;
-            this.heapUsed = heapUsed;
-        }
-    }
-
-    private static StatsResult runTest(String name, SubAggCollectionMode[] collectionModes) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name + "s_value").field("s_value").collectMode(collectionModes[0])
-                            .subAggregation(AggregationBuilders.terms(name + "l_value").field("l_value").collectMode(collectionModes[1])
-                                    .subAggregation(AggregationBuilders.terms(name + "s_value_dv").field("s_value_dv").collectMode(collectionModes[2])
-                                            .subAggregation(AggregationBuilders.terms(name + "l_value_dv").field("l_value_dv").collectMode(collectionModes[3])))))
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading : took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name + "s_value").field("s_value").collectMode(collectionModes[0])
-                            .subAggregation(AggregationBuilders.terms(name + "l_value").field("l_value").collectMode(collectionModes[1])
-                                    .subAggregation(AggregationBuilders.terms(name + "s_value_dv").field("s_value_dv").collectMode(collectionModes[2])
-                                            .subAggregation(AggregationBuilders.terms(name + "l_value_dv").field("l_value_dv").collectMode(collectionModes[3])))))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-        ByteSizeValue heapUsed = clusterStateResponse.getNodesStats().getJvm().getHeapUsed();
-        System.out.println("--> Heap used: " + heapUsed);
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed, heapUsed);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
deleted file mode 100644
index 9b544a7..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
+++ /dev/null
@@ -1,354 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.benchmark.search.aggregations.TermsAggregationSearchBenchmark.Method;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TermsAggregationSearchAndIndexingBenchmark {
-
-    static String indexName = "test";
-    static String typeName = "type1";
-    static Random random = new Random();
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int NUMBER_OF_TERMS = (int) SizeValue.parseSizeValue("100k").singles();
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Node[] nodes;
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TermsAggregationSearchAndIndexingBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node1"))
-                    .clusterName(clusterName)
-                    .node();
-        }
-        Client client = nodes[0].client();
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().prepareCreate(indexName)
-                    .addMapping(typeName, generateMapping("eager", "lazy"))
-                    .get();
-            Thread.sleep(5000);
-
-            long startTime = System.currentTimeMillis();
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[counter % sValues.length];
-                    builder.field("s_value", sValue);
-                    builder.field("s_value_dv", sValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    request.add(Requests.indexRequest(indexName).type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH));
-                }
-            }
-
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().preparePutMapping(indexName)
-                .setType(typeName)
-                .setSource(generateMapping("lazy", "lazy"))
-                .get();
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        List<TestRun> testRuns = new ArrayList<>();
-        testRuns.add(new TestRun("Regular field ordinals", "eager", "lazy", "s_value", "ordinals"));
-        testRuns.add(new TestRun("Docvalues field ordinals", "lazy", "eager", "s_value_dv", "ordinals"));
-        testRuns.add(new TestRun("Regular field global ordinals", "eager_global_ordinals", "lazy", "s_value", null));
-        testRuns.add(new TestRun("Docvalues field global", "lazy", "eager_global_ordinals", "s_value_dv", null));
-
-        List<TestResult> testResults = new ArrayList<>();
-        for (TestRun testRun : testRuns) {
-            client.admin().indices().preparePutMapping(indexName).setType(typeName)
-                    .setSource(generateMapping(testRun.indexedFieldEagerLoading, testRun.docValuesEagerLoading)).get();
-            client.admin().indices().prepareClearCache(indexName).setFieldDataCache(true).get();
-            SearchThread searchThread = new SearchThread(client, testRun.termsAggsField, testRun.termsAggsExecutionHint);
-            RefreshThread refreshThread = new RefreshThread(client);
-            System.out.println("--> Running '" + testRun.name + "' round...");
-            new Thread(refreshThread).start();
-            new Thread(searchThread).start();
-            Thread.sleep(2 * 60 * 1000);
-            refreshThread.stop();
-            searchThread.stop();
-
-            System.out.println("--> Avg refresh time: " + refreshThread.avgRefreshTime + " ms");
-            System.out.println("--> Avg query time: " + searchThread.avgQueryTime + " ms");
-
-            ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-            System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-            ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-            System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-            testResults.add(new TestResult(testRun.name, refreshThread.avgRefreshTime, searchThread.avgQueryTime, fieldDataMemoryUsed));
-        }
-
-        System.out.println("----------------------------------------- SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%30s%18s%15s%15s\n", "name", "avg refresh time", "avg query time", "fieldata size");
-        for (TestResult testResult : testResults) {
-            System.out.format(Locale.ENGLISH, "%30s%18s%15s%15s\n", testResult.name, testResult.avgRefreshTime, testResult.avgQueryTime, testResult.fieldDataSizeInMemory);
-        }
-        System.out.println("----------------------------------------- SUMMARY ----------------------------------------------");
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    static class RefreshThread implements Runnable {
-
-        private final Client client;
-        private volatile boolean run = true;
-        private volatile boolean stopped = false;
-        private volatile long avgRefreshTime = 0;
-
-        RefreshThread(Client client) throws IOException {
-            this.client = client;
-        }
-
-        @Override
-        public void run() {
-            long totalRefreshTime = 0;
-            int numExecutedRefreshed = 0;
-            while (run) {
-                long docIdLimit = COUNT;
-                for (long docId = 1; run && docId < docIdLimit;) {
-                    try {
-                        for (int j = 0; j < 8; j++) {
-                            GetResponse getResponse = client
-                                    .prepareGet(indexName, "type1", String.valueOf(++docId))
-                                    .get();
-                            client.prepareIndex(indexName, "type1", getResponse.getId())
-                                    .setSource(getResponse.getSource())
-                                    .get();
-                        }
-                        long startTime = System.currentTimeMillis();
-                        client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                        totalRefreshTime += System.currentTimeMillis() - startTime;
-                        numExecutedRefreshed++;
-                        Thread.sleep(500);
-                    } catch (Throwable e) {
-                        e.printStackTrace();
-                    }
-                }
-            }
-            avgRefreshTime = totalRefreshTime / numExecutedRefreshed;
-            stopped = true;
-        }
-
-        public void stop() throws InterruptedException {
-            run = false;
-            while (!stopped) {
-                Thread.sleep(100);
-            }
-        }
-
-    }
-
-    private static class TestRun {
-
-        final String name;
-        final String indexedFieldEagerLoading;
-        final String docValuesEagerLoading;
-        final String termsAggsField;
-        final String termsAggsExecutionHint;
-
-        private TestRun(String name, String indexedFieldEagerLoading, String docValuesEagerLoading, String termsAggsField, String termsAggsExecutionHint) {
-            this.name = name;
-            this.indexedFieldEagerLoading = indexedFieldEagerLoading;
-            this.docValuesEagerLoading = docValuesEagerLoading;
-            this.termsAggsField = termsAggsField;
-            this.termsAggsExecutionHint = termsAggsExecutionHint;
-        }
-    }
-
-    private static class TestResult {
-
-        final String name;
-        final TimeValue avgRefreshTime;
-        final TimeValue avgQueryTime;
-        final ByteSizeValue fieldDataSizeInMemory;
-
-        private TestResult(String name, long avgRefreshTime, long avgQueryTime, ByteSizeValue fieldDataSizeInMemory) {
-            this.name = name;
-            this.avgRefreshTime = TimeValue.timeValueMillis(avgRefreshTime);
-            this.avgQueryTime = TimeValue.timeValueMillis(avgQueryTime);
-            this.fieldDataSizeInMemory = fieldDataSizeInMemory;
-        }
-    }
-
-    static class SearchThread implements Runnable {
-
-        private final Client client;
-        private final String field;
-        private final String executionHint;
-        private volatile boolean run = true;
-        private volatile boolean stopped = false;
-        private volatile long avgQueryTime = 0;
-
-        SearchThread(Client client, String field, String executionHint) {
-            this.client = client;
-            this.field = field;
-            this.executionHint = executionHint;
-        }
-
-        @Override
-        public void run() {
-            long totalQueryTime = 0;
-            int numExecutedQueries = 0;
-            while (run) {
-                try {
-                    SearchResponse searchResponse = Method.AGGREGATION.addTermsAgg(client.prepareSearch()
-                            .setSize(0)
-                            .setQuery(matchAllQuery()), "test", field, executionHint)
-                            .execute().actionGet();
-                    if (searchResponse.getHits().totalHits() != COUNT) {
-                        System.err.println("--> mismatch on hits");
-                    }
-                    totalQueryTime += searchResponse.getTookInMillis();
-                    numExecutedQueries++;
-                } catch (Throwable e) {
-                    e.printStackTrace();
-                }
-            }
-            avgQueryTime = totalQueryTime / numExecutedQueries;
-            stopped = true;
-        }
-
-        public void stop() throws InterruptedException {
-            run = false;
-            while (!stopped) {
-                Thread.sleep(100);
-            }
-        }
-
-    }
-
-    private static XContentBuilder generateMapping(String loading1, String loading2) throws IOException {
-        return jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("s_value")
-                .field("type", "string")
-                .field("index", "not_analyzed")
-                .startObject("fielddata")
-                .field("loading", loading1)
-                .endObject()
-                .endObject()
-                .startObject("s_value_dv")
-                .field("type", "string")
-                .field("index", "no")
-                .startObject("fielddata")
-                .field("loading", loading2)
-                .field("format", "doc_values")
-                .endObject()
-                .endObject()
-                .endObject().endObject().endObject();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
deleted file mode 100644
index e63fbfe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,403 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TermsAggregationSearchBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int QUERY_WARMUP = 10;
-    static int QUERY_COUNT = 100;
-    static int NUMBER_OF_TERMS = 200;
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Client client;
-    static Node[] nodes;
-
-    public enum Method {
-        AGGREGATION {
-            @Override
-            SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint) {
-                return builder.addAggregation(AggregationBuilders.terms(name).executionHint(executionHint).field(field));
-            }
-
-            @Override
-            SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField) {
-                return builder.addAggregation(AggregationBuilders.terms(name).field(keyField).subAggregation(AggregationBuilders.stats("stats").field(valueField)));
-            }
-        },
-        AGGREGATION_DEFERRED {
-            @Override
-            SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint) {
-                return builder.addAggregation(AggregationBuilders.terms(name).executionHint(executionHint).field(field).collectMode(SubAggCollectionMode.BREADTH_FIRST));
-            }
-
-            @Override
-            SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField) {
-                return builder.addAggregation(AggregationBuilders.terms(name).field(keyField).collectMode(SubAggCollectionMode.BREADTH_FIRST).subAggregation(AggregationBuilders.stats("stats").field(valueField)));
-            }
-        };
-        abstract SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint);
-        abstract SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField);
-    }
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TermsAggregationSearchBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("path.home", "."))
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "client"))
-                .settings(settingsBuilder().put(settings).put("path.home", ".")).client(true).node();
-
-        client = clientNode.client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test").mapping("type1", jsonBuilder()
-              .startObject()
-                .startObject("type1")
-                  .startObject("properties")
-                    .startObject("s_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("sm_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("l_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("lm_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                  .endObject()
-                .endObject()
-              .endObject())).actionGet();
-
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[ThreadLocalRandom.current().nextInt(sValues.length)];
-                    final long lValue = ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS);
-                    builder.field("s_value", sValue);
-                    builder.field("l_value", lValue);
-                    builder.field("s_value_dv", sValue);
-                    builder.field("l_value_dv", lValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    for (String field : new String[] {"lm_value", "lm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS));
-                        }
-                        builder.endArray();
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForYellowStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(terms("terms_agg_s", Method.AGGREGATION, "s_value", null));
-        stats.add(terms("terms_agg_s_dv", Method.AGGREGATION, "s_value_dv", null));
-        stats.add(terms("terms_agg_map_s", Method.AGGREGATION, "s_value", "map"));
-        stats.add(terms("terms_agg_map_s_dv", Method.AGGREGATION, "s_value_dv", "map"));
-        stats.add(terms("terms_agg_def_s", Method.AGGREGATION_DEFERRED, "s_value", null));
-        stats.add(terms("terms_agg_def_s_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", null));
-        stats.add(terms("terms_agg_def_map_s", Method.AGGREGATION_DEFERRED, "s_value", "map"));
-        stats.add(terms("terms_agg_def_map_s_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "map"));
-        stats.add(terms("terms_agg_l", Method.AGGREGATION, "l_value", null));
-        stats.add(terms("terms_agg_l_dv", Method.AGGREGATION, "l_value_dv", null));
-        stats.add(terms("terms_agg_def_l", Method.AGGREGATION_DEFERRED, "l_value", null));
-        stats.add(terms("terms_agg_def_l_dv", Method.AGGREGATION_DEFERRED, "l_value_dv", null));
-        stats.add(terms("terms_agg_sm", Method.AGGREGATION, "sm_value", null));
-        stats.add(terms("terms_agg_sm_dv", Method.AGGREGATION, "sm_value_dv", null));
-        stats.add(terms("terms_agg_map_sm", Method.AGGREGATION, "sm_value", "map"));
-        stats.add(terms("terms_agg_map_sm_dv", Method.AGGREGATION, "sm_value_dv", "map"));
-        stats.add(terms("terms_agg_def_sm", Method.AGGREGATION_DEFERRED, "sm_value", null));
-        stats.add(terms("terms_agg_def_sm_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", null));
-        stats.add(terms("terms_agg_def_map_sm", Method.AGGREGATION_DEFERRED, "sm_value", "map"));
-        stats.add(terms("terms_agg_def_map_sm_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "map"));
-        stats.add(terms("terms_agg_lm", Method.AGGREGATION, "lm_value", null));
-        stats.add(terms("terms_agg_lm_dv", Method.AGGREGATION, "lm_value_dv", null));
-        stats.add(terms("terms_agg_def_lm", Method.AGGREGATION_DEFERRED, "lm_value", null));
-        stats.add(terms("terms_agg_def_lm_dv", Method.AGGREGATION_DEFERRED, "lm_value_dv", null));
-
-        stats.add(termsStats("terms_stats_agg_s_l", Method.AGGREGATION, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_s_l_dv", Method.AGGREGATION, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l", Method.AGGREGATION_DEFERRED, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_s_lm", Method.AGGREGATION, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_s_lm_dv", Method.AGGREGATION, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm", Method.AGGREGATION_DEFERRED, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_sm_l", Method.AGGREGATION, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_sm_l_dv", Method.AGGREGATION, "sm_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l", Method.AGGREGATION_DEFERRED, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "l_value_dv", null));
-
-        stats.add(termsStats("terms_stats_agg_s_l", Method.AGGREGATION, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_s_l_dv", Method.AGGREGATION, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l", Method.AGGREGATION_DEFERRED, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_s_lm", Method.AGGREGATION, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_s_lm_dv", Method.AGGREGATION, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm", Method.AGGREGATION_DEFERRED, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_sm_l", Method.AGGREGATION, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_sm_l_dv", Method.AGGREGATION, "sm_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l", Method.AGGREGATION_DEFERRED, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "l_value_dv", null));
-
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%35s%10s%10s%15s\n", "name", "took", "millis", "fieldata size");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%35s%10s%10d%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed);
-        }
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-
-        clientNode.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    public static class StatsResult {
-        final String name;
-        final long took;
-        final ByteSizeValue fieldDataMemoryUsed;
-
-        public StatsResult(String name, long took, ByteSizeValue fieldDataMemoryUsed) {
-            this.name = name;
-            this.took = took;
-            this.fieldDataMemoryUsed = fieldDataMemoryUsed;
-        }
-    }
-
-    private static StatsResult terms(String name, Method method, String field, String executionHint) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = method.addTermsAgg(client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, field, executionHint)
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = method.addTermsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, field, executionHint)
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-        System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed);
-    }
-
-    private static StatsResult termsStats(String name, Method method, String keyField, String valueField, String executionHint) {
-        long totalQueryTime;
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = method.addTermsStatsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, keyField, valueField)
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading (" + name + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = method.addTermsStatsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, keyField, valueField)
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms stats agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-        return new StatsResult(name, totalQueryTime, ByteSizeValue.parseBytesSizeValue("0b", "StatsResult"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java
deleted file mode 100644
index 0e16b07..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java
+++ /dev/null
@@ -1,262 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TimeDataHistogramAggregationBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static long TIME_PERIOD = 24 * 3600 * 1000;
-    static int BATCH = 100;
-    static int QUERY_WARMUP = 50;
-    static int QUERY_COUNT = 500;
-    static IndexFieldData.CommonSettings.MemoryStorageFormat MEMORY_FORMAT = IndexFieldData.CommonSettings.MemoryStorageFormat.PAGED;
-    static double ACCEPTABLE_OVERHEAD_RATIO = 0.5;
-    static float MATCH_PERCENTAGE = 0.1f;
-
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put("node.local", true)
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TimeDataHistogramAggregationBenchmark.class.getSimpleName();
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        client = nodes[0].client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test")).actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            long[] currentTimeInMillis1 = new long[]{System.currentTimeMillis()};
-            long[] currentTimeInMillis2 = new long[]{System.currentTimeMillis()};
-            long startTimeInMillis = currentTimeInMillis1[0];
-            long averageMillisChange = TIME_PERIOD / COUNT * 2;
-            long backwardSkew = Math.max(1, (long) (averageMillisChange * 0.1));
-            long bigOutOfOrder = 1;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    // move forward in time and sometimes a little bit back (delayed delivery)
-                    long diff = ThreadLocalRandom.current().nextLong(2 * averageMillisChange + 2 * backwardSkew) - backwardSkew;
-                    long[] currentTime = counter % 2 == 0 ? currentTimeInMillis1 : currentTimeInMillis2;
-                    currentTime[0] += diff;
-                    if (ThreadLocalRandom.current().nextLong(100) <= bigOutOfOrder) {
-                        builder.field("l_value", currentTime[0] - 60000); // 1m delays
-                    } else {
-                        builder.field("l_value", currentTime[0]);
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-            System.out.println("Time range 1: " + (currentTimeInMillis1[0] - startTimeInMillis) / 1000.0 / 3600 + " hours");
-            System.out.println("Time range 2: " + (currentTimeInMillis2[0] - startTimeInMillis) / 1000.0 / 3600 + " hours");
-            System.out.println("--> optimizing index");
-            client.admin().indices().prepareForceMerge().setMaxNumSegments(1).get();
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        // load with the reverse options to make sure jit doesn't optimize one away
-        setMapping(ACCEPTABLE_OVERHEAD_RATIO, MEMORY_FORMAT.equals(IndexFieldData.CommonSettings.MemoryStorageFormat.PACKED) ? IndexFieldData.CommonSettings.MemoryStorageFormat.PAGED : IndexFieldData.CommonSettings.MemoryStorageFormat.PACKED);
-        warmUp("hist_l", "l_value", MATCH_PERCENTAGE);
-
-        setMapping(ACCEPTABLE_OVERHEAD_RATIO, MEMORY_FORMAT);
-        warmUp("hist_l", "l_value", MATCH_PERCENTAGE);
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(measureAgg("hist_l", "l_value", MATCH_PERCENTAGE));
-
-        NodesStatsResponse nodeStats = client.admin().cluster().prepareNodesStats(nodes[0].settings().get("name")).clear()
-                .setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.FieldData)).get();
-
-
-        System.out.println("------------------ SUMMARY -------------------------------");
-
-        System.out.println("docs: " + COUNT);
-        System.out.println("match percentage: " + MATCH_PERCENTAGE);
-        System.out.println("memory format hint: " + MEMORY_FORMAT);
-        System.out.println("acceptable_overhead_ratio: " + ACCEPTABLE_OVERHEAD_RATIO);
-        System.out.println("field data: " + nodeStats.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.format(Locale.ROOT, "%25s%10s%10s\n", "name", "took", "millis");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ROOT, "%25s%10s%10d\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT));
-        }
-        System.out.println("------------------ SUMMARY -------------------------------");
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    protected static void setMapping(double acceptableOverheadRatio, IndexFieldData.CommonSettings.MemoryStorageFormat fielddataStorageFormat) throws IOException {
-        XContentBuilder mapping = JsonXContent.contentBuilder();
-        mapping.startObject().startObject("type1").startObject("properties").startObject("l_value")
-                .field("type", "long")
-                .startObject("fielddata")
-                .field("acceptable_transient_overhead_ratio", acceptableOverheadRatio)
-                .field("acceptable_overhead_ratio", acceptableOverheadRatio)
-                .field(IndexFieldData.CommonSettings.SETTING_MEMORY_STORAGE_HINT, fielddataStorageFormat.name().toLowerCase(Locale.ROOT))
-                .endObject()
-                .endObject().endObject().endObject().endObject();
-        client.admin().indices().preparePutMapping("test").setType("type1").setSource(mapping).get();
-    }
-
-    static class StatsResult {
-        final String name;
-        final long took;
-
-        StatsResult(String name, long took) {
-            this.name = name;
-            this.took = took;
-        }
-    }
-
-    private static SearchResponse doTermsAggsSearch(String name, String field, float matchPercentage) {
-        Map<String, Object> params = new HashMap<>();
-        params.put("matchP", matchPercentage);
-        SearchResponse response = client.prepareSearch()
-                .setSize(0)
-                .setQuery(
-                        QueryBuilders.constantScoreQuery(QueryBuilders.scriptQuery(new Script("random()<matchP", ScriptType.INLINE, null,
-                                params))))
-                .addAggregation(AggregationBuilders.histogram(name).field(field).interval(3600 * 1000)).get();
-
-        if (response.getHits().totalHits() < COUNT * matchPercentage * 0.7) {
-            System.err.println("--> warning - big deviation from expected count: " + response.getHits().totalHits() + " expected: " + COUNT * matchPercentage);
-        }
-
-        return response;
-    }
-
-    private static StatsResult measureAgg(String name, String field, float matchPercentage) {
-        long totalQueryTime;// LM VALUE
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        long previousCount = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = doTermsAggsSearch(name, field, matchPercentage);
-            if (previousCount == 0) {
-                previousCount = searchResponse.getHits().getTotalHits();
-            } else if (searchResponse.getHits().totalHits() != previousCount) {
-                System.err.println("*** HIT COUNT CHANGE -> CACHE EXPIRED? ***");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram aggregations (" + field + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-        return new StatsResult(name, totalQueryTime);
-    }
-
-    private static void warmUp(String name, String field, float matchPercentage) {
-        System.out.println("--> Warmup (" + name + ")...");
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = doTermsAggsSearch(name, field, matchPercentage);
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java
deleted file mode 100644
index ffc7eb9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java
+++ /dev/null
@@ -1,216 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-
-import java.util.Arrays;
-import java.util.Random;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchAndIndexingBenchmark {
-
-    static int PARENT_COUNT = (int) SizeValue.parseSizeValue("1m").singles();
-    static int NUM_CHILDREN_PER_PARENT = 12;
-    static int QUERY_VALUE_RATIO_PER_PARENT = 3;
-    static int QUERY_COUNT = 50;
-    static String indexName = "test";
-    static Random random = new Random();
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ChildSearchAndIndexingBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node1"))
-                .clusterName(clusterName)
-                .node();
-        Client client = node1.client();
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-
-            long startTime = System.currentTimeMillis();
-            ParentChildIndexGenerator generator = new ParentChildIndexGenerator(client, PARENT_COUNT, NUM_CHILDREN_PER_PARENT, QUERY_VALUE_RATIO_PER_PARENT);
-            generator.index();
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        SearchThread searchThread = new SearchThread(client);
-        new Thread(searchThread).start();
-        IndexThread indexThread = new IndexThread(client);
-        new Thread(indexThread).start();
-
-        System.in.read();
-
-        indexThread.stop();
-        searchThread.stop();
-        client.close();
-        node1.close();
-    }
-
-    static class IndexThread implements Runnable {
-
-        private final Client client;
-        private volatile boolean run = true;
-
-        IndexThread(Client client) {
-            this.client = client;
-        }
-
-        @Override
-        public void run() {
-            while (run) {
-                int childIdLimit = PARENT_COUNT * NUM_CHILDREN_PER_PARENT;
-                for (int childId = 1; run && childId < childIdLimit;) {
-                    try {
-                        for (int j = 0; j < 8; j++) {
-                            GetResponse getResponse = client
-                                    .prepareGet(indexName, "child", String.valueOf(++childId))
-                                    .setFields("_source", "_parent")
-                                    .setRouting("1") // Doesn't matter what value, since there is only one shard
-                                    .get();
-                            client.prepareIndex(indexName, "child", Integer.toString(childId) + "_" + j)
-                                    .setParent(getResponse.getField("_parent").getValue().toString())
-                                    .setSource(getResponse.getSource())
-                                    .get();
-                        }
-                        client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                        Thread.sleep(1000);
-                        if (childId % 500 == 0) {
-                            NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                                    .clear().setIndices(true).execute().actionGet();
-                            System.out.println("Deleted docs: " + statsResponse.getAt(0).getIndices().getDocs().getDeleted());
-                        }
-                    } catch (Throwable e) {
-                        e.printStackTrace();
-                    }
-                }
-            }
-        }
-
-        public void stop() {
-            run = false;
-        }
-
-    }
-
-    static class SearchThread implements Runnable {
-
-        private final Client client;
-        private final int numValues;
-        private volatile boolean run = true;
-
-        SearchThread(Client client) {
-            this.client = client;
-            this.numValues = NUM_CHILDREN_PER_PARENT / NUM_CHILDREN_PER_PARENT;
-        }
-
-        @Override
-        public void run() {
-            while (run) {
-                try {
-                    long totalQueryTime = 0;
-                    for (int j = 0; j < QUERY_COUNT; j++) {
-                        SearchResponse searchResponse = client.prepareSearch(indexName)
-                                .setQuery(
-                                        boolQuery()
-                                        .must(matchAllQuery())
-                                        .filter(hasChildQuery("child", termQuery("field2", "value" + random.nextInt(numValues)))
-                                        )
-                                )
-                                .execute().actionGet();
-                        if (searchResponse.getFailedShards() > 0) {
-                            System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-                        }
-                        totalQueryTime += searchResponse.getTookInMillis();
-                    }
-                    System.out.println("--> has_child filter with term filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-                    totalQueryTime = 0;
-                    for (int j = 1; j <= QUERY_COUNT; j++) {
-                        SearchResponse searchResponse = client.prepareSearch(indexName)
-                                .setQuery(
-                                        boolQuery()
-                                        .must(matchAllQuery())
-                                        .filter(hasChildQuery("child", matchAllQuery()))
-                                )
-                                .execute().actionGet();
-                        if (searchResponse.getFailedShards() > 0) {
-                            System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-                        }
-                        totalQueryTime += searchResponse.getTookInMillis();
-                    }
-                    System.out.println("--> has_child filter with match_all child query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-                    NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                            .setJvm(true).execute().actionGet();
-                    System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-                    System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-                    Thread.sleep(1000);
-                } catch (Throwable e) {
-                    e.printStackTrace();
-                }
-            }
-        }
-
-        public void stop() {
-            run = false;
-        }
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
deleted file mode 100644
index 8889801..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
+++ /dev/null
@@ -1,344 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.bucket.children.Children;
-
-import java.util.Arrays;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchBenchmark {
-
-    /*
-        Run: MAVEN_OPTS=-Xmx4g mvn test-compile exec:java -Dexec.mainClass="org.elasticsearch.benchmark.search.child.ChildSearchBenchmark" -Dexec.classpathScope="test" -Dexec.args="bwc false"
-     */
-
-    public static void main(String[] args) throws Exception {
-        boolean bwcMode = false;
-        int numParents = (int) SizeValue.parseSizeValue("2m").singles();;
-
-        if (args.length % 2 != 0) {
-            throw new IllegalArgumentException("Uneven number of arguments");
-        }
-        for (int i = 0; i < args.length; i += 2) {
-            String value = args[i + 1];
-            if ("--bwc_mode".equals(args[i])) {
-                bwcMode = Boolean.valueOf(value);
-            } else if ("--num_parents".equals(args[i])) {
-                numParents = Integer.valueOf(value);
-            }
-        }
-
-
-        Settings.Builder settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0);
-
-        // enable bwc parent child mode:
-        if (bwcMode) {
-            settings.put("tests.mock.version", Version.V_1_6_0);
-        }
-
-        String clusterName = ChildSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings.build()).put("name", "node1")).node();
-        Client client = node1.client();
-
-        int CHILD_COUNT = 15;
-        int QUERY_VALUE_RATIO = 3;
-        int QUERY_WARMUP = 10;
-        int QUERY_COUNT = 20;
-        String indexName = "test";
-
-        ParentChildIndexGenerator parentChildIndexGenerator = new ParentChildIndexGenerator(client, numParents, CHILD_COUNT, QUERY_VALUE_RATIO);
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-            long startTime = System.currentTimeMillis();
-            parentChildIndexGenerator.index();
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch(indexName).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        System.out.println("--> Running just child query");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(termQuery("child.tag", "tag1")).execute().actionGet();
-        }
-
-        long totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(termQuery("child.tag", "tag1")).execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Just Child Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-        
-        // run parent child constant query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running has_child filter with match_all child query");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", matchAllQuery()))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child filter with match_all child query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-
-        System.out.println("--> Running children agg");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(matchQuery("field1", parentChildIndexGenerator.getQueryValue()))
-                    .addAggregation(
-                            AggregationBuilders.children("to-child").childType("child")
-                    )
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            Children children = searchResponse.getAggregations().get("to-child");
-            if (j % 10 == 0) {
-                System.out.println("--> children doc count [" + j + "], got [" + children.getDocCount() + "]");
-            }
-        }
-        System.out.println("--> children agg, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running children agg with match_all");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .addAggregation(
-                            AggregationBuilders.children("to-child").childType("child")
-                    )
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            Children children = searchResponse.getAggregations().get("to-child");
-            if (j % 10 == 0) {
-                System.out.println("--> children doc count [" + j + "], got [" + children.getDocCount() + "]");
-            }
-        }
-        System.out.println("--> children agg, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        // run parent child constant query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-        }
-
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running has_parent filter with match_all parent query ");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", matchAllQuery()))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent filter with match_all parent query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        System.out.println("--> Running has_child query with score type");
-        // run parent child score query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", matchAllQuery()).scoreMode(ScoreMode.Max)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child query with match_all Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        
-        System.out.println("--> Running has_parent query with score type");
-        // run parent child score query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
-        }
-
-        totalQueryTime = 0;
-        for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", matchAllQuery()).score(true)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent query with match_all Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.gc();
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        client.close();
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
deleted file mode 100644
index 0db0303..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
+++ /dev/null
@@ -1,210 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchShortCircuitBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ChildSearchShortCircuitBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1"))
-                .node();
-        Client client = node1.client();
-
-        long PARENT_COUNT = SizeValue.parseSizeValue("10M").singles();
-        int BATCH = 100;
-        int QUERY_WARMUP = 5;
-        int QUERY_COUNT = 25;
-        String indexName = "test";
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + PARENT_COUNT + "] parent document and some child documents");
-            long ITERS = PARENT_COUNT / BATCH;
-            int i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-                    request.add(Requests.indexRequest(indexName).type("parent").id(Integer.toString(counter))
-                            .source(parentSource(counter)));
-
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + "parent docs; took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            int id = 0;
-            for (i = 1; i <= PARENT_COUNT; i *= 2) {
-                int parentId = 1;
-                for (int j = 0; j < i; j++) {
-                    client.prepareIndex(indexName, "child", Integer.toString(id++))
-                            .setParent(Integer.toString(parentId++))
-                            .setSource(childSource(i))
-                            .execute().actionGet();
-                }
-            }
-
-            System.out.println("--> Indexing took " + stopWatch.totalTime());
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch(indexName).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        System.out.println("--> Running just child query");
-        // run just the child query, warm up first
-        for (int i = 1; i <= 10000; i *= 2) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(matchQuery("child.field2", i)).execute().actionGet();
-            System.out.println("--> Warmup took["+ i +"]: " + searchResponse.getTook());
-            if (searchResponse.getHits().totalHits() != i) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        // run parent child constant query
-        for (int j = 1; j < QUERY_WARMUP; j *= 2) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            hasChildQuery("child", matchQuery("field2", j))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (searchResponse.getHits().totalHits() != j) {
-                System.err.println("--> mismatch on hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "], expected [" + PARENT_COUNT + "]");
-            }
-        }
-
-        long totalQueryTime = 0;
-        for (int i = 1; i < PARENT_COUNT; i *= 2) {
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch(indexName)
-                        .setQuery(boolQuery().must(matchAllQuery()).filter(hasChildQuery("child", matchQuery("field2", i))))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != i) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> has_child filter " + i +" Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        totalQueryTime = 0;
-        for (int i = 1; i < PARENT_COUNT; i *= 2) {
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch(indexName)
-                        .setQuery(hasChildQuery("child", matchQuery("field2", i)).scoreMode(ScoreMode.Max))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != i) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> has_child query " + i +" Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        System.gc();
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        client.close();
-        node1.close();
-    }
-
-    private static XContentBuilder parentSource(int val) throws IOException {
-        return jsonBuilder().startObject().field("field1", Integer.toString(val)).endObject();
-    }
-
-    private static XContentBuilder childSource(int val) throws IOException {
-        return jsonBuilder().startObject().field("field2", Integer.toString(val)).endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java
deleted file mode 100644
index 1d02a1f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.child;
-
-import com.carrotsearch.hppc.ObjectArrayList;
-import com.carrotsearch.hppc.ObjectHashSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-
-import java.util.Random;
-
-/**
- */
-public class ParentChildIndexGenerator {
-
-    private final static Random RANDOM = new Random();
-
-    private final Client client;
-    private final int numParents;
-    private final int numChildrenPerParent;
-    private final int queryValueRatio;
-
-    public ParentChildIndexGenerator(Client client, int numParents, int numChildrenPerParent, int queryValueRatio) {
-        this.client = client;
-        this.numParents = numParents;
-        this.numChildrenPerParent = numChildrenPerParent;
-        this.queryValueRatio = queryValueRatio;
-    }
-
-    public void index() {
-        // Memory intensive...
-        ObjectHashSet<String> usedParentIds = new ObjectHashSet<>(numParents, 0.5d);
-        ObjectArrayList<ParentDocument> parents = new ObjectArrayList<>(numParents);
-
-        for (int i = 0; i < numParents; i++) {
-            String parentId;
-            do {
-                parentId = RandomStrings.randomAsciiOfLength(RANDOM, 10);
-            } while (!usedParentIds.add(parentId));
-            String[] queryValues = new String[numChildrenPerParent];
-            for (int j = 0; j < numChildrenPerParent; j++) {
-                queryValues[j] = getQueryValue();
-            }
-            parents.add(new ParentDocument(parentId, queryValues));
-        }
-
-        int indexCounter = 0;
-        int childIdCounter = 0;
-        while (!parents.isEmpty()) {
-            BulkRequestBuilder request = client.prepareBulk();
-            for (int i = 0; !parents.isEmpty() && i < 100; i++) {
-                int index = RANDOM.nextInt(parents.size());
-                ParentDocument parentDocument = parents.get(index);
-
-                if (parentDocument.indexCounter == -1) {
-                    request.add(Requests.indexRequest("test").type("parent")
-                            .id(parentDocument.parentId)
-                            .source("field1", getQueryValue()));
-                } else {
-                    request.add(Requests.indexRequest("test").type("child")
-                            .parent(parentDocument.parentId)
-                            .id(String.valueOf(++childIdCounter))
-                            .source("field2", parentDocument.queryValues[parentDocument.indexCounter]));
-                }
-
-                if (++parentDocument.indexCounter == parentDocument.queryValues.length) {
-                    parents.remove(index);
-                }
-            }
-
-            BulkResponse response = request.execute().actionGet();
-            if (response.hasFailures()) {
-                System.err.println("--> failures...");
-            }
-
-            indexCounter += response.getItems().length;
-            if (indexCounter % 100000 == 0) {
-                System.out.println("--> Indexed " + indexCounter + " documents");
-            }
-        }
-    }
-
-    public String getQueryValue() {
-        return "value" + RANDOM.nextInt(numChildrenPerParent / queryValueRatio);
-    }
-
-    class ParentDocument {
-
-        final String parentId;
-        final String[] queryValues;
-        int indexCounter;
-
-        ParentDocument(String parentId, String[] queryValues) {
-            this.parentId = parentId;
-            this.queryValues = queryValues;
-            this.indexCounter = -1;
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java
deleted file mode 100644
index 55c2918..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java
+++ /dev/null
@@ -1,207 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.geo;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.geoDistanceQuery;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-/**
- */
-public class GeoDistanceSearchBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        Node node = NodeBuilder.nodeBuilder().clusterName(GeoDistanceSearchBenchmark.class.getSimpleName()).node();
-        Client client = node.client();
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("Failed to wait for green status, bailing");
-            exit(1);
-        }
-
-        final long NUM_DOCS = SizeValue.parseSizeValue("1m").singles();
-        final long NUM_WARM = 50;
-        final long NUM_RUNS = 100;
-
-        if (client.admin().indices().prepareExists("test").execute().actionGet().isExists()) {
-            System.out.println("Found an index, count: " + client.prepareSearch("test").setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet().getHits().totalHits());
-        } else {
-            String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("properties").startObject("location").field("type", "geo_point").field("lat_lon", true).endObject().endObject()
-                    .endObject().endObject().string();
-            client.admin().indices().prepareCreate("test")
-                    .setSettings(Settings.settingsBuilder().put("index.number_of_shards", 1).put("index.number_of_replicas", 0))
-                    .addMapping("type1", mapping)
-                    .execute().actionGet();
-
-            System.err.println("--> Indexing [" + NUM_DOCS + "]");
-            for (long i = 0; i < NUM_DOCS; ) {
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "New York")
-                        .startObject("location").field("lat", 40.7143528).field("lon", -74.0059731).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 5.286 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Times Square")
-                        .startObject("location").field("lat", 40.759011).field("lon", -73.9844722).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 0.4621 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Tribeca")
-                        .startObject("location").field("lat", 40.718266).field("lon", -74.007819).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 1.258 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Soho")
-                        .startObject("location").field("lat", 40.7247222).field("lon", -74).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 8.572 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Brooklyn")
-                        .startObject("location").field("lat", 40.65).field("lon", -73.95).endObject()
-                        .endObject()).execute().actionGet();
-
-                if ((i % 10000) == 0) {
-                    System.err.println("--> indexed " + i);
-                }
-            }
-            System.err.println("Done indexed");
-            client.admin().indices().prepareFlush("test").execute().actionGet();
-            client.admin().indices().prepareRefresh().execute().actionGet();
-        }
-
-        System.err.println("--> Warming up (ARC) - optimize_bbox");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "memory");
-        }
-        long totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC)  - optimize_bbox (memory) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - optimize_bbox (memory)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - optimize_bbox " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (ARC)  - optimize_bbox (indexed)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "indexed");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC) - optimize_bbox (indexed) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - optimize_bbox (indexed)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "indexed");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - optimize_bbox (indexed) " + (totalTime / NUM_RUNS) + "ms");
-
-
-        System.err.println("--> Warming up (ARC)  - no optimize_bbox");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "none");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC) - no optimize_bbox " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - no optimize_bbox");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "none");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - no optimize_bbox " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (SLOPPY_ARC)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.SLOPPY_ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (SLOPPY_ARC) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (SLOPPY_ARC)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.SLOPPY_ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (SLOPPY_ARC) " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (PLANE)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.PLANE, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (PLANE) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (PLANE)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.PLANE, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (PLANE) " + (totalTime / NUM_RUNS) + "ms");
-
-        node.close();
-    }
-
-    public static void run(Client client, GeoDistance geoDistance, String optimizeBbox) {
-        client.prepareSearch() // from NY
-                .setSize(0)
-                .setQuery(boolQuery().must(matchAllQuery()).filter(geoDistanceQuery("location")
-                        .distance("2km")
-                        .optimizeBbox(optimizeBbox)
-                        .geoDistance(geoDistance)
-                        .point(40.7143528, -74.0059731)))
-                .execute().actionGet();
-    }
-
-    @SuppressForbidden(reason = "Allowed to exit explicitly from #main()")
-    private static void exit(int status) {
-        System.exit(status);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java
deleted file mode 100644
index 1aa3310..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.nested;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class NestedSearchBenchMark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node node1 = nodeBuilder()
-                .settings(settingsBuilder().put(settings).put("name", "node1"))
-                .node();
-        Client client = node1.client();
-
-        int count = (int) SizeValue.parseSizeValue("1m").singles();
-        int nestedCount = 10;
-        int rootDocs = count / nestedCount;
-        int batch = 100;
-        int queryWarmup = 5;
-        int queryCount = 500;
-        String indexName = "test";
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth()
-                .setWaitForGreenStatus().execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        try {
-            client.admin().indices().prepareCreate(indexName)
-                    .addMapping("type", XContentFactory.jsonBuilder()
-                            .startObject()
-                            .startObject("type")
-                            .startObject("properties")
-                            .startObject("field1")
-                            .field("type", "integer")
-                            .endObject()
-                            .startObject("field2")
-                            .field("type", "nested")
-                            .startObject("properties")
-                            .startObject("field3")
-                            .field("type", "integer")
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                    ).execute().actionGet();
-            clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + rootDocs + "] root documents and [" + (rootDocs * nestedCount) + "] nested objects");
-            long ITERS = rootDocs / batch;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < batch; j++) {
-                    counter++;
-                    XContentBuilder doc = XContentFactory.jsonBuilder().startObject()
-                            .field("field1", counter)
-                            .startArray("field2");
-                    for (int k = 0; k < nestedCount; k++) {
-                        doc = doc.startObject()
-                                .field("field3", k)
-                                .endObject();
-                    }
-                    doc = doc.endArray();
-                    request.add(
-                            Requests.indexRequest(indexName).type("type").id(Integer.toString(counter)).source(doc)
-                    );
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * batch) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * batch) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (count * (1 + nestedCount))) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        System.out.println("--> Running match_all with sorting on nested field");
-        // run just the child query, warm up first
-        for (int j = 0; j < queryWarmup; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addSort(
-                            SortBuilders.fieldSort("field2.field3")
-                                    .setNestedPath("field2")
-                                    .sortMode("avg")
-                                    .order(SortOrder.ASC)
-                    )
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Warmup took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != rootDocs) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-
-        long totalQueryTime = 0;
-        for (int j = 0; j < queryCount; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addSort(
-                            SortBuilders.fieldSort("field2.field3")
-                                    .setNestedPath("field2")
-                                    .sortMode("avg")
-                                    .order(j % 2 == 0 ? SortOrder.ASC : SortOrder.DESC)
-                    )
-                    .execute().actionGet();
-
-            if (searchResponse.getHits().totalHits() != rootDocs) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Sorting by nested fields took: " + (totalQueryTime / queryCount) + "ms");
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java
deleted file mode 100644
index 363facc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.scroll;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.monitor.jvm.JvmStats;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.Locale;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class ScrollSearchBenchmark {
-
-    // Run with: -Xms1G -Xms1G
-    public static void main(String[] args) {
-        String indexName = "test";
-        String typeName = "type";
-        String clusterName = ScrollSearchBenchmark.class.getSimpleName();
-        long numDocs = SizeValue.parseSizeValue("300k").singles();
-        int requestSize = 50;
-
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 3)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[3];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder()
-                    .clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Client client = nodes[0].client();
-
-        try {
-            client.admin().indices().prepareCreate(indexName).get();
-            for (int counter = 1; counter <= numDocs;) {
-                BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();
-                for (int bulkCounter = 0; bulkCounter < 100; bulkCounter++) {
-                    if (counter > numDocs) {
-                        break;
-                    }
-                    bulkRequestBuilder.add(
-                            client.prepareIndex(indexName, typeName, String.valueOf(counter))
-                                    .setSource("field1", counter++)
-                    );
-                }
-                int indexedDocs = counter - 1;
-                if (indexedDocs % 100000 == 0) {
-                     System.out.printf(Locale.ENGLISH, "--> Indexed %d so far\n", indexedDocs);
-                }
-                bulkRequestBuilder.get();
-            }
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-
-        client.admin().indices().prepareRefresh(indexName).get();
-        System.out.printf(Locale.ENGLISH, "--> Number of docs in index: %d\n", client.prepareSearch().setSize(0).get().getHits().totalHits());
-
-        Long counter = numDocs;
-        SearchResponse searchResponse = client.prepareSearch(indexName)
-                .addSort("field1", SortOrder.DESC)
-                .setSize(requestSize)
-                .setScroll("10m").get();
-
-        if (searchResponse.getHits().getTotalHits() != numDocs) {
-            System.err.printf(Locale.ENGLISH, "Expected total hits [%d] but got [%d]\n", numDocs, searchResponse.getHits().getTotalHits());
-        }
-
-        if (searchResponse.getHits().hits().length != requestSize) {
-            System.err.printf(Locale.ENGLISH, "Expected hits length [%d] but got [%d]\n", requestSize, searchResponse.getHits().hits().length);
-        }
-
-        for (SearchHit hit : searchResponse.getHits()) {
-            if (!hit.sortValues()[0].equals(counter--)) {
-                System.err.printf(Locale.ENGLISH, "Expected sort value [%d] but got [%s]\n", counter + 1, hit.sortValues()[0]);
-            }
-        }
-        String scrollId = searchResponse.getScrollId();
-        int scrollRequestCounter = 0;
-        long sumTimeSpent = 0;
-        while (true) {
-            long timeSpent = System.currentTimeMillis();
-            searchResponse = client.prepareSearchScroll(scrollId).setScroll("10m").get();
-            sumTimeSpent += (System.currentTimeMillis() - timeSpent);
-            scrollRequestCounter++;
-            if (searchResponse.getHits().getTotalHits() != numDocs) {
-                System.err.printf(Locale.ENGLISH, "Expected total hits [%d] but got [%d]\n", numDocs, searchResponse.getHits().getTotalHits());
-            }
-            if (scrollRequestCounter % 20 == 0) {
-                long avgTimeSpent = sumTimeSpent / 20;
-                JvmStats.Mem mem = JvmStats.jvmStats().getMem();
-                System.out.printf(Locale.ENGLISH, "Cursor location=%d, avg time spent=%d ms\n", (requestSize * scrollRequestCounter), (avgTimeSpent));
-                System.out.printf(Locale.ENGLISH, "heap max=%s, used=%s, percentage=%d\n", mem.getHeapMax(), mem.getHeapUsed(), mem.getHeapUsedPercent());
-                sumTimeSpent = 0;
-            }
-            if (searchResponse.getHits().hits().length == 0) {
-                break;
-            }
-            if (searchResponse.getHits().hits().length != requestSize) {
-                System.err.printf(Locale.ENGLISH, "Expected hits length [%d] but got [%d]\n", requestSize, searchResponse.getHits().hits().length);
-            }
-            for (SearchHit hit : searchResponse.getHits()) {
-                if (!hit.sortValues()[0].equals(counter--)) {
-                    System.err.printf(Locale.ENGLISH, "Expected sort value [%d] but got [%s]\n", counter + 1, hit.sortValues()[0]);
-                }
-            }
-            scrollId = searchResponse.getScrollId();
-        }
-        if (counter != 0) {
-            System.err.printf(Locale.ENGLISH, "Counter should be 0 because scroll has been consumed\n");
-        }
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java
deleted file mode 100644
index 37b20bc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.time;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class SimpleTimeBenchmark {
-
-    private static boolean USE_NANO_TIME = false;
-    private static long NUMBER_OF_ITERATIONS = 1000000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            System.currentTimeMillis();
-        }
-        System.out.println("Took " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Running using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    if (USE_NANO_TIME) {
-                        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                            System.nanoTime();
-                        }
-                    } else {
-                        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                            System.currentTimeMillis();
-                        }
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Took " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java
deleted file mode 100644
index 2978c5c..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.transport.TransportRequest;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class BenchmarkMessageRequest extends TransportRequest {
-
-    long id;
-    byte[] payload;
-
-    public BenchmarkMessageRequest(long id, byte[] payload) {
-        this.id = id;
-        this.payload = payload;
-    }
-
-    public BenchmarkMessageRequest() {
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readLong();
-        payload = new byte[in.readVInt()];
-        in.readFully(payload);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeLong(id);
-        out.writeVInt(payload.length);
-        out.writeBytes(payload);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java
deleted file mode 100644
index 7a7e3d9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.transport.TransportResponse;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class BenchmarkMessageResponse extends TransportResponse {
-
-    long id;
-    byte[] payload;
-
-    public BenchmarkMessageResponse(BenchmarkMessageRequest request) {
-        this.id = request.id;
-        this.payload = request.payload;
-    }
-
-    public BenchmarkMessageResponse(long id, byte[] payload) {
-        this.id = id;
-        this.payload = payload;
-    }
-
-    public BenchmarkMessageResponse() {
-    }
-
-    public long id() {
-        return id;
-    }
-
-    public byte[] payload() {
-        return payload;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readLong();
-        payload = new byte[in.readVInt()];
-        in.readFully(payload);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeLong(id);
-        out.writeVInt(payload.length);
-        out.writeBytes(payload);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
deleted file mode 100644
index 553ef0c..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.settings.DynamicSettings;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-import org.elasticsearch.transport.netty.NettyTransport;
-
-import java.net.InetAddress;
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class BenchmarkNettyLargeMessages {
-
-    public static void main(String[] args) throws Exception {
-        final ByteSizeValue payloadSize = new ByteSizeValue(10, ByteSizeUnit.MB);
-        final int NUMBER_OF_ITERATIONS = 100000;
-        final int NUMBER_OF_CLIENTS = 5;
-        final byte[] payload = new byte[(int) payloadSize.bytes()];
-
-        Settings settings = Settings.settingsBuilder()
-                .build();
-
-        NetworkService networkService = new NetworkService(settings);
-
-        final ThreadPool threadPool = new ThreadPool("BenchmarkNettyLargeMessages");
-        final TransportService transportServiceServer = new TransportService(
-                new NettyTransport(settings, threadPool, networkService, BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry()), threadPool
-        ).start();
-        final TransportService transportServiceClient = new TransportService(
-                new NettyTransport(settings, threadPool, networkService, BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry()), threadPool
-        ).start();
-
-        final DiscoveryNode bigNode = new DiscoveryNode("big", new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300), Version.CURRENT);
-//        final DiscoveryNode smallNode = new DiscoveryNode("small", new InetSocketTransportAddress("localhost", 9300));
-        final DiscoveryNode smallNode = bigNode;
-
-        transportServiceClient.connectToNode(bigNode);
-        transportServiceClient.connectToNode(smallNode);
-
-        transportServiceServer.registerRequestHandler("benchmark", BenchmarkMessageRequest::new, ThreadPool.Names.GENERIC, new TransportRequestHandler<BenchmarkMessageRequest>() {
-            @Override
-            public void messageReceived(BenchmarkMessageRequest request, TransportChannel channel) throws Exception {
-                channel.sendResponse(new BenchmarkMessageResponse(request));
-            }
-        });
-
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_CLIENTS);
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (int i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        BenchmarkMessageRequest message = new BenchmarkMessageRequest(1, payload);
-                        transportServiceClient.submitRequest(bigNode, "benchmark", message, TransportRequestOptions.builder().withType(TransportRequestOptions.Type.BULK).build(), new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                            @Override
-                            public BenchmarkMessageResponse newInstance() {
-                                return new BenchmarkMessageResponse();
-                            }
-
-                            @Override
-                            public String executor() {
-                                return ThreadPool.Names.SAME;
-                            }
-
-                            @Override
-                            public void handleResponse(BenchmarkMessageResponse response) {
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                exp.printStackTrace();
-                            }
-                        }).txGet();
-                    }
-                    latch.countDown();
-                }
-            }).start();
-        }
-
-        new Thread(new Runnable() {
-            @Override
-            public void run() {
-                for (int i = 0; i < 1; i++) {
-                    BenchmarkMessageRequest message = new BenchmarkMessageRequest(2, BytesRef.EMPTY_BYTES);
-                    long start = System.currentTimeMillis();
-                    transportServiceClient.submitRequest(smallNode, "benchmark", message, TransportRequestOptions.builder().withType(TransportRequestOptions.Type.STATE).build(), new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                        @Override
-                        public BenchmarkMessageResponse newInstance() {
-                            return new BenchmarkMessageResponse();
-                        }
-
-                        @Override
-                        public String executor() {
-                            return ThreadPool.Names.SAME;
-                        }
-
-                        @Override
-                        public void handleResponse(BenchmarkMessageResponse response) {
-                        }
-
-                        @Override
-                        public void handleException(TransportException exp) {
-                            exp.printStackTrace();
-                        }
-                    }).txGet();
-                    long took = System.currentTimeMillis() - start;
-                    System.out.println("Took " + took + "ms");
-                }
-            }
-        }).start();
-
-        latch.await();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java
deleted file mode 100644
index 5ccc264..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.elasticsearch.transport.netty.NettyTransport;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- *
- */
-public class TransportBenchmark {
-
-    static enum Type {
-        LOCAL {
-            @Override
-            public Transport newTransport(Settings settings, ThreadPool threadPool) {
-                return new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry());
-            }
-        },
-        NETTY {
-            @Override
-            public Transport newTransport(Settings settings, ThreadPool threadPool) {
-                return new NettyTransport(settings, threadPool, new NetworkService(Settings.EMPTY), BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry());
-            }
-        };
-
-        public abstract Transport newTransport(Settings settings, ThreadPool threadPool);
-    }
-
-    public static void main(String[] args) {
-        final String executor = ThreadPool.Names.GENERIC;
-        final boolean waitForRequest = true;
-        final ByteSizeValue payloadSize = new ByteSizeValue(100, ByteSizeUnit.BYTES);
-        final int NUMBER_OF_CLIENTS = 10;
-        final int NUMBER_OF_ITERATIONS = 100000;
-        final byte[] payload = new byte[(int) payloadSize.bytes()];
-        final AtomicLong idGenerator = new AtomicLong();
-        final Type type = Type.NETTY;
-
-
-        Settings settings = Settings.settingsBuilder()
-                .build();
-
-        final ThreadPool serverThreadPool = new ThreadPool("server");
-        final TransportService serverTransportService = new TransportService(type.newTransport(settings, serverThreadPool), serverThreadPool).start();
-
-        final ThreadPool clientThreadPool = new ThreadPool("client");
-        final TransportService clientTransportService = new TransportService(type.newTransport(settings, clientThreadPool), clientThreadPool).start();
-
-        final DiscoveryNode node = new DiscoveryNode("server", serverTransportService.boundAddress().publishAddress(), Version.CURRENT);
-
-        serverTransportService.registerRequestHandler("benchmark", BenchmarkMessageRequest::new, executor, new TransportRequestHandler<BenchmarkMessageRequest>() {
-            @Override
-            public void messageReceived(BenchmarkMessageRequest request, TransportChannel channel) throws Exception {
-                channel.sendResponse(new BenchmarkMessageResponse(request));
-            }
-        });
-
-        clientTransportService.connectToNode(node);
-
-        for (int i = 0; i < 10000; i++) {
-            BenchmarkMessageRequest message = new BenchmarkMessageRequest(1, payload);
-            clientTransportService.submitRequest(node, "benchmark", message, new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                @Override
-                public BenchmarkMessageResponse newInstance() {
-                    return new BenchmarkMessageResponse();
-                }
-
-                @Override
-                public String executor() {
-                    return ThreadPool.Names.SAME;
-                }
-
-                @Override
-                public void handleResponse(BenchmarkMessageResponse response) {
-                }
-
-                @Override
-                public void handleException(TransportException exp) {
-                    exp.printStackTrace();
-                }
-            }).txGet();
-        }
-
-
-        Thread[] clients = new Thread[NUMBER_OF_CLIENTS];
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS);
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            clients[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (int j = 0; j < NUMBER_OF_ITERATIONS; j++) {
-                        final long id = idGenerator.incrementAndGet();
-                        BenchmarkMessageRequest request = new BenchmarkMessageRequest(id, payload);
-                        BaseTransportResponseHandler<BenchmarkMessageResponse> handler = new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                            @Override
-                            public BenchmarkMessageResponse newInstance() {
-                                return new BenchmarkMessageResponse();
-                            }
-
-                            @Override
-                            public String executor() {
-                                return executor;
-                            }
-
-                            @Override
-                            public void handleResponse(BenchmarkMessageResponse response) {
-                                if (response.id() != id) {
-                                    System.out.println("NO ID MATCH [" + response.id() + "] and [" + id + "]");
-                                }
-                                latch.countDown();
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                exp.printStackTrace();
-                                latch.countDown();
-                            }
-                        };
-
-                        if (waitForRequest) {
-                            clientTransportService.submitRequest(node, "benchmark", request, handler).txGet();
-                        } else {
-                            clientTransportService.sendRequest(node, "benchmark", request, handler);
-                        }
-                    }
-                }
-            });
-        }
-
-        StopWatch stopWatch = new StopWatch().start();
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            clients[i].start();
-        }
-
-        try {
-            latch.await();
-        } catch (InterruptedException e) {
-            e.printStackTrace();
-        }
-        stopWatch.stop();
-
-        System.out.println("Ran [" + NUMBER_OF_CLIENTS + "], each with [" + NUMBER_OF_ITERATIONS + "] iterations, payload [" + payloadSize + "]: took [" + stopWatch.totalTime() + "], TPS: " + (NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS) / stopWatch.totalTime().secondsFrac());
-
-        clientTransportService.close();
-        clientThreadPool.shutdownNow();
-
-        serverTransportService.close();
-        serverThreadPool.shutdownNow();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
deleted file mode 100644
index fd76504..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport.netty;
-
-import org.jboss.netty.bootstrap.ClientBootstrap;
-import org.jboss.netty.bootstrap.ServerBootstrap;
-import org.jboss.netty.buffer.ChannelBuffer;
-import org.jboss.netty.buffer.ChannelBuffers;
-import org.jboss.netty.channel.*;
-import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
-import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
-
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Executors;
-
-public class NettyEchoBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        final int payloadSize = 100;
-        int CYCLE_SIZE = 50000;
-        final long NUMBER_OF_ITERATIONS = 500000;
-
-        ChannelBuffer message = ChannelBuffers.buffer(100);
-        for (int i = 0; i < message.capacity(); i++) {
-            message.writeByte((byte) i);
-        }
-
-        // Configure the server.
-        ServerBootstrap serverBootstrap = new ServerBootstrap(
-                new NioServerSocketChannelFactory(
-                        Executors.newCachedThreadPool(),
-                        Executors.newCachedThreadPool()));
-
-        // Set up the pipeline factory.
-        serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(new EchoServerHandler());
-            }
-        });
-
-        // Bind and start to accept incoming connections.
-        serverBootstrap.bind(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
-
-        ClientBootstrap clientBootstrap = new ClientBootstrap(
-                new NioClientSocketChannelFactory(
-                        Executors.newCachedThreadPool(),
-                        Executors.newCachedThreadPool()));
-
-//        ClientBootstrap clientBootstrap = new ClientBootstrap(
-//                new OioClientSocketChannelFactory(Executors.newCachedThreadPool()));
-
-        // Set up the pipeline factory.
-        final EchoClientHandler clientHandler = new EchoClientHandler();
-        clientBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(clientHandler);
-            }
-        });
-
-        // Start the connection attempt.
-        ChannelFuture future = clientBootstrap.connect(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
-        future.awaitUninterruptibly();
-        Channel clientChannel = future.getChannel();
-
-        System.out.println("Warming up...");
-        for (long i = 0; i < 10000; i++) {
-            clientHandler.latch = new CountDownLatch(1);
-            clientChannel.write(message);
-            try {
-                clientHandler.latch.await();
-            } catch (InterruptedException e) {
-                e.printStackTrace();
-            }
-        }
-        System.out.println("Warmed up");
-
-
-        long start = System.currentTimeMillis();
-        long cycleStart = System.currentTimeMillis();
-        for (long i = 1; i < NUMBER_OF_ITERATIONS; i++) {
-            clientHandler.latch = new CountDownLatch(1);
-            clientChannel.write(message);
-            try {
-                clientHandler.latch.await();
-            } catch (InterruptedException e) {
-                e.printStackTrace();
-            }
-            if ((i % CYCLE_SIZE) == 0) {
-                long cycleEnd = System.currentTimeMillis();
-                System.out.println("Ran 50000, TPS " + (CYCLE_SIZE / ((double) (cycleEnd - cycleStart) / 1000)));
-                cycleStart = cycleEnd;
-            }
-        }
-        long end = System.currentTimeMillis();
-        long seconds = (end - start) / 1000;
-        System.out.println("Ran [" + NUMBER_OF_ITERATIONS + "] iterations, payload [" + payloadSize + "]: took [" + seconds + "], TPS: " + ((double) NUMBER_OF_ITERATIONS) / seconds);
-
-        clientChannel.close().awaitUninterruptibly();
-        clientBootstrap.releaseExternalResources();
-        serverBootstrap.releaseExternalResources();
-    }
-
-    public static class EchoClientHandler extends SimpleChannelUpstreamHandler {
-
-        public volatile CountDownLatch latch;
-
-        public EchoClientHandler() {
-        }
-
-        @Override
-        public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
-            latch.countDown();
-        }
-
-        @Override
-        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-            e.getCause().printStackTrace();
-            e.getChannel().close();
-        }
-    }
-
-
-    public static class EchoServerHandler extends SimpleChannelUpstreamHandler {
-
-        @Override
-        public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
-            e.getChannel().write(e.getMessage());
-        }
-
-        @Override
-        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-            // Close the connection when an exception is raised.
-            e.getCause().printStackTrace();
-            e.getChannel().close();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java
deleted file mode 100644
index d9995e1..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.uuid;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.UUID;
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class SimpleUuidBenchmark {
-
-    private static long NUMBER_OF_ITERATIONS = 10000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            UUID.randomUUID().toString();
-        }
-        System.out.println("Generated in " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Generating using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        UUID.randomUUID().toString();
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Generate in " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
index 5a82384..39ed23b 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
@@ -69,7 +69,7 @@ public class NodesStatsBasicBackwardsCompatIT extends ESBackcompatTestCase {
             NodesStatsRequestBuilder nsBuilder = tc.admin().cluster().prepareNodesStats();
 
             Class c = nsBuilder.getClass();
-            for (Method method : c.getDeclaredMethods()) {
+            for (Method method : c.getMethods()) {
                 if (method.getName().startsWith("set")) {
                     if (method.getParameterTypes().length == 1 && method.getParameterTypes()[0] == boolean.class) {
                         method.invoke(nsBuilder, randomBoolean());
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
index bccd429..228f1a6 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
@@ -92,7 +92,7 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
         }
 
         SortedSet<String> expectedVersions = new TreeSet<>();
-        for (java.lang.reflect.Field field : Version.class.getDeclaredFields()) {
+        for (java.lang.reflect.Field field : Version.class.getFields()) {
             if (Modifier.isStatic(field.getModifiers()) && field.getType() == Version.class) {
                 Version v = (Version) field.get(Version.class);
                 if (v.snapshot()) continue;
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
index 4d82445..f01fdff 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
@@ -32,7 +32,6 @@ import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.elasticsearch.transport.TransportService;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.is;
@@ -51,13 +50,15 @@ public class TransportClientIT extends ESIntegTestCase {
     public void testNodeVersionIsUpdated() {
         TransportClient client = (TransportClient)  internalCluster().client();
         TransportClientNodesService nodeService = client.nodeService();
-        Node node = nodeBuilder().data(false).settings(Settings.builder()
+        Node node = new Node(Settings.builder()
                 .put(internalCluster().getDefaultSettings())
                 .put("path.home", createTempDir())
                 .put("node.name", "testNodeVersionIsUpdated")
                 .put("http.enabled", false)
+                .put("node.data", false)
+                .put("cluster.name", "foobar")
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
-                .build()).clusterName("foobar").build();
+                .build());
         node.start();
         try {
             TransportAddress transportAddress = node.injector().getInstance(TransportService.class).boundAddress().publishAddress();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
index 29281e2..e8be4e3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
@@ -26,13 +26,11 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.VersionUtils;
 
 import java.io.BufferedReader;
 import java.io.InputStreamReader;
-import java.nio.file.Path;
 import java.util.Arrays;
 
 public class RoutingBackwardCompatibilityTests extends ESTestCase {
diff --git a/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java b/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java
deleted file mode 100644
index 886d9b9..0000000
--- a/core/src/test/java/org/elasticsearch/common/SearchScrollIteratorTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common;
-
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-// Not a real unit tests with mocks, but with a single node, because we mock the scroll
-// search behaviour and it changes then this test will not catch this.
-public class SearchScrollIteratorTests extends ESSingleNodeTestCase {
-
-    public void testSearchScrollIterator() {
-        createIndex("index");
-        int numDocs = scaledRandomIntBetween(0, 128);
-        for (int i = 0; i < numDocs; i++) {
-            client().prepareIndex("index", "type", Integer.toString(i))
-                    .setSource("field", "value" + i)
-                    .get();
-        }
-        client().admin().indices().prepareRefresh().get();
-
-        int i = 0;
-        SearchRequest searchRequest = new SearchRequest("index");
-        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
-        // randomize size, because that also controls how many actual searches will happen:
-        sourceBuilder.size(scaledRandomIntBetween(1, 10));
-        searchRequest.source(sourceBuilder);
-        Iterable<SearchHit> hits = SearchScrollIterator.createIterator(client(), TimeValue.timeValueSeconds(10), searchRequest);
-        for (SearchHit hit : hits) {
-            assertThat(hit.getId(), equalTo(Integer.toString(i)));
-            assertThat(hit.getSource().get("field"), equalTo("value" + i));
-            i++;
-        }
-        assertThat(i, equalTo(numDocs));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
index 13c2211..7ec4756 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
@@ -24,14 +24,16 @@ import org.elasticsearch.test.ESTestCase;
 
 import java.net.InetAddress;
 
+import static org.hamcrest.Matchers.is;
+
 /**
  * Tests for network service... try to keep them safe depending upon configuration
  * please don't actually bind to anything, just test the addresses.
  */
 public class NetworkServiceTests extends ESTestCase {
 
-    /** 
-     * ensure exception if we bind to multicast ipv4 address 
+    /**
+     * ensure exception if we bind to multicast ipv4 address
      */
     public void testBindMulticastV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -42,9 +44,8 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we bind to multicast ipv6 address 
+    /**
+     * ensure exception if we bind to multicast ipv6 address
      */
     public void testBindMulticastV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -55,9 +56,9 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we publish to multicast ipv4 address 
+
+    /**
+     * ensure exception if we publish to multicast ipv4 address
      */
     public void testPublishMulticastV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -68,9 +69,9 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we publish to multicast ipv6 address 
+
+    /**
+     * ensure exception if we publish to multicast ipv6 address
      */
     public void testPublishMulticastV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -82,24 +83,24 @@ public class NetworkServiceTests extends ESTestCase {
         }
     }
 
-    /** 
-     * ensure specifying wildcard ipv4 address will bind to all interfaces 
+    /**
+     * ensure specifying wildcard ipv4 address will bind to all interfaces
      */
     public void testBindAnyLocalV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         assertEquals(InetAddress.getByName("0.0.0.0"), service.resolveBindHostAddresses(new String[] { "0.0.0.0" })[0]);
     }
-    
-    /** 
-     * ensure specifying wildcard ipv6 address will bind to all interfaces 
+
+    /**
+     * ensure specifying wildcard ipv6 address will bind to all interfaces
      */
     public void testBindAnyLocalV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         assertEquals(InetAddress.getByName("::"), service.resolveBindHostAddresses(new String[] { "::" })[0]);
     }
 
-    /** 
-     * ensure specifying wildcard ipv4 address selects reasonable publish address 
+    /**
+     * ensure specifying wildcard ipv4 address selects reasonable publish address
      */
     public void testPublishAnyLocalV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -107,12 +108,34 @@ public class NetworkServiceTests extends ESTestCase {
         assertFalse(address.isAnyLocalAddress());
     }
 
-    /** 
-     * ensure specifying wildcard ipv6 address selects reasonable publish address 
+    /**
+     * ensure specifying wildcard ipv6 address selects reasonable publish address
      */
     public void testPublishAnyLocalV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         InetAddress address = service.resolvePublishHostAddresses(new String[] { "::" });
         assertFalse(address.isAnyLocalAddress());
     }
+
+    /**
+     * ensure we can bind to multiple addresses
+     */
+    public void testBindMultipleAddresses() throws Exception {
+        NetworkService service = new NetworkService(Settings.EMPTY);
+        InetAddress[] addresses = service.resolveBindHostAddresses(new String[]{"127.0.0.1", "127.0.0.2"});
+        assertThat(addresses.length, is(2));
+    }
+
+    /**
+     * ensure we can't bind to multiple addresses when using wildcard
+     */
+    public void testBindMultipleAddressesWithWildcard() throws Exception {
+        NetworkService service = new NetworkService(Settings.EMPTY);
+        try {
+            service.resolveBindHostAddresses(new String[]{"0.0.0.0", "127.0.0.1"});
+            fail("should have hit exception");
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("is wildcard, but multiple addresses specified"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java b/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
index ee0756f..85c2be7 100644
--- a/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
+++ b/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
@@ -28,7 +28,7 @@ import static org.hamcrest.Matchers.equalTo;
 public class RegexTests extends ESTestCase {
     public void testFlags() {
         String[] supportedFlags = new String[]{"CASE_INSENSITIVE", "MULTILINE", "DOTALL", "UNICODE_CASE", "CANON_EQ", "UNIX_LINES",
-                "LITERAL", "COMMENTS", "UNICODE_CHAR_CLASS"};
+                "LITERAL", "COMMENTS", "UNICODE_CHAR_CLASS", "UNICODE_CHARACTER_CLASS"};
         int[] flags = new int[]{Pattern.CASE_INSENSITIVE, Pattern.MULTILINE, Pattern.DOTALL, Pattern.UNICODE_CASE, Pattern.CANON_EQ,
                 Pattern.UNIX_LINES, Pattern.LITERAL, Pattern.COMMENTS, Regex.UNICODE_CHARACTER_CLASS};
         Random random = getRandom();
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java b/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java
new file mode 100644
index 0000000..1b91726
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent;
+
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+
+public abstract class BaseXContentTestCase extends ESTestCase {
+
+    public abstract XContentType xcontentType();
+
+    public void testBasics() throws IOException {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeEndObject();
+        }
+        byte[] data = os.toByteArray();
+        assertEquals(xcontentType(), XContentFactory.xContentType(data));
+    }
+
+    public void testRawField() throws Exception {
+        for (boolean useStream : new boolean[] {false, true}) {
+            for (XContentType xcontentType : XContentType.values()) {
+                doTestRawField(xcontentType.xContent(), useStream);
+            }
+        }
+    }
+
+    void doTestRawField(XContent source, boolean useStream) throws Exception {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = source.createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeFieldName("foo");
+            generator.writeNull();
+            generator.writeEndObject();
+        }
+        final byte[] rawData = os.toByteArray();
+
+        os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeStartObject();
+            if (useStream) {
+                generator.writeRawField("bar", new ByteArrayInputStream(rawData));
+            } else {
+                generator.writeRawField("bar", new BytesArray(rawData));
+            }
+            generator.writeEndObject();
+        }
+
+        XContentParser parser = xcontentType().xContent().createParser(os.toByteArray());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("bar", parser.currentName());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("foo", parser.currentName());
+        assertEquals(Token.VALUE_NULL, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertNull(parser.nextToken());
+    }
+
+    public void testRawValue() throws Exception {
+        for (XContentType xcontentType : XContentType.values()) {
+            doTestRawValue(xcontentType.xContent());
+        }
+    }
+
+    void doTestRawValue(XContent source) throws Exception {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = source.createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeFieldName("foo");
+            generator.writeNull();
+            generator.writeEndObject();
+        }
+        final byte[] rawData = os.toByteArray();
+
+        os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeRawValue(new BytesArray(rawData));
+        }
+
+        XContentParser parser = xcontentType().xContent().createParser(os.toByteArray());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("foo", parser.currentName());
+        assertEquals(Token.VALUE_NULL, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertNull(parser.nextToken());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java
deleted file mode 100644
index 9bb26b6..0000000
--- a/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.xcontent.builder;
-
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class BuilderRawFieldTests extends ESTestCase {
-    public void testJsonRawField() throws IOException {
-        testRawField(XContentType.JSON);
-    }
-
-    public void testSmileRawField() throws IOException {
-        testRawField(XContentType.SMILE);
-    }
-
-    public void testYamlRawField() throws IOException {
-        testRawField(XContentType.YAML);
-    }
-
-    public void testCborRawField() throws IOException {
-        testRawField(XContentType.CBOR);
-    }
-
-    private void testRawField(XContentType type) throws IOException {
-        XContentBuilder builder = XContentFactory.contentBuilder(type);
-        builder.startObject();
-        builder.field("field1", "value1");
-        builder.rawField("_source", XContentFactory.contentBuilder(type).startObject().field("s_field", "s_value").endObject().bytes());
-        builder.field("field2", "value2");
-        builder.rawField("payload_i", new BytesArray(Long.toString(1)));
-        builder.field("field3", "value3");
-        builder.rawField("payload_d", new BytesArray(Double.toString(1.1)));
-        builder.field("field4", "value4");
-        builder.rawField("payload_s", new BytesArray("test"));
-        builder.field("field5", "value5");
-        builder.endObject();
-
-        XContentParser parser = XContentFactory.xContent(type).createParser(builder.bytes());
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field1"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value1"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("_source"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("s_field"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("s_value"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field2"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value2"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_i"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER));
-        assertThat(parser.numberType(), equalTo(XContentParser.NumberType.INT));
-        assertThat(parser.longValue(), equalTo(1l));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field3"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value3"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_d"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER));
-        assertThat(parser.numberType(), equalTo(XContentParser.NumberType.DOUBLE));
-        assertThat(parser.doubleValue(), equalTo(1.1d));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field4"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value4"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_s"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("test"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field5"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value5"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
index d6cec17..7ffafc0 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.xcontent.builder;
 
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.io.FastCharArrayWriter;
@@ -32,6 +33,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.test.ESTestCase;
 
+import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.util.ArrayList;
@@ -54,8 +56,8 @@ import static org.hamcrest.Matchers.equalTo;
  */
 public class XContentBuilderTests extends ESTestCase {
     public void testPrettyWithLfAtEnd() throws Exception {
-        FastCharArrayWriter writer = new FastCharArrayWriter();
-        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(writer);
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(os);
         generator.usePrettyPrint();
         generator.usePrintLineFeedAtEnd();
 
@@ -68,27 +70,28 @@ public class XContentBuilderTests extends ESTestCase {
         // double close, and check there is no error...
         generator.close();
 
-        assertThat(writer.unsafeCharArray()[writer.size() - 1], equalTo('\n'));
+        byte[] bytes = os.toByteArray();
+        assertThat((char) bytes[bytes.length - 1], equalTo('\n'));
     }
 
     public void testReuseJsonGenerator() throws Exception {
-        FastCharArrayWriter writer = new FastCharArrayWriter();
-        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(writer);
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(os);
         generator.writeStartObject();
         generator.writeStringField("test", "value");
         generator.writeEndObject();
         generator.flush();
 
-        assertThat(writer.toStringTrim(), equalTo("{\"test\":\"value\"}"));
+        assertThat(new BytesRef(os.toByteArray()), equalTo(new BytesRef("{\"test\":\"value\"}")));
 
         // try again...
-        writer.reset();
+        os.reset();
         generator.writeStartObject();
         generator.writeStringField("test", "value");
         generator.writeEndObject();
         generator.flush();
         // we get a space at the start here since it thinks we are not in the root object (fine, we will ignore it in the real code we use)
-        assertThat(writer.toStringTrim(), equalTo("{\"test\":\"value\"}"));
+        assertThat(new BytesRef(os.toByteArray()), equalTo(new BytesRef(" {\"test\":\"value\"}")));
     }
 
     public void testRaw() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java
new file mode 100644
index 0000000..928b8a6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.cbor;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class CborXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.CBOR;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java
new file mode 100644
index 0000000..8a739ee
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.json;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class JsonXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.JSON;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java
new file mode 100644
index 0000000..6961e84
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.smile;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class SmileXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.SMILE;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
index 9216f48..ed7aee3 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
@@ -475,15 +475,10 @@ public abstract class AbstractFilteringJsonGeneratorTestCase extends ESTestCase
         assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw).endObject());
         assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw).endObject());
 
-        // Test method: rawField(String fieldName, byte[] content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-
         // Test method: rawField(String fieldName, InputStream content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
+        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
     }
 
     public void testArrays() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java
deleted file mode 100644
index 97ce4fc..0000000
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.xcontent.support.filtering;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Locale;
-
-/**
- * Benchmark class to compare filtered and unfiltered XContent generators.
- */
-public class FilteringJsonGeneratorBenchmark {
-
-    public static void main(String[] args) throws IOException {
-        final XContent XCONTENT = JsonXContent.jsonXContent;
-
-        System.out.println("Executing " + FilteringJsonGeneratorBenchmark.class + "...");
-
-        System.out.println("Warming up...");
-        run(XCONTENT, 500_000, 100, 0.5);
-        System.out.println("Warmed up.");
-
-        System.out.println("nb documents | nb fields | nb fields written | % fields written | time (millis) | rate (docs/sec) | avg size");
-
-        for (int nbFields : Arrays.asList(10, 25, 50, 100, 250)) {
-            for (int nbDocs : Arrays.asList(100, 1000, 10_000, 100_000, 500_000)) {
-                for (double ratio : Arrays.asList(0.0, 1.0, 0.99, 0.95, 0.9, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01)) {
-                    run(XCONTENT, nbDocs, nbFields, ratio);
-                }
-            }
-        }
-        System.out.println("Done.");
-    }
-
-    private static void run(XContent xContent, long nbIterations, int nbFields, double ratio) throws IOException {
-        String[] fields = fields(nbFields);
-        String[] filters = fields((int) (nbFields * ratio));
-
-        long size = 0;
-        BytesStreamOutput os = new BytesStreamOutput();
-
-        long start = System.nanoTime();
-        for (int i = 0; i < nbIterations; i++) {
-            XContentBuilder builder = new XContentBuilder(xContent, os, filters);
-            builder.startObject();
-
-            for (String field : fields) {
-                builder.field(field, System.nanoTime());
-            }
-            builder.endObject();
-
-            size += builder.bytes().length();
-            os.reset();
-        }
-        double milliseconds = (System.nanoTime() - start) / 1_000_000d;
-
-        System.out.printf(Locale.ROOT, "%12d | %9d | %17d | %14.2f %% | %10.3f ms | %15.2f | %8.0f %n",
-                nbIterations, nbFields,
-                (int) (nbFields * ratio),
-                (ratio * 100d),
-                milliseconds,
-                ((double) nbIterations) / (milliseconds / 1000d),
-                size / ((double) nbIterations));
-    }
-
-    /**
-     * Returns a String array of field names starting from "field_0" with a length of n.
-     * If n=3, the array is ["field_0","field_1","field_2"]
-     */
-    private static String[] fields(int n) {
-        String[] fields = new String[n];
-        for (int i = 0; i < n; i++) {
-            fields[i] = "field_" + i;
-        }
-        return fields;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java
new file mode 100644
index 0000000..17c2a59
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.yaml;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class YamlXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.YAML;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java b/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
index f124e19..b696c44 100644
--- a/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
+++ b/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
@@ -53,7 +53,7 @@ public class WriteConsistencyLevelIT extends ESIntegTestCase {
             fail("can't index, does not match consistency");
         } catch (UnavailableShardsException e) {
             assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE));
-            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [QUORUM] (have 1, needed 2). Timeout: [100ms], request: index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}"));
+            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [QUORUM] (have 1, needed 2). Timeout: [100ms], request: [index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}]"));
             // but really, all is well
         }
 
@@ -76,7 +76,7 @@ public class WriteConsistencyLevelIT extends ESIntegTestCase {
             fail("can't index, does not match consistency");
         } catch (UnavailableShardsException e) {
             assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE));
-            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [ALL] (have 2, needed 3). Timeout: [100ms], request: index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}"));
+            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [ALL] (have 2, needed 3). Timeout: [100ms], request: [index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}]"));
             // but really, all is well
         }
 
diff --git a/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java b/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
index d4907d8..4f28cf1 100644
--- a/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
+++ b/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.document;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
 import org.elasticsearch.action.bulk.BulkItemResponse;
@@ -117,11 +117,11 @@ public class ShardInfoIT extends ESIntegTestCase {
         }
     }
 
-    private void assertShardInfo(ActionWriteResponse response) {
+    private void assertShardInfo(ReplicationResponse response) {
         assertShardInfo(response, numCopies, numNodes);
     }
 
-    private void assertShardInfo(ActionWriteResponse response, int expectedTotal, int expectedSuccessful) {
+    private void assertShardInfo(ReplicationResponse response, int expectedTotal, int expectedSuccessful) {
         assertThat(response.getShardInfo().getTotal(), greaterThanOrEqualTo(expectedTotal));
         assertThat(response.getShardInfo().getSuccessful(), greaterThanOrEqualTo(expectedSuccessful));
     }
diff --git a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
index 2b76d03..c230613 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
@@ -59,7 +59,6 @@ import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.ScriptContextRegistry;
 import org.elasticsearch.script.ScriptEngineService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.IndexSettingsModule;
 import org.elasticsearch.test.engine.MockEngineFactory;
@@ -102,7 +101,6 @@ public class IndexModuleTests extends ESTestCase {
         BigArrays bigArrays = new BigArrays(recycler, circuitBreakerService);
         IndicesFieldDataCache indicesFieldDataCache = new IndicesFieldDataCache(settings, new IndicesFieldDataCacheListener(circuitBreakerService), threadPool);
         Set<ScriptEngineService> scriptEngines = new HashSet<>();
-        scriptEngines.add(new MustacheScriptEngineService(settings));
         scriptEngines.addAll(Arrays.asList(scriptEngineServices));
         ScriptService scriptService = new ScriptService(settings, environment, scriptEngines, new ResourceWatcherService(settings, threadPool), new ScriptContextRegistry(Collections.emptyList()));
         IndicesQueriesRegistry indicesQueriesRegistry = new IndicesQueriesRegistry(settings, Collections.emptySet(), new NamedWriteableRegistry());
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
index b7bdbb2..d931b47 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
@@ -237,7 +237,7 @@ public class NGramTokenizerFactoryTests extends ESTokenStreamTestCase {
 
 
     private Version randomVersion(Random random) throws IllegalArgumentException, IllegalAccessException {
-        Field[] declaredFields = Version.class.getDeclaredFields();
+        Field[] declaredFields = Version.class.getFields();
         List<Field> versionFields = new ArrayList<>();
         for (Field field : declaredFields) {
             if ((field.getModifiers() & Modifier.STATIC) != 0 && field.getName().startsWith("V_") && field.getType() == Version.class) {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
index 2b20052..f4a7507 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.mapper;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -117,8 +118,9 @@ public class MapperServiceTests extends ESSingleNodeTestCase {
             if (t instanceof ExecutionException) {
                 t = ((ExecutionException) t).getCause();
             }
-            if (t instanceof IllegalArgumentException) {
-                assertEquals("It is forbidden to index into the default mapping [_default_]", t.getMessage());
+            final Throwable throwable = ExceptionsHelper.unwrapCause(t);
+            if (throwable instanceof IllegalArgumentException) {
+                assertEquals("It is forbidden to index into the default mapping [_default_]", throwable.getMessage());
             } else {
                 throw t;
             }
@@ -133,8 +135,9 @@ public class MapperServiceTests extends ESSingleNodeTestCase {
             if (t instanceof ExecutionException) {
                 t = ((ExecutionException) t).getCause();
             }
-            if (t instanceof IllegalArgumentException) {
-                assertEquals("It is forbidden to index into the default mapping [_default_]", t.getMessage());
+            final Throwable throwable = ExceptionsHelper.unwrapCause(t);
+            if (throwable instanceof IllegalArgumentException) {
+                assertEquals("It is forbidden to index into the default mapping [_default_]", throwable.getMessage());
             } else {
                 throw t;
             }
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index aebf00e..aa97d72 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -84,7 +84,6 @@ import org.elasticsearch.indices.mapper.MapperRegistry;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptParseException;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.IndexSettingsModule;
@@ -205,15 +204,8 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                         MockScriptEngine mockScriptEngine = new MockScriptEngine();
                         Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
                         multibinder.addBinding().toInstance(mockScriptEngine);
-                        try {
-                            Class.forName("com.github.mustachejava.Mustache");
-                        } catch(ClassNotFoundException e) {
-                            throw new IllegalStateException("error while loading mustache", e);
-                        }
-                        MustacheScriptEngineService mustacheScriptEngineService = new MustacheScriptEngineService(settings);
                         Set<ScriptEngineService> engines = new HashSet<>();
                         engines.add(mockScriptEngine);
-                        engines.add(mustacheScriptEngineService);
                         List<ScriptContext.Plugin> customContexts = new ArrayList<>();
                         bind(ScriptContextRegistry.class).toInstance(new ScriptContextRegistry(customContexts));
                         try {
@@ -836,21 +828,21 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
         AbstractQueryTestCase delegate;
         @Override
         public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
-            if (method.equals(Client.class.getDeclaredMethod("get", GetRequest.class))) {
+            if (method.equals(Client.class.getMethod("get", GetRequest.class))) {
                 return new PlainActionFuture<GetResponse>() {
                     @Override
                     public GetResponse get() throws InterruptedException, ExecutionException {
                         return delegate.executeGet((GetRequest) args[0]);
                     }
                 };
-            } else if (method.equals(Client.class.getDeclaredMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
+            } else if (method.equals(Client.class.getMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
                     return new PlainActionFuture<MultiTermVectorsResponse>() {
                         @Override
                         public MultiTermVectorsResponse get() throws InterruptedException, ExecutionException {
                             return delegate.executeMultiTermVectors((MultiTermVectorsRequest) args[0]);
                         }
                     };
-            } else if (method.equals(Object.class.getDeclaredMethod("toString"))) {
+            } else if (method.equals(Object.class.getMethod("toString"))) {
                 return "MockClient";
             }
             throw new UnsupportedOperationException("this test can't handle calls to: " + method);
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index cdf0c5d..df7eb3c 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -51,7 +51,7 @@ public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQue
 
     @Override
     protected TemplateQueryBuilder doCreateTestQueryBuilder() {
-        return new TemplateQueryBuilder(new Template(templateBase.toString()));
+        return new TemplateQueryBuilder(new Template(templateBase.toString(), ScriptType.INLINE, "mockscript", null, null));
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
deleted file mode 100644
index d4816f8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ /dev/null
@@ -1,503 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.action.index.IndexRequest.OpType;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
-import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
-import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
-import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-/**
- * Full integration test of the template query plugin.
- */
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
-public class TemplateQueryIT extends ESIntegTestCase {
-
-    @Before
-    public void setup() throws IOException {
-        createIndex("test");
-        ensureGreen("test");
-
-        index("test", "testtype", "1", jsonBuilder().startObject().field("text", "value1").endObject());
-        index("test", "testtype", "2", jsonBuilder().startObject().field("text", "value2").endObject());
-        refresh();
-    }
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config")).build();
-    }
-
-    public void testTemplateInBody() throws IOException {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("template", "all");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("{\"match_{{template}}\": {}}\"", ScriptType.INLINE, null,
-                null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testTemplateInBodyWithSize() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        SearchResponse sr = client().prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().size(0).query(
-                                QueryBuilders.templateQuery(new Template("{ \"match_{{template}}\": {} }",
-                                        ScriptType.INLINE, null, null, params)))).execute()
-                .actionGet();
-        assertNoFailures(sr);
-        assertThat(sr.getHits().hits().length, equalTo(0));
-    }
-
-    public void testTemplateWOReplacementInBody() throws IOException {
-        Map<String, Object> vars = new HashMap<>();
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "{\"match_all\": {}}\"", ScriptType.INLINE, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testTemplateInFile() {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("template", "all");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "storedTemplate", ScriptService.ScriptType.FILE, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testRawFSTemplate() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
-        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
-        assertHitCount(sr, 2);
-    }
-
-    public void testSearchRequestTemplateSource() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-
-        String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
-        searchRequest.template(parseTemplate(query));
-
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertHitCount(searchResponse, 2);
-    }
-
-    private Template parseTemplate(String template) throws IOException {
-        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
-            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
-        }
-    }
-
-    // Releates to #6318
-    public void testSearchRequestFail() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        try {
-            String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
-            searchRequest.template(parseTemplate(query));
-            client().search(searchRequest).get();
-            fail("expected exception");
-        } catch (Exception ex) {
-            // expected - no params
-        }
-        String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
-        searchRequest.template(parseTemplate(query));
-
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    public void testThatParametersCanBeSet() throws Exception {
-        index("test", "type", "1", jsonBuilder().startObject().field("theField", "foo").endObject());
-        index("test", "type", "2", jsonBuilder().startObject().field("theField", "foo 2").endObject());
-        index("test", "type", "3", jsonBuilder().startObject().field("theField", "foo 3").endObject());
-        index("test", "type", "4", jsonBuilder().startObject().field("theField", "foo 4").endObject());
-        index("test", "type", "5", jsonBuilder().startObject().field("otherField", "foo").endObject());
-        refresh();
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("mySize", "2");
-        templateParams.put("myField", "theField");
-        templateParams.put("myValue", "foo");
-
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-        // size kicks in here...
-        assertThat(searchResponse.getHits().getHits().length, is(2));
-
-        templateParams.put("myField", "otherField");
-        searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 1);
-    }
-
-    public void testSearchTemplateQueryFromFile() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
-                + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can be expressed as a single escaped string.
-     */
-    public void testTemplateQueryAsEscapedString() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can contain conditional clause. In this case it is at
-     * the beginning of the string.
-     */
-    public void testTemplateQueryAsEscapedStringStartingWithConditionalClause() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String templateString = "{"
-                + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can contain conditional clause. In this case it is at
-     * the end of the string.
-     */
-    public void testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String templateString = "{"
-                + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
-                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    public void testIndexedTemplateClient() throws Exception {
-        createIndex(ScriptService.SCRIPT_INDEX);
-        ensureGreen(ScriptService.SCRIPT_INDEX);
-
-        PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
-                "\"template\":{" +
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}" +
-                "}").get();
-
-        assertTrue(scriptResponse.isCreated());
-
-        scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
-                "\"template\":{" +
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}" +
-                "}").get();
-
-        assertEquals(scriptResponse.getVersion(), 2);
-
-        GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
-        assertTrue(getResponse.isExists());
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-
-        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-        indexRandom(true, builders);
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("fieldParam", "foo");
-
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("testTemplate", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-
-        DeleteIndexedScriptResponse deleteResponse = client().prepareDeleteIndexedScript(MustacheScriptEngineService.NAME, "testTemplate")
-                .get();
-        assertTrue(deleteResponse.isFound());
-
-        getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
-        assertFalse(getResponse.isExists());
-
-        try {
-            client().prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/template_index/mustache/1000", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                    templateParams)).get();
-            fail("Expected SearchPhaseExecutionException");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("Illegal index script format"));
-        }
-    }
-
-    public void testIndexedTemplate() throws Exception {
-        createIndex(ScriptService.SCRIPT_INDEX);
-        ensureGreen(ScriptService.SCRIPT_INDEX);
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "1a").setSource("{" +
-                "\"template\":{"+
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                    "}" +
-                "}"));
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "2").setSource("{" +
-                "\"template\":{"+
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                    "}" +
-                "}"));
-
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "3").setSource("{" +
-                "\"template\":{"+
-                "             \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}"));
-
-        indexRandom(true, builders);
-
-        builders.clear();
-
-        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-        indexRandom(true, builders);
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("fieldParam", "foo");
-
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                templateParams)).get();
-        assertHitCount(searchResponse, 4);
-
-        try {
-            client().prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/template_index/mustache/1000", ScriptService.ScriptType.INDEXED,
-                                    MustacheScriptEngineService.NAME, null, templateParams)).get();
-            fail("shouldn't get here");
-        } catch (SearchPhaseExecutionException spee) {
-            //all good
-        }
-
-        try {
-            searchResponse = client()
-                    .prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/myindex/mustache/1", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                    templateParams)).get();
-            assertFailures(searchResponse);
-        } catch (SearchPhaseExecutionException spee) {
-            //all good
-        }
-
-        searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-
-        templateParams.put("fieldParam", "bar");
-        searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/2", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                templateParams)).get();
-        assertHitCount(searchResponse, 1);
-
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("fieldParam", "bar");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "3", ScriptService.ScriptType.INDEXED, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 1);
-
-        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("fieldParam", "foo");
-        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
-        assertHitCount(sr, 4);
-
-        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
-        assertHitCount(sr, 4);
-    }
-
-    // Relates to #10397
-    public void testIndexedTemplateOverwrite() throws Exception {
-        createIndex("testindex");
-        ensureGreen("testindex");
-
-        index("testindex", "test", "1", jsonBuilder().startObject().field("searchtext", "dev1").endObject());
-        refresh();
-
-        int iterations = randomIntBetween(2, 11);
-        for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
-                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
-            assertEquals(i * 2 - 1, scriptResponse.getVersion());
-
-            GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "git01").get();
-            assertTrue(getResponse.isExists());
-
-            Map<String, Object> templateParams = new HashMap<>();
-            templateParams.put("P_Keyword1", "dev");
-
-            try {
-                client().prepareSearch("testindex")
-                        .setTypes("test")
-                        .setTemplate(
-                                new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                        templateParams)).get();
-                fail("Broken test template is parsing w/o error.");
-            } catch (SearchPhaseExecutionException e) {
-                // the above is expected to fail
-            }
-
-            PutIndexedScriptRequestBuilder builder = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
-                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"phrase_prefix\"}}}}").setOpType(
-                    OpType.INDEX);
-            scriptResponse = builder.get();
-            assertEquals(i * 2, scriptResponse.getVersion());
-            SearchResponse searchResponse = client()
-                    .prepareSearch("testindex")
-                    .setTypes("test")
-                    .setTemplate(
-                            new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                    .get();
-            assertHitCount(searchResponse, 1);
-        }
-    }
-
-    public void testIndexedTemplateWithArray() throws Exception {
-      createIndex(ScriptService.SCRIPT_INDEX);
-      ensureGreen(ScriptService.SCRIPT_INDEX);
-      List<IndexRequestBuilder> builders = new ArrayList<>();
-
-      String multiQuery = "{\"query\":{\"terms\":{\"theField\":[\"{{#fieldParam}}\",\"{{.}}\",\"{{/fieldParam}}\"]}}}";
-
-      builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "4").setSource(jsonBuilder().startObject().field("template", multiQuery).endObject()));
-
-      indexRandom(true,builders);
-
-      builders.clear();
-
-      builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-      builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-      builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-      builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-      builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-      indexRandom(true,builders);
-
-      Map<String, Object> arrayTemplateParams = new HashMap<>();
-      String[] fieldParams = {"foo","bar"};
-      arrayTemplateParams.put("fieldParam", fieldParams);
-
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/4", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                arrayTemplateParams)).get();
-        assertHitCount(searchResponse, 5);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
deleted file mode 100644
index d62a110..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Accountable;
-import org.elasticsearch.Version;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexSettings;
-import org.elasticsearch.index.analysis.AnalysisRegistry;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.IndicesWarmer;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
-import org.elasticsearch.indices.mapper.MapperRegistry;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.lang.reflect.Proxy;
-import java.util.Collections;
-
-import static org.hamcrest.Matchers.containsString;
-
-/**
- * Test parsing and executing a template request.
- */
-// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
-public class TemplateQueryParserTests extends ESTestCase {
-
-    private Injector injector;
-    private QueryShardContext context;
-
-    @Before
-    public void setup() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", this.getDataPath("config"))
-                .put("name", getClass().getName())
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-        final Client proxy = (Client) Proxy.newProxyInstance(
-                Client.class.getClassLoader(),
-                new Class[]{Client.class}, (proxy1, method, args) -> {
-                    throw new UnsupportedOperationException("client is just a dummy");
-                });
-        Index index = new Index("test");
-        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule() {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(Client.class).toInstance(proxy); // not needed here
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                    }
-                }
-        ).createInjector();
-
-        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
-        ScriptService scriptService = injector.getInstance(ScriptService.class);
-        SimilarityService similarityService = new SimilarityService(idxSettings, Collections.emptyMap());
-        MapperRegistry mapperRegistry = new IndicesModule().getMapperRegistry();
-        MapperService mapperService = new MapperService(idxSettings, analysisService, similarityService, mapperRegistry);
-        IndexFieldDataService indexFieldDataService =new IndexFieldDataService(idxSettings, injector.getInstance(IndicesFieldDataCache.class), injector.getInstance(CircuitBreakerService.class), mapperService);
-        BitsetFilterCache bitsetFilterCache = new BitsetFilterCache(idxSettings, new IndicesWarmer(idxSettings.getNodeSettings(), null), new BitsetFilterCache.Listener() {
-            @Override
-            public void onCache(ShardId shardId, Accountable accountable) {
-
-            }
-
-            @Override
-            public void onRemoval(ShardId shardId, Accountable accountable) {
-
-            }
-        });
-        IndicesQueriesRegistry indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        context = new QueryShardContext(idxSettings, proxy, bitsetFilterCache, indexFieldDataService, mapperService, similarityService, scriptService, indicesQueriesRegistry);
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        terminate(injector.getInstance(ThreadPool.class));
-    }
-
-    public void testParser() throws IOException {
-        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
-                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    /**
-     * Test that the template query parser can parse and evaluate template
-     * expressed as a single string but still it expects only the query
-     * specification (thus this test should fail with specific exception).
-     */
-    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        try {
-            parser.fromXContent(context.parseContext()).toQuery(context);
-            fail("Expected ParsingException");
-        } catch (ParsingException e) {
-            assertThat(e.getMessage(), containsString("query malformed, no field after start_object"));
-        }
-    }
-
-    public void testParserCanExtractTemplateNames() throws Exception {
-        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java b/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
index 9f4f2b5..8099322 100644
--- a/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
@@ -22,11 +22,14 @@ import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.rest.action.admin.indices.analyze.RestAnalyzeAction;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.hamcrest.core.IsNull;
 
 import java.io.IOException;
 
@@ -36,8 +39,10 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.startsWith;
 
+
 /**
  *
  */
@@ -201,7 +206,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
 
         AnalyzeRequest analyzeRequest = new AnalyzeRequest("for test");
 
-        RestAnalyzeAction.buildFromContent(content, analyzeRequest);
+        RestAnalyzeAction.buildFromContent(content, analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
 
         assertThat(analyzeRequest.text().length, equalTo(1));
         assertThat(analyzeRequest.text(), equalTo(new String[]{"THIS IS A TEST"}));
@@ -213,7 +218,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
         AnalyzeRequest analyzeRequest = new AnalyzeRequest("for test");
 
         try {
-            RestAnalyzeAction.buildFromContent(new BytesArray("{invalid_json}"), analyzeRequest);
+            RestAnalyzeAction.buildFromContent(new BytesArray("{invalid_json}"), analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
             fail("shouldn't get here");
         } catch (Exception e) {
             assertThat(e, instanceOf(IllegalArgumentException.class));
@@ -230,7 +235,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
             .endObject().bytes();
 
         try {
-            RestAnalyzeAction.buildFromContent(invalidContent, analyzeRequest);
+            RestAnalyzeAction.buildFromContent(invalidContent, analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
             fail("shouldn't get here");
         } catch (Exception e) {
             assertThat(e, instanceOf(IllegalArgumentException.class));
@@ -267,4 +272,235 @@ public class AnalyzeActionIT extends ESIntegTestCase {
 
     }
 
+    public void testDetailAnalyze() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+            .setSettings(
+                settingsBuilder()
+                    .put("index.analysis.char_filter.my_mapping.type", "mapping")
+                    .putArray("index.analysis.char_filter.my_mapping.mappings", "PH=>F")
+                    .put("index.analysis.analyzer.test_analyzer.type", "custom")
+                    .put("index.analysis.analyzer.test_analyzer.position_increment_gap", "100")
+                    .put("index.analysis.analyzer.test_analyzer.tokenizer", "standard")
+                    .putArray("index.analysis.analyzer.test_analyzer.char_filter", "my_mapping")
+                    .putArray("index.analysis.analyzer.test_analyzer.filter", "snowball")));
+        ensureGreen();
+
+        for (int i = 0; i < 10; i++) {
+            AnalyzeResponse analyzeResponse = admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText("THIS IS A PHISH")
+                .setExplain(true).setCharFilters("my_mapping").setTokenizer("keyword").setTokenFilters("lowercase").get();
+
+            assertThat(analyzeResponse.detail().analyzer(), IsNull.nullValue());
+            //charfilters
+            // global charfilter is not change text.
+            assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+            assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("my_mapping"));
+            assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(1));
+            assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("THIS IS A FISH"));
+            //tokenizer
+            assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("keyword"));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getTerm(), equalTo("THIS IS A FISH"));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getStartOffset(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getEndOffset(), equalTo(15));
+            //tokenfilters
+            assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("lowercase"));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getTerm(), equalTo("this is a fish"));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getPosition(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getStartOffset(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getEndOffset(), equalTo(15));
+        }
+    }
+
+    public void testDetailAnalyzeWithNoIndex() throws Exception {
+        //analyzer only
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST")
+            .setExplain(true).setAnalyzer("simple").get();
+
+        assertThat(analyzeResponse.detail().tokenizer(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().tokenfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().charfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(4));
+    }
+
+    public void testDetailAnalyzeCustomAnalyzerWithNoIndex() throws Exception {
+        //analyzer only
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST")
+            .setExplain(true).setAnalyzer("simple").get();
+
+        assertThat(analyzeResponse.detail().tokenizer(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().tokenfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().charfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(4));
+
+        //custom analyzer
+        analyzeResponse = client().admin().indices().prepareAnalyze("<text>THIS IS A TEST</text>")
+            .setExplain(true).setCharFilters("html_strip").setTokenizer("keyword").setTokenFilters("lowercase").get();
+        assertThat(analyzeResponse.detail().analyzer(), IsNull.nullValue());
+        //charfilters
+        // global charfilter is not change text.
+        assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("html_strip"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("\nTHIS IS A TEST\n"));
+        //tokenizer
+        assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("keyword"));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getTerm(), equalTo("\nTHIS IS A TEST\n"));
+        //tokenfilters
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("lowercase"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getTerm(), equalTo("\nthis is a test\n"));
+
+
+        //check other attributes
+        analyzeResponse = client().admin().indices().prepareAnalyze("This is troubled")
+            .setExplain(true).setTokenizer("standard").setTokenFilters("snowball").get();
+
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(3));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getTerm(), equalTo("troubl"));
+        String[] expectedAttributesKey = {
+            "bytes",
+            "positionLength",
+            "keyword"};
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().size(), equalTo(expectedAttributesKey.length));
+        Object extendedAttribute;
+
+        for (String key : expectedAttributesKey) {
+            extendedAttribute = analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().get(key);
+            assertThat(extendedAttribute, notNullValue());
+        }
+    }
+
+    public void testDetailAnalyzeSpecifyAttributes() throws Exception {
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("This is troubled")
+            .setExplain(true).setTokenizer("standard").setTokenFilters("snowball").setAttributes("keyword").get();
+
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(3));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getTerm(), equalTo("troubl"));
+        String[] expectedAttributesKey = {
+            "keyword"};
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().size(), equalTo(expectedAttributesKey.length));
+        Object extendedAttribute;
+
+        for (String key : expectedAttributesKey) {
+            extendedAttribute = analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().get(key);
+            assertThat(extendedAttribute, notNullValue());
+        }
+    }
+
+    public void testDetailAnalyzeWithMultiValues() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
+        ensureGreen();
+        client().admin().indices().preparePutMapping("test")
+            .setType("document").setSource("simple", "type=string,analyzer=simple,position_increment_gap=100").get();
+
+        String[] texts = new String[]{"THIS IS A TEST", "THE SECOND TEXT"};
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText(texts)
+            .setExplain(true).setField("simple").setText(texts).execute().get();
+
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(7));
+        AnalyzeResponse.AnalyzeToken token = analyzeResponse.detail().analyzer().getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("test"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(14));
+
+        token = analyzeResponse.detail().analyzer().getTokens()[5];
+        assertThat(token.getTerm(), equalTo("second"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(19));
+        assertThat(token.getEndOffset(), equalTo(25));
+    }
+
+    public void testDetailAnalyzeWithMultiValuesWithCustomAnalyzer() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+            .setSettings(
+                settingsBuilder()
+                    .put("index.analysis.char_filter.my_mapping.type", "mapping")
+                    .putArray("index.analysis.char_filter.my_mapping.mappings", "PH=>F")
+                    .put("index.analysis.analyzer.test_analyzer.type", "custom")
+                    .put("index.analysis.analyzer.test_analyzer.position_increment_gap", "100")
+                    .put("index.analysis.analyzer.test_analyzer.tokenizer", "standard")
+                    .putArray("index.analysis.analyzer.test_analyzer.char_filter", "my_mapping")
+                    .putArray("index.analysis.analyzer.test_analyzer.filter", "snowball", "lowercase")));
+        ensureGreen();
+
+        client().admin().indices().preparePutMapping("test")
+            .setType("document").setSource("simple", "type=string,analyzer=simple,position_increment_gap=100").get();
+
+        //only analyzer =
+        String[] texts = new String[]{"this is a PHISH", "the troubled text"};
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText(texts)
+            .setExplain(true).setAnalyzer("test_analyzer").setText(texts).execute().get();
+
+        // charfilter
+        assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("my_mapping"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(2));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("this is a FISH"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[1], equalTo("the troubled text"));
+
+        // tokenizer
+        assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("standard"));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(7));
+        AnalyzeResponse.AnalyzeToken token = analyzeResponse.detail().tokenizer().getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("FISH"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenizer().getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubled"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+        // tokenfilter(snowball)
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(2));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(7));
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("FISH"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubl"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+        // tokenfilter(lowercase)
+        assertThat(analyzeResponse.detail().tokenfilters()[1].getName(), equalTo("lowercase"));
+        assertThat(analyzeResponse.detail().tokenfilters()[1].getTokens().length, equalTo(7));
+        token = analyzeResponse.detail().tokenfilters()[1].getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("fish"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubl"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
index daefc20..fc888c7 100644
--- a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.script;
 
 import org.elasticsearch.common.ContextAndHeaderHolder;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
 
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -32,9 +29,6 @@ import java.util.Collections;
 import java.util.HashSet;
 import java.util.Set;
 
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.containsString;
-
 // TODO: these really should just be part of ScriptService tests, there is nothing special about them
 public class FileScriptTests extends ESTestCase {
 
diff --git a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
deleted file mode 100644
index 1cdac14..0000000
--- a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * A dummy script engine used for testing. Scripts must be a number. Running the script
- */
-public class MockScriptEngine implements ScriptEngineService {
-    public static final String NAME = "mockscript";
-
-    public static class TestPlugin extends Plugin {
-
-        public TestPlugin() {
-        }
-
-        @Override
-        public String name() {
-            return NAME;
-        }
-
-        @Override
-        public String description() {
-            return "Mock script engine for integration tests";
-        }
-
-        public void onModule(ScriptModule module) {
-            module.addScriptEngine(MockScriptEngine.class);
-        }
-
-    }
-
-    @Override
-    public String[] types() {
-        return new String[]{ NAME };
-    }
-
-    @Override
-    public String[] extensions() {
-        return types();
-    }
-
-    @Override
-    public boolean sandboxed() {
-        return true;
-    }
-
-    @Override
-    public Object compile(String script) {
-        return Integer.parseInt(script);
-    }
-
-    @Override
-    public ExecutableScript executable(CompiledScript compiledScript, @Nullable Map<String, Object> vars) {
-        return null;
-    }
-
-    @Override
-    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
-        return new SearchScript() {
-            @Override
-            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
-                AbstractSearchScript leafSearchScript = new AbstractSearchScript() {
-
-                    @Override
-                    public Object run() {
-                        return compiledScript.compiled();
-                    }
-
-                };
-                leafSearchScript.setLookup(lookup.getLeafSearchLookup(context));
-                return leafSearchScript;
-            }
-
-            @Override
-            public boolean needsScores() {
-                return false;
-            }
-        };
-    }
-
-    @Override
-    public void scriptRemoved(@Nullable CompiledScript script) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index 38ab78b..3e476d2 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -22,7 +22,6 @@ package org.elasticsearch.script;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.After;
@@ -45,7 +44,7 @@ import static org.hamcrest.Matchers.containsString;
 // TODO: this needs to be a base test class, and all scripting engines extend it
 public class ScriptModesTests extends ESTestCase {
     private static final Set<String> ALL_LANGS = unmodifiableSet(
-            newHashSet(MustacheScriptEngineService.NAME, "custom", "test"));
+            newHashSet("custom", "test"));
 
     static final String[] ENABLE_VALUES = new String[]{"on", "true", "yes", "1"};
     static final String[] DISABLE_VALUES = new String[]{"off", "false", "no", "0"};
@@ -73,7 +72,6 @@ public class ScriptModesTests extends ESTestCase {
         scriptContextRegistry = new ScriptContextRegistry(contexts.values());
         scriptContexts = scriptContextRegistry.scriptContexts().toArray(new ScriptContext[scriptContextRegistry.scriptContexts().size()]);
         scriptEngines = buildScriptEnginesByLangMap(newHashSet(
-                new MustacheScriptEngineService(Settings.EMPTY),
                 //add the native engine just to make sure it gets filtered out
                 new NativeScriptEngineService(Settings.EMPTY, Collections.<String, NativeScriptFactory>emptyMap()),
                 new CustomScriptEngineService()));
@@ -93,8 +91,8 @@ public class ScriptModesTests extends ESTestCase {
     public void assertAllSettingsWereChecked() {
         if (assertScriptModesNonNull) {
             assertThat(scriptModes, notNullValue());
-            //3 is the number of engines (native excluded), custom is counted twice though as it's associated with two different names
-            int numberOfSettings = 3 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
+            //2 is the number of engines (native excluded), custom is counted twice though as it's associated with two different names
+            int numberOfSettings = 2 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
             assertThat(scriptModes.scriptModes.size(), equalTo(numberOfSettings));
             if (assertAllSettingsWereChecked) {
                 assertThat(checkedSettings.size(), equalTo(numberOfSettings));
@@ -190,21 +188,6 @@ public class ScriptModesTests extends ESTestCase {
         assertScriptModes(ScriptMode.SANDBOX, ALL_LANGS, new ScriptType[]{ScriptType.INLINE}, complementOf);
     }
 
-    public void testInteractionBetweenGenericAndEngineSpecificSettings() {
-        Settings.Builder builder = Settings.builder().put("script.inline", randomFrom(DISABLE_VALUES))
-                .put(specificEngineOpSettings(MustacheScriptEngineService.NAME, ScriptType.INLINE, ScriptContext.Standard.AGGS), randomFrom(ENABLE_VALUES))
-                .put(specificEngineOpSettings(MustacheScriptEngineService.NAME, ScriptType.INLINE, ScriptContext.Standard.SEARCH), randomFrom(ENABLE_VALUES));
-        Set<String> mustacheLangSet = singleton(MustacheScriptEngineService.NAME);
-        Set<String> allButMustacheLangSet = new HashSet<>(ALL_LANGS);
-        allButMustacheLangSet.remove(MustacheScriptEngineService.NAME);
-        this.scriptModes = new ScriptModes(scriptEngines, scriptContextRegistry, builder.build());
-        assertScriptModes(ScriptMode.ON, mustacheLangSet, new ScriptType[]{ScriptType.INLINE}, ScriptContext.Standard.AGGS, ScriptContext.Standard.SEARCH);
-        assertScriptModes(ScriptMode.OFF, mustacheLangSet, new ScriptType[]{ScriptType.INLINE}, complementOf(ScriptContext.Standard.AGGS, ScriptContext.Standard.SEARCH));
-        assertScriptModesAllOps(ScriptMode.OFF, allButMustacheLangSet, ScriptType.INLINE);
-        assertScriptModesAllOps(ScriptMode.SANDBOX, ALL_LANGS, ScriptType.INDEXED);
-        assertScriptModesAllOps(ScriptMode.ON, ALL_LANGS, ScriptType.FILE);
-    }
-
     private void assertScriptModesAllOps(ScriptMode expectedScriptMode, Set<String> langs, ScriptType... scriptTypes) {
         assertScriptModes(expectedScriptMode, langs, scriptTypes, scriptContexts);
     }
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
index aa7df3f..23cada0 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.watcher.ResourceWatcherService;
@@ -73,8 +72,7 @@ public class ScriptServiceTests extends ESTestCase {
                 .put("path.conf", genericConfigFolder)
                 .build();
         resourceWatcherService = new ResourceWatcherService(baseSettings, null);
-        scriptEngineServices = newHashSet(new TestEngineService(),
-                                               new MustacheScriptEngineService(baseSettings));
+        scriptEngineServices = newHashSet(new TestEngineService());
         scriptEnginesByLangMap = ScriptModesTests.buildScriptEnginesByLangMap(scriptEngineServices);
         //randomly register custom script contexts
         int randomInt = randomIntBetween(0, 3);
@@ -199,10 +197,6 @@ public class ScriptServiceTests extends ESTestCase {
         createFileScripts("groovy", "mustache", "test");
 
         for (ScriptContext scriptContext : scriptContexts) {
-            //mustache engine is sandboxed, all scripts are enabled by default
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "file_script", ScriptType.FILE, scriptContext, contextAndHeaders);
             //custom engine is sandboxed, all scripts are enabled by default
             assertCompileAccepted("test", "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
             assertCompileAccepted("test", "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
deleted file mode 100644
index ce29bf2..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.io.StringWriter;
-import java.nio.charset.Charset;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- * Mustache based templating test
- */
-public class MustacheScriptEngineTests extends ESTestCase {
-    private MustacheScriptEngineService qe;
-    private JsonEscapingMustacheFactory escaper;
-
-    @Before
-    public void setup() {
-        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        escaper = new JsonEscapingMustacheFactory();
-    }
-
-    public void testSimpleParameterReplace() {
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            vars.put("body_val", "\"quick brown\"");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-    }
-
-    public void testEscapeJson() throws IOException {
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("hello \n world", writer);
-            assertThat(writer.toString(), equalTo("hello \\n world"));
-        }
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("\n", writer);
-            assertThat(writer.toString(), equalTo("\\n"));
-        }
-
-        Character[] specialChars = new Character[]{
-                '\"',
-                '\\',
-                '\u0000',
-                '\u0001',
-                '\u0002',
-                '\u0003',
-                '\u0004',
-                '\u0005',
-                '\u0006',
-                '\u0007',
-                '\u0008',
-                '\u0009',
-                '\u000B',
-                '\u000C',
-                '\u000E',
-                '\u000F',
-                '\u001F'};
-        String[] escapedChars = new String[]{
-                "\\\"",
-                "\\\\",
-                "\\u0000",
-                "\\u0001",
-                "\\u0002",
-                "\\u0003",
-                "\\u0004",
-                "\\u0005",
-                "\\u0006",
-                "\\u0007",
-                "\\u0008",
-                "\\u0009",
-                "\\u000B",
-                "\\u000C",
-                "\\u000E",
-                "\\u000F",
-                "\\u001F"};
-        int iters = scaledRandomIntBetween(100, 1000);
-        for (int i = 0; i < iters; i++) {
-            int rounds = scaledRandomIntBetween(1, 20);
-            StringWriter expect = new StringWriter();
-            StringWriter writer = new StringWriter();
-            for (int j = 0; j < rounds; j++) {
-                String s = getChars();
-                writer.write(s);
-                expect.write(s);
-
-                int charIndex = randomInt(7);
-                writer.append(specialChars[charIndex]);
-                expect.append(escapedChars[charIndex]);
-            }
-            StringWriter target = new StringWriter();
-            escaper.encode(writer.toString(), target);
-            assertThat(expect.toString(), equalTo(target.toString()));
-        }
-    }
-
-    private String getChars() {
-        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
-        for (int i = 0; i < string.length(); i++) {
-            if (isEscapeChar(string.charAt(i))) {
-                return string.substring(0, i);
-            }
-        }
-        return string;
-    }
-
-    /**
-     * From https://www.ietf.org/rfc/rfc4627.txt:
-     *
-     * All Unicode characters may be placed within the
-     * quotation marks except for the characters that must be escaped:
-     * quotation mark, reverse solidus, and the control characters (U+0000
-     * through U+001F).
-     * */
-    private static boolean isEscapeChar(char c) {
-        switch (c) {
-        case '"':
-        case '\\':
-            return true;
-        }
-
-        if (c < '\u002F')
-            return true;
-        return false;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
deleted file mode 100644
index 76c8678..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheFactory;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.StringReader;
-import java.io.StringWriter;
-import java.util.HashMap;
-
-/**
- * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
- * */
-public class MustacheTests extends ESTestCase {
-    public void test() {
-        HashMap<String, Object> scopes = new HashMap<>();
-        scopes.put("boost_val", "0.2");
-
-        String template = "GET _search {\"query\": " + "{\"boosting\": {"
-                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
-                + "}}, \"negative_boost\": {{boost_val}} } }}";
-        MustacheFactory f = new DefaultMustacheFactory();
-        Mustache mustache = f.compile(new StringReader(template), "example");
-        StringWriter writer = new StringWriter();
-        mustache.execute(writer, scopes);
-        writer.flush();
-        assertEquals(
-                "Mustache templating broken",
-                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
-                writer.toString());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
index 8d6edff..75fc9f9 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
@@ -21,6 +21,8 @@ package org.elasticsearch.search.highlight;
 
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -32,6 +34,13 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.IdsQueryParser;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
@@ -39,11 +48,15 @@ import org.elasticsearch.index.query.MatchAllQueryParser;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.TermQueryBuilder;
 import org.elasticsearch.index.query.TermQueryParser;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder.Field;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.IndexSettingsModule;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
@@ -51,6 +64,7 @@ import java.io.IOException;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -128,7 +142,7 @@ public class HighlightBuilderTests extends ESTestCase {
     }
 
     /**
-     * Generic test that creates new highlighter from the test highlighter and checks both for equality
+     *  creates random highlighter, renders it to xContent and back to new instance that should be equal to original
      */
     public void testFromXContent() throws IOException {
         QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
@@ -261,6 +275,70 @@ public class HighlightBuilderTests extends ESTestCase {
         } catch (ParsingException e) {
             assertEquals("cannot parse object with name [bad_fieldname]", e.getMessage());
         }
+     }
+
+     /**
+     * test that build() outputs a {@link SearchContextHighlight} that is similar to the one
+     * we would get when parsing the xContent the test highlight builder is rendering out
+     */
+    public void testBuildSearchContextHighlight() throws IOException {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
+        Index index = new Index(randomAsciiOfLengthBetween(1, 10));
+        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, indexSettings);
+        // shard context will only need indicesQueriesRegistry for building Query objects nested in highlighter
+        QueryShardContext mockShardContext = new QueryShardContext(idxSettings, null, null, null, null, null, null, indicesQueriesRegistry) {
+            @Override
+            public MappedFieldType fieldMapper(String name) {
+                StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+                return builder.build(new Mapper.BuilderContext(idxSettings.getSettings(), new ContentPath(1))).fieldType();
+            }
+        };
+        mockShardContext.setMapUnmappedFieldAsString(true);
+
+        for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
+            HighlightBuilder highlightBuilder = randomHighlighterBuilder();
+            SearchContextHighlight highlight = highlightBuilder.build(mockShardContext);
+            XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+            if (randomBoolean()) {
+                builder.prettyPrint();
+            }
+            builder.startObject();
+            highlightBuilder.innerXContent(builder);
+            builder.endObject();
+            XContentParser parser = XContentHelper.createParser(builder.bytes());
+
+            SearchContextHighlight parsedHighlight = new HighlighterParseElement().parse(parser, mockShardContext);
+            assertNotSame(highlight, parsedHighlight);
+            assertEquals(highlight.globalForceSource(), parsedHighlight.globalForceSource());
+            assertEquals(highlight.fields().size(), parsedHighlight.fields().size());
+
+            Iterator<org.elasticsearch.search.highlight.SearchContextHighlight.Field> iterator = parsedHighlight.fields().iterator();
+            for (org.elasticsearch.search.highlight.SearchContextHighlight.Field field : highlight.fields()) {
+                org.elasticsearch.search.highlight.SearchContextHighlight.Field otherField = iterator.next();
+                assertEquals(field.field(), otherField.field());
+                FieldOptions options = field.fieldOptions();
+                FieldOptions otherOptions = otherField.fieldOptions();
+                assertArrayEquals(options.boundaryChars(), options.boundaryChars());
+                assertEquals(options.boundaryMaxScan(), otherOptions.boundaryMaxScan());
+                assertEquals(options.encoder(), otherOptions.encoder());
+                assertEquals(options.fragmentCharSize(), otherOptions.fragmentCharSize());
+                assertEquals(options.fragmenter(), otherOptions.fragmenter());
+                assertEquals(options.fragmentOffset(), otherOptions.fragmentOffset());
+                assertEquals(options.highlighterType(), otherOptions.highlighterType());
+                assertEquals(options.highlightFilter(), otherOptions.highlightFilter());
+                assertEquals(options.highlightQuery(), otherOptions.highlightQuery());
+                assertEquals(options.matchedFields(), otherOptions.matchedFields());
+                assertEquals(options.noMatchSize(), otherOptions.noMatchSize());
+                assertEquals(options.numberOfFragments(), otherOptions.numberOfFragments());
+                assertEquals(options.options(), otherOptions.options());
+                assertEquals(options.phraseLimit(), otherOptions.phraseLimit());
+                assertArrayEquals(options.preTags(), otherOptions.preTags());
+                assertArrayEquals(options.postTags(), otherOptions.postTags());
+                assertEquals(options.requireFieldMatch(), otherOptions.requireFieldMatch());
+                assertEquals(options.scoreOrdered(), otherOptions.scoreOrdered());
+            }
+        }
     }
 
     /**
@@ -277,9 +355,9 @@ public class HighlightBuilderTests extends ESTestCase {
 
         context.reset(parser);
         HighlightBuilder highlightBuilder = HighlightBuilder.fromXContent(context);
-        assertArrayEquals("setting tags_schema 'styled' should alter pre_tags", HighlighterParseElement.STYLED_PRE_TAG,
+        assertArrayEquals("setting tags_schema 'styled' should alter pre_tags", HighlightBuilder.DEFAULT_STYLED_PRE_TAG,
                 highlightBuilder.preTags());
-        assertArrayEquals("setting tags_schema 'styled' should alter post_tags", HighlighterParseElement.STYLED_POST_TAGS,
+        assertArrayEquals("setting tags_schema 'styled' should alter post_tags", HighlightBuilder.DEFAULT_STYLED_POST_TAGS,
                 highlightBuilder.postTags());
 
         highlightElement = "{\n" +
@@ -289,9 +367,9 @@ public class HighlightBuilderTests extends ESTestCase {
 
         context.reset(parser);
         highlightBuilder = HighlightBuilder.fromXContent(context);
-        assertArrayEquals("setting tags_schema 'default' should alter pre_tags", HighlighterParseElement.DEFAULT_PRE_TAGS,
+        assertArrayEquals("setting tags_schema 'default' should alter pre_tags", HighlightBuilder.DEFAULT_PRE_TAGS,
                 highlightBuilder.preTags());
-        assertArrayEquals("setting tags_schema 'default' should alter post_tags", HighlighterParseElement.DEFAULT_POST_TAGS,
+        assertArrayEquals("setting tags_schema 'default' should alter post_tags", HighlightBuilder.DEFAULT_POST_TAGS,
                 highlightBuilder.postTags());
 
         highlightElement = "{\n" +
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
deleted file mode 100644
index 1850abc..0000000
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ /dev/null
@@ -1,1291 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.suggest;
-
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.ReduceSearchPhaseException;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.action.suggest.SuggestRequestBuilder;
-import org.elasticsearch.action.suggest.SuggestResponse;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.DirectCandidateGenerator;
-import org.elasticsearch.search.suggest.term.TermSuggestionBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
-
-import java.io.IOException;
-import java.net.URISyntaxException;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ExecutionException;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
-import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
-import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
- * possible these tests should declare for the first request, make the request, modify the configuration for the next request, make that
- * request, modify again, request again, etc.  This makes it very obvious what changes between requests.
- */
-public class SuggestSearchIT extends ESIntegTestCase {
-    // see #3196
-    public void testSuggestAcrossMultipleIndices() throws IOException {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "text", "abcd");
-        index("test", "type1", "2", "text", "aacd");
-        index("test", "type1", "3", "text", "abbd");
-        index("test", "type1", "4", "text", "abcc");
-        refresh();
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        logger.info("--> run suggestions with one index");
-        searchSuggest( termSuggest);
-        createIndex("test_1");
-        ensureGreen();
-
-        index("test_1", "type1", "1", "text", "ab cd");
-        index("test_1", "type1", "2", "text", "aa cd");
-        index("test_1", "type1", "3", "text", "ab bd");
-        index("test_1", "type1", "4", "text", "ab cc");
-        refresh();
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ab cd")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with two indices");
-        searchSuggest( termSuggest);
-
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("text").field("type", "string").field("analyzer", "keyword").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(prepareCreate("test_2").addMapping("type1", mapping));
-        ensureGreen();
-
-        index("test_2", "type1", "1", "text", "ab cd");
-        index("test_2", "type1", "2", "text", "aa cd");
-        index("test_2", "type1", "3", "text", "ab bd");
-        index("test_2", "type1", "4", "text", "ab cc");
-        index("test_2", "type1", "1", "text", "abcd");
-        index("test_2", "type1", "2", "text", "aacd");
-        index("test_2", "type1", "3", "text", "abbd");
-        index("test_2", "type1", "4", "text", "abcc");
-        refresh();
-
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ab cd")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with three indices");
-        try {
-            searchSuggest( termSuggest);
-            fail(" can not suggest across multiple indices with different analysis chains");
-        } catch (ReduceSearchPhaseException ex) {
-            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
-            assertThat(ex.getCause().getMessage(),
-                    anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
-                            endsWith("Suggest entries have different sizes actual [2] expected [1]")));
-        } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
-                    endsWith("Suggest entries have different sizes actual [2] expected [1]")));
-        }
-
-
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ABCD")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with four indices");
-        try {
-            searchSuggest( termSuggest);
-            fail(" can not suggest across multiple indices with different analysis chains");
-        } catch (ReduceSearchPhaseException ex) {
-            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
-            assertThat(ex.getCause().getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
-                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
-        } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
-                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
-        }
-    }
-
-    // see #3037
-    public void testSuggestModes() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put("index.analysis.analyzer.biword.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("name")
-                    .field("type", "multi_field")
-                    .startObject("fields")
-                        .startObject("name")
-                            .field("type", "string")
-                        .endObject()
-                        .startObject("shingled")
-                            .field("type", "string")
-                            .field("analyzer", "biword")
-                            .field("search_analyzer", "standard")
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-
-        index("test", "type1", "1", "name", "I like iced tea");
-        index("test", "type1", "2", "name", "I like tea.");
-        index("test", "type1", "3", "name", "I like ice cream.");
-        refresh();
-
-        DirectCandidateGenerator generator = candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2);
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
-                .addCandidateGenerator(generator)
-                .gramSize(3);
-        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, "did_you_mean", "iced tea");
-
-        generator.suggestMode(null);
-        searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestionSize(searchSuggest, 0, 0, "did_you_mean");
-    }
-
-    // see #2729
-    public void testSizeOneShard() throws Exception {
-        prepareCreate("test").setSettings(
-                SETTING_NUMBER_OF_SHARDS, 1,
-                SETTING_NUMBER_OF_REPLICAS, 0).get();
-        ensureGreen();
-
-        for (int i = 0; i < 15; i++) {
-            index("test", "type1", Integer.toString(i), "text", "abc" + i);
-        }
-        refresh();
-
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellchecker")).get();
-        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
-        TermSuggestionBuilder termSuggestion = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text")
-                .size(10);
-        Suggest suggest = searchSuggest( termSuggestion);
-        assertSuggestion(suggest, 0, "test", 10, "abc0");
-
-        termSuggestion.text("abcd").shardSize(5);
-        suggest = searchSuggest( termSuggestion);
-        assertSuggestion(suggest, 0, "test", 5, "abc0");
-    }
-
-    public void testUnmappedField() throws IOException, InterruptedException, ExecutionException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.biword.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 3));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("name")
-                    .field("type", "multi_field")
-                    .startObject("fields")
-                        .startObject("name")
-                            .field("type", "string")
-                        .endObject()
-                        .startObject("shingled")
-                            .field("type", "string")
-                            .field("analyzer", "biword")
-                            .field("search_analyzer", "standard")
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        indexRandom(true, client().prepareIndex("test", "type1").setSource("name", "I like iced tea"),
-        client().prepareIndex("test", "type1").setSource("name", "I like tea."),
-        client().prepareIndex("test", "type1").setSource("name", "I like ice cream."));
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2))
-                .gramSize(3);
-        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, 0, "did_you_mean", "iced tea");
-
-        phraseSuggestion.field("nosuchField");
-        {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
-        }
-        {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
-        }
-    }
-
-    public void testSimple() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "text", "abcd");
-        index("test", "type1", "2", "text", "aacd");
-        index("test", "type1", "3", "text", "abbd");
-        index("test", "type1", "4", "text", "abcc");
-        refresh();
-
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellcecker")).get();
-        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        Suggest suggest = searchSuggest( termSuggest);
-        assertSuggestion(suggest, 0, "test", "aacd", "abbd", "abcc");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-
-        suggest = searchSuggest( termSuggest);
-        assertSuggestion(suggest, 0, "test", "aacd","abbd", "abcc");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-    }
-
-    public void testEmpty() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "foo", "bar");
-        refresh();
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        Suggest suggest = searchSuggest( termSuggest);
-        assertSuggestionSize(suggest, 0, 0, "test");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-
-        suggest = searchSuggest( termSuggest);
-        assertSuggestionSize(suggest, 0, 0, "test");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-    }
-
-    public void testWithMultipleCommands() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "typ1", "1", "field1", "prefix_abcd", "field2", "prefix_efgh");
-        index("test", "typ1", "2", "field1", "prefix_aacd", "field2", "prefix_eeeh");
-        index("test", "typ1", "3", "field1", "prefix_abbd", "field2", "prefix_efff");
-        index("test", "typ1", "4", "field1", "prefix_abcc", "field2", "prefix_eggg");
-        refresh();
-
-        Suggest suggest = searchSuggest(
-                termSuggestion("size1")
-                        .size(1).text("prefix_abcd").maxTermFreq(10).prefixLength(1).minDocFreq(0)
-                        .field("field1").suggestMode("always"),
-                termSuggestion("field2")
-                        .field("field2").text("prefix_eeeh prefix_efgh")
-                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"),
-                termSuggestion("accuracy")
-                        .field("field2").text("prefix_efgh").setAccuracy(1f)
-                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"));
-        assertSuggestion(suggest, 0, "size1", "prefix_aacd");
-        assertThat(suggest.getSuggestion("field2").getEntries().get(0).getText().string(), equalTo("prefix_eeeh"));
-        assertSuggestion(suggest, 0, "field2", "prefix_efgh");
-        assertThat(suggest.getSuggestion("field2").getEntries().get(1).getText().string(), equalTo("prefix_efgh"));
-        assertSuggestion(suggest, 1, "field2", "prefix_eeeh", "prefix_efff", "prefix_eggg");
-        assertSuggestionSize(suggest, 0, 0, "accuracy");
-    }
-
-    public void testSizeAndSort() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        Map<String, Integer> termsAndDocCount = new HashMap<>();
-        termsAndDocCount.put("prefix_aaad", 20);
-        termsAndDocCount.put("prefix_abbb", 18);
-        termsAndDocCount.put("prefix_aaca", 16);
-        termsAndDocCount.put("prefix_abba", 14);
-        termsAndDocCount.put("prefix_accc", 12);
-        termsAndDocCount.put("prefix_addd", 10);
-        termsAndDocCount.put("prefix_abaa", 8);
-        termsAndDocCount.put("prefix_dbca", 6);
-        termsAndDocCount.put("prefix_cbad", 4);
-        termsAndDocCount.put("prefix_aacd", 1);
-        termsAndDocCount.put("prefix_abcc", 1);
-        termsAndDocCount.put("prefix_accd", 1);
-
-        for (Map.Entry<String, Integer> entry : termsAndDocCount.entrySet()) {
-            for (int i = 0; i < entry.getValue(); i++) {
-                index("test", "type1", entry.getKey() + i, "field1", entry.getKey());
-            }
-        }
-        refresh();
-
-        Suggest suggest = searchSuggest( "prefix_abcd",
-                termSuggestion("size3SortScoreFirst")
-                        .size(3).minDocFreq(0).field("field1").suggestMode("always"),
-                termSuggestion("size10SortScoreFirst")
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always").shardSize(50),
-                termSuggestion("size3SortScoreFirstMaxEdits1")
-                        .maxEdits(1)
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always"),
-                termSuggestion("size10SortFrequencyFirst")
-                        .size(10).sort("frequency").shardSize(1000)
-                        .minDocFreq(0).field("field1").suggestMode("always"));
-
-        // The commented out assertions fail sometimes because suggestions are based off of shard frequencies instead of index frequencies.
-        assertSuggestion(suggest, 0, "size3SortScoreFirst", "prefix_aacd", "prefix_abcc", "prefix_accd");
-        assertSuggestion(suggest, 0, "size10SortScoreFirst", 10, "prefix_aacd", "prefix_abcc", "prefix_accd" /*, "prefix_aaad" */);
-        assertSuggestion(suggest, 0, "size3SortScoreFirstMaxEdits1", "prefix_aacd", "prefix_abcc", "prefix_accd");
-        assertSuggestion(suggest, 0, "size10SortFrequencyFirst", "prefix_aaad", "prefix_abbb", "prefix_aaca", "prefix_abba",
-                "prefix_accc", "prefix_addd", "prefix_abaa", "prefix_dbca", "prefix_cbad", "prefix_aacd");
-
-        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
-        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
-    }
-
-    // see #2817
-    public void testStopwordsOnlyPhraseSuggest() throws IOException {
-        assertAcked(prepareCreate("test").addMapping("typ1", "body", "type=string,analyzer=stopwd").setSettings(
-                settingsBuilder()
-                        .put("index.analysis.analyzer.stopwd.tokenizer", "whitespace")
-                        .putArray("index.analysis.analyzer.stopwd.filter", "stop")
-        ));
-        ensureGreen();
-        index("test", "typ1", "1", "body", "this is a test");
-        refresh();
-
-        Suggest searchSuggest = searchSuggest( "a an the",
-                phraseSuggestion("simple_phrase").field("body").gramSize(1)
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always"))
-                        .size(1));
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-    }
-
-    public void testPrefixLength() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
-                .startObject("properties")
-                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
-                .startObject("body_reverse").field("type", "string").field("analyzer", "reverse").endObject()
-                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        index("test", "type1", "1", "body", "hello world");
-        index("test", "type1", "2", "body", "hello world");
-        index("test", "type1", "3", "body", "hello words");
-        refresh();
-
-        Suggest searchSuggest = searchSuggest( "hello word",
-                phraseSuggestion("simple_phrase").field("body")
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(4).minWordLength(1).suggestMode("always"))
-                        .size(1).confidence(1.0f));
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello words");
-
-        searchSuggest = searchSuggest( "hello word",
-                phraseSuggestion("simple_phrase").field("body")
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(2).minWordLength(1).suggestMode("always"))
-                        .size(1).confidence(1.0f));
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello world");
-    }
-
-    @Nightly
-    public void testMarvelHerosPhraseSuggest() throws IOException, URISyntaxException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("_all")
-                        .field("store", "yes")
-                        .field("termVector", "with_positions_offsets")
-                    .endObject()
-                    .startObject("properties")
-                        .startObject("body").
-                            field("type", "string").
-                            field("analyzer", "body")
-                        .endObject()
-                        .startObject("body_reverse").
-                            field("type", "string").
-                            field("analyzer", "reverse")
-                         .endObject()
-                         .startObject("bigram").
-                             field("type", "string").
-                             field("analyzer", "bigram")
-                         .endObject()
-                     .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        for (String line : readMarvelHeroNames()) {
-            index("test", "type1", line, "body", line, "body_reverse", line, "bigram", line);
-        }
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggest = phraseSuggestion("simple_phrase")
-                .field("bigram").gramSize(2).analyzer("body")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-                .size(1);
-        Suggest searchSuggest = searchSuggest( "american ame", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("american ame"));
-
-        phraseSuggest.realWordErrorLikelihood(0.95f);
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-        // Check the "text" field this one time.
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel"));
-
-        // Ask for highlighting
-        phraseSuggest.highlight("<em>", "</em>");
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getOptions().get(0).getHighlighted().string(), equalTo("<em>xorr</em> the <em>god</em> jewel"));
-
-        // pass in a correct phrase
-        phraseSuggest.highlight(null, null).confidence(0f).size(1).maxErrors(0.5f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // pass in a correct phrase - set confidence to 2
-        phraseSuggest.confidence(2f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // pass in a correct phrase - set confidence to 0.99
-        phraseSuggest.confidence(0.99f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        //test reverse suggestions with pre & post filter
-        phraseSuggest
-            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-            .addCandidateGenerator(candidateGenerator("body_reverse").minWordLength(1).suggestMode("always").preFilter("reverse").postFilter("reverse"));
-        searchSuggest = searchSuggest( "xor the yod-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // set all mass to trigrams (not indexed)
-        phraseSuggest.clearCandidateGenerators()
-            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-            .smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(1,0,0));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // set all mass to bigrams
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0,1,0));
-        searchSuggest =  searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // distribute mass
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        searchSuggest = searchSuggest( "american ame", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
-
-        // try all smoothing methods
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.Laplace(0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // check tokenLimit
-        phraseSuggest.smoothingModel(null).tokenLimit(4);
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        phraseSuggest.tokenLimit(15).smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel xorr the god jewel xorr the god jewel");
-        // Check the name this time because we're repeating it which is funky
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
-    }
-    
-    private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
-        return Files.readAllLines(PathUtils.get(SuggestSearchIT.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
-    }
-
-    public void testSizePararm() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("_all")
-                            .field("store", "yes")
-                            .field("termVector", "with_positions_offsets")
-                        .endObject()
-                        .startObject("properties")
-                            .startObject("body")
-                                .field("type", "string")
-                                .field("analyzer", "body")
-                            .endObject()
-                         .startObject("body_reverse")
-                             .field("type", "string")
-                             .field("analyzer", "reverse")
-                         .endObject()
-                         .startObject("bigram")
-                             .field("type", "string")
-                             .field("analyzer", "bigram")
-                         .endObject()
-                     .endObject()
-                 .endObject()
-             .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        String line = "xorr the god jewel";
-        index("test", "type1", "1", "body", line, "body_reverse", line, "bigram", line);
-        line = "I got it this time";
-        index("test", "type1", "2", "body", line, "body_reverse", line, "bigram", line);
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
-                .realWordErrorLikelihood(0.95f)
-                .field("bigram")
-                .gramSize(2)
-                .analyzer("body")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(1).accuracy(0.1f))
-                .smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1))
-                .maxErrors(1.0f)
-                .size(5);
-        Suggest searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // we allow a size of 2 now on the shard generator level so "god" will be found since it's LD2
-        phraseSuggestion.clearCandidateGenerators()
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(2).accuracy(0.1f));
-        searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-    }
-
-    @Nightly
-    public void testPhraseBoundaryCases() throws IOException, URISyntaxException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings()).put(SETTING_NUMBER_OF_SHARDS, 1) // to get reliable statistics we should put this all into one shard
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.analyzer.ngram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.ngram.filter", "my_shingle2", "lowercase")
-                .put("index.analysis.analyzer.myDefAnalyzer.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.myDefAnalyzer.filter", "shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle2.type", "shingle")
-                .put("index.analysis.filter.my_shingle2.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle2.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle2.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                    .startObject().startObject("type1")
-                    .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
-                .startObject("properties")
-                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
-                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
-                .startObject("ngram").field("type", "string").field("analyzer", "ngram").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        for (String line : readMarvelHeroNames()) {
-            index("test", "type1", line, "body", line, "bigram", line, "ngram", line);
-        }
-        refresh();
-
-        NumShards numShards = getNumShards("test");
-
-        // Lets make sure some things throw exceptions
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
-                .field("bigram")
-                .analyzer("body")
-                .addCandidateGenerator(candidateGenerator("does_not_exist").minWordLength(1).suggestMode("always"))
-                .realWordErrorLikelihood(0.95f)
-                .maxErrors(0.5f)
-                .size(1);
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("field does not exists");
-        } catch (SearchPhaseExecutionException e) {}
-
-        phraseSuggestion.clearCandidateGenerators().analyzer(null);
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("analyzer does only produce ngrams");
-        } catch (SearchPhaseExecutionException e) {
-        }
-
-        phraseSuggestion.analyzer("bigram");
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("analyzer does only produce ngrams");
-        } catch (SearchPhaseExecutionException e) {
-        }
-
-        // Now we'll make sure some things don't
-        phraseSuggestion.forceUnigrams(false);
-        searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // Field doesn't produce unigrams but the analyzer does
-        phraseSuggestion.forceUnigrams(true).field("bigram").analyzer("ngram");
-        searchSuggest( "Xor the Got-Jewel",
-                phraseSuggestion);
-
-        phraseSuggestion.field("ngram").analyzer("myDefAnalyzer")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"));
-        Suggest suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // "xorr the god jewel" and and "xorn the god jewel" have identical scores (we are only using unigrams to score), so we tie break by
-        // earlier term (xorn):
-        assertSuggestion(suggest, 0, "simple_phrase", "xorn the god jewel");
-
-        phraseSuggestion.analyzer(null);
-        suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // In this case xorr has a better score than xorn because we set the field back to the default (my_shingle2) analyzer, so the
-        // probability that the term is not in the dictionary but is NOT a misspelling is relatively high in this case compared to the
-        // others that have no n-gram with the other terms in the phrase :) you can set this realWorldErrorLikelyhood
-        assertSuggestion(suggest, 0, "simple_phrase", "xorr the god jewel");
-    }
-
-    public void testDifferentShardSize() throws Exception {
-        createIndex("test");
-        ensureGreen();
-        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("field1", "foobar1").setRouting("1"),
-                client().prepareIndex("test", "type1", "2").setSource("field1", "foobar2").setRouting("2"),
-                client().prepareIndex("test", "type1", "3").setSource("field1", "foobar3").setRouting("3"));
-
-        Suggest suggest = searchSuggest( "foobar",
-                termSuggestion("simple")
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always"));
-        ElasticsearchAssertions.assertSuggestionSize(suggest, 0, 3, "simple");
-    }
-
-    // see #3469
-    public void testShardFailures() throws IOException, InterruptedException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 5)
-                .put("index.analysis.filter.shingler.output_unigrams", true));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type2")
-                .startObject("properties")
-                    .startObject("name")
-                        .field("type", "multi_field")
-                        .startObject("fields")
-                            .startObject("name")
-                                .field("type", "string")
-                                .field("analyzer", "suggest")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type2", mapping));
-        ensureGreen();
-
-        index("test", "type2", "1", "foo", "bar");
-        index("test", "type2", "2", "foo", "bar");
-        index("test", "type2", "3", "foo", "bar");
-        index("test", "type2", "4", "foo", "bar");
-        index("test", "type2", "5", "foo", "bar");
-        index("test", "type2", "1", "name", "Just testing the suggestions api");
-        index("test", "type2", "2", "name", "An other title about equal length");
-        // Note that the last document has to have about the same length as the other or cutoff rechecking will remove the useful suggestion.
-        refresh();
-
-        // When searching on a shard with a non existing mapping, we should fail
-        SearchRequestBuilder request = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
-        assertThrows(request, SearchPhaseExecutionException.class);
-
-        // When searching on a shard which does not hold yet any document of an existing type, we should not fail
-        SearchResponse searchResponse = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
-            .get();
-        ElasticsearchAssertions.assertNoFailures(searchResponse);
-        ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
-    }
-
-    // see #3469
-    public void testEmptyShards() throws IOException, InterruptedException {
-        XContentBuilder mappingBuilder = XContentFactory.jsonBuilder().
-                startObject().
-                    startObject("type1").
-                        startObject("properties").
-                            startObject("name").
-                                field("type", "multi_field").
-                                startObject("fields").
-                                    startObject("name").
-                                        field("type", "string").
-                                        field("analyzer", "suggest").
-                                    endObject().
-                                endObject().
-                            endObject().
-                        endObject().
-                    endObject().
-                endObject();
-        assertAcked(prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 5)
-                .put("index.analysis.filter.shingler.output_unigrams", true)).addMapping("type1", mappingBuilder));
-        ensureGreen();
-
-        index("test", "type2", "1", "foo", "bar");
-        index("test", "type2", "2", "foo", "bar");
-        index("test", "type1", "1", "name", "Just testing the suggestions api");
-        index("test", "type1", "2", "name", "An other title about equal length");
-        refresh();
-
-        SearchResponse searchResponse = client().prepareSearch()
-                .setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
-                .get();
-
-        assertNoFailures(searchResponse);
-        assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
-    }
-
-    /**
-     * Searching for a rare phrase shouldn't provide any suggestions if confidence &gt; 1.  This was possible before we rechecked the cutoff
-     * score during the reduce phase.  Failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
-     */
-    public void testSearchForRarePhrase() throws IOException {
-        // If there isn't enough chaf per shard then shards can become unbalanced, making the cutoff recheck this is testing do more harm then good.
-        int chafPerShard = 100;
-
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("_all")
-                            .field("store", "yes")
-                            .field("termVector", "with_positions_offsets")
-                        .endObject()
-                        .startObject("properties")
-                            .startObject("body")
-                                .field("type", "string")
-                                .field("analyzer", "body")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        NumShards test = getNumShards("test");
-
-        List<String> phrases = new ArrayList<>();
-        Collections.addAll(phrases, "nobel prize", "noble gases", "somethingelse prize", "pride and joy", "notes are fun");
-        for (int i = 0; i < 8; i++) {
-            phrases.add("noble somethingelse" + i);
-        }
-        for (int i = 0; i < test.numPrimaries * chafPerShard; i++) {
-            phrases.add("chaff" + i);
-        }
-        for (String phrase: phrases) {
-            index("test", "type1", phrase, "body", phrase);
-        }
-        refresh();
-
-        Suggest searchSuggest = searchSuggest("nobel prize", phraseSuggestion("simple_phrase")
-                .field("body")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
-                .confidence(2f)
-                .maxErrors(5f)
-                .size(1));
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        searchSuggest = searchSuggest("noble prize", phraseSuggestion("simple_phrase")
-                .field("body")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
-                .confidence(2f)
-                .maxErrors(5f)
-                .size(1));
-        assertSuggestion(searchSuggest, 0, 0, "simple_phrase", "nobel prize");
-    }
-
-    @Nightly
-    public void testSuggestWithManyCandidates() throws InterruptedException, ExecutionException, IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("properties")
-                            .startObject("title")
-                                .field("type", "string")
-                                .field("analyzer", "text")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        // We're going to be searching for:
-        //   united states house of representatives elections in washington 2006
-        // But we need to make sure we generate a ton of suggestions so we add a bunch of candidates.
-        // Many of these candidates are drawn from page names on English Wikipedia.
-
-        // Tons of different options very near the exact query term
-        titles.add("United States House of Representatives Elections in Washington 1789");
-        for (int year = 1790; year < 2014; year+= 2) {
-            titles.add("United States House of Representatives Elections in Washington " + year);
-        }
-        // Six of these are near enough to be viable suggestions, just not the top one
-
-        // But we can't stop there!  Titles that are just a year are pretty common so lets just add one per year
-        // since 0.  Why not?
-        for (int year = 0; year < 2015; year++) {
-            titles.add(Integer.toString(year));
-        }
-        // That ought to provide more less good candidates for the last term
-
-        // Now remove or add plural copies of every term we can
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        // Now some possessive
-        titles.add("Washington's Birthday");
-
-        // And some conjugation
-        titles.add("Unified Modeling Language");
-        titles.add("Unite Against Fascism");
-        titles.add("Stated Income Tax");
-        titles.add("Media organizations housed within colleges");
-
-        // And other stuff
-        titles.add("Untied shoelaces");
-        titles.add("Unit circle");
-        titles.add("Untitled");
-        titles.add("Unicef");
-        titles.add("Unrated");
-        titles.add("UniRed");
-        titles.add("Jalan Uniten–Dengkil"); // Highway in Malaysia
-        titles.add("UNITAS");
-        titles.add("UNITER");
-        titles.add("Un-Led-Ed");
-        titles.add("STATS LLC");
-        titles.add("Staples");
-        titles.add("Skates");
-        titles.add("Statues of the Liberators");
-        titles.add("Staten Island");
-        titles.add("Statens Museum for Kunst");
-        titles.add("Hause"); // The last name or the German word, whichever.
-        titles.add("Hose");
-        titles.add("Hoses");
-        titles.add("Howse Peak");
-        titles.add("The Hoose-Gow");
-        titles.add("Hooser");
-        titles.add("Electron");
-        titles.add("Electors");
-        titles.add("Evictions");
-        titles.add("Coronal mass ejection");
-        titles.add("Wasington"); // A film?
-        titles.add("Warrington"); // A town in England
-        titles.add("Waddington"); // Lots of places have this name
-        titles.add("Watlington"); // Ditto
-        titles.add("Waplington"); // Yup, also a town
-        titles.add("Washing of the Spears"); // Book
-
-        for (char c = 'A'; c <= 'Z'; c++) {
-            // Can't forget lists, glorious lists!
-            titles.add("List of former members of the United States House of Representatives (" + c + ")");
-
-            // Lots of people are named Washington <Middle Initial>. LastName
-            titles.add("Washington " + c + ". Lastname");
-
-            // Lets just add some more to be evil
-            titles.add("United " + c);
-            titles.add("States " + c);
-            titles.add("House " + c);
-            titles.add("Elections " + c);
-            titles.add("2006 " + c);
-            titles.add(c + " United");
-            titles.add(c + " States");
-            titles.add(c + " House");
-            titles.add(c + " Elections");
-            titles.add(c + " 2006");
-        }
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
-        }
-        indexRandom(true, builders);
-
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(1000) // Setting a silly high size helps of generate a larger list of candidates for testing.
-                        .maxInspections(1000) // This too
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(30000);
-        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
-        assertSuggestionSize(searchSuggest, 0, 25480, "title");  // Just to prove that we've run through a ton of options
-
-        suggest.size(1);
-        long start = System.currentTimeMillis();
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        long total = System.currentTimeMillis() - start;
-        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
-        // assertThat(total, lessThan(1000L)); // Takes many seconds without fix - just for debugging
-    }
-
-    public void testPhraseSuggesterCollate() throws InterruptedException, ExecutionException, IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("type1")
-                .startObject("properties")
-                .startObject("title")
-                .field("type", "string")
-                .field("analyzer", "text")
-                .endObject()
-                .endObject()
-                .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        titles.add("United States House of Representatives Elections in Washington 2006");
-        titles.add("United States House of Representatives Elections in Washington 2005");
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
-        }
-        indexRandom(true, builders);
-
-        // suggest without collate
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(10)
-                        .maxInspections(200)
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(10);
-        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        assertSuggestionSize(searchSuggest, 0, 10, "title");
-
-        // suggest with collate
-        String filterString = XContentFactory.jsonBuilder()
-                    .startObject()
-                        .startObject("match_phrase")
-                            .field("title", "{{suggestion}}")
-                        .endObject()
-                    .endObject()
-                .string();
-        PhraseSuggestionBuilder filteredQuerySuggest = suggest.collateQuery(filterString);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredQuerySuggest);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate suggest with no result (boundary case)
-        searchSuggest = searchSuggest("Elections of Representatives Parliament", filteredQuerySuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "title");
-
-        NumShards numShards = getNumShards("test");
-
-        // collate suggest with bad query
-        String incorrectFilterString = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("test")
-                        .field("title", "{{suggestion}}")
-                    .endObject()
-                .endObject()
-                .string();
-        PhraseSuggestionBuilder incorrectFilteredSuggest = suggest.collateQuery(incorrectFilterString);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, incorrectFilteredSuggest);
-            fail("Post query error has been swallowed");
-        } catch(ElasticsearchException e) {
-            // expected
-        }
-
-        // suggest with collation
-        String filterStringAsFilter = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("match_phrase")
-                .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(filterStringAsFilter);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredFilterSuggest);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate suggest with bad query
-        String filterStr = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("pprefix")
-                        .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PhraseSuggestionBuilder in = suggest.collateQuery(filterStr);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, in);
-            fail("Post filter error has been swallowed");
-        } catch(ElasticsearchException e) {
-            //expected
-        }
-
-        // collate script failure due to no additional params
-        String collateWithParams = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("{{query_type}}")
-                    .field("{{query_field}}", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-
-        PhraseSuggestionBuilder phraseSuggestWithNoParams = suggest.collateQuery(collateWithParams);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, phraseSuggestWithNoParams);
-            fail("Malformed query (lack of additional params) should fail");
-        } catch (ElasticsearchException e) {
-            // expected
-        }
-
-        // collate script with additional params
-        Map<String, Object> params = new HashMap<>();
-        params.put("query_type", "match_phrase");
-        params.put("query_field", "title");
-
-        PhraseSuggestionBuilder phraseSuggestWithParams = suggest.collateQuery(collateWithParams).collateParams(params);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParams);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate query request with prune set to true
-        PhraseSuggestionBuilder phraseSuggestWithParamsAndReturn = suggest.collateQuery(collateWithParams).collateParams(params).collatePrune(true);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParamsAndReturn);
-        assertSuggestionSize(searchSuggest, 0, 10, "title");
-        assertSuggestionPhraseCollateMatchExists(searchSuggest, "title", 2);
-    }
-
-    protected Suggest searchSuggest(SuggestionBuilder<?>... suggestion) {
-        return searchSuggest(null, suggestion);
-    }
-
-    protected Suggest searchSuggest(String suggestText, SuggestionBuilder<?>... suggestions) {
-        return searchSuggest(suggestText, 0, suggestions);
-    }
-
-    protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
-        if (randomBoolean()) {
-            SearchRequestBuilder builder = client().prepareSearch().setSize(0);
-            SuggestBuilder suggestBuilder = new SuggestBuilder();
-            if (suggestText != null) {
-                suggestBuilder.setText(suggestText);
-            }
-            for (SuggestionBuilder<?> suggestion : suggestions) {
-                suggestBuilder.addSuggestion(suggestion);
-            }
-            builder.suggest(suggestBuilder);
-            SearchResponse actionGet = builder.execute().actionGet();
-            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
-            return actionGet.getSuggest();
-        } else {
-            SuggestRequestBuilder builder = client().prepareSuggest();
-            if (suggestText != null) {
-                builder.setSuggestText(suggestText);
-            }
-            for (SuggestionBuilder<?> suggestion : suggestions) {
-                builder.addSuggestion(suggestion);
-            }
-
-            SuggestResponse actionGet = builder.execute().actionGet();
-            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
-            if (expectShardsFailed > 0) {
-                throw new SearchPhaseExecutionException("suggest", "Suggest execution failed", new ShardSearchFailure[0]);
-            }
-            return actionGet.getSuggest();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java b/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
index 0bfc76f..b0de061 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
@@ -94,7 +94,8 @@ public class RepositoriesIT extends AbstractSnapshotIntegTestCase {
         assertThat(repositoriesMetaData.repository("test-repo-2").type(), equalTo("fs"));
 
         logger.info("--> check that both repositories can be retrieved by getRepositories query");
-        GetRepositoriesResponse repositoriesResponse = client.admin().cluster().prepareGetRepositories().get();
+        GetRepositoriesResponse repositoriesResponse = client.admin().cluster()
+            .prepareGetRepositories(randomFrom("_all", "*", "test-repo-*")).get();
         assertThat(repositoriesResponse.repositories().size(), equalTo(2));
         assertThat(findRepository(repositoriesResponse.repositories(), "test-repo-1"), notNullValue());
         assertThat(findRepository(repositoriesResponse.repositories(), "test-repo-2"), notNullValue());
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index afbdf9d..57a22c0 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -149,7 +149,10 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
         assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
         assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
 
-        SnapshotInfo snapshotInfo = client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0);
+        List<SnapshotInfo> snapshotInfos = client.admin().cluster().prepareGetSnapshots("test-repo")
+            .setSnapshots(randomFrom("test-snap", "_all", "*", "*-snap", "test*")).get().getSnapshots();
+        assertThat(snapshotInfos.size(), equalTo(1));
+        SnapshotInfo snapshotInfo = snapshotInfos.get(0);
         assertThat(snapshotInfo.state(), equalTo(SnapshotState.SUCCESS));
         assertThat(snapshotInfo.version(), equalTo(Version.CURRENT));
 
diff --git a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
index 838c2a6..60f1bad 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.NodeBuilder;
+import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.ESIntegTestCase.Scope;
@@ -46,13 +46,20 @@ import java.lang.management.ThreadMXBean;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.*;
+import java.util.concurrent.BrokenBarrierException;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.Executor;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Pattern;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.sameInstance;
 
 /**
  */
@@ -185,7 +192,7 @@ public class SimpleThreadPoolIT extends ESIntegTestCase {
                 .put("tribe.t1.plugin.mandatory", "non_existing").build();
 
         try {
-            NodeBuilder.nodeBuilder().settings(settings).build();
+            new Node(settings);
             fail("The node startup is supposed to fail");
         } catch(Throwable t) {
             //all good
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index 19def40..28a3dea 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -21,13 +21,13 @@ package org.elasticsearch.tribe;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Priority;
@@ -37,7 +37,6 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
 import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.NodeConfigurationSource;
@@ -143,9 +142,7 @@ public class TribeIT extends ESIntegTestCase {
                 .put("node.name", "tribe_node") // make sure we can identify threads from this node
                 .build();
 
-        tribeNode = NodeBuilder.nodeBuilder()
-                .settings(merged)
-                .node();
+        tribeNode = new Node(merged).start();
         tribeClient = tribeNode.client();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
deleted file mode 100644
index e819286..0000000
--- a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.validate;
-
-import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.SuiteScopeTestCase
-public class RenderSearchTemplateIT extends ESIntegTestCase {
-    private static final String TEMPLATE_CONTENTS = "{\"size\":\"{{size}}\",\"query\":{\"match\":{\"foo\":\"{{value}}\"}},\"aggs\":{\"objects\":{\"terms\":{\"field\":\"{{value}}\",\"size\":\"{{size}}\"}}}}";
-
-    @Override
-    protected void setupSuiteScopeCluster() throws Exception {
-        client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "index_template_1", "{ \"template\": " + TEMPLATE_CONTENTS + " }").get();
-    }
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        //Set path so ScriptService will pick up the test scripts
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config")).build();
-    }
-
-    public void testInlineTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-
-    public void testIndexedTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-
-    public void testFileTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache b/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache
deleted file mode 100644
index 5191414..0000000
--- a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-  "query": {
-    "match": { "{{myField}}" : "{{myValue}}" }
-  },
-  "size" : {{mySize}}
-}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache b/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache
deleted file mode 100644
index a779da7..0000000
--- a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache
+++ /dev/null
@@ -1,3 +0,0 @@
-{
-    "match_{{template}}": {}
-}
diff --git a/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache b/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache
deleted file mode 100644
index 969dc8d..0000000
--- a/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache
+++ /dev/null
@@ -1 +0,0 @@
-{"size":"{{size}}","query":{"match":{"foo":"{{value}}"}},"aggs":{"objects":{"terms":{"field":"{{value}}","size":"{{size}}"}}}}
\ No newline at end of file
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index b7bc00d..3fa61c4 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -70,6 +70,7 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "lang-expression",
                    "lang-groovy",
                    "lang-javascript",
+                   "lang-plan-a",
                    "lang-python",
                    "mapper-murmur3",
                    "mapper-size",
diff --git a/distribution/build.gradle b/distribution/build.gradle
index 56a602c..4da1641 100644
--- a/distribution/build.gradle
+++ b/distribution/build.gradle
@@ -47,7 +47,7 @@ ext.dependencyFiles = project(':core').configurations.runtime.copyRecursive().ex
  *                                  Modules                                  *
  *****************************************************************************/
 
-task buildModules(type: Copy) {
+task buildModules(type: Sync) {
   into 'build/modules'
 } 
 
@@ -55,21 +55,31 @@ ext.restTestExpansions = [
   'expected.modules.count': 0,
 ]
 // we create the buildModules task above so the distribution subprojects can
-// depend on it, but we don't actually configure it until projects are evaluated
-// so it can depend on the bundling of plugins (ie modules must have been configured)
-project.gradle.projectsEvaluated {
-  project.rootProject.subprojects.findAll { it.path.startsWith(':modules:') }.each { Project module ->
-    buildModules {
-      dependsOn module.bundlePlugin
-      into(module.name) {
-        from { zipTree(module.bundlePlugin.outputs.files.singleFile) }
-      }
-    }
-    configure(subprojects.findAll { it.name != 'integ-test-zip' }) { Project distribution ->
-      distribution.integTest.mustRunAfter(module.integTest)      
+// depend on it, but we don't actually configure it until here so we can do a single
+// loop over modules to also setup cross task dependencies and increment our modules counter
+project.rootProject.subprojects.findAll { it.path.startsWith(':modules:') }.each { Project module ->
+  buildModules {
+    dependsOn({ project(module.path).bundlePlugin })
+    into(module.name) {
+      from { zipTree(project(module.path).bundlePlugin.outputs.files.singleFile) }
     }
-    restTestExpansions['expected.modules.count'] += 1
   }
+  // We would like to make sure integ tests for the distribution run after
+  // integ tests for the modules included in the distribution. However, gradle
+  // has a bug where depending on a task with a finalizer can sometimes not make
+  // the finalizer task follow the original task immediately. To work around this,
+  // we make the mustRunAfter the finalizer task itself.
+  // See https://discuss.gradle.org/t/cross-project-task-dependencies-ordering-screws-up-finalizers/13190
+  project.configure(project.subprojects.findAll { it.name != 'integ-test-zip' }) { Project distribution ->
+    distribution.afterEvaluate({
+      distribution.integTest.mustRunAfter("${module.path}:integTest#stop")
+    })
+  }
+  // also want to make sure the module's integration tests run after the integ-test-zip (ie rest tests)
+  module.afterEvaluate({
+    module.integTest.mustRunAfter(':distribution:integ-test-zip:integTest#stop')
+  })
+  restTestExpansions['expected.modules.count'] += 1
 }
 
 // make sure we have a clean task since we aren't a java project, but we have tasks that
@@ -84,11 +94,15 @@ subprojects {
    *****************************************************************************/
   apply plugin: 'elasticsearch.rest-test'
   project.integTest {
-    dependsOn(project.assemble)
+    dependsOn project.assemble
     includePackaged project.name == 'integ-test-zip'
     cluster {
       distribution = project.name
     }
+    if (project.name != 'integ-test-zip') {
+      // see note above with module mustRunAfter about why integTest#stop is used here
+      mustRunAfter ':distribution:integ-test-zip:integTest#stop'
+    }
   }
   
   processTestResources {
diff --git a/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/licenses/compiler-0.9.1.jar.sha1 b/distribution/licenses/compiler-0.9.1.jar.sha1
deleted file mode 100644
index 96152e0..0000000
--- a/distribution/licenses/compiler-0.9.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-14aec5344639782ee76441401b773946c65eb2b3
diff --git a/distribution/licenses/compiler-LICENSE.txt b/distribution/licenses/compiler-LICENSE.txt
deleted file mode 100644
index ac68303..0000000
--- a/distribution/licenses/compiler-LICENSE.txt
+++ /dev/null
@@ -1,14 +0,0 @@
-Copyright 2010 RightTime, Inc.
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
diff --git a/distribution/licenses/compiler-NOTICE.txt b/distribution/licenses/compiler-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/distribution/licenses/compiler-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ff69dc3..0000000
--- a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-feaf885ed4155fb7202c1f90ac2eb40503961efc
\ No newline at end of file
diff --git a/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..5d95f64
--- /dev/null
+++ b/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9f2b9811a4f4a57a1b3a98bdc1e1b63476b9f628
\ No newline at end of file
diff --git a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 1341c03..0000000
--- a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5b5b5c950b4fcac38cf48fab911f75da61e780fa
\ No newline at end of file
diff --git a/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..0ae258b
--- /dev/null
+++ b/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+038071889a5dbeb279e37fa46225e194139a427c
\ No newline at end of file
diff --git a/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 10ffbd1..0000000
--- a/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-84685d37a34b4d87e2928566ed266a7f005ca67d
\ No newline at end of file
diff --git a/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..aee7c10
--- /dev/null
+++ b/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+b986d0ad8ee4dda8172a5a61875c47631e4b21d4
\ No newline at end of file
diff --git a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 6eed3e2..0000000
--- a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ff92011208ed5c28f041acc37bd77728a89fc6a5
\ No newline at end of file
diff --git a/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..aa1011e
--- /dev/null
+++ b/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f46574fbdfbcc81d936c77e15ba5b3af2c2b7253
\ No newline at end of file
diff --git a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ac8fb4d..0000000
--- a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5d46f26a6cb36aede89b8728b6fcbc427d4f9416
\ No newline at end of file
diff --git a/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..561f17e
--- /dev/null
+++ b/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f620262d667a294d390e8df7575cc2cca2626559
\ No newline at end of file
diff --git a/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index fade025..0000000
--- a/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-726ea07bbfdfbfbee80522353496fc6667dc33c9
\ No newline at end of file
diff --git a/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..4735bdf
--- /dev/null
+++ b/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4c44b07242fd706f6f7f14c9063a725e0e5b98cd
\ No newline at end of file
diff --git a/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 4ce0f78..0000000
--- a/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d8d7a7b573a4cfc54745a126e905ccfd523b7a24
\ No newline at end of file
diff --git a/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..9c19a6a
--- /dev/null
+++ b/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+1e33e0aa5fc227e90c8314f61b4cba1090035e33
\ No newline at end of file
diff --git a/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 245438c..0000000
--- a/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-cd9d4fb4492bd2680cea2f038a051311329f6443
\ No newline at end of file
diff --git a/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..c4a61bf
--- /dev/null
+++ b/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+e416893f7b781239a15d3e2c7200ff26574d14de
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 5244f41..0000000
--- a/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a1a04d191443e51f992ed3dd02d0e14fd48493c9
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..edc5675
--- /dev/null
+++ b/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+b153b63b9333feedb18af2673eb6ccaf95bcc8bf
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 4600767..0000000
--- a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c4d34b29b8b14ad3deb300a6d699e9d8965a3c2c
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..eddd3d6
--- /dev/null
+++ b/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+0aa2758d70a79f2e0f33a87624fd9d31e155c864
\ No newline at end of file
diff --git a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 7ad16ae..0000000
--- a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-bf45dbd653d66ce9d2c3f19b69997b8098d8b416
\ No newline at end of file
diff --git a/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..571903c
--- /dev/null
+++ b/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+873c716ba629dae389b12ddb1aedf2f5c5f57fea
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index e366610..0000000
--- a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2bddfda70f5c657064d12860b03c2cd8a5029bfc
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..5e6a27b
--- /dev/null
+++ b/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9d7e47c2fb73c614cc5ca41529b2c273c73b0ce7
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ed120db..0000000
--- a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-881b8cd571fb3ccdcc69f1316468d816812513fb
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..cf841e1
--- /dev/null
+++ b/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4766305088797a66fe02d5aaa98e086867816e42
\ No newline at end of file
diff --git a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index adab682..0000000
--- a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-466e2bc02f45f04cbf516e5df78b9c2ebd99e944
\ No newline at end of file
diff --git a/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..1fbb60a
--- /dev/null
+++ b/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f0ee6fb780ea8aa9ec6d31e6a9cc7d48700bd2ca
\ No newline at end of file
diff --git a/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/src/main/resources/config/elasticsearch.yml b/distribution/src/main/resources/config/elasticsearch.yml
index 51630fe..4b335ce 100644
--- a/distribution/src/main/resources/config/elasticsearch.yml
+++ b/distribution/src/main/resources/config/elasticsearch.yml
@@ -60,19 +60,8 @@
 # For more information, see the documentation at:
 # <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html>
 #
-# ---------------------------------- Gateway -----------------------------------
-#
-# Block initial recovery after a full cluster restart until N nodes are started:
-#
-# gateway.recover_after_nodes: 3
-#
-# For more information, see the documentation at:
-# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html>
-#
 # --------------------------------- Discovery ----------------------------------
 #
-# Elasticsearch nodes will find each other via unicast, by default.
-#
 # Pass an initial list of hosts to perform discovery when new node is started:
 # The default list of hosts is ["127.0.0.1", "[::1]"]
 #
@@ -85,6 +74,15 @@
 # For more information, see the documentation at:
 # <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html>
 #
+# ---------------------------------- Gateway -----------------------------------
+#
+# Block initial recovery after a full cluster restart until N nodes are started:
+#
+# gateway.recover_after_nodes: 3
+#
+# For more information, see the documentation at:
+# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html>
+#
 # ---------------------------------- Various -----------------------------------
 #
 # Disable starting multiple nodes on a single system:
diff --git a/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/docs/java-api/client.asciidoc b/docs/java-api/client.asciidoc
index cfc45b7..87aa729 100644
--- a/docs/java-api/client.asciidoc
+++ b/docs/java-api/client.asciidoc
@@ -37,11 +37,10 @@ that can execute operations against elasticsearch.
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
-Node node = nodeBuilder().node();
+Node node = new Node(Settings.EMPTY).start();
 Client client = node.client();
 
 // on shutdown
@@ -86,17 +85,15 @@ it):
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
 // Embedded node clients behave just like standalone nodes,
 // which means that they will leave the HTTP port open!
-Node node =
-    nodeBuilder()
-        .settings(Settings.settingsBuilder().put("http.enabled", false))
-        .client(true)
-    .node();
+Node node = new Node(Settings.settingsBuilder()
+        .put("http.enabled", false)
+        .put("node.client", true).build())
+    .start();
 
 Client client = node.client();
 
@@ -115,11 +112,10 @@ and form a cluster.
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
-Node node = nodeBuilder().local(true).node();
+Node node = new Node(Settings.builder().put("node.local", true).build()).start();
 Client client = node.client();
 
 // on shutdown
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
index a2b8049..bdd46fb 100644
--- a/docs/plugins/discovery-ec2.asciidoc
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -64,16 +64,19 @@ cloud:
             protocol: https
 ----
 
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
-`http` or `https`):
+In addition, a proxy can be configured with the `proxy.host`, `proxy.port`, `proxy.username` and `proxy.password` settings
+(note that protocol can be `http` or `https`):
 
 [source,yaml]
 ----
 cloud:
     aws:
         protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
+        proxy:
+            host: proxy1.company.com
+            port: 8083
+            username: myself
+            password: theBestPasswordEver!
 ----
 
 You can also set different proxies for `ec2` and `s3`:
@@ -83,11 +86,17 @@ You can also set different proxies for `ec2` and `s3`:
 cloud:
     aws:
         s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy1.company.com
+                port: 8083
+                username: myself1
+                password: theBestPasswordEver1!
         ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy2.company.com
+                port: 8083
+                username: myself2
+                password: theBestPasswordEver2!
 ----
 
 [[discovery-ec2-usage-region]]
diff --git a/docs/plugins/ingest.asciidoc b/docs/plugins/ingest.asciidoc
deleted file mode 100644
index 4c0cc6a..0000000
--- a/docs/plugins/ingest.asciidoc
+++ /dev/null
@@ -1,524 +0,0 @@
-[[ingest]]
-== Ingest Plugin
-
-=== Processors
-
-==== Set processor
-Sets one field and associates it with the specified value. If the field already exists,
-its value will be replaced with the provided one.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field1": 582.1
-  }
-}
---------------------------------------------------
-
-==== Remove processor
-Removes an existing field. If the field doesn't exist, an exception will be thrown
-
-[source,js]
---------------------------------------------------
-{
-  "remove": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Rename processor
-Renames an existing field. If the field doesn't exist, an exception will be thrown. Also, the new field
-name must not exist.
-
-[source,js]
---------------------------------------------------
-{
-  "rename": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-
-==== Convert processor
-Converts an existing field's value to a different type, like turning a string to an integer.
-If the field value is an array, all members will be converted.
-
-The supported types include: `integer`, `float`, `string`, and `boolean`.
-
-`boolean` will set the field to true if its string value is equal to `true` (ignore case), to
-false if its string value is equal to `false` (ignore case) and it will throw exception otherwise.
-
-[source,js]
---------------------------------------------------
-{
-  "convert": {
-    "foo": "integer"
-  }
-}
---------------------------------------------------
-
-==== Gsub processor
-Converts a string field by applying a regular expression and a replacement.
-If the field is not a string, the processor will throw an exception.
-
-This configuration takes a `field` for the field name, `pattern` for the
-pattern to be replaced, and `replacement` for the string to replace the matching patterns with.
-
-
-[source,js]
---------------------------------------------------
-{
-  "gsub": {
-    "field": "field1",
-    "pattern": "\.",
-    "replacement": "-"
-  }
-}
---------------------------------------------------
-
-==== Join processor
-Joins each element of an array into a single string using a separator character between each element.
-Throws error when the field is not an array.
-
-[source,js]
---------------------------------------------------
-{
-  "join": {
-    "field": "joined_array_field",
-    "separator": "-"
-  }
-}
---------------------------------------------------
-
-==== Split processor
-Split a field to an array using a separator character. Only works on string fields.
-
-[source,js]
---------------------------------------------------
-{
-  "split": {
-    "field": ","
-  }
-}
---------------------------------------------------
-
-==== Lowercase processor
-Converts a string to its lowercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "lowercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Uppercase processor
-Converts a string to its uppercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "uppercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Trim processor
-Trims whitespace from field. NOTE: this only works on leading and trailing whitespaces.
-
-[source,js]
---------------------------------------------------
-{
-  "trim": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Grok Processor
-
-The Grok Processor extracts structured fields out of a single text field within a document. You choose which field to
-extract matched fields from, as well as the Grok Pattern you expect will match. A Grok Pattern is like a regular
-expression that supports aliased expressions that can be reused.
-
-This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format
-that is generally written for humans and not computer consumption.
-
-The processor comes packaged with over 120 reusable patterns that are located at `$ES_HOME/config/ingest/grok/patterns`.
-Here, you can add your own custom grok pattern files with custom grok expressions to be used by the processor.
-
-If you need help building patterns to match your logs, you will find the <http://grokdebug.herokuapp.com> and
-<http://grokconstructor.appspot.com/> applications quite useful!
-
-===== Grok Basics
-
-Grok sits on top of regular expressions, so any regular expressions are valid in grok as well.
-The regular expression library is Oniguruma, and you can see the full supported regexp syntax
-https://github.com/kkos/oniguruma/blob/master/doc/RE[on the Onigiruma site].
-
-Grok works by leveraging this regular expression language to allow naming existing patterns and combining them into more
-complex patterns that match your fields.
-
-The syntax for re-using a grok pattern comes in three forms: `%{SYNTAX:SEMANTIC}`, `%{SYNTAX}`, `%{SYNTAX:SEMANTIC:TYPE}`.
-
-The `SYNTAX` is the name of the pattern that will match your text. For example, `3.44` will be matched by the `NUMBER`
-pattern and `55.3.244.1` will be matched by the `IP` pattern. The syntax is how you match. `NUMBER` and `IP` are both
-patterns that are provided within the default patterns set.
-
-The `SEMANTIC` is the identifier you give to the piece of text being matched. For example, `3.44` could be the
-duration of an event, so you could call it simply `duration`. Further, a string `55.3.244.1` might identify
-the `client` making a request.
-
-The `TYPE` is the type you wish to cast your named field. `int` and `float` are currently the only types supported for coercion.
-
-For example, here is a grok pattern that would match the above example given. We would like to match a text with the following
-contents:
-
-[source,js]
---------------------------------------------------
-3.44 55.3.244.1
---------------------------------------------------
-
-We may know that the above message is a number followed by an IP-address. We can match this text with the following
-Grok expression.
-
-[source,js]
---------------------------------------------------
-%{NUMBER:duration} %{IP:client}
---------------------------------------------------
-
-===== Custom Patterns and Pattern Files
-
-The Grok Processor comes pre-packaged with a base set of pattern files. These patterns may not always have
-what you are looking for. These pattern files have a very basic format. Each line describes a named pattern with
-the following format:
-
-[source,js]
---------------------------------------------------
-NAME ' '+ PATTERN '\n'
---------------------------------------------------
-
-You can add this pattern to an existing file, or add your own file in the patterns directory here: `$ES_HOME/config/ingest/grok/patterns`.
-The Ingest Plugin will pick up files in this directory to be loaded into the grok processor's known patterns. These patterns are loaded
-at startup, so you will need to do a restart your ingest node if you wish to update these files while running.
-
-Example snippet of pattern definitions found in the `grok-patterns` patterns file:
-
-[source,js]
---------------------------------------------------
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
---------------------------------------------------
-
-===== Using Grok Processor in a Pipeline
-
-[[grok-options]]
-.Grok Options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to use for grok expression parsing
-| `match_pattern`        | yes       | -                   | The grok expression to match and extract named captures with
-| `pattern_definitions`  | no        | -                   | A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.
-|======
-
-Here is an example of using the provided patterns to extract out and name structured fields from a string field in
-a document.
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043"
-}
---------------------------------------------------
-
-The pattern for this could be
-
-[source]
---------------------------------------------------
-%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
---------------------------------------------------
-
-An example pipeline for processing the above document using Grok:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-This pipeline will insert these named captures as new fields within the document, like so:
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043",
-  "client": "55.3.244.1",
-  "method": "GET",
-  "request": "/index.html",
-  "bytes": 15824,
-  "duration": "0.043"
-}
---------------------------------------------------
-
-An example of a pipeline specifying custom pattern definitions:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "my %{FAVORITE_DOG:dog} is colored %{RGB:color}"
-        "pattern_definitions" : {
-          "FAVORITE_DOG" : "beagle",
-          "RGB" : "RED|GREEN|BLUE"
-        }
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-
-==== Geoip processor
-
-The GeoIP processor adds information about the geographical location of IP addresses, based on data from the Maxmind databases.
-This processor adds this information by default under the `geoip` field.
-
-The ingest plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
-under the CCA-ShareAlike 3.0 license. For more details see, http://dev.maxmind.com/geoip/geoip2/geolite2/
-
-The GeoIP processor can run with other geoip2 databases from Maxmind. The files must be copied into the geoip config directory
-and the `database_file` option should be used to specify the filename of the custom database. The geoip config directory
-is located at `$ES_HOME/config/ingest/geoip` and holds the shipped databases too.
-
-[[geoip-options]]
-.Geoip options
-[options="header"]
-|======
-| Name                   | Required  | Default                                                                            | Description
-| `source_field`         | yes       | -                                                                                  | The field to get the ip address or hostname from for the geographical lookup.
-| `target_field`         | no        | geoip                                                                              | The field that will hold the geographical information looked up from the Maxmind database.
-| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
-| `fields`               | no        | [`continent_name`, `country_iso_code`, `region_name`, `city_name`, `location`] <1> | Controls what properties are added to the `target_field` based on the geoip lookup.
-|======
-
-<1> Depends on what is available in `database_field`:
-* If the GeoLite2 City database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name`, `continent_name`, `region_name`, `city_name`, `timezone`, `latitude`, `longitude`
-and `location`. The fields actually added depend on what has been found and which fields were configured in `fields`.
-* If the GeoLite2 Country database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name` and `continent_name`.The fields actually added depend on what has been found and which fields were configured in `fields`.
-
-An example that uses the default city database and adds the geographical information to the `geoip` field based on the `ip` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-An example that uses the default country database and add the geographical information to the `geo` field based on the `ip` field`:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip",
-        "target_field" : "geo",
-        "database_file" : "GeoLite2-Country.mmdb"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Date processor
-
-The date processor is used for parsing dates from fields, and then using that date or timestamp as the timestamp for that document.
-The date processor adds by default the parsed date as a new field called `@timestamp`, configurable by setting the `target_field`
-configuration parameter. Multiple date formats are supported as part of the same date processor definition. They will be used
-sequentially to attempt parsing the date field, in the same order they were defined as part of the processor definition.
-
-[[date-options]]
-.Date options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to get the date from.
-| `target_field`         | no        | @timestamp          | The field that will hold the parsed date.
-| `match_formats`        | yes       | -                   | Array of the expected date formats. Can be a joda pattern or one of the following formats: ISO8601, UNIX, UNIX_MS, TAI64N.
-| `timezone`             | no        | UTC                 | The timezone to use when parsing the date.
-| `locale`               | no        | ENGLISH             | The locale to use when parsing the date, relevant when parsing month names or week days.
-|======
-
-An example that adds the parsed date to the `timestamp` field based on the `initial_date` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "date" : {
-        "match_field" : "initial_date",
-        "target_field" : "timestamp",
-        "match_formats" : ["dd/MM/yyyy hh:mm:ss"],
-        "timezone" : "Europe/Amsterdam"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Meta processor
-
-The `meta` processor allows to modify metadata properties of a document being processed.
-
-The following example changes the index of a document to `alternative_index` instead of indexing it into an index
-that was specified in the index or bulk request:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "meta" : {
-        "_index" : "alternative_index"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-The following metadata attributes can be modified in this processor: `_index`, `_type`, `_id`, `_routing`, `_parent`,
-`_timestamp` and `_ttl`. All these metadata attributes can be specified in the body of the `meta` processor.
-
-Also the metadata settings in this processor are templatable which allows metadata field values to be replaced with
-field values in the source of the document being indexed. The mustache template language is used and anything between
-`{{` and `}}` can contain a template and point to any field in the source of the document.
-
-The following example documents being processed end up being indexed into an index based on the resolved city name by
-the `geoip` processor. (for example `city-amsterdam`)
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source" : "ip"
-      }
-    },
-    {
-      "meta" : {
-        "_index" : "city-{{geoip.city_name}}"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-=== Put pipeline API
-
-The put pipeline api adds pipelines and updates existing pipelines in the cluster.
-
-[source,js]
---------------------------------------------------
-PUT _ingest/pipeline/my-pipeline-id
-{
-  "description" : "describe pipeline",
-  "processors" : [
-    {
-      "simple" : {
-        // settings
-      }
-    },
-    // other processors
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-NOTE: Each ingest node updates its processors asynchronously in the background, so it may take a few seconds for all
-      nodes to have the latest version of the pipeline.
-
-=== Get pipeline API
-
-The get pipeline api returns pipelines based on id. This api always returns a local reference of the pipeline.
-
-[source,js]
---------------------------------------------------
-GET _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
-
-Example response:
-
-[source,js]
---------------------------------------------------
-{
-   "my-pipeline-id": {
-      "_source" : {
-        "description": "describe pipeline",
-        "processors": [
-          {
-            "simple" : {
-              // settings
-            }
-          },
-          // other processors
-        ]
-      },
-      "_version" : 0
-   }
-}
---------------------------------------------------
-
-For each returned pipeline the source and the version is returned.
-The version is useful for knowing what version of the pipeline the node has.
-Multiple ids can be provided at the same time. Also wildcards are supported.
-
-=== Delete pipeline API
-
-The delete pipeline api deletes pipelines by id.
-
-[source,js]
---------------------------------------------------
-DELETE _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
diff --git a/docs/plugins/repository-s3.asciidoc b/docs/plugins/repository-s3.asciidoc
index 1650588..faaa873 100644
--- a/docs/plugins/repository-s3.asciidoc
+++ b/docs/plugins/repository-s3.asciidoc
@@ -67,16 +67,19 @@ cloud:
             protocol: https
 ----
 
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
-`http` or `https`):
+In addition, a proxy can be configured with the `proxy.host`, `proxy.port`, `proxy.username` and `proxy.password` settings
+(note that protocol can be `http` or `https`):
 
 [source,yaml]
 ----
 cloud:
     aws:
         protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
+        proxy:
+            host: proxy1.company.com
+            port: 8083
+            username: myself
+            password: theBestPasswordEver!
 ----
 
 You can also set different proxies for `ec2` and `s3`:
@@ -86,11 +89,17 @@ You can also set different proxies for `ec2` and `s3`:
 cloud:
     aws:
         s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy1.company.com
+                port: 8083
+                username: myself1
+                password: theBestPasswordEver1!
         ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy2.company.com
+                port: 8083
+                username: myself2
+                password: theBestPasswordEver2!
 ----
 
 [[repository-s3-usage-region]]
diff --git a/docs/reference/api-conventions.asciidoc b/docs/reference/api-conventions.asciidoc
index 98fbcae..6bbd041 100644
--- a/docs/reference/api-conventions.asciidoc
+++ b/docs/reference/api-conventions.asciidoc
@@ -360,6 +360,22 @@ are:
 `s`::   Second
 `ms`::  Milli-second
 
+[[size-units]]
+[float]
+=== Data size units
+
+Whenever the size of data needs to be specified, eg when setting a buffer size
+parameter, the value must specify the unit, like `10kb` for 10 kilobytes.  The
+supported units are:
+
+[horizontal]
+`b`::   Bytes
+`kb`::  Kilobytes
+`mb`::  Megabytes
+`gb`::  Gigabytes
+`tb`::  Terabytes
+`pb`::  Petabytes
+
 [[distance-units]]
 [float]
 === Distance Units
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index 34d1cba..4acd1f1 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -7,6 +7,7 @@
 :jdk:           1.8.0_25
 :defguide:      https://www.elastic.co/guide/en/elasticsearch/guide/current
 :plugins:       https://www.elastic.co/guide/en/elasticsearch/plugins/master
+:javaclient:    https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/
 :issue:         https://github.com/elastic/elasticsearch/issues/
 :pull:          https://github.com/elastic/elasticsearch/pull/
 
diff --git a/docs/reference/indices/aliases.asciidoc b/docs/reference/indices/aliases.asciidoc
index 9a65c89..57faa97 100644
--- a/docs/reference/indices/aliases.asciidoc
+++ b/docs/reference/indices/aliases.asciidoc
@@ -63,7 +63,22 @@ curl -XPOST 'http://localhost:9200/_aliases' -d '
 }'
 --------------------------------------------------
 
-Alternatively, you can use a glob pattern to associate an alias to
+Multiple indices can be specified for an action with the `indices` array syntax:
+
+[source,js]
+--------------------------------------------------
+curl -XPOST 'http://localhost:9200/_aliases' -d '
+{
+    "actions" : [
+        { "add" : { "indices" : ["test1", "test2"], "alias" : "alias1" } }
+    ]
+}'
+--------------------------------------------------
+
+To specify multiple aliases in one action, the corresponding `aliases` array
+syntax exists as well.
+
+For the example above, a glob pattern can also be used to associate an alias to
 more than one index that share a common name:
 
 [source,js]
diff --git a/docs/reference/indices/analyze.asciidoc b/docs/reference/indices/analyze.asciidoc
index 1a256a6..1e8cd77 100644
--- a/docs/reference/indices/analyze.asciidoc
+++ b/docs/reference/indices/analyze.asciidoc
@@ -100,3 +100,74 @@ provided it doesn't start with `{` :
 --------------------------------------------------
 curl -XGET 'localhost:9200/_analyze?tokenizer=keyword&token_filters=lowercase&char_filters=html_strip' -d 'this is a <b>test</b>'
 --------------------------------------------------
+
+=== Explain Analyze
+
+If you want to get more advanced details, set `explain` to `true` (defaults to `false`). It will output all token attributes for each token.
+You can filter token attributes you want to output by setting `attributes` option.
+
+experimental[The format of the additional detail information is experimental and can change at any time]
+
+[source,js]
+--------------------------------------------------
+GET test/_analyze
+{
+  "tokenizer" : "standard",
+  "token_filters" : ["snowball"],
+  "text" : "detailed output",
+  "explain" : true,
+  "attributes" : ["keyword"] <1>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Set "keyword" to output "keyword" attribute only
+
+coming[2.0.0, body based parameters were added in 2.0.0]
+
+The request returns the following result:
+
+[source,js]
+--------------------------------------------------
+{
+  "detail" : {
+    "custom_analyzer" : true,
+    "charfilters" : [ ],
+    "tokenizer" : {
+      "name" : "standard",
+      "tokens" : [ {
+        "token" : "detailed",
+        "start_offset" : 0,
+        "end_offset" : 8,
+        "type" : "<ALPHANUM>",
+        "position" : 0
+      }, {
+        "token" : "output",
+        "start_offset" : 9,
+        "end_offset" : 15,
+        "type" : "<ALPHANUM>",
+        "position" : 1
+      } ]
+    },
+    "tokenfilters" : [ {
+      "name" : "snowball",
+      "tokens" : [ {
+        "token" : "detail",
+        "start_offset" : 0,
+        "end_offset" : 8,
+        "type" : "<ALPHANUM>",
+        "position" : 0,
+        "keyword" : false <1>
+      }, {
+        "token" : "output",
+        "start_offset" : 9,
+        "end_offset" : 15,
+        "type" : "<ALPHANUM>",
+        "position" : 1,
+        "keyword" : false <1>
+      } ]
+    } ]
+  }
+}
+--------------------------------------------------
+<1> Output only "keyword" attribute, since specify "attributes" in the request.
+
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 112b508..1fbcc28 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -244,6 +244,15 @@ Cloud AWS plugin has been split in two plugins:
 * {plugins}/discovery-ec2.html[Discovery EC2 plugin]
 * {plugins}/repository-s3.html[Repository S3 plugin]
 
+Proxy settings for both plugins have been renamed:
+
+* from `cloud.aws.proxy_host` to `cloud.aws.proxy.host`
+* from `cloud.aws.ec2.proxy_host` to `cloud.aws.ec2.proxy.host`
+* from `cloud.aws.s3.proxy_host` to `cloud.aws.s3.proxy.host`
+* from `cloud.aws.proxy_port` to `cloud.aws.proxy.port`
+* from `cloud.aws.ec2.proxy_port` to `cloud.aws.ec2.proxy.port`
+* from `cloud.aws.s3.proxy_port` to `cloud.aws.s3.proxy.port`
+
 ==== Cloud Azure plugin changes
 
 Cloud Azure plugin has been split in three plugins:
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index 4572efe..5a71059 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -1,94 +1,175 @@
 [[modules-network]]
 == Network Settings
 
-There are several modules within a Node that use network based
-configuration, for example, the
-<<modules-transport,transport>> and
-<<modules-http,http>> modules. Node level
-network settings allows to set common settings that will be shared among
-all network based modules (unless explicitly overridden in each module).
+Elasticsearch binds to localhost only by default.  This is sufficient for you
+to run a local development server (or even a development cluster, if you start
+multiple nodes on the same machine), but you will need to configure some
+<<common-network-settings,basic network settings>> in order to run a real
+production cluster across multiple servers.
+
+[WARNING]
+.Be careful with the network configuration!
+=============================
+Never expose an unprotected node to the public internet.
+=============================
 
-Be careful with host configuration! Never expose an unprotected instance
-to the public internet.
+[float]
+[[common-network-settings]]
+=== Commonly Used Network Settings
+
+`network.host`::
+
+The node will bind to this hostname or IP address and _publish_ (advertise)
+this host to other nodes in the cluster. Accepts an IP address, hostname, or a
+<<network-interface-values,special value>>.
++
+Defaults to `_local_`.
+
+`discovery.zen.ping.unicast.hosts`::
+
+In order to join a cluster, a node needs to know the hostname or IP address of
+at least some of the other nodes in the cluster.  This settting provides the
+initial list of other nodes that this node will try to contact. Accepts IP
+addresses or hostnames.
++
+Defaults to `["127.0.0.1", "[::1]"]`.
+
+`http.port`::
+
+Port to bind to for incoming HTTP requests. Accepts a single value or a range.
+If a range is specified, the node will bind to the first available port in the
+range.
++
+Defaults to `9200-9300`.
+
+`transport.tcp.port`::
+
+Port to bind for communication between nodes. Accepts a single value or a
+range. If a range is specified, the node will bind to the first available port
+in the range.
++
+Defaults to `9300-9400`.
+
+[float]
+[[network-interface-values]]
+=== Special values for `network.host`
+
+The following special values may be passed to `network.host`:
+
+[horizontal]
+`_[networkInterface]_`::
+
+  Addresses of a network interface, for example `_en0_`.
 
-The `network.bind_host` setting allows to control the host different network
-components will bind on. By default, the bind host will be `_local_`
-(loopback addresses such as `127.0.0.1`, `::1`).
+`_local_`::
 
-The `network.publish_host` setting allows to control the host the node will
-publish itself within the cluster so other nodes will be able to connect to it.
-Currently an elasticsearch node may be bound to multiple addresses, but only
-publishes one.  If not specified, this defaults to the "best" address from 
-`network.bind_host`, sorted by IPv4/IPv6 stack preference, then by reachability.
+  Any loopback addresses on the system, for example `127.0.0.1`.
 
-The `network.host` setting is a simple setting to automatically set both
-`network.bind_host` and `network.publish_host` to the same host value.
+`_site_`::
 
-Both settings allows to be configured with either explicit host address(es)
-or host name(s). The settings also accept logical setting value(s) explained
-in the following table:
+  Any site-local addresses on the system, for example `192.168.0.1`.
 
-[cols="<,<",options="header",]
-|=======================================================================
-|Logical Host Setting Value |Description
-|`_local_` |Will be resolved to loopback addresses
+`_global_`::
 
-|`_local:ipv4_` |Will be resolved to loopback IPv4 addresses (e.g. 127.0.0.1)
+  Any globally-scoped addresses on the system, for example `8.8.8.8`.
 
-|`_local:ipv6_` |Will be resolved to loopback IPv6 addresses (e.g. ::1)
 
-|`_site_` |Will be resolved to site-local addresses ("private network")
+[float]
+==== IPv4 vs IPv6
 
-|`_site:ipv4_` |Will be resolved to site-local IPv4 addresses (e.g. 192.168.0.1)
+These special values will work over both IPv4 and IPv6 by default, but you can
+also limit this with the use of `:ipv4` of `:ipv6` specifiers. For example,
+`_en0:ipv4_` would only bind to the IPv4 addresses of interface `en0`.
 
-|`_site:ipv6_` |Will be resolved to site-local IPv6 addresses (e.g. fec0::1)
+[TIP]
+.Discovery in the cloud
+================================
 
-|`_global_` |Will be resolved to globally-scoped addresses ("publicly reachable")
+More special settings are available when running in the cloud with either the
+{plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[EC2 discovery plugin] or the
+{plugins}/discovery-gce-network-host.html#discovery-gce-network-host[Google Compute Engine discovery plugin]
+installed.
 
-|`_global:ipv4_` |Will be resolved to globally-scoped IPv4 addresses (e.g. 8.8.8.8)
+================================
 
-|`_global:ipv6_` |Will be resolved to globally-scoped IPv6 addresses (e.g. 2001:4860:4860::8888)
+[float]
+[[advanced-network-settings]]
+=== Advanced network settings
 
-|`_[networkInterface]_` |Resolves to the addresses of the provided
-network interface. For example `_en0_`.
+The `network.host` setting explained in <<common-network-settings,Commonly used network settings>>
+is a shortcut which sets the _bind host_ and the _publish host_ at the same
+time. In advanced used cases, such as when running behind a proxy server, you
+may need to set these settings to different values:
 
-|`_[networkInterface]:ipv4_` |Resolves to the ipv4 addresses of the
-provided network interface. For example `_en0:ipv4_`.
+`network.bind_host`::
 
-|`_[networkInterface]:ipv6_` |Resolves to the ipv6 addresses of the
-provided network interface. For example `_en0:ipv6_`.
-|=======================================================================
+This specifies which network interface(s) a node should bind to in order to
+listen for incoming requests.  A node can bind to multiple interfaces, e.g.
+two network cards, or a site-local address and a local address. Defaults to
+`network.host`.
 
-When the `discovery-ec2` plugin is installed, you can use
-{plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[ec2 specific host settings].
+`network.publish_host`::
 
-When the `discovery-gce` plugin is installed, you can use
-{plugins}/discovery-gce-network-host.html[gce specific host settings].
+The publish host is the single interface that the node advertises to other
+nodes in the cluster, so that those nodes can connect to it.   Currently an
+elasticsearch node may be bound to multiple addresses, but only publishes one.
+If not specified, this defaults to the ``best'' address from
+`network.bind_host`, sorted by IPv4/IPv6 stack preference, then by
+reachability.
 
+Both of the above settings can be configured just like `network.host` -- they
+accept IP addresses, host names, and
+<<network-interface-values,special values>>.
 
 [float]
 [[tcp-settings]]
-=== TCP Settings
+=== Advanced TCP Settings
+
+Any component that uses TCP (like the <<modules-http,HTTP>> and
+<<modules-transport,Transport>> modules) share the following settings:
+
+[horizontal]
+`network.tcp.no_delay`::
 
-Any component that uses TCP (like the HTTP, Transport and Memcached)
-share the following allowed settings:
+Enable or disable the https://en.wikipedia.org/wiki/Nagle%27s_algorithm[TCP no delay]
+setting. Defaults to `true`.
 
-[cols="<,<",options="header",]
-|=======================================================================
-|Setting |Description
-|`network.tcp.no_delay` |Enable or disable tcp no delay setting.
+`network.tcp.keep_alive`::
+
+Enable or disable https://en.wikipedia.org/wiki/Keepalive[TCP keep alive].
 Defaults to `true`.
 
-|`network.tcp.keep_alive` |Enable or disable tcp keep alive. Defaults
-to `true`.
+`network.tcp.reuse_address`::
+
+Should an address be reused or not. Defaults to `true` on non-windows
+machines.
+
+`network.tcp.send_buffer_size`::
+
+The size of the TCP send buffer (specified with <<size-units,size units>>).
+By default not explicitly set.
+
+`network.tcp.receive_buffer_size`::
+
+The size of the TCP receive buffer (specified with <<size-units,size units>>).
+By default not explicitly set.
+
+[float]
+=== Transport and HTTP protocols
+
+An Elasticsearch node exposes two network protocols which inherit the above
+settings, but may be further configured independently:
+
+TCP Transport::
 
-|`network.tcp.reuse_address` |Should an address be reused or not.
-Defaults to `true` on non-windows machines.
+Used for communication between nodes in the cluster and by the Java
+{javaclient}/node-client.html[Node client],
+{javaclient}/transport-client.html[Transport client], and by the
+<<modules-tribe,Tribe node>>.  See the <<modules-transport,Transport module>>
+for more information.
 
-|`network.tcp.send_buffer_size` |The size of the tcp send buffer size
-(in size setting format). By default not explicitly set.
+HTTP::
 
-|`network.tcp.receive_buffer_size` |The size of the tcp receive buffer
-size (in size setting format). By default not explicitly set.
-|=======================================================================
+Exposes the JSON-over-HTTP interface used by all clients other than the Java
+clients. See the <<modules-http,HTTP module>> for more information.
 
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index b6371dc..dbb9f39 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -45,6 +45,15 @@ which returns:
 }
 -----------------------------------
 
+Information about multiple repositories can be fetched in one go by using a comma-delimited list of repository names.
+Star wildcards are supported as well. For example, information about repositories that start with `repo` or that contain `backup`
+can be obtained using the following command:
+
+[source,js]
+-----------------------------------
+GET /_snapshot/repo*,*backup*
+-----------------------------------
+
 If a repository name is not specified, or `_all` is used as repository name Elasticsearch will return information about
 all repositories currently registered in the cluster:
 
@@ -251,6 +260,14 @@ GET /_snapshot/my_backup/snapshot_1
 -----------------------------------
 // AUTOSENSE
 
+Similar as for repositories, information about multiple snapshots can be queried in one go, supporting wildcards as well:
+
+[source,sh]
+-----------------------------------
+GET /_snapshot/my_backup/snapshot_*,some_other_snapshot
+-----------------------------------
+// AUTOSENSE
+
 All snapshots currently stored in the repository can be listed using the following command:
 
 [source,sh]
diff --git a/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1 b/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index d81842d..0000000
--- a/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-414dfcf600b6c02b90a21bd219a5e115bbda0d14
\ No newline at end of file
diff --git a/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1 b/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..50bb58f
--- /dev/null
+++ b/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+787356d4ae6142bb8ca7e9713d0a281a797b57fb
\ No newline at end of file
diff --git a/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java b/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
index 48c7b4c..c72428c 100644
--- a/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
+++ b/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
@@ -19,38 +19,11 @@
 
 package org.elasticsearch.script.expression;
 
-import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.ScriptModule;
 
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.text.ParseException;
-
 public class ExpressionPlugin extends Plugin {
     
-    // lucene expressions has crazy checks in its clinit for the functions map
-    // it violates rules of classloaders to detect accessibility
-    // TODO: clean that up
-    static {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        AccessController.doPrivileged(new PrivilegedAction<Void>() {
-            @Override
-            public Void run() {
-                try {
-                    JavascriptCompiler.compile("0");
-                } catch (ParseException e) {
-                    throw new RuntimeException(e);
-                }
-                return null;
-            }
-        });
-    }
-
     @Override
     public String name() {
         return "lang-expression";
diff --git a/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy b/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
index 9f50be3..c11af51 100644
--- a/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
+++ b/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
@@ -20,8 +20,6 @@
 grant {
   // needed to generate runtime classes
   permission java.lang.RuntimePermission "createClassLoader";
-  // needed because of security problems in JavascriptCompiler
-  permission java.lang.RuntimePermission "getClassLoader";
   
   // expression runtime
   permission org.elasticsearch.script.ClassPermission "java.lang.String";
diff --git a/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy b/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
index 7de3e1a..e1fd920 100644
--- a/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
+++ b/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
@@ -23,6 +23,7 @@ grant {
   // needed by IndyInterface
   permission java.lang.RuntimePermission "getClassLoader";
   // needed by groovy engine
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
   // needed by GroovyScriptEngineService to close its classloader (why?)
   permission java.lang.RuntimePermission "closeClassLoader";
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 1362975..728a932 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -27,16 +27,13 @@ import org.elasticsearch.action.ActionModule;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestBuilder;
 import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
 import org.elasticsearch.action.get.GetRequest;
 import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.support.ActionFilter;
 import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
@@ -47,8 +44,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.GeoShapeQueryBuilder;
@@ -62,15 +57,8 @@ import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders;
-import org.elasticsearch.search.suggest.Suggest;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
@@ -79,13 +67,10 @@ import org.junit.After;
 import org.junit.Before;
 
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
-import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
@@ -284,172 +269,6 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
         assertRequestsContainHeader(PutIndexedScriptRequest.class);
     }
 
-    public void testThatIndexedScriptGetRequestInTemplateQueryContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient()
-                .preparePutIndexedScript(
-                        MustacheScriptEngineService.NAME,
-                        "my_script",
-                        jsonBuilder().startObject().field("script", "{ \"match\": { \"name\": \"Star Wars\" }}").endObject()
-                                .string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject()).get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        SearchResponse searchResponse = transportClient()
-                .prepareSearch(queryIndex)
-                .setQuery(
-                        QueryBuilders.templateQuery(new Template("my_script", ScriptType.INDEXED,
-                                MustacheScriptEngineService.NAME, null, null))).get();
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatIndexedScriptGetRequestInReducePhaseContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(GroovyScriptEngineService.NAME, "my_script",
-                jsonBuilder().startObject().field("script", "_value0 * 10").endObject().string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("s_field", "foo").field("l_field", 10).endObject()).get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        SearchResponse searchResponse = transportClient()
-                .prepareSearch(queryIndex)
-                .addAggregation(
-                        AggregationBuilders
-                                .terms("terms")
-                                .field("s_field")
-                                .subAggregation(AggregationBuilders.max("max").field("l_field"))
-                                .subAggregation(
-                                        PipelineAggregatorBuilders.bucketScript("scripted").setBucketsPaths("max").script(
-                                                new Script("my_script", ScriptType.INDEXED, GroovyScriptEngineService.NAME, null)))).get();
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatSearchTemplatesWithIndexedTemplatesGetRequestContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(MustacheScriptEngineService.NAME, "the_template",
-                jsonBuilder().startObject().startObject("template").startObject("query").startObject("match")
-                        .field("name", "{{query_string}}").endObject().endObject().endObject().endObject().string()
-        ).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject())
-                .get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("query_string", "star wars");
-
-        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex).setTemplate(new Template("the_template", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params))
-                .get();
-
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatIndexedScriptGetRequestInPhraseSuggestContainsContextAndHeaders() throws Exception {
-        CreateIndexRequestBuilder builder = transportClient().admin().indices().prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("type1")
-                .startObject("properties")
-                .startObject("title")
-                .field("type", "string")
-                .field("analyzer", "text")
-                .endObject()
-                .endObject()
-                .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        titles.add("United States House of Representatives Elections in Washington 2006");
-        titles.add("United States House of Representatives Elections in Washington 2005");
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            transportClient().prepareIndex("test", "type1").setSource("title", title).get();
-        }
-        transportClient().admin().indices().prepareRefresh("test").get();
-
-        String filterStringAsFilter = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("match_phrase")
-                .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PutIndexedScriptResponse scriptResponse = transportClient()
-                .preparePutIndexedScript(
-                        MustacheScriptEngineService.NAME,
-                        "my_script",
-                jsonBuilder().startObject().field("script", filterStringAsFilter).endObject()
-                                .string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(10)
-                        .maxInspections(200)
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(10);
-
-        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(new Template("my_script", ScriptType.INDEXED,
-                MustacheScriptEngineService.NAME, null, null));
-
-        SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
-        String suggestText = "united states house of representatives elections in washington 2006";
-        if (suggestText != null) {
-            suggestBuilder.setText(suggestText);
-        }
-        suggestBuilder.addSuggestion(filteredFilterSuggest);
-        searchRequestBuilder.suggest(suggestBuilder);
-        SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
-        assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
-        Suggest searchSuggest = actionGet.getSuggest();
-
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
     public void testThatRelevantHttpHeadersBecomeRequestHeaders() throws Exception {
         String releventHeaderName = "relevant_" + randomHeaderKey;
         for (RestController restController : internalCluster().getDataNodeInstances(RestController.class)) {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
index 4291f00..66a764d 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
@@ -178,7 +178,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testIndex() {
-        String[] indexShardActions = new String[]{IndexAction.NAME, IndexAction.NAME + "[r]"};
+        String[] indexShardActions = new String[]{IndexAction.NAME, IndexAction.NAME + "[p]", IndexAction.NAME + "[r]"};
         interceptTransportActions(indexShardActions);
 
         IndexRequest indexRequest = new IndexRequest(randomIndexOrAlias(), "type", "id").source("field", "value");
@@ -189,7 +189,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testDelete() {
-        String[] deleteShardActions = new String[]{DeleteAction.NAME, DeleteAction.NAME + "[r]"};
+        String[] deleteShardActions = new String[]{DeleteAction.NAME, DeleteAction.NAME + "[p]", DeleteAction.NAME + "[r]"};
         interceptTransportActions(deleteShardActions);
 
         DeleteRequest deleteRequest = new DeleteRequest(randomIndexOrAlias(), "type", "id");
@@ -244,7 +244,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testBulk() {
-        String[] bulkShardActions = new String[]{BulkAction.NAME + "[s]", BulkAction.NAME + "[s][r]"};
+        String[] bulkShardActions = new String[]{BulkAction.NAME + "[s][p]", BulkAction.NAME + "[s][r]"};
         interceptTransportActions(bulkShardActions);
 
         List<String> indices = new ArrayList<>();
@@ -344,7 +344,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testFlush() {
-        String[] indexShardActions = new String[]{TransportShardFlushAction.NAME + "[r]", TransportShardFlushAction.NAME};
+        String[] indexShardActions = new String[]{TransportShardFlushAction.NAME, TransportShardFlushAction.NAME + "[r]", TransportShardFlushAction.NAME + "[p]"};
         interceptTransportActions(indexShardActions);
 
         FlushRequest flushRequest = new FlushRequest(randomIndicesOrAliases());
@@ -367,7 +367,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testRefresh() {
-        String[] indexShardActions = new String[]{TransportShardRefreshAction.NAME + "[r]", TransportShardRefreshAction.NAME};
+        String[] indexShardActions = new String[]{TransportShardRefreshAction.NAME, TransportShardRefreshAction.NAME + "[r]", TransportShardRefreshAction.NAME + "[p]"};
         interceptTransportActions(indexShardActions);
 
         RefreshRequest refreshRequest = new RefreshRequest(randomIndicesOrAliases());
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
index af27047..adf3492 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -42,6 +42,7 @@
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/CardinalityIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/CardinalityTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ChildQuerySearchTests.java
   renamed:    core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+  ^^^^^ note: the methods from this test using mustache were moved to the mustache module under its messy tests package.
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateHistogramTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
diff --git a/modules/lang-mustache/build.gradle b/modules/lang-mustache/build.gradle
new file mode 100644
index 0000000..4e8e9cc
--- /dev/null
+++ b/modules/lang-mustache/build.gradle
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+esplugin {
+  description 'Mustache scripting integration for Elasticsearch'
+  classname 'org.elasticsearch.script.mustache.MustachePlugin'
+}
+
+dependencies {
+  compile "com.github.spullara.mustache.java:compiler:0.9.1"
+}
+
+compileTestJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked'
+
+integTest {
+  cluster {
+    systemProperty 'es.script.inline', 'on'
+    systemProperty 'es.script.indexed', 'on'
+  }
+}
diff --git a/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1 b/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1
new file mode 100644
index 0000000..96152e0
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1
@@ -0,0 +1 @@
+14aec5344639782ee76441401b773946c65eb2b3
diff --git a/modules/lang-mustache/licenses/compiler-LICENSE.txt b/modules/lang-mustache/licenses/compiler-LICENSE.txt
new file mode 100644
index 0000000..ac68303
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-LICENSE.txt
@@ -0,0 +1,14 @@
+Copyright 2010 RightTime, Inc.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
diff --git a/modules/lang-mustache/licenses/compiler-NOTICE.txt b/modules/lang-mustache/licenses/compiler-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
new file mode 100644
index 0000000..7734d03
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.fasterxml.jackson.core.io.JsonStringEncoder;
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.MustacheException;
+
+import java.io.IOException;
+import java.io.Writer;
+
+/**
+ * A MustacheFactory that does simple JSON escaping.
+ */
+public final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
+    
+    @Override
+    public void encode(String value, Writer writer) {
+        try {
+            JsonStringEncoder utils = new JsonStringEncoder();
+            writer.write(utils.quoteAsString(value));;
+        } catch (IOException e) {
+            throw new MustacheException("Failed to encode value: " + value);
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java
new file mode 100644
index 0000000..3f6f6e0
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.mustache;
+
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptModule;
+
+public class MustachePlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "lang-mustache";
+    }
+
+    @Override
+    public String description() {
+        return "Mustache scripting integration for Elasticsearch";
+    }
+
+    public void onModule(ScriptModule module) {
+        module.addScriptEngine(MustacheScriptEngineService.class);
+    }
+}
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
new file mode 100644
index 0000000..9317205
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
@@ -0,0 +1,184 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.Mustache;
+
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.io.FastStringReader;
+import org.elasticsearch.common.io.UTF8StreamWriter;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptException;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.lang.ref.SoftReference;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.Collections;
+import java.util.Map;
+
+/**
+ * Main entry point handling template registration, compilation and
+ * execution.
+ *
+ * Template handling is based on Mustache. Template handling is a two step
+ * process: First compile the string representing the template, the resulting
+ * {@link Mustache} object can then be re-used for subsequent executions.
+ */
+public class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
+
+    public static final String NAME = "mustache";
+
+    /** Thread local UTF8StreamWriter to store template execution results in, thread local to save object creation.*/
+    private static ThreadLocal<SoftReference<UTF8StreamWriter>> utf8StreamWriter = new ThreadLocal<>();
+
+    /** If exists, reset and return, otherwise create, reset and return a writer.*/
+    private static UTF8StreamWriter utf8StreamWriter() {
+        SoftReference<UTF8StreamWriter> ref = utf8StreamWriter.get();
+        UTF8StreamWriter writer = (ref == null) ? null : ref.get();
+        if (writer == null) {
+            writer = new UTF8StreamWriter(1024 * 4);
+            utf8StreamWriter.set(new SoftReference<>(writer));
+        }
+        writer.reset();
+        return writer;
+    }
+
+    /**
+     * @param settings automatically wired by Guice.
+     * */
+    @Inject
+    public MustacheScriptEngineService(Settings settings) {
+        super(settings);
+    }
+
+    /**
+     * Compile a template string to (in this case) a Mustache object than can
+     * later be re-used for execution to fill in missing parameter values.
+     *
+     * @param template
+     *            a string representing the template to compile.
+     * @return a compiled template object for later execution.
+     * */
+    @Override
+    public Object compile(String template) {
+        /** Factory to generate Mustache objects from. */
+        return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
+    }
+
+    @Override
+    public String[] types() {
+        return new String[] {NAME};
+    }
+
+    @Override
+    public String[] extensions() {
+        return new String[] {NAME};
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript,
+            @Nullable Map<String, Object> vars) {
+        return new MustacheExecutableScript(compiledScript, vars);
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup,
+            @Nullable Map<String, Object> vars) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void close() {
+        // Nothing to do here
+    }
+
+    @Override
+    public void scriptRemoved(CompiledScript script) {
+        // Nothing to do here
+    }
+
+    // permission checked before doing crazy reflection
+    static final SpecialPermission SPECIAL_PERMISSION = new SpecialPermission();
+
+    /**
+     * Used at query execution time by script service in order to execute a query template.
+     * */
+    private class MustacheExecutableScript implements ExecutableScript {
+        /** Compiled template object wrapper. */
+        private CompiledScript template;
+        /** Parameters to fill above object with. */
+        private Map<String, Object> vars;
+
+        /**
+         * @param template the compiled template object wrapper
+         * @param vars the parameters to fill above object with
+         **/
+        public MustacheExecutableScript(CompiledScript template, Map<String, Object> vars) {
+            this.template = template;
+            this.vars = vars == null ? Collections.<String, Object>emptyMap() : vars;
+        }
+
+        @Override
+        public void setNextVar(String name, Object value) {
+            this.vars.put(name, value);
+        }
+
+        @Override
+        public Object run() {
+            final BytesStreamOutput result = new BytesStreamOutput();
+            try (UTF8StreamWriter writer = utf8StreamWriter().setOutput(result)) {
+                // crazy reflection here
+                SecurityManager sm = System.getSecurityManager();
+                if (sm != null) {
+                    sm.checkPermission(SPECIAL_PERMISSION);
+                }
+                AccessController.doPrivileged(new PrivilegedAction<Void>() {
+                    @Override
+                    public Void run() {
+                        ((Mustache) template.compiled()).execute(writer, vars);
+                        return null;
+                    }
+                });
+            } catch (Exception e) {
+                logger.error("Error running " + template, e);
+                throw new ScriptException("Error running " + template, e);
+            }
+            return result.bytes();
+        }
+
+        @Override
+        public Object unwrap(Object value) {
+            return value;
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy b/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy
new file mode 100644
index 0000000..ea2db55
--- /dev/null
+++ b/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy
@@ -0,0 +1,23 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+grant {
+  // needed to do crazy reflection
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+};
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
new file mode 100644
index 0000000..92d1533
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -0,0 +1,389 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionModule;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
+import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.search.SearchRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.FilterClient;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.search.suggest.Suggest;
+import org.elasticsearch.search.suggest.SuggestBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.concurrent.CopyOnWriteArrayList;
+
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.node.Node.HTTP_ENABLED;
+import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
+import static org.elasticsearch.test.ESIntegTestCase.Scope.SUITE;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
+
+@ClusterScope(scope = SUITE)
+public class ContextAndHeaderTransportTests extends ESIntegTestCase {
+    private static final List<ActionRequest> requests =  new CopyOnWriteArrayList<>();
+    private String randomHeaderKey = randomAsciiOfLength(10);
+    private String randomHeaderValue = randomAsciiOfLength(20);
+    private String queryIndex = "query-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT);
+    private String lookupIndex = "lookup-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT);
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return settingsBuilder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("script.indexed", "on")
+                .put(HTTP_ENABLED, true)
+                .build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(ActionLoggingPlugin.class, MustachePlugin.class);
+    }
+
+    @Before
+    public void createIndices() throws Exception {
+        String mapping = jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                .startObject("location").field("type", "geo_shape").endObject()
+                .startObject("name").field("type", "string").endObject()
+                .endObject()
+                .endObject().endObject().string();
+
+        Settings settings = settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .build();
+        assertAcked(transportClient().admin().indices().prepareCreate(lookupIndex)
+                .setSettings(settings).addMapping("type", mapping));
+        assertAcked(transportClient().admin().indices().prepareCreate(queryIndex)
+                .setSettings(settings).addMapping("type", mapping));
+        ensureGreen(queryIndex, lookupIndex);
+
+        requests.clear();
+    }
+
+    @After
+    public void checkAllRequestsContainHeaders() {
+        assertRequestsContainHeader(IndexRequest.class);
+        assertRequestsContainHeader(RefreshRequest.class);
+    }
+
+    public void testThatIndexedScriptGetRequestInTemplateQueryContainsContextAndHeaders() throws Exception {
+        PutIndexedScriptResponse scriptResponse = transportClient()
+                .preparePutIndexedScript(
+                        MustacheScriptEngineService.NAME,
+                        "my_script",
+                        jsonBuilder().startObject().field("script", "{ \"match\": { \"name\": \"Star Wars\" }}").endObject()
+                                .string()).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        transportClient().prepareIndex(queryIndex, "type", "1")
+                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject()).get();
+        transportClient().admin().indices().prepareRefresh(queryIndex).get();
+
+        SearchResponse searchResponse = transportClient()
+                .prepareSearch(queryIndex)
+                .setQuery(
+                        QueryBuilders.templateQuery(new Template("my_script", ScriptType.INDEXED,
+                                MustacheScriptEngineService.NAME, null, null))).get();
+        assertNoFailures(searchResponse);
+        assertHitCount(searchResponse, 1);
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    public void testThatSearchTemplatesWithIndexedTemplatesGetRequestContainsContextAndHeaders() throws Exception {
+        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(MustacheScriptEngineService.NAME, "the_template",
+                jsonBuilder().startObject().startObject("template").startObject("query").startObject("match")
+                        .field("name", "{{query_string}}").endObject().endObject().endObject().endObject().string()
+        ).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        transportClient().prepareIndex(queryIndex, "type", "1")
+                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject())
+                .get();
+        transportClient().admin().indices().prepareRefresh(queryIndex).get();
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("query_string", "star wars");
+
+        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex).setTemplate(new Template("the_template", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params))
+                .get();
+
+        assertNoFailures(searchResponse);
+        assertHitCount(searchResponse, 1);
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    public void testThatIndexedScriptGetRequestInPhraseSuggestContainsContextAndHeaders() throws Exception {
+        CreateIndexRequestBuilder builder = transportClient().admin().indices().prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("title")
+                .field("type", "string")
+                .field("analyzer", "text")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        titles.add("United States House of Representatives Elections in Washington 2006");
+        titles.add("United States House of Representatives Elections in Washington 2005");
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            transportClient().prepareIndex("test", "type1").setSource("title", title).get();
+        }
+        transportClient().admin().indices().prepareRefresh("test").get();
+
+        String filterStringAsFilter = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("match_phrase")
+                .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PutIndexedScriptResponse scriptResponse = transportClient()
+                .preparePutIndexedScript(
+                        MustacheScriptEngineService.NAME,
+                        "my_script",
+                jsonBuilder().startObject().field("script", filterStringAsFilter).endObject()
+                                .string()).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(10)
+                        .maxInspections(200)
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(10);
+
+        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(new Template("my_script", ScriptType.INDEXED,
+                MustacheScriptEngineService.NAME, null, null));
+
+        SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
+        SuggestBuilder suggestBuilder = new SuggestBuilder();
+        String suggestText = "united states house of representatives elections in washington 2006";
+        if (suggestText != null) {
+            suggestBuilder.setText(suggestText);
+        }
+        suggestBuilder.addSuggestion(filteredFilterSuggest);
+        searchRequestBuilder.suggest(suggestBuilder);
+        SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
+        assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
+        Suggest searchSuggest = actionGet.getSuggest();
+
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    private <T> List<T> getRequests(Class<T> clazz) {
+        List<T> results = new ArrayList<>();
+        for (ActionRequest request : requests) {
+            if (request.getClass().equals(clazz)) {
+                results.add((T) request);
+            }
+        }
+
+        return results;
+    }
+
+    private void assertRequestsContainHeader(Class<? extends ActionRequest> clazz) {
+        List<? extends ActionRequest> classRequests = getRequests(clazz);
+        for (ActionRequest request : classRequests) {
+            assertRequestContainsHeader(request);
+        }
+    }
+
+    private void assertGetRequestsContainHeaders() {
+        assertGetRequestsContainHeaders(this.lookupIndex);
+    }
+
+    private void assertGetRequestsContainHeaders(String index) {
+        List<GetRequest> getRequests = getRequests(GetRequest.class);
+        assertThat(getRequests, hasSize(greaterThan(0)));
+
+        for (GetRequest request : getRequests) {
+            if (!request.index().equals(index)) {
+                continue;
+            }
+            assertRequestContainsHeader(request);
+        }
+    }
+
+    private void assertRequestContainsHeader(ActionRequest request) {
+        String msg = String.format(Locale.ROOT, "Expected header %s to be in request %s", randomHeaderKey, request.getClass().getName());
+        if (request instanceof IndexRequest) {
+            IndexRequest indexRequest = (IndexRequest) request;
+            msg = String.format(Locale.ROOT, "Expected header %s to be in index request %s/%s/%s", randomHeaderKey,
+                    indexRequest.index(), indexRequest.type(), indexRequest.id());
+        }
+        assertThat(msg, request.hasHeader(randomHeaderKey), is(true));
+        assertThat(request.getHeader(randomHeaderKey).toString(), is(randomHeaderValue));
+    }
+
+    /**
+     * a transport client that adds our random header
+     */
+    private Client transportClient() {
+        Client transportClient = internalCluster().transportClient();
+        FilterClient filterClient = new FilterClient(transportClient) {
+            @Override
+            protected <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {
+                request.putHeader(randomHeaderKey, randomHeaderValue);
+                super.doExecute(action, request, listener);
+            }
+        };
+
+        return filterClient;
+    }
+
+    public static class ActionLoggingPlugin extends Plugin {
+
+        @Override
+        public String name() {
+            return "test-action-logging";
+        }
+
+        @Override
+        public String description() {
+            return "Test action logging";
+        }
+
+        @Override
+        public Collection<Module> nodeModules() {
+            return Collections.<Module>singletonList(new ActionLoggingModule());
+        }
+
+        public void onModule(ActionModule module) {
+            module.registerFilter(LoggingFilter.class);
+        }
+    }
+
+    public static class ActionLoggingModule extends AbstractModule {
+        @Override
+        protected void configure() {
+            bind(LoggingFilter.class).asEagerSingleton();
+        }
+
+    }
+
+    public static class LoggingFilter extends ActionFilter.Simple {
+
+        @Inject
+        public LoggingFilter(Settings settings) {
+            super(settings);
+        }
+
+        @Override
+        public int order() {
+            return 999;
+        }
+
+        @Override
+        protected boolean apply(String action, ActionRequest request, ActionListener listener) {
+            requests.add(request);
+            return true;
+        }
+
+        @Override
+        protected boolean apply(String action, ActionResponse response, ActionListener listener) {
+            return true;
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
new file mode 100644
index 0000000..87cc51c
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.rest.support.FileUtils;
+
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.SuiteScopeTestCase
+public class RenderSearchTemplateTests extends ESIntegTestCase {
+    private static final String TEMPLATE_CONTENTS = "{\"size\":\"{{size}}\",\"query\":{\"match\":{\"foo\":\"{{value}}\"}},\"aggs\":{\"objects\":{\"terms\":{\"field\":\"{{value}}\",\"size\":\"{{size}}\"}}}}";
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+
+    @Override
+    protected void setupSuiteScopeCluster() throws Exception {
+        client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "index_template_1", "{ \"template\": " + TEMPLATE_CONTENTS + " }").get();
+    }
+
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        Path configDir = createTempDir();
+        Path scriptsDir = configDir.resolve("scripts");
+        try {
+            Files.createDirectories(scriptsDir);
+            Files.write(scriptsDir.resolve("file_template_1.mustache"), TEMPLATE_CONTENTS.getBytes("UTF-8"));
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
+                .put("path.conf", configDir).build();
+    }
+
+    public void testInlineTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+
+    public void testIndexedTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+
+    public void testFileTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
new file mode 100644
index 0000000..a0699a3
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
@@ -0,0 +1,1302 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.ReduceSearchPhaseException;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.action.suggest.SuggestRequestBuilder;
+import org.elasticsearch.action.suggest.SuggestResponse;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.search.suggest.Suggest;
+import org.elasticsearch.search.suggest.SuggestBuilder;
+import org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.DirectCandidateGenerator;
+import org.elasticsearch.search.suggest.term.TermSuggestionBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
+
+import java.io.IOException;
+import java.net.URISyntaxException;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
+import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
+import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
+import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
+import static org.hamcrest.Matchers.anyOf;
+import static org.hamcrest.Matchers.endsWith;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.nullValue;
+
+/**
+ * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
+ * possible these tests should declare for the first request, make the request, modify the configuration for the next request, make that
+ * request, modify again, request again, etc.  This makes it very obvious what changes between requests.
+ */
+public class SuggestSearchTests extends ESIntegTestCase {
+    
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+    
+    // see #3196
+    public void testSuggestAcrossMultipleIndices() throws IOException {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "text", "abcd");
+        index("test", "type1", "2", "text", "aacd");
+        index("test", "type1", "3", "text", "abbd");
+        index("test", "type1", "4", "text", "abcc");
+        refresh();
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        logger.info("--> run suggestions with one index");
+        searchSuggest( termSuggest);
+        createIndex("test_1");
+        ensureGreen();
+
+        index("test_1", "type1", "1", "text", "ab cd");
+        index("test_1", "type1", "2", "text", "aa cd");
+        index("test_1", "type1", "3", "text", "ab bd");
+        index("test_1", "type1", "4", "text", "ab cc");
+        refresh();
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ab cd")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with two indices");
+        searchSuggest( termSuggest);
+
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("text").field("type", "string").field("analyzer", "keyword").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(prepareCreate("test_2").addMapping("type1", mapping));
+        ensureGreen();
+
+        index("test_2", "type1", "1", "text", "ab cd");
+        index("test_2", "type1", "2", "text", "aa cd");
+        index("test_2", "type1", "3", "text", "ab bd");
+        index("test_2", "type1", "4", "text", "ab cc");
+        index("test_2", "type1", "1", "text", "abcd");
+        index("test_2", "type1", "2", "text", "aacd");
+        index("test_2", "type1", "3", "text", "abbd");
+        index("test_2", "type1", "4", "text", "abcc");
+        refresh();
+
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ab cd")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with three indices");
+        try {
+            searchSuggest( termSuggest);
+            fail(" can not suggest across multiple indices with different analysis chains");
+        } catch (ReduceSearchPhaseException ex) {
+            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
+            assertThat(ex.getCause().getMessage(),
+                    anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
+                            endsWith("Suggest entries have different sizes actual [2] expected [1]")));
+        } catch (IllegalStateException ex) {
+            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
+                    endsWith("Suggest entries have different sizes actual [2] expected [1]")));
+        }
+
+
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ABCD")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with four indices");
+        try {
+            searchSuggest( termSuggest);
+            fail(" can not suggest across multiple indices with different analysis chains");
+        } catch (ReduceSearchPhaseException ex) {
+            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
+            assertThat(ex.getCause().getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
+                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
+        } catch (IllegalStateException ex) {
+            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
+                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
+        }
+    }
+
+    // see #3037
+    public void testSuggestModes() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put(SETTING_NUMBER_OF_REPLICAS, 0)
+                .put("index.analysis.analyzer.biword.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("name")
+                    .field("type", "multi_field")
+                    .startObject("fields")
+                        .startObject("name")
+                            .field("type", "string")
+                        .endObject()
+                        .startObject("shingled")
+                            .field("type", "string")
+                            .field("analyzer", "biword")
+                            .field("search_analyzer", "standard")
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+
+        index("test", "type1", "1", "name", "I like iced tea");
+        index("test", "type1", "2", "name", "I like tea.");
+        index("test", "type1", "3", "name", "I like ice cream.");
+        refresh();
+
+        DirectCandidateGenerator generator = candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2);
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
+                .addCandidateGenerator(generator)
+                .gramSize(3);
+        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, "did_you_mean", "iced tea");
+
+        generator.suggestMode(null);
+        searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestionSize(searchSuggest, 0, 0, "did_you_mean");
+    }
+
+    // see #2729
+    public void testSizeOneShard() throws Exception {
+        prepareCreate("test").setSettings(
+                SETTING_NUMBER_OF_SHARDS, 1,
+                SETTING_NUMBER_OF_REPLICAS, 0).get();
+        ensureGreen();
+
+        for (int i = 0; i < 15; i++) {
+            index("test", "type1", Integer.toString(i), "text", "abc" + i);
+        }
+        refresh();
+
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellchecker")).get();
+        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
+
+        TermSuggestionBuilder termSuggestion = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text")
+                .size(10);
+        Suggest suggest = searchSuggest( termSuggestion);
+        assertSuggestion(suggest, 0, "test", 10, "abc0");
+
+        termSuggestion.text("abcd").shardSize(5);
+        suggest = searchSuggest( termSuggestion);
+        assertSuggestion(suggest, 0, "test", 5, "abc0");
+    }
+
+    public void testUnmappedField() throws IOException, InterruptedException, ExecutionException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.biword.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 3));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("name")
+                    .field("type", "multi_field")
+                    .startObject("fields")
+                        .startObject("name")
+                            .field("type", "string")
+                        .endObject()
+                        .startObject("shingled")
+                            .field("type", "string")
+                            .field("analyzer", "biword")
+                            .field("search_analyzer", "standard")
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        indexRandom(true, client().prepareIndex("test", "type1").setSource("name", "I like iced tea"),
+        client().prepareIndex("test", "type1").setSource("name", "I like tea."),
+        client().prepareIndex("test", "type1").setSource("name", "I like ice cream."));
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2))
+                .gramSize(3);
+        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, 0, "did_you_mean", "iced tea");
+
+        phraseSuggestion.field("nosuchField");
+        {
+            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
+            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
+            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+        }
+        {
+            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
+            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
+            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+        }
+    }
+
+    public void testSimple() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "text", "abcd");
+        index("test", "type1", "2", "text", "aacd");
+        index("test", "type1", "3", "text", "abbd");
+        index("test", "type1", "4", "text", "abcc");
+        refresh();
+
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellcecker")).get();
+        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        Suggest suggest = searchSuggest( termSuggest);
+        assertSuggestion(suggest, 0, "test", "aacd", "abbd", "abcc");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+
+        suggest = searchSuggest( termSuggest);
+        assertSuggestion(suggest, 0, "test", "aacd","abbd", "abcc");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+    }
+
+    public void testEmpty() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "foo", "bar");
+        refresh();
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        Suggest suggest = searchSuggest( termSuggest);
+        assertSuggestionSize(suggest, 0, 0, "test");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+
+        suggest = searchSuggest( termSuggest);
+        assertSuggestionSize(suggest, 0, 0, "test");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+    }
+
+    public void testWithMultipleCommands() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "typ1", "1", "field1", "prefix_abcd", "field2", "prefix_efgh");
+        index("test", "typ1", "2", "field1", "prefix_aacd", "field2", "prefix_eeeh");
+        index("test", "typ1", "3", "field1", "prefix_abbd", "field2", "prefix_efff");
+        index("test", "typ1", "4", "field1", "prefix_abcc", "field2", "prefix_eggg");
+        refresh();
+
+        Suggest suggest = searchSuggest(
+                termSuggestion("size1")
+                        .size(1).text("prefix_abcd").maxTermFreq(10).prefixLength(1).minDocFreq(0)
+                        .field("field1").suggestMode("always"),
+                termSuggestion("field2")
+                        .field("field2").text("prefix_eeeh prefix_efgh")
+                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"),
+                termSuggestion("accuracy")
+                        .field("field2").text("prefix_efgh").setAccuracy(1f)
+                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"));
+        assertSuggestion(suggest, 0, "size1", "prefix_aacd");
+        assertThat(suggest.getSuggestion("field2").getEntries().get(0).getText().string(), equalTo("prefix_eeeh"));
+        assertSuggestion(suggest, 0, "field2", "prefix_efgh");
+        assertThat(suggest.getSuggestion("field2").getEntries().get(1).getText().string(), equalTo("prefix_efgh"));
+        assertSuggestion(suggest, 1, "field2", "prefix_eeeh", "prefix_efff", "prefix_eggg");
+        assertSuggestionSize(suggest, 0, 0, "accuracy");
+    }
+
+    public void testSizeAndSort() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        Map<String, Integer> termsAndDocCount = new HashMap<>();
+        termsAndDocCount.put("prefix_aaad", 20);
+        termsAndDocCount.put("prefix_abbb", 18);
+        termsAndDocCount.put("prefix_aaca", 16);
+        termsAndDocCount.put("prefix_abba", 14);
+        termsAndDocCount.put("prefix_accc", 12);
+        termsAndDocCount.put("prefix_addd", 10);
+        termsAndDocCount.put("prefix_abaa", 8);
+        termsAndDocCount.put("prefix_dbca", 6);
+        termsAndDocCount.put("prefix_cbad", 4);
+        termsAndDocCount.put("prefix_aacd", 1);
+        termsAndDocCount.put("prefix_abcc", 1);
+        termsAndDocCount.put("prefix_accd", 1);
+
+        for (Map.Entry<String, Integer> entry : termsAndDocCount.entrySet()) {
+            for (int i = 0; i < entry.getValue(); i++) {
+                index("test", "type1", entry.getKey() + i, "field1", entry.getKey());
+            }
+        }
+        refresh();
+
+        Suggest suggest = searchSuggest( "prefix_abcd",
+                termSuggestion("size3SortScoreFirst")
+                        .size(3).minDocFreq(0).field("field1").suggestMode("always"),
+                termSuggestion("size10SortScoreFirst")
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always").shardSize(50),
+                termSuggestion("size3SortScoreFirstMaxEdits1")
+                        .maxEdits(1)
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always"),
+                termSuggestion("size10SortFrequencyFirst")
+                        .size(10).sort("frequency").shardSize(1000)
+                        .minDocFreq(0).field("field1").suggestMode("always"));
+
+        // The commented out assertions fail sometimes because suggestions are based off of shard frequencies instead of index frequencies.
+        assertSuggestion(suggest, 0, "size3SortScoreFirst", "prefix_aacd", "prefix_abcc", "prefix_accd");
+        assertSuggestion(suggest, 0, "size10SortScoreFirst", 10, "prefix_aacd", "prefix_abcc", "prefix_accd" /*, "prefix_aaad" */);
+        assertSuggestion(suggest, 0, "size3SortScoreFirstMaxEdits1", "prefix_aacd", "prefix_abcc", "prefix_accd");
+        assertSuggestion(suggest, 0, "size10SortFrequencyFirst", "prefix_aaad", "prefix_abbb", "prefix_aaca", "prefix_abba",
+                "prefix_accc", "prefix_addd", "prefix_abaa", "prefix_dbca", "prefix_cbad", "prefix_aacd");
+
+        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
+        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
+    }
+
+    // see #2817
+    public void testStopwordsOnlyPhraseSuggest() throws IOException {
+        assertAcked(prepareCreate("test").addMapping("typ1", "body", "type=string,analyzer=stopwd").setSettings(
+                settingsBuilder()
+                        .put("index.analysis.analyzer.stopwd.tokenizer", "whitespace")
+                        .putArray("index.analysis.analyzer.stopwd.filter", "stop")
+        ));
+        ensureGreen();
+        index("test", "typ1", "1", "body", "this is a test");
+        refresh();
+
+        Suggest searchSuggest = searchSuggest( "a an the",
+                phraseSuggestion("simple_phrase").field("body").gramSize(1)
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always"))
+                        .size(1));
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+    }
+
+    public void testPrefixLength() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
+                .startObject("properties")
+                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
+                .startObject("body_reverse").field("type", "string").field("analyzer", "reverse").endObject()
+                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        index("test", "type1", "1", "body", "hello world");
+        index("test", "type1", "2", "body", "hello world");
+        index("test", "type1", "3", "body", "hello words");
+        refresh();
+
+        Suggest searchSuggest = searchSuggest( "hello word",
+                phraseSuggestion("simple_phrase").field("body")
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(4).minWordLength(1).suggestMode("always"))
+                        .size(1).confidence(1.0f));
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello words");
+
+        searchSuggest = searchSuggest( "hello word",
+                phraseSuggestion("simple_phrase").field("body")
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(2).minWordLength(1).suggestMode("always"))
+                        .size(1).confidence(1.0f));
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello world");
+    }
+
+    @Nightly
+    public void testMarvelHerosPhraseSuggest() throws IOException, URISyntaxException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                    .startObject("_all")
+                        .field("store", "yes")
+                        .field("termVector", "with_positions_offsets")
+                    .endObject()
+                    .startObject("properties")
+                        .startObject("body").
+                            field("type", "string").
+                            field("analyzer", "body")
+                        .endObject()
+                        .startObject("body_reverse").
+                            field("type", "string").
+                            field("analyzer", "reverse")
+                         .endObject()
+                         .startObject("bigram").
+                             field("type", "string").
+                             field("analyzer", "bigram")
+                         .endObject()
+                     .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        for (String line : readMarvelHeroNames()) {
+            index("test", "type1", line, "body", line, "body_reverse", line, "bigram", line);
+        }
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggest = phraseSuggestion("simple_phrase")
+                .field("bigram").gramSize(2).analyzer("body")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+                .size(1);
+        Suggest searchSuggest = searchSuggest( "american ame", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("american ame"));
+
+        phraseSuggest.realWordErrorLikelihood(0.95f);
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+        // Check the "text" field this one time.
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel"));
+
+        // Ask for highlighting
+        phraseSuggest.highlight("<em>", "</em>");
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getOptions().get(0).getHighlighted().string(), equalTo("<em>xorr</em> the <em>god</em> jewel"));
+
+        // pass in a correct phrase
+        phraseSuggest.highlight(null, null).confidence(0f).size(1).maxErrors(0.5f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // pass in a correct phrase - set confidence to 2
+        phraseSuggest.confidence(2f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // pass in a correct phrase - set confidence to 0.99
+        phraseSuggest.confidence(0.99f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        //test reverse suggestions with pre & post filter
+        phraseSuggest
+            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+            .addCandidateGenerator(candidateGenerator("body_reverse").minWordLength(1).suggestMode("always").preFilter("reverse").postFilter("reverse"));
+        searchSuggest = searchSuggest( "xor the yod-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // set all mass to trigrams (not indexed)
+        phraseSuggest.clearCandidateGenerators()
+            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+            .smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(1,0,0));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // set all mass to bigrams
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0,1,0));
+        searchSuggest =  searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // distribute mass
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        searchSuggest = searchSuggest( "american ame", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
+
+        // try all smoothing methods
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.Laplace(0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // check tokenLimit
+        phraseSuggest.smoothingModel(null).tokenLimit(4);
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        phraseSuggest.tokenLimit(15).smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel xorr the god jewel xorr the god jewel");
+        // Check the name this time because we're repeating it which is funky
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
+    }
+    
+    private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
+        return Files.readAllLines(PathUtils.get(Suggest.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
+    }
+
+    public void testSizePararm() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("_all")
+                            .field("store", "yes")
+                            .field("termVector", "with_positions_offsets")
+                        .endObject()
+                        .startObject("properties")
+                            .startObject("body")
+                                .field("type", "string")
+                                .field("analyzer", "body")
+                            .endObject()
+                         .startObject("body_reverse")
+                             .field("type", "string")
+                             .field("analyzer", "reverse")
+                         .endObject()
+                         .startObject("bigram")
+                             .field("type", "string")
+                             .field("analyzer", "bigram")
+                         .endObject()
+                     .endObject()
+                 .endObject()
+             .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        String line = "xorr the god jewel";
+        index("test", "type1", "1", "body", line, "body_reverse", line, "bigram", line);
+        line = "I got it this time";
+        index("test", "type1", "2", "body", line, "body_reverse", line, "bigram", line);
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
+                .realWordErrorLikelihood(0.95f)
+                .field("bigram")
+                .gramSize(2)
+                .analyzer("body")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(1).accuracy(0.1f))
+                .smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1))
+                .maxErrors(1.0f)
+                .size(5);
+        Suggest searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // we allow a size of 2 now on the shard generator level so "god" will be found since it's LD2
+        phraseSuggestion.clearCandidateGenerators()
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(2).accuracy(0.1f));
+        searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+    }
+
+    @Nightly
+    public void testPhraseBoundaryCases() throws IOException, URISyntaxException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings()).put(SETTING_NUMBER_OF_SHARDS, 1) // to get reliable statistics we should put this all into one shard
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.analyzer.ngram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.ngram.filter", "my_shingle2", "lowercase")
+                .put("index.analysis.analyzer.myDefAnalyzer.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.myDefAnalyzer.filter", "shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle2.type", "shingle")
+                .put("index.analysis.filter.my_shingle2.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle2.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle2.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                    .startObject().startObject("type1")
+                    .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
+                .startObject("properties")
+                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
+                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
+                .startObject("ngram").field("type", "string").field("analyzer", "ngram").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        for (String line : readMarvelHeroNames()) {
+            index("test", "type1", line, "body", line, "bigram", line, "ngram", line);
+        }
+        refresh();
+
+        NumShards numShards = getNumShards("test");
+
+        // Lets make sure some things throw exceptions
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
+                .field("bigram")
+                .analyzer("body")
+                .addCandidateGenerator(candidateGenerator("does_not_exist").minWordLength(1).suggestMode("always"))
+                .realWordErrorLikelihood(0.95f)
+                .maxErrors(0.5f)
+                .size(1);
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("field does not exists");
+        } catch (SearchPhaseExecutionException e) {}
+
+        phraseSuggestion.clearCandidateGenerators().analyzer(null);
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("analyzer does only produce ngrams");
+        } catch (SearchPhaseExecutionException e) {
+        }
+
+        phraseSuggestion.analyzer("bigram");
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("analyzer does only produce ngrams");
+        } catch (SearchPhaseExecutionException e) {
+        }
+
+        // Now we'll make sure some things don't
+        phraseSuggestion.forceUnigrams(false);
+        searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // Field doesn't produce unigrams but the analyzer does
+        phraseSuggestion.forceUnigrams(true).field("bigram").analyzer("ngram");
+        searchSuggest( "Xor the Got-Jewel",
+                phraseSuggestion);
+
+        phraseSuggestion.field("ngram").analyzer("myDefAnalyzer")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"));
+        Suggest suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // "xorr the god jewel" and and "xorn the god jewel" have identical scores (we are only using unigrams to score), so we tie break by
+        // earlier term (xorn):
+        assertSuggestion(suggest, 0, "simple_phrase", "xorn the god jewel");
+
+        phraseSuggestion.analyzer(null);
+        suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // In this case xorr has a better score than xorn because we set the field back to the default (my_shingle2) analyzer, so the
+        // probability that the term is not in the dictionary but is NOT a misspelling is relatively high in this case compared to the
+        // others that have no n-gram with the other terms in the phrase :) you can set this realWorldErrorLikelyhood
+        assertSuggestion(suggest, 0, "simple_phrase", "xorr the god jewel");
+    }
+
+    public void testDifferentShardSize() throws Exception {
+        createIndex("test");
+        ensureGreen();
+        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("field1", "foobar1").setRouting("1"),
+                client().prepareIndex("test", "type1", "2").setSource("field1", "foobar2").setRouting("2"),
+                client().prepareIndex("test", "type1", "3").setSource("field1", "foobar3").setRouting("3"));
+
+        Suggest suggest = searchSuggest( "foobar",
+                termSuggestion("simple")
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always"));
+        ElasticsearchAssertions.assertSuggestionSize(suggest, 0, 3, "simple");
+    }
+
+    // see #3469
+    public void testShardFailures() throws IOException, InterruptedException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 5)
+                .put("index.analysis.filter.shingler.output_unigrams", true));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type2")
+                .startObject("properties")
+                    .startObject("name")
+                        .field("type", "multi_field")
+                        .startObject("fields")
+                            .startObject("name")
+                                .field("type", "string")
+                                .field("analyzer", "suggest")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type2", mapping));
+        ensureGreen();
+
+        index("test", "type2", "1", "foo", "bar");
+        index("test", "type2", "2", "foo", "bar");
+        index("test", "type2", "3", "foo", "bar");
+        index("test", "type2", "4", "foo", "bar");
+        index("test", "type2", "5", "foo", "bar");
+        index("test", "type2", "1", "name", "Just testing the suggestions api");
+        index("test", "type2", "2", "name", "An other title about equal length");
+        // Note that the last document has to have about the same length as the other or cutoff rechecking will remove the useful suggestion.
+        refresh();
+
+        // When searching on a shard with a non existing mapping, we should fail
+        SearchRequestBuilder request = client().prepareSearch().setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
+        assertThrows(request, SearchPhaseExecutionException.class);
+
+        // When searching on a shard which does not hold yet any document of an existing type, we should not fail
+        SearchResponse searchResponse = client().prepareSearch().setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+            .get();
+        ElasticsearchAssertions.assertNoFailures(searchResponse);
+        ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
+    }
+
+    // see #3469
+    public void testEmptyShards() throws IOException, InterruptedException {
+        XContentBuilder mappingBuilder = XContentFactory.jsonBuilder().
+                startObject().
+                    startObject("type1").
+                        startObject("properties").
+                            startObject("name").
+                                field("type", "multi_field").
+                                startObject("fields").
+                                    startObject("name").
+                                        field("type", "string").
+                                        field("analyzer", "suggest").
+                                    endObject().
+                                endObject().
+                            endObject().
+                        endObject().
+                    endObject().
+                endObject();
+        assertAcked(prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 5)
+                .put("index.analysis.filter.shingler.output_unigrams", true)).addMapping("type1", mappingBuilder));
+        ensureGreen();
+
+        index("test", "type2", "1", "foo", "bar");
+        index("test", "type2", "2", "foo", "bar");
+        index("test", "type1", "1", "name", "Just testing the suggestions api");
+        index("test", "type1", "2", "name", "An other title about equal length");
+        refresh();
+
+        SearchResponse searchResponse = client().prepareSearch()
+                .setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+                .get();
+
+        assertNoFailures(searchResponse);
+        assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
+    }
+
+    /**
+     * Searching for a rare phrase shouldn't provide any suggestions if confidence &gt; 1.  This was possible before we rechecked the cutoff
+     * score during the reduce phase.  Failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
+     */
+    public void testSearchForRarePhrase() throws IOException {
+        // If there isn't enough chaf per shard then shards can become unbalanced, making the cutoff recheck this is testing do more harm then good.
+        int chafPerShard = 100;
+
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("_all")
+                            .field("store", "yes")
+                            .field("termVector", "with_positions_offsets")
+                        .endObject()
+                        .startObject("properties")
+                            .startObject("body")
+                                .field("type", "string")
+                                .field("analyzer", "body")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        NumShards test = getNumShards("test");
+
+        List<String> phrases = new ArrayList<>();
+        Collections.addAll(phrases, "nobel prize", "noble gases", "somethingelse prize", "pride and joy", "notes are fun");
+        for (int i = 0; i < 8; i++) {
+            phrases.add("noble somethingelse" + i);
+        }
+        for (int i = 0; i < test.numPrimaries * chafPerShard; i++) {
+            phrases.add("chaff" + i);
+        }
+        for (String phrase: phrases) {
+            index("test", "type1", phrase, "body", phrase);
+        }
+        refresh();
+
+        Suggest searchSuggest = searchSuggest("nobel prize", phraseSuggestion("simple_phrase")
+                .field("body")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
+                .confidence(2f)
+                .maxErrors(5f)
+                .size(1));
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        searchSuggest = searchSuggest("noble prize", phraseSuggestion("simple_phrase")
+                .field("body")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
+                .confidence(2f)
+                .maxErrors(5f)
+                .size(1));
+        assertSuggestion(searchSuggest, 0, 0, "simple_phrase", "nobel prize");
+    }
+
+    @Nightly
+    public void testSuggestWithManyCandidates() throws InterruptedException, ExecutionException, IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("properties")
+                            .startObject("title")
+                                .field("type", "string")
+                                .field("analyzer", "text")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        // We're going to be searching for:
+        //   united states house of representatives elections in washington 2006
+        // But we need to make sure we generate a ton of suggestions so we add a bunch of candidates.
+        // Many of these candidates are drawn from page names on English Wikipedia.
+
+        // Tons of different options very near the exact query term
+        titles.add("United States House of Representatives Elections in Washington 1789");
+        for (int year = 1790; year < 2014; year+= 2) {
+            titles.add("United States House of Representatives Elections in Washington " + year);
+        }
+        // Six of these are near enough to be viable suggestions, just not the top one
+
+        // But we can't stop there!  Titles that are just a year are pretty common so lets just add one per year
+        // since 0.  Why not?
+        for (int year = 0; year < 2015; year++) {
+            titles.add(Integer.toString(year));
+        }
+        // That ought to provide more less good candidates for the last term
+
+        // Now remove or add plural copies of every term we can
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        // Now some possessive
+        titles.add("Washington's Birthday");
+
+        // And some conjugation
+        titles.add("Unified Modeling Language");
+        titles.add("Unite Against Fascism");
+        titles.add("Stated Income Tax");
+        titles.add("Media organizations housed within colleges");
+
+        // And other stuff
+        titles.add("Untied shoelaces");
+        titles.add("Unit circle");
+        titles.add("Untitled");
+        titles.add("Unicef");
+        titles.add("Unrated");
+        titles.add("UniRed");
+        titles.add("Jalan Uniten–Dengkil"); // Highway in Malaysia
+        titles.add("UNITAS");
+        titles.add("UNITER");
+        titles.add("Un-Led-Ed");
+        titles.add("STATS LLC");
+        titles.add("Staples");
+        titles.add("Skates");
+        titles.add("Statues of the Liberators");
+        titles.add("Staten Island");
+        titles.add("Statens Museum for Kunst");
+        titles.add("Hause"); // The last name or the German word, whichever.
+        titles.add("Hose");
+        titles.add("Hoses");
+        titles.add("Howse Peak");
+        titles.add("The Hoose-Gow");
+        titles.add("Hooser");
+        titles.add("Electron");
+        titles.add("Electors");
+        titles.add("Evictions");
+        titles.add("Coronal mass ejection");
+        titles.add("Wasington"); // A film?
+        titles.add("Warrington"); // A town in England
+        titles.add("Waddington"); // Lots of places have this name
+        titles.add("Watlington"); // Ditto
+        titles.add("Waplington"); // Yup, also a town
+        titles.add("Washing of the Spears"); // Book
+
+        for (char c = 'A'; c <= 'Z'; c++) {
+            // Can't forget lists, glorious lists!
+            titles.add("List of former members of the United States House of Representatives (" + c + ")");
+
+            // Lots of people are named Washington <Middle Initial>. LastName
+            titles.add("Washington " + c + ". Lastname");
+
+            // Lets just add some more to be evil
+            titles.add("United " + c);
+            titles.add("States " + c);
+            titles.add("House " + c);
+            titles.add("Elections " + c);
+            titles.add("2006 " + c);
+            titles.add(c + " United");
+            titles.add(c + " States");
+            titles.add(c + " House");
+            titles.add(c + " Elections");
+            titles.add(c + " 2006");
+        }
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
+        }
+        indexRandom(true, builders);
+
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(1000) // Setting a silly high size helps of generate a larger list of candidates for testing.
+                        .maxInspections(1000) // This too
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(30000);
+        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
+        assertSuggestionSize(searchSuggest, 0, 25480, "title");  // Just to prove that we've run through a ton of options
+
+        suggest.size(1);
+        long start = System.currentTimeMillis();
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        long total = System.currentTimeMillis() - start;
+        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
+        // assertThat(total, lessThan(1000L)); // Takes many seconds without fix - just for debugging
+    }
+
+    public void testPhraseSuggesterCollate() throws InterruptedException, ExecutionException, IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("title")
+                .field("type", "string")
+                .field("analyzer", "text")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        titles.add("United States House of Representatives Elections in Washington 2006");
+        titles.add("United States House of Representatives Elections in Washington 2005");
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
+        }
+        indexRandom(true, builders);
+
+        // suggest without collate
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(10)
+                        .maxInspections(200)
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(10);
+        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        assertSuggestionSize(searchSuggest, 0, 10, "title");
+
+        // suggest with collate
+        String filterString = XContentFactory.jsonBuilder()
+                    .startObject()
+                        .startObject("match_phrase")
+                            .field("title", "{{suggestion}}")
+                        .endObject()
+                    .endObject()
+                .string();
+        PhraseSuggestionBuilder filteredQuerySuggest = suggest.collateQuery(filterString);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredQuerySuggest);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate suggest with no result (boundary case)
+        searchSuggest = searchSuggest("Elections of Representatives Parliament", filteredQuerySuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "title");
+
+        NumShards numShards = getNumShards("test");
+
+        // collate suggest with bad query
+        String incorrectFilterString = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("test")
+                        .field("title", "{{suggestion}}")
+                    .endObject()
+                .endObject()
+                .string();
+        PhraseSuggestionBuilder incorrectFilteredSuggest = suggest.collateQuery(incorrectFilterString);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, incorrectFilteredSuggest);
+            fail("Post query error has been swallowed");
+        } catch(ElasticsearchException e) {
+            // expected
+        }
+
+        // suggest with collation
+        String filterStringAsFilter = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("match_phrase")
+                .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(filterStringAsFilter);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredFilterSuggest);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate suggest with bad query
+        String filterStr = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("pprefix")
+                        .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PhraseSuggestionBuilder in = suggest.collateQuery(filterStr);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, in);
+            fail("Post filter error has been swallowed");
+        } catch(ElasticsearchException e) {
+            //expected
+        }
+
+        // collate script failure due to no additional params
+        String collateWithParams = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("{{query_type}}")
+                    .field("{{query_field}}", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+
+        PhraseSuggestionBuilder phraseSuggestWithNoParams = suggest.collateQuery(collateWithParams);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, phraseSuggestWithNoParams);
+            fail("Malformed query (lack of additional params) should fail");
+        } catch (ElasticsearchException e) {
+            // expected
+        }
+
+        // collate script with additional params
+        Map<String, Object> params = new HashMap<>();
+        params.put("query_type", "match_phrase");
+        params.put("query_field", "title");
+
+        PhraseSuggestionBuilder phraseSuggestWithParams = suggest.collateQuery(collateWithParams).collateParams(params);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParams);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate query request with prune set to true
+        PhraseSuggestionBuilder phraseSuggestWithParamsAndReturn = suggest.collateQuery(collateWithParams).collateParams(params).collatePrune(true);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParamsAndReturn);
+        assertSuggestionSize(searchSuggest, 0, 10, "title");
+        assertSuggestionPhraseCollateMatchExists(searchSuggest, "title", 2);
+    }
+
+    protected Suggest searchSuggest(SuggestionBuilder<?>... suggestion) {
+        return searchSuggest(null, suggestion);
+    }
+
+    protected Suggest searchSuggest(String suggestText, SuggestionBuilder<?>... suggestions) {
+        return searchSuggest(suggestText, 0, suggestions);
+    }
+
+    protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
+        if (randomBoolean()) {
+            SearchRequestBuilder builder = client().prepareSearch().setSize(0);
+            SuggestBuilder suggestBuilder = new SuggestBuilder();
+            if (suggestText != null) {
+                suggestBuilder.setText(suggestText);
+            }
+            for (SuggestionBuilder<?> suggestion : suggestions) {
+                suggestBuilder.addSuggestion(suggestion);
+            }
+            builder.suggest(suggestBuilder);
+            SearchResponse actionGet = builder.execute().actionGet();
+            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
+            return actionGet.getSuggest();
+        } else {
+            SuggestRequestBuilder builder = client().prepareSuggest();
+            if (suggestText != null) {
+                builder.setSuggestText(suggestText);
+            }
+            for (SuggestionBuilder<?> suggestion : suggestions) {
+                builder.addSuggestion(suggestion);
+            }
+
+            SuggestResponse actionGet = builder.execute().actionGet();
+            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
+            if (expectShardsFailed > 0) {
+                throw new SearchPhaseExecutionException("suggest", "Suggest execution failed", new ShardSearchFailure[0]);
+            }
+            return actionGet.getSuggest();
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
new file mode 100644
index 0000000..29213f0
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
@@ -0,0 +1,210 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.messy.tests;
+
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Accountable;
+import org.elasticsearch.Version;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.ModulesBuilder;
+import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.inject.util.Providers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+import org.elasticsearch.common.settings.SettingsModule;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.EnvironmentModule;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.analysis.AnalysisRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
+import org.elasticsearch.index.fielddata.IndexFieldDataService;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.TemplateQueryParser;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.indices.IndicesModule;
+import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.breaker.CircuitBreakerService;
+import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
+import org.elasticsearch.indices.mapper.MapperRegistry;
+import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.IndexSettingsModule;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.threadpool.ThreadPoolModule;
+import org.junit.After;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.lang.reflect.Proxy;
+import java.util.Collections;
+
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test parsing and executing a template request.
+ */
+// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
+public class TemplateQueryParserTests extends ESTestCase {
+
+    private Injector injector;
+    private QueryShardContext context;
+
+    @Before
+    public void setup() throws IOException {
+        Settings settings = Settings.settingsBuilder()
+                .put("path.home", createTempDir().toString())
+                .put("path.conf", this.getDataPath("config"))
+                .put("name", getClass().getName())
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+                .build();
+        final Client proxy = (Client) Proxy.newProxyInstance(
+                Client.class.getClassLoader(),
+                new Class<?>[]{Client.class}, (proxy1, method, args) -> {
+                    throw new UnsupportedOperationException("client is just a dummy");
+                });
+        Index index = new Index("test");
+        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
+        ScriptModule scriptModule = new ScriptModule(settings);
+        // TODO: make this use a mock engine instead of mustache and it will no longer be messy!
+        scriptModule.addScriptEngine(MustacheScriptEngineService.class);
+        injector = new ModulesBuilder().add(
+                new EnvironmentModule(new Environment(settings)),
+                new SettingsModule(settings, new SettingsFilter(settings)),
+                new ThreadPoolModule(new ThreadPool(settings)),
+                new IndicesModule() {
+                    @Override
+                    public void configure() {
+                        // skip services
+                        bindQueryParsersExtension();
+                    }
+                },
+                scriptModule,
+                new IndexSettingsModule(index, settings),
+                new AbstractModule() {
+                    @Override
+                    protected void configure() {
+                        bind(Client.class).toInstance(proxy); // not needed here
+                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
+                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
+                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
+                    }
+                }
+        ).createInjector();
+
+        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
+        ScriptService scriptService = injector.getInstance(ScriptService.class);
+        SimilarityService similarityService = new SimilarityService(idxSettings, Collections.emptyMap());
+        MapperRegistry mapperRegistry = new IndicesModule().getMapperRegistry();
+        MapperService mapperService = new MapperService(idxSettings, analysisService, similarityService, mapperRegistry);
+        IndexFieldDataService indexFieldDataService =new IndexFieldDataService(idxSettings, injector.getInstance(IndicesFieldDataCache.class), injector.getInstance(CircuitBreakerService.class), mapperService);
+        BitsetFilterCache bitsetFilterCache = new BitsetFilterCache(idxSettings, new IndicesWarmer(idxSettings.getNodeSettings(), null), new BitsetFilterCache.Listener() {
+            @Override
+            public void onCache(ShardId shardId, Accountable accountable) {
+
+            }
+
+            @Override
+            public void onRemoval(ShardId shardId, Accountable accountable) {
+
+            }
+        });
+        IndicesQueriesRegistry indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
+        context = new QueryShardContext(idxSettings, proxy, bitsetFilterCache, indexFieldDataService, mapperService, similarityService, scriptService, indicesQueriesRegistry);
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        terminate(injector.getInstance(ThreadPool.class));
+    }
+
+    public void testParser() throws IOException {
+        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
+                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    /**
+     * Test that the template query parser can parse and evaluate template
+     * expressed as a single string but still it expects only the query
+     * specification (thus this test should fail with specific exception).
+     */
+    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        try {
+            parser.fromXContent(context.parseContext()).toQuery(context);
+            fail("Expected ParsingException");
+        } catch (ParsingException e) {
+            assertThat(e.getMessage(), containsString("query malformed, no field after start_object"));
+        }
+    }
+
+    public void testParserCanExtractTemplateNames() throws Exception {
+        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
new file mode 100644
index 0000000..7029826
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
@@ -0,0 +1,515 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.index.IndexRequest.OpType;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
+import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.TemplateQueryBuilder;
+import org.elasticsearch.index.query.TemplateQueryParser;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ * Full integration test of the template query plugin.
+ */
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
+public class TemplateQueryTests extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+
+    @Before
+    public void setup() throws IOException {
+        createIndex("test");
+        ensureGreen("test");
+
+        index("test", "testtype", "1", jsonBuilder().startObject().field("text", "value1").endObject());
+        index("test", "testtype", "2", jsonBuilder().startObject().field("text", "value2").endObject());
+        refresh();
+    }
+
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
+                .put("path.conf", this.getDataPath("config")).build();
+    }
+
+    public void testTemplateInBody() throws IOException {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("template", "all");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("{\"match_{{template}}\": {}}\"", ScriptType.INLINE, null,
+                null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testTemplateInBodyWithSize() throws IOException {
+        Map<String, Object> params = new HashMap<>();
+        params.put("template", "all");
+        SearchResponse sr = client().prepareSearch()
+                .setSource(
+                        new SearchSourceBuilder().size(0).query(
+                                QueryBuilders.templateQuery(new Template("{ \"match_{{template}}\": {} }",
+                                        ScriptType.INLINE, null, null, params)))).execute()
+                .actionGet();
+        assertNoFailures(sr);
+        assertThat(sr.getHits().hits().length, equalTo(0));
+    }
+
+    public void testTemplateWOReplacementInBody() throws IOException {
+        Map<String, Object> vars = new HashMap<>();
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "{\"match_all\": {}}\"", ScriptType.INLINE, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testTemplateInFile() {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("template", "all");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "storedTemplate", ScriptService.ScriptType.FILE, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testRawFSTemplate() throws IOException {
+        Map<String, Object> params = new HashMap<>();
+        params.put("template", "all");
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
+        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
+        assertHitCount(sr, 2);
+    }
+
+    public void testSearchRequestTemplateSource() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+
+        String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
+        searchRequest.template(parseTemplate(query));
+
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertHitCount(searchResponse, 2);
+    }
+
+    private Template parseTemplate(String template) throws IOException {
+        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
+            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
+        }
+    }
+
+    // Releates to #6318
+    public void testSearchRequestFail() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        try {
+            String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
+            searchRequest.template(parseTemplate(query));
+            client().search(searchRequest).get();
+            fail("expected exception");
+        } catch (Exception ex) {
+            // expected - no params
+        }
+        String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
+        searchRequest.template(parseTemplate(query));
+
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    public void testThatParametersCanBeSet() throws Exception {
+        index("test", "type", "1", jsonBuilder().startObject().field("theField", "foo").endObject());
+        index("test", "type", "2", jsonBuilder().startObject().field("theField", "foo 2").endObject());
+        index("test", "type", "3", jsonBuilder().startObject().field("theField", "foo 3").endObject());
+        index("test", "type", "4", jsonBuilder().startObject().field("theField", "foo 4").endObject());
+        index("test", "type", "5", jsonBuilder().startObject().field("otherField", "foo").endObject());
+        refresh();
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("mySize", "2");
+        templateParams.put("myField", "theField");
+        templateParams.put("myValue", "foo");
+
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+        // size kicks in here...
+        assertThat(searchResponse.getHits().getHits().length, is(2));
+
+        templateParams.put("myField", "otherField");
+        searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 1);
+    }
+
+    public void testSearchTemplateQueryFromFile() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
+                + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
+        searchRequest.template(parseTemplate(query));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can be expressed as a single escaped string.
+     */
+    public void testTemplateQueryAsEscapedString() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
+        searchRequest.template(parseTemplate(query));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can contain conditional clause. In this case it is at
+     * the beginning of the string.
+     */
+    public void testTemplateQueryAsEscapedStringStartingWithConditionalClause() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String templateString = "{"
+                + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
+        searchRequest.template(parseTemplate(templateString));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can contain conditional clause. In this case it is at
+     * the end of the string.
+     */
+    public void testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String templateString = "{"
+                + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
+                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
+        searchRequest.template(parseTemplate(templateString));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    public void testIndexedTemplateClient() throws Exception {
+        createIndex(ScriptService.SCRIPT_INDEX);
+        ensureGreen(ScriptService.SCRIPT_INDEX);
+
+        PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
+                "\"template\":{" +
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}" +
+                "}").get();
+
+        assertTrue(scriptResponse.isCreated());
+
+        scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
+                "\"template\":{" +
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}" +
+                "}").get();
+
+        assertEquals(scriptResponse.getVersion(), 2);
+
+        GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
+        assertTrue(getResponse.isExists());
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+
+        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+        indexRandom(true, builders);
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("fieldParam", "foo");
+
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("testTemplate", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+
+        DeleteIndexedScriptResponse deleteResponse = client().prepareDeleteIndexedScript(MustacheScriptEngineService.NAME, "testTemplate")
+                .get();
+        assertTrue(deleteResponse.isFound());
+
+        getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
+        assertFalse(getResponse.isExists());
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/template_index/mustache/1000", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                    templateParams)).get();
+            fail("Expected SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Illegal index script format"));
+        }
+    }
+
+    public void testIndexedTemplate() throws Exception {
+        createIndex(ScriptService.SCRIPT_INDEX);
+        ensureGreen(ScriptService.SCRIPT_INDEX);
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "1a").setSource("{" +
+                "\"template\":{"+
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                    "}" +
+                "}"));
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "2").setSource("{" +
+                "\"template\":{"+
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                    "}" +
+                "}"));
+
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "3").setSource("{" +
+                "\"template\":{"+
+                "             \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}"));
+
+        indexRandom(true, builders);
+
+        builders.clear();
+
+        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+        indexRandom(true, builders);
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("fieldParam", "foo");
+
+        SearchResponse searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                templateParams)).get();
+        assertHitCount(searchResponse, 4);
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/template_index/mustache/1000", ScriptService.ScriptType.INDEXED,
+                                    MustacheScriptEngineService.NAME, null, templateParams)).get();
+            fail("shouldn't get here");
+        } catch (SearchPhaseExecutionException spee) {
+            //all good
+        }
+
+        try {
+            searchResponse = client()
+                    .prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/myindex/mustache/1", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                    templateParams)).get();
+            assertFailures(searchResponse);
+        } catch (SearchPhaseExecutionException spee) {
+            //all good
+        }
+
+        searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+
+        templateParams.put("fieldParam", "bar");
+        searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/2", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                templateParams)).get();
+        assertHitCount(searchResponse, 1);
+
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("fieldParam", "bar");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "3", ScriptService.ScriptType.INDEXED, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 1);
+
+        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        Map<String, Object> params = new HashMap<>();
+        params.put("fieldParam", "foo");
+        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
+        sr = client().prepareSearch().setQuery(templateQuery).get();
+        assertHitCount(sr, 4);
+
+        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
+        sr = client().prepareSearch().setQuery(templateQuery).get();
+        assertHitCount(sr, 4);
+    }
+
+    // Relates to #10397
+    public void testIndexedTemplateOverwrite() throws Exception {
+        createIndex("testindex");
+        ensureGreen("testindex");
+
+        index("testindex", "test", "1", jsonBuilder().startObject().field("searchtext", "dev1").endObject());
+        refresh();
+
+        int iterations = randomIntBetween(2, 11);
+        for (int i = 1; i < iterations; i++) {
+            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
+            assertEquals(i * 2 - 1, scriptResponse.getVersion());
+
+            GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "git01").get();
+            assertTrue(getResponse.isExists());
+
+            Map<String, Object> templateParams = new HashMap<>();
+            templateParams.put("P_Keyword1", "dev");
+
+            try {
+                client().prepareSearch("testindex")
+                        .setTypes("test")
+                        .setTemplate(
+                                new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                        templateParams)).get();
+                fail("Broken test template is parsing w/o error.");
+            } catch (SearchPhaseExecutionException e) {
+                // the above is expected to fail
+            }
+
+            PutIndexedScriptRequestBuilder builder = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"phrase_prefix\"}}}}").setOpType(
+                    OpType.INDEX);
+            scriptResponse = builder.get();
+            assertEquals(i * 2, scriptResponse.getVersion());
+            SearchResponse searchResponse = client()
+                    .prepareSearch("testindex")
+                    .setTypes("test")
+                    .setTemplate(
+                            new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                    .get();
+            assertHitCount(searchResponse, 1);
+        }
+    }
+
+    public void testIndexedTemplateWithArray() throws Exception {
+      createIndex(ScriptService.SCRIPT_INDEX);
+      ensureGreen(ScriptService.SCRIPT_INDEX);
+      List<IndexRequestBuilder> builders = new ArrayList<>();
+
+      String multiQuery = "{\"query\":{\"terms\":{\"theField\":[\"{{#fieldParam}}\",\"{{.}}\",\"{{/fieldParam}}\"]}}}";
+
+      builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "4").setSource(jsonBuilder().startObject().field("template", multiQuery).endObject()));
+
+      indexRandom(true,builders);
+
+      builders.clear();
+
+      builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+      builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+      builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+      builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+      builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+      indexRandom(true,builders);
+
+      Map<String, Object> arrayTemplateParams = new HashMap<>();
+      String[] fieldParams = {"foo","bar"};
+      arrayTemplateParams.put("fieldParam", fieldParams);
+
+        SearchResponse searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/4", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                arrayTemplateParams)).get();
+        assertHitCount(searchResponse, 5);
+    }
+
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java
new file mode 100644
index 0000000..a2325b2
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+/**
+ * This package contains tests that use mustache to test what looks
+ * to be unrelated functionality, or functionality that should be 
+ * tested with a mock instead. Instead of doing an epic battle
+ * with these tests, they are temporarily moved here to the mustache
+ * module's tests, but that is likely not where they belong. Please 
+ * help by cleaning them up and we can remove this package!
+ *
+ * <ul>
+ *   <li>If the test is actually testing mustache specifically, move to 
+ *       the org.elasticsearch.script.mustache tests package of this module</li>
+ *   <li>If the test is testing templating integration with another core subsystem,
+ *       fix it to use a mock instead, so it can be in the core tests again</li>
+ *   <li>If the test is just being lazy, and does not really need templating to test
+ *       something, clean it up!</li>
+ * </ul>
+ */
+/* List of renames that took place:  
+renamed:    core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
+renamed:    core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
+renamed:    core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
+renamed:    core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
+renamed:    core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java -> module/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+  ^^^^^ note: just the methods from this test using mustache were moved here, the others use groovy and are in the groovy module under its messy tests package.
+renamed:    rest-api-spec/test/msearch/10_basic.yaml -> module/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml 
+ */
+
+package org.elasticsearch.messy.tests;
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java
new file mode 100644
index 0000000..0c489b3
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.mustache;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class MustacheRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(MustachePlugin.class);
+    }
+
+    public MustacheRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
new file mode 100644
index 0000000..ce29bf2
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
@@ -0,0 +1,170 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.io.StringWriter;
+import java.nio.charset.Charset;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ * Mustache based templating test
+ */
+public class MustacheScriptEngineTests extends ESTestCase {
+    private MustacheScriptEngineService qe;
+    private JsonEscapingMustacheFactory escaper;
+
+    @Before
+    public void setup() {
+        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        escaper = new JsonEscapingMustacheFactory();
+    }
+
+    public void testSimpleParameterReplace() {
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            vars.put("body_val", "\"quick brown\"");
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+    }
+
+    public void testEscapeJson() throws IOException {
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("hello \n world", writer);
+            assertThat(writer.toString(), equalTo("hello \\n world"));
+        }
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("\n", writer);
+            assertThat(writer.toString(), equalTo("\\n"));
+        }
+
+        Character[] specialChars = new Character[]{
+                '\"',
+                '\\',
+                '\u0000',
+                '\u0001',
+                '\u0002',
+                '\u0003',
+                '\u0004',
+                '\u0005',
+                '\u0006',
+                '\u0007',
+                '\u0008',
+                '\u0009',
+                '\u000B',
+                '\u000C',
+                '\u000E',
+                '\u000F',
+                '\u001F'};
+        String[] escapedChars = new String[]{
+                "\\\"",
+                "\\\\",
+                "\\u0000",
+                "\\u0001",
+                "\\u0002",
+                "\\u0003",
+                "\\u0004",
+                "\\u0005",
+                "\\u0006",
+                "\\u0007",
+                "\\u0008",
+                "\\u0009",
+                "\\u000B",
+                "\\u000C",
+                "\\u000E",
+                "\\u000F",
+                "\\u001F"};
+        int iters = scaledRandomIntBetween(100, 1000);
+        for (int i = 0; i < iters; i++) {
+            int rounds = scaledRandomIntBetween(1, 20);
+            StringWriter expect = new StringWriter();
+            StringWriter writer = new StringWriter();
+            for (int j = 0; j < rounds; j++) {
+                String s = getChars();
+                writer.write(s);
+                expect.write(s);
+
+                int charIndex = randomInt(7);
+                writer.append(specialChars[charIndex]);
+                expect.append(escapedChars[charIndex]);
+            }
+            StringWriter target = new StringWriter();
+            escaper.encode(writer.toString(), target);
+            assertThat(expect.toString(), equalTo(target.toString()));
+        }
+    }
+
+    private String getChars() {
+        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
+        for (int i = 0; i < string.length(); i++) {
+            if (isEscapeChar(string.charAt(i))) {
+                return string.substring(0, i);
+            }
+        }
+        return string;
+    }
+
+    /**
+     * From https://www.ietf.org/rfc/rfc4627.txt:
+     *
+     * All Unicode characters may be placed within the
+     * quotation marks except for the characters that must be escaped:
+     * quotation mark, reverse solidus, and the control characters (U+0000
+     * through U+001F).
+     * */
+    private static boolean isEscapeChar(char c) {
+        switch (c) {
+        case '"':
+        case '\\':
+            return true;
+        }
+
+        if (c < '\u002F')
+            return true;
+        return false;
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
new file mode 100644
index 0000000..76c8678
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.Mustache;
+import com.github.mustachejava.MustacheFactory;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.util.HashMap;
+
+/**
+ * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
+ * */
+public class MustacheTests extends ESTestCase {
+    public void test() {
+        HashMap<String, Object> scopes = new HashMap<>();
+        scopes.put("boost_val", "0.2");
+
+        String template = "GET _search {\"query\": " + "{\"boosting\": {"
+                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
+                + "}}, \"negative_boost\": {{boost_val}} } }}";
+        MustacheFactory f = new DefaultMustacheFactory();
+        Mustache mustache = f.compile(new StringReader(template), "example");
+        StringWriter writer = new StringWriter();
+        mustache.execute(writer, scopes);
+        writer.flush();
+        assertEquals(
+                "Mustache templating broken",
+                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
+                writer.toString());
+    }
+}
diff --git a/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache
new file mode 100644
index 0000000..5191414
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache
@@ -0,0 +1,6 @@
+{
+  "query": {
+    "match": { "{{myField}}" : "{{myValue}}" }
+  },
+  "size" : {{mySize}}
+}
diff --git a/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache
new file mode 100644
index 0000000..a779da7
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache
@@ -0,0 +1,3 @@
+{
+    "match_{{template}}": {}
+}
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
new file mode 100644
index 0000000..9bfea28
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
@@ -0,0 +1,71 @@
+# Integration tests for Mustache scripts
+#
+"Mustache loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.modules.0.name: lang-mustache  }
+    - match:  { nodes.$master.modules.0.jvm: true  }
+
+---
+"Indexed template":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match_all": {}}, "size": "{{my_size}}" } }
+  - match: { _id: "1" }
+
+  - do:
+      get_template:
+        id: 1
+  - match: { found: true }
+  - match: { lang: mustache }
+  - match: { _id: "1" }
+  - match: { _version: 1 }
+  - match: { template: /.*query\S\S\S\Smatch_all.*/ }
+
+  - do:
+      catch: missing
+      get_template:
+        id: 2
+  - match: { found: false }
+  - match: { lang: mustache }
+  - match: { _id: "2" }
+  - is_false: _version
+  - is_false: template
+
+  - do:
+      delete_template:
+        id: "1"
+  - match: { found: true }
+  - match: { _index: ".scripts" }
+  - match: { _id: "1" }
+  - match: { _version: 2}
+
+  - do:
+      catch: missing
+      delete_template:
+        id: "non_existing"
+  - match: { found: false }
+  - match: { _index: ".scripts" }
+  - match: { _id: "non_existing" }
+  - match: { _version: 1 }
+
+  - do:
+      catch: request
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
+
+  - do:
+      catch: /Unable\sto\sparse.*/
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml
new file mode 100644
index 0000000..4da748a
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml
@@ -0,0 +1,38 @@
+---
+"Indexed Template query tests":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1_foo" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2_foo value3_foo" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match" : { "text": "{{my_value}}" } }, "size": "{{my_size}}" } }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+
+  - do:
+      search_template:
+        body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
+  - match: { hits.total: 1 }
+
+  - do:
+      catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
+      search_template:
+        body: { "file" : "simple1"}
+
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml
new file mode 100644
index 0000000..5d5c3d5
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml
@@ -0,0 +1,110 @@
+---
+"Indexed Template validate tests":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } } }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "foo", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "bar", "my_field": "my_other_field" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
+
+  - do:
+      render_search_template:
+        id: "1"
+        body: { "params": { "my_value": "bar", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+---
+"Inline Template validate tests":
+
+  - do:
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "foo", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+  - do:
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "my_other_field" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
+
+  - do:
+      catch: /Improperly.closed.variable.in.query-template/
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "field1" } }
+---
+"Escaped Indexed Template validate tests":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }" }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "foo", "my_size": 20 } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.size: 20 }
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+  - do:
+      render_search_template:
+        id: "1"
+        body: { "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+---
+"Escaped Inline Template validate tests":
+
+  - do:
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "foo", "my_size": 20 } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.size: 20 }
+
+  - do:
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+  - do:
+      catch: /Improperly.closed.variable.in.query-template/
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml
new file mode 100644
index 0000000..d2474b7
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml
@@ -0,0 +1,48 @@
+---
+"Template query":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2 value3" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": { "term": { "text": { "value": "{{template}}" } } }, "params": { "template": "value1" } } } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search: 
+        body: { "query": { "template": { "query": {"match_{{template}}": {}}, "params" : { "template" : "all" } } } }
+
+  - match: { hits.total: 2 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{ \"term\": { \"text\": { \"value\": \"{{template}}\" } } }", "params": { "template": "value1" } } } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{\"match_{{template}}\": {}}", "params" : { "template" : "all" } } } }
+
+  - match: { hits.total: 2 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{\"query_string\": { \"query\" : \"{{query}}\" }}", "params" : { "query" : "text:\"value2 value3\"" } } } }
+
+
+  - match: { hits.total: 1 }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml
new file mode 100644
index 0000000..a0a6695
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml
@@ -0,0 +1,29 @@
+---
+"Template search request":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      search_template:
+        body: { "template" : { "query": { "term": { "text": { "value": "{{template}}" } } } }, "params": { "template": "value1" } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search_template:
+        body: { "template" : { "query": { "match_{{template}}": {} } }, "params" : { "template" : "all" } }
+
+  - match: { hits.total: 2 }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml
new file mode 100644
index 0000000..205070be
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml
@@ -0,0 +1,54 @@
+---
+"Basic multi-search":
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     1
+          body:   { foo: bar }
+
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     2
+          body:   { foo: baz }
+
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     3
+          body:   { foo: foo }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      msearch:
+        body:
+          - index: test_1
+          - query:
+              match_all: {}
+          - index: test_2
+          - query:
+              match_all: {}
+          - search_type: query_then_fetch
+            index: test_1
+          - query:
+              match: {foo: bar}
+
+  - match:  { responses.0.hits.total:     3  }
+  - match:  { responses.1.error.root_cause.0.type: index_not_found_exception }
+  - match:  { responses.1.error.root_cause.0.reason: "/no.such.index/" }
+  - match:  { responses.1.error.root_cause.0.index: test_2 }
+  - match:  { responses.2.hits.total:     1  }
+
+  - do:
+      msearch:
+        body: 
+          - index: test_1
+          - query:
+              { "template": { "query": { "term": { "foo": { "value": "{{template}}" } } }, "params": { "template": "bar" } } }
+  - match: { responses.0.hits.total: 1 }
+
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 22ca2f2..0000000
--- a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d957c3956797c9c057e65f6342eeb104fa20951e
\ No newline at end of file
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..4942bbc
--- /dev/null
+++ b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4e56ba76d6b23756b2bd4d9e42b2b00122cd4fa5
\ No newline at end of file
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 3a31061..0000000
--- a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2ea1253cd3a704dea02382d6f9abe7c2a58874ac
\ No newline at end of file
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..1ba2a93
--- /dev/null
+++ b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+d6ccac802dc1e4c177be043a173377cf5e517cff
\ No newline at end of file
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 44a895a..0000000
--- a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-7f2132a00895c6eacfc735a4d6056275a692363b
\ No newline at end of file
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..2b61186
--- /dev/null
+++ b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+70ad9f6c3738727229867419d949527cc7789f62
\ No newline at end of file
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index d00f2ec..0000000
--- a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5eff0c934476356fcd608d574ce0af4104e5e2a4
\ No newline at end of file
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..e28887a
--- /dev/null
+++ b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+75504fd906929700e7d11f9600e4a79de48e1090
\ No newline at end of file
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ccc82e3..0000000
--- a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-210cab15ecf74e5a1bf35173ef5b0d420475694c
\ No newline at end of file
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..739ecc4
--- /dev/null
+++ b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9eeeeabeab89ec305e831d80bdcc7e85a1140fbb
\ No newline at end of file
diff --git a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
index 2b70834..c44608c 100644
--- a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
+++ b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.util.concurrent.CountDown;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.internal.InternalSearchHit;
@@ -225,7 +226,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
                 } else {
                     deleted++;
                 }
-                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse("test", "type", String.valueOf(i), 1, delete));
+                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse(new ShardId("test", 0), "type", String.valueOf(i), 1, delete));
             } else {
                 items[i] = new BulkItemResponse(i, "delete", new BulkItemResponse.Failure("test", "type", String.valueOf(i), new Throwable("item failed")));
                 failed++;
@@ -281,7 +282,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
                     deleted[0] = deleted[0] + 1;
                     deleted[index] = deleted[index] + 1;
                 }
-                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse("test-" + index, "type", String.valueOf(i), 1, delete));
+                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse(new ShardId("test-" + index, 0), "type", String.valueOf(i), 1, delete));
             } else {
                 items[i] = new BulkItemResponse(i, "delete", new BulkItemResponse.Failure("test-" + index, "type", String.valueOf(i), new Throwable("item failed")));
                 failed[0] = failed[0] + 1;
diff --git a/plugins/discovery-ec2/build.gradle b/plugins/discovery-ec2/build.gradle
index 2570661..77cfd66 100644
--- a/plugins/discovery-ec2/build.gradle
+++ b/plugins/discovery-ec2/build.gradle
@@ -42,7 +42,7 @@ dependencyLicenses {
   mapping from: /jackson-.*/, to: 'jackson'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-rawtypes'
+compileJava.options.compilerArgs << '-Xlint:-rawtypes,-deprecation'
 
 test {
   // this is needed for insecure plugins, remove if possible!
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
index a427b4a..d71d9df 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
@@ -27,20 +27,32 @@ public interface AwsEc2Service extends LifecycleComponent<AwsEc2Service> {
         public static final String KEY = "cloud.aws.access_key";
         public static final String SECRET = "cloud.aws.secret_key";
         public static final String PROTOCOL = "cloud.aws.protocol";
-        public static final String PROXY_HOST = "cloud.aws.proxy_host";
-        public static final String PROXY_PORT = "cloud.aws.proxy_port";
+        public static final String PROXY_HOST = "cloud.aws.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
         public static final String SIGNER = "cloud.aws.signer";
         public static final String REGION = "cloud.aws.region";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
     }
 
     final class CLOUD_EC2 {
         public static final String KEY = "cloud.aws.ec2.access_key";
         public static final String SECRET = "cloud.aws.ec2.secret_key";
         public static final String PROTOCOL = "cloud.aws.ec2.protocol";
-        public static final String PROXY_HOST = "cloud.aws.ec2.proxy_host";
-        public static final String PROXY_PORT = "cloud.aws.ec2.proxy_port";
+        public static final String PROXY_HOST = "cloud.aws.ec2.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.ec2.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.ec2.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.ec2.proxy.password";
         public static final String SIGNER = "cloud.aws.ec2.signer";
         public static final String ENDPOINT = "cloud.aws.ec2.endpoint";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.ec2.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.ec2.proxy_port";
     }
 
     final class DISCOVERY_EC2 {
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
index 76c3262..ec1ffd5 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
@@ -56,8 +56,10 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         // Filter global settings
         settingsFilter.addFilter(CLOUD_AWS.KEY);
         settingsFilter.addFilter(CLOUD_AWS.SECRET);
+        settingsFilter.addFilter(CLOUD_AWS.PROXY_PASSWORD);
         settingsFilter.addFilter(CLOUD_EC2.KEY);
         settingsFilter.addFilter(CLOUD_EC2.SECRET);
+        settingsFilter.addFilter(CLOUD_EC2.PROXY_PASSWORD);
         // add specific ec2 name resolver
         networkService.addCustomNameResolver(new Ec2NameResolver(settings));
         discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
@@ -83,16 +85,25 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         String account = settings.get(CLOUD_EC2.KEY, settings.get(CLOUD_AWS.KEY));
         String key = settings.get(CLOUD_EC2.SECRET, settings.get(CLOUD_AWS.SECRET));
 
-        String proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, settings.get(CLOUD_AWS.PROXY_HOST));
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
+        proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, settings.get(CLOUD_EC2.DEPRECATED_PROXY_HOST, proxyHost));
         if (proxyHost != null) {
-            String portString = settings.get(CLOUD_EC2.PROXY_PORT, settings.get(CLOUD_AWS.PROXY_PORT, "80"));
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
+            portString = settings.get(CLOUD_EC2.PROXY_PORT, settings.get(CLOUD_EC2.DEPRECATED_PROXY_PORT, portString));
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
             } catch (NumberFormatException ex) {
                 throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
             }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+            String proxyUsername = settings.get(CLOUD_EC2.PROXY_USERNAME, settings.get(CLOUD_AWS.PROXY_USERNAME));
+            String proxyPassword = settings.get(CLOUD_EC2.PROXY_PASSWORD, settings.get(CLOUD_AWS.PROXY_PASSWORD));
+
+            clientConfiguration
+                .withProxyHost(proxyHost)
+                .withProxyPort(proxyPort)
+                .withProxyUsername(proxyUsername)
+                .withProxyPassword(proxyPassword);
         }
 
         // #155: we might have 3rd party users using older EC2 API version
diff --git a/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy b/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
index 42bd707..d5c92a9 100644
--- a/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
@@ -19,7 +19,8 @@
 
 grant {
   // needed because of problems in ClientConfiguration
-  // TODO: get this fixed in aws sdk
+  // TODO: get these fixed in aws sdk
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   // NOTE: no tests fail without this, but we know the problem
   // exists in AWS sdk, and tests here are not thorough
   permission java.lang.RuntimePermission "getClassLoader";
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
index 943b1cd..07e05f0 100644
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
@@ -45,6 +45,7 @@ import java.io.IOException;
 import java.net.URL;
 import java.security.AccessController;
 import java.security.GeneralSecurityException;
+import java.security.PrivilegedAction;
 import java.security.PrivilegedActionException;
 import java.security.PrivilegedExceptionAction;
 import java.util.*;
@@ -103,23 +104,33 @@ public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceCompute
     public String metadata(String metadataPath) throws IOException {
         String urlMetadataNetwork = GCE_METADATA_URL + "/" + metadataPath;
         logger.debug("get metadata from [{}]", urlMetadataNetwork);
-        URL url = new URL(urlMetadataNetwork);
+        final URL url = new URL(urlMetadataNetwork);
         HttpHeaders headers;
         try {
             // hack around code messiness in GCE code
             // TODO: get this fixed
+            SecurityManager sm = System.getSecurityManager();
+            if (sm != null) {
+                sm.checkPermission(new SpecialPermission());
+            }
             headers = AccessController.doPrivileged(new PrivilegedExceptionAction<HttpHeaders>() {
                 @Override
                 public HttpHeaders run() throws IOException {
                     return new HttpHeaders();
                 }
             });
+            GenericUrl genericUrl = AccessController.doPrivileged(new PrivilegedAction<GenericUrl>() {
+                @Override
+                public GenericUrl run() {
+                    return new GenericUrl(url);
+                }
+            });
 
             // This is needed to query meta data: https://cloud.google.com/compute/docs/metadata
             headers.put("Metadata-Flavor", "Google");
             HttpResponse response;
             response = getGceHttpTransport().createRequestFactory()
-                    .buildGetRequest(new GenericUrl(url))
+                    .buildGetRequest(genericUrl)
                     .setHeaders(headers)
                     .execute();
             String metadata = response.parseAsString();
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
index 6d48943..1d73e1d 100644
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
@@ -20,13 +20,18 @@
 package org.elasticsearch.discovery.gce;
 
 import com.google.api.client.auth.oauth2.Credential;
+import com.google.api.client.googleapis.testing.auth.oauth2.MockGoogleCredential;
 import com.google.api.client.http.*;
 import com.google.api.client.util.ExponentialBackOff;
 import com.google.api.client.util.Sleeper;
+
+import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 
 import java.io.IOException;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
 import java.util.Objects;
 
 public class RetryHttpInitializerWrapper implements HttpRequestInitializer {
@@ -60,6 +65,21 @@ public class RetryHttpInitializerWrapper implements HttpRequestInitializer {
         this.sleeper = sleeper;
         this.maxWait = maxWait;
     }
+    
+    // Use only for testing
+    static MockGoogleCredential.Builder newMockCredentialBuilder() {
+        // TODO: figure out why GCE is so bad like this
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        return AccessController.doPrivileged(new PrivilegedAction<MockGoogleCredential.Builder>() {
+            @Override
+            public MockGoogleCredential.Builder run() {
+                return new MockGoogleCredential.Builder();
+            }
+        });
+    }
 
     @Override
     public void initialize(HttpRequest httpRequest) {
diff --git a/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy b/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
index 80a9978..429c472 100644
--- a/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
@@ -19,5 +19,6 @@
 
 grant {
   // needed because of problems in gce
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
index dcbd878..ef92bd7 100644
--- a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
@@ -97,7 +97,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, 3);
 
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
         MockSleeper mockSleeper = new MockSleeper();
 
@@ -122,7 +122,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, maxRetryTimes);
         JsonFactory jsonFactory = new JacksonFactory();
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
 
         MockSleeper oneTimeSleeper = new MockSleeper() {
@@ -155,7 +155,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, 1, true);
 
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
         MockSleeper mockSleeper = new MockSleeper();
         RetryHttpInitializerWrapper retryHttpInitializerWrapper = new RetryHttpInitializerWrapper(credential, mockSleeper, 500);
diff --git a/plugins/ingest/build.gradle b/plugins/ingest/build.gradle
deleted file mode 100644
index 2d43139..0000000
--- a/plugins/ingest/build.gradle
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-  description 'Plugin that allows to configure pipelines to preprocess documents before indexing'
-  classname 'org.elasticsearch.plugin.ingest.IngestPlugin'
-}
-
-dependencies {
-  compile 'org.jruby.joni:joni:2.1.6'
-  // joni dependencies:
-  compile 'org.jruby.jcodings:jcodings:1.0.12'
-
-  compile ('com.maxmind.geoip2:geoip2:2.3.1')
-  // geoip2 dependencies:
-  compile('com.fasterxml.jackson.core:jackson-annotations:2.5.0')
-  compile('com.fasterxml.jackson.core:jackson-databind:2.5.3')
-  compile('com.maxmind.db:maxmind-db:1.0.0')
-
-  compile 'joda-time:joda-time:2.8.2'
-  testCompile 'org.elasticsearch:geolite2-databases:20151029'
-  testCompile 'org.elasticsearch:securemock:1.1'
-}
-
-sourceSets {
-  test {
-    resources {
-      srcDir "src/main/packaging/config"
-    }
-  }
-}
-
-task copyDefaultGeoIp2DatabaseFiles(type: Copy) {
-  from zipTree(configurations.testCompile.files.find { it.name.contains('geolite2-databases')})
-  into "${project.buildDir}/geoip"
-  include "*.mmdb"
-}
-
-project.bundlePlugin.dependsOn(copyDefaultGeoIp2DatabaseFiles)
-
-compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
-compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
-
-bundlePlugin {
-  from("${project.buildDir}/geoip") {
-    into 'config/geoip'
-  }
-}
diff --git a/plugins/ingest/licenses/geoip2-2.3.1.jar.sha1 b/plugins/ingest/licenses/geoip2-2.3.1.jar.sha1
deleted file mode 100644
index cb1982f..0000000
--- a/plugins/ingest/licenses/geoip2-2.3.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0b128448f5bcfafb6caa82ae079ab39aa56dafb4
diff --git a/plugins/ingest/licenses/geoip2-LICENSE.txt b/plugins/ingest/licenses/geoip2-LICENSE.txt
deleted file mode 100644
index 7a4a3ea..0000000
--- a/plugins/ingest/licenses/geoip2-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
\ No newline at end of file
diff --git a/plugins/ingest/licenses/geoip2-NOTICE.txt b/plugins/ingest/licenses/geoip2-NOTICE.txt
deleted file mode 100644
index 448b71d..0000000
--- a/plugins/ingest/licenses/geoip2-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2013 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
\ No newline at end of file
diff --git a/plugins/ingest/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/ingest/licenses/jackson-annotations-2.5.0.jar.sha1
deleted file mode 100644
index 862ac6f..0000000
--- a/plugins/ingest/licenses/jackson-annotations-2.5.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/ingest/licenses/jackson-annotations-LICENSE b/plugins/ingest/licenses/jackson-annotations-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest/licenses/jackson-annotations-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest/licenses/jackson-annotations-NOTICE b/plugins/ingest/licenses/jackson-annotations-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest/licenses/jackson-annotations-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/ingest/licenses/jackson-databind-2.5.3.jar.sha1
deleted file mode 100644
index cdc6695..0000000
--- a/plugins/ingest/licenses/jackson-databind-2.5.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/ingest/licenses/jackson-databind-LICENSE b/plugins/ingest/licenses/jackson-databind-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest/licenses/jackson-databind-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest/licenses/jackson-databind-NOTICE b/plugins/ingest/licenses/jackson-databind-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest/licenses/jackson-databind-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest/licenses/jcodings-1.0.12.jar.sha1 b/plugins/ingest/licenses/jcodings-1.0.12.jar.sha1
deleted file mode 100644
index dac1d7a..0000000
--- a/plugins/ingest/licenses/jcodings-1.0.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-6bc17079fcaa8823ea8cd0d4c66516335b558db8
diff --git a/plugins/ingest/licenses/jcodings-LICENSE.txt b/plugins/ingest/licenses/jcodings-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/plugins/ingest/licenses/jcodings-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/plugins/ingest/licenses/jcodings-NOTICE.txt b/plugins/ingest/licenses/jcodings-NOTICE.txt
deleted file mode 100644
index f6c4948..0000000
--- a/plugins/ingest/licenses/jcodings-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-JCodings is released under the MIT License.
\ No newline at end of file
diff --git a/plugins/ingest/licenses/joni-2.1.6.jar.sha1 b/plugins/ingest/licenses/joni-2.1.6.jar.sha1
deleted file mode 100644
index 110752b..0000000
--- a/plugins/ingest/licenses/joni-2.1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0f23c95a06eaecbc8c74c7458a8bfd13e4fd2d3a
diff --git a/plugins/ingest/licenses/joni-LICENSE.txt b/plugins/ingest/licenses/joni-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/plugins/ingest/licenses/joni-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/plugins/ingest/licenses/joni-NOTICE.txt b/plugins/ingest/licenses/joni-NOTICE.txt
deleted file mode 100644
index 45bc517..0000000
--- a/plugins/ingest/licenses/joni-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-Joni is released under the MIT License.
diff --git a/plugins/ingest/licenses/maxmind-db-1.0.0.jar.sha1 b/plugins/ingest/licenses/maxmind-db-1.0.0.jar.sha1
deleted file mode 100644
index 4437d02..0000000
--- a/plugins/ingest/licenses/maxmind-db-1.0.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b140295a52005aaf224b6c711ad4ecb38b1da155
diff --git a/plugins/ingest/licenses/maxmind-db-LICENSE.txt b/plugins/ingest/licenses/maxmind-db-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/ingest/licenses/maxmind-db-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/ingest/licenses/maxmind-db-NOTICE.txt b/plugins/ingest/licenses/maxmind-db-NOTICE.txt
deleted file mode 100644
index 1ebe2b0..0000000
--- a/plugins/ingest/licenses/maxmind-db-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2014 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/IngestDocument.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/IngestDocument.java
deleted file mode 100644
index ca4d55e..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/IngestDocument.java
+++ /dev/null
@@ -1,482 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.Strings;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.*;
-
-/**
- * Represents a single document being captured before indexing and holds the source and metadata (like id, type and index).
- */
-public final class IngestDocument {
-
-    static final String TIMESTAMP = "timestamp";
-
-    private final Map<String, Object> sourceAndMetadata;
-    private final Map<String, String> ingestMetadata;
-
-    public IngestDocument(String index, String type, String id, String routing, String parent, String timestamp, String ttl, Map<String, Object> source) {
-        this.sourceAndMetadata = new HashMap<>();
-        this.sourceAndMetadata.putAll(source);
-        this.sourceAndMetadata.put(MetaData.INDEX.getFieldName(), index);
-        this.sourceAndMetadata.put(MetaData.TYPE.getFieldName(), type);
-        this.sourceAndMetadata.put(MetaData.ID.getFieldName(), id);
-        if (routing != null) {
-            this.sourceAndMetadata.put(MetaData.ROUTING.getFieldName(), routing);
-        }
-        if (parent != null) {
-            this.sourceAndMetadata.put(MetaData.PARENT.getFieldName(), parent);
-        }
-        if (timestamp != null) {
-            this.sourceAndMetadata.put(MetaData.TIMESTAMP.getFieldName(), timestamp);
-        }
-        if (ttl != null) {
-            this.sourceAndMetadata.put(MetaData.TTL.getFieldName(), ttl);
-        }
-
-        this.ingestMetadata = new HashMap<>();
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        df.setTimeZone(TimeZone.getTimeZone("UTC"));
-        this.ingestMetadata.put(TIMESTAMP, df.format(new Date()));
-    }
-
-    /**
-     * Copy constructor that creates a new {@link IngestDocument} which has exactly the same properties as the one provided as argument
-     */
-    public IngestDocument(IngestDocument other) {
-        this(new HashMap<>(other.sourceAndMetadata), new HashMap<>(other.ingestMetadata));
-    }
-
-    /**
-     * Constructor needed for testing that allows to create a new {@link IngestDocument} given the provided elasticsearch metadata,
-     * source and ingest metadata. This is needed because the ingest metadata will be initialized with the current timestamp at
-     * init time, which makes equality comparisons impossible in tests.
-     */
-    public IngestDocument(Map<String, Object> sourceAndMetadata, Map<String, String> ingestMetadata) {
-        this.sourceAndMetadata = sourceAndMetadata;
-        this.ingestMetadata = ingestMetadata;
-    }
-
-    /**
-     * Returns the value contained in the document for the provided path
-     * @param path The path within the document in dot-notation
-     * @param clazz The expected class of the field value
-     * @return the value for the provided path if existing, null otherwise
-     * @throws IllegalArgumentException if the path is null, empty, invalid, if the field doesn't exist
-     * or if the field that is found at the provided path is not of the expected type.
-     */
-    public <T> T getFieldValue(String path, Class<T> clazz) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (String pathElement : fieldPath.pathElements) {
-            context = resolve(pathElement, path, context);
-        }
-        return cast(path, context, clazz);
-    }
-
-    /**
-     * Checks whether the document contains a value for the provided path
-     * @param path The path within the document in dot-notation
-     * @return true if the document contains a value for the field, false otherwise
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public boolean hasField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                return false;
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                context = map.get(pathElement);
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                try {
-                    int index = Integer.parseInt(pathElement);
-                    if (index < 0 || index >= list.size()) {
-                        return false;
-                    }
-                    context = list.get(index);
-                } catch (NumberFormatException e) {
-                    return false;
-                }
-
-            } else {
-                return false;
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            return map.containsKey(leafKey);
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            try {
-                int index = Integer.parseInt(leafKey);
-                return index >= 0 && index < list.size();
-            } catch (NumberFormatException e) {
-                return false;
-            }
-        }
-        return false;
-    }
-
-    /**
-     * Removes the field identified by the provided path.
-     * @param path the path of the field to be removed
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void removeField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            context = resolve(fieldPath.pathElements[i], path, context);
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(leafKey)) {
-                map.remove(leafKey);
-                return;
-            }
-            throw new IllegalArgumentException("field [" + leafKey + "] not present as part of path [" + path + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            list.remove(index);
-            return;
-        }
-
-        if (context == null) {
-            throw new IllegalArgumentException("cannot remove [" + leafKey + "] from null as part of path [" + path + "]");
-        }
-        throw new IllegalArgumentException("cannot remove [" + leafKey + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-    }
-
-    private static Object resolve(String pathElement, String fullPath, Object context) {
-        if (context == null) {
-            throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + fullPath + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(pathElement)) {
-                return map.get(pathElement);
-            }
-            throw new IllegalArgumentException("field [" + pathElement + "] not present as part of path [" + fullPath + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(pathElement);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + fullPath + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + fullPath + "]");
-            }
-            return list.get(index);
-        }
-        throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + fullPath + "]");
-    }
-
-    /**
-     * Appends the provided value to the provided path in the document.
-     * Any non existing path element will be created. Same as {@link #setFieldValue(String, Object)}
-     * but if the last element is a list, the value will be appended to the existing list.
-     * @param path The path within the document in dot-notation
-     * @param value The value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void appendFieldValue(String path, Object value) {
-        setFieldValue(path, value, true);
-    }
-
-    /**
-     * Sets the provided value to the provided path in the document.
-     * Any non existing path element will be created. If the last element is a list,
-     * the value will replace the existing list.
-     * @param path The path within the document in dot-notation
-     * @param value The value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void setFieldValue(String path, Object value) {
-        setFieldValue(path, value, false);
-    }
-
-    private void setFieldValue(String path, Object value, boolean append) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-
-        value = deepCopy(value);
-
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + path + "]");
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                if (map.containsKey(pathElement)) {
-                    context = map.get(pathElement);
-                } else {
-                    HashMap<Object, Object> newMap = new HashMap<>();
-                    map.put(pathElement, newMap);
-                    context = newMap;
-                }
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                int index;
-                try {
-                    index = Integer.parseInt(pathElement);
-                } catch (NumberFormatException e) {
-                    throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-                }
-                if (index < 0 || index >= list.size()) {
-                    throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-                }
-                context = list.get(index);
-            } else {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context == null) {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with null parent as part of path [" + path + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (append) {
-                if (map.containsKey(leafKey)) {
-                    Object object = map.get(leafKey);
-                    if (object instanceof List) {
-                        @SuppressWarnings("unchecked")
-                        List<Object> list = (List<Object>) object;
-                        list.add(value);
-                        return;
-                    }
-                }
-            }
-            map.put(leafKey, value);
-        } else if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            list.set(index, value);
-        } else {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with parent object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-        }
-    }
-
-    private static <T> T cast(String path, Object object, Class<T> clazz) {
-        if (object == null) {
-            return null;
-        }
-        if (clazz.isInstance(object)) {
-            return clazz.cast(object);
-        }
-        throw new IllegalArgumentException("field [" + path + "] of type [" + object.getClass().getName() + "] cannot be cast to [" + clazz.getName() + "]");
-    }
-
-    /**
-     * one time operation that extracts the metadata fields from the ingest document and returns them.
-     * Metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.
-     */
-    public Map<MetaData, String> extractMetadata() {
-        Map<MetaData, String> metadataMap = new HashMap<>();
-        for (MetaData metaData : MetaData.values()) {
-            metadataMap.put(metaData, cast(metaData.getFieldName(), sourceAndMetadata.remove(metaData.getFieldName()), String.class));
-        }
-        return metadataMap;
-    }
-
-    /**
-     * Returns the available ingest metadata fields, by default only timestamp, but it is possible to set additional ones.
-     * Use only for reading values, modify them instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, String> getIngestMetadata() {
-        return this.ingestMetadata;
-    }
-
-    /**
-     * Returns the document including its metadata fields, unless {@link #extractMetadata()} has been called, in which case the
-     * metadata fields will not be present anymore. Should be used only for reading.
-     * Modify the document instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, Object> getSourceAndMetadata() {
-        return this.sourceAndMetadata;
-    }
-
-    static Object deepCopy(Object value) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<Object, Object> mapValue = (Map<Object, Object>) value;
-            Map<Object, Object> copy = new HashMap<>(mapValue.size());
-            for (Map.Entry<Object, Object> entry : mapValue.entrySet()) {
-                copy.put(entry.getKey(), deepCopy(entry.getValue()));
-            }
-            return copy;
-        } else if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> listValue = (List<Object>) value;
-            List<Object> copy = new ArrayList<>(listValue.size());
-            for (Object itemValue : listValue) {
-                copy.add(deepCopy(itemValue));
-            }
-            return copy;
-        } else if (value == null || value instanceof String || value instanceof Integer ||
-                value instanceof Long || value instanceof Float ||
-                value instanceof Double || value instanceof Boolean) {
-            return value;
-        } else {
-            throw new IllegalArgumentException("unexpected value type [" + value.getClass() + "]");
-        }
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == this) { return true; }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-
-        IngestDocument other = (IngestDocument) obj;
-        return Objects.equals(sourceAndMetadata, other.sourceAndMetadata) &&
-                Objects.equals(ingestMetadata, other.ingestMetadata);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(sourceAndMetadata, ingestMetadata);
-    }
-
-    @Override
-    public String toString() {
-        return "IngestDocument{" +
-                " sourceAndMetadata=" + sourceAndMetadata +
-                ", ingestMetadata=" + ingestMetadata +
-                '}';
-    }
-
-    public enum MetaData {
-        INDEX("_index"),
-        TYPE("_type"),
-        ID("_id"),
-        ROUTING("_routing"),
-        PARENT("_parent"),
-        TIMESTAMP("_timestamp"),
-        TTL("_ttl");
-
-        private final String fieldName;
-
-        MetaData(String fieldName) {
-            this.fieldName = fieldName;
-        }
-
-        public String getFieldName() {
-            return fieldName;
-        }
-
-        public static MetaData fromString(String value) {
-            switch (value) {
-                case "_index":
-                    return INDEX;
-                case "_type":
-                    return TYPE;
-                case "_id":
-                    return ID;
-                case "_routing":
-                    return ROUTING;
-                case "_parent":
-                    return PARENT;
-                case "_timestamp":
-                    return TIMESTAMP;
-                case "_ttl":
-                    return TTL;
-                default:
-                    throw new IllegalArgumentException("no valid metadata field name [" + value + "]");
-            }
-        }
-    }
-
-    private class FieldPath {
-        private final String[] pathElements;
-        private final Object initialContext;
-
-        private FieldPath(String path) {
-            if (Strings.isEmpty(path)) {
-                throw new IllegalArgumentException("path cannot be null nor empty");
-            }
-            String newPath;
-            if (path.startsWith("_ingest.")) {
-                initialContext = ingestMetadata;
-                newPath = path.substring(8, path.length());
-            } else {
-                initialContext = sourceAndMetadata;
-                if (path.startsWith("_source.")) {
-                    newPath = path.substring(8, path.length());
-                } else {
-                    newPath = path;
-                }
-            }
-            this.pathElements = Strings.splitStringToArray(newPath, '.');
-            if (pathElements.length == 0) {
-                throw new IllegalArgumentException("path [" + path + "] is not valid");
-            }
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/Pipeline.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/Pipeline.java
deleted file mode 100644
index 4fb26bc..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/Pipeline.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.io.IOException;
-import java.util.*;
-
-/**
- * A pipeline is a list of {@link Processor} instances grouped under a unique id.
- */
-public final class Pipeline {
-
-    private final String id;
-    private final String description;
-    private final List<Processor> processors;
-
-    public Pipeline(String id, String description, List<Processor> processors) {
-        this.id = id;
-        this.description = description;
-        this.processors = processors;
-    }
-
-    /**
-     * Modifies the data of a document to be indexed based on the processor this pipeline holds
-     */
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        for (Processor processor : processors) {
-            processor.execute(ingestDocument);
-        }
-    }
-
-    /**
-     * The unique id of this pipeline
-     */
-    public String getId() {
-        return id;
-    }
-
-    /**
-     * An optional description of what this pipeline is doing to the data gets processed by this pipeline.
-     */
-    public String getDescription() {
-        return description;
-    }
-
-    /**
-     * Unmodifiable list containing each processor that operates on the data.
-     */
-    public List<Processor> getProcessors() {
-        return processors;
-    }
-
-    public final static class Factory {
-
-        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws Exception {
-            String description = ConfigurationUtils.readOptionalStringProperty(config, "description");
-            List<Processor> processors = new ArrayList<>();
-            @SuppressWarnings("unchecked")
-            List<Map<String, Map<String, Object>>> processorConfigs = (List<Map<String, Map<String, Object>>>) config.get("processors");
-            if (processorConfigs != null ) {
-                for (Map<String, Map<String, Object>> processor : processorConfigs) {
-                    for (Map.Entry<String, Map<String, Object>> entry : processor.entrySet()) {
-                        Processor.Factory factory = processorRegistry.get(entry.getKey());
-                        if (factory != null) {
-                            Map<String, Object> processorConfig = entry.getValue();
-                            processors.add(factory.create(processorConfig));
-                            if (processorConfig.isEmpty() == false) {
-                                throw new IllegalArgumentException("processor [" + entry.getKey() + "] doesn't support one or more provided configuration parameters " + Arrays.toString(processorConfig.keySet().toArray()));
-                            }
-                        } else {
-                            throw new IllegalArgumentException("No processor type exist with name [" + entry.getKey() + "]");
-                        }
-                    }
-                }
-            }
-            return new Pipeline(id, description, Collections.unmodifiableList(processors));
-        }
-
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
deleted file mode 100644
index 475ace1..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.IngestDocument;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Base class for processors that manipulate strings and require a single "fields" array config value, which
- * holds a list of field names in string format.
- */
-public abstract class AbstractStringProcessor implements Processor {
-
-    private final String field;
-
-    protected AbstractStringProcessor(String field) {
-        this.field = field;
-    }
-
-    public String getField() {
-        return field;
-    }
-
-    @Override
-    public final void execute(IngestDocument document) {
-        String val = document.getFieldValue(field, String.class);
-        if (val == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot process it.");
-        }
-        document.setFieldValue(field, process(val));
-    }
-
-    protected abstract String process(String value);
-
-    public static abstract class Factory<T extends AbstractStringProcessor> implements Processor.Factory<T> {
-        @Override
-        public T create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return newProcessor(field);
-        }
-
-        protected abstract T newProcessor(String field);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/ConfigurationUtils.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/ConfigurationUtils.java
deleted file mode 100644
index 7ba737e..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/ConfigurationUtils.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.List;
-import java.util.Map;
-
-public final class ConfigurationUtils {
-
-    private ConfigurationUtils() {
-    }
-
-    /**
-     * Returns and removes the specified optional property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     */
-    public static String readOptionalStringProperty(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        return readString(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName) {
-        return readStringProperty(configuration, propertyName, null);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     * If the property is missing and no default value has been specified a {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName, String defaultValue) {
-        Object value = configuration.remove(propertyName);
-        if (value == null && defaultValue != null) {
-            return defaultValue;
-        } else if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return readString(propertyName, value);
-    }
-
-    private static String readString(String propertyName, Object value) {
-        if (value == null) {
-            return null;
-        }
-        if (value instanceof String) {
-            return (String) value;
-        }
-        throw new IllegalArgumentException("property [" + propertyName + "] isn't a string, but of type [" + value.getClass().getName() + "]");
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> List<T> readOptionalList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-        return readList(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> List<T> readList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readList(propertyName, value);
-    }
-
-    private static <T> List<T> readList(String propertyName, Object value) {
-        if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<T> stringList = (List<T>) value;
-            return stringList;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a list, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> Map<String, T> readMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> Map<String, T> readOptionalMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    private static <T> Map<String, T> readMap(String propertyName, Object value) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, T> map = (Map<String, T>) value;
-            return map;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a map, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property as an {@link Object} from the specified configuration map.
-     */
-    public static Object readObject(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return value;
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/Processor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/Processor.java
deleted file mode 100644
index d9c788b..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/Processor.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.IngestDocument;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Map;
-
-/**
- * A processor implementation may modify the data belonging to a document.
- * Whether changes are made and what exactly is modified is up to the implementation.
- */
-public interface Processor {
-
-    /**
-     * Introspect and potentially modify the incoming data.
-     */
-    void execute(IngestDocument ingestDocument) throws Exception;
-
-    /**
-     * Gets the type of a processor
-     */
-    String getType();
-
-    /**
-     * A factory that knows how to construct a processor based on a map of maps.
-     */
-    interface Factory<P extends Processor> {
-
-        /**
-         * Creates a processor based on the specified map of maps config.
-         *
-         * Implementations are responsible for removing the used keys, so that after creating a pipeline ingest can
-         * verify if all configurations settings have been used.
-         */
-        P create(Map<String, Object> config) throws Exception;
-
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/convert/ConvertProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/convert/ConvertProcessor.java
deleted file mode 100644
index 0944305..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/convert/ConvertProcessor.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.convert;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.*;
-
-/**
- * Processor that converts fields content to a different type. Supported types are: integer, float, boolean and string.
- * Throws exception if the field is not there or the conversion fails.
- */
-public class ConvertProcessor implements Processor {
-
-    enum Type {
-        INTEGER {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Integer.parseInt(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to integer", e);
-                }
-
-            }
-        }, FLOAT {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Float.parseFloat(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to float", e);
-                }
-            }
-        }, BOOLEAN {
-            @Override
-            public Object convert(Object value) {
-                if (value.toString().equalsIgnoreCase("true")) {
-                    return true;
-                } else if (value.toString().equalsIgnoreCase("false")) {
-                    return false;
-                } else {
-                    throw new IllegalArgumentException("[" + value + "] is not a boolean value, cannot convert to boolean");
-                }
-            }
-        }, STRING {
-            @Override
-            public Object convert(Object value) {
-                return value.toString();
-            }
-        };
-
-        @Override
-        public final String toString() {
-            return name().toLowerCase(Locale.ROOT);
-        }
-
-        public abstract Object convert(Object value);
-
-        public static Type fromString(String type) {
-            try {
-                return Type.valueOf(type.toUpperCase(Locale.ROOT));
-            } catch(IllegalArgumentException e) {
-                throw new IllegalArgumentException("type [" + type + "] not supported, cannot convert field.", e);
-            }
-        }
-    }
-
-    public static final String TYPE = "convert";
-
-    private final String field;
-    private final Type convertType;
-
-    ConvertProcessor(String field, Type convertType) {
-        this.field = field;
-        this.convertType = convertType;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Type getConvertType() {
-        return convertType;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        Object oldValue = document.getFieldValue(field, Object.class);
-        Object newValue;
-        if (oldValue == null) {
-            throw new IllegalArgumentException("Field [" + field + "] is null, cannot be converted to type [" + convertType + "]");
-        }
-
-        if (oldValue instanceof List) {
-            List<?> list = (List<?>) oldValue;
-            List<Object> newList = new ArrayList<>();
-            for (Object value : list) {
-                newList.add(convertType.convert(value));
-            }
-            newValue = newList;
-        } else {
-            newValue = convertType.convert(oldValue);
-        }
-        document.setFieldValue(field, newValue);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<ConvertProcessor> {
-        @Override
-        public ConvertProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Type convertType = Type.fromString(ConfigurationUtils.readStringProperty(config, "type"));
-            return new ConvertProcessor(field, convertType);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateFormat.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateFormat.java
deleted file mode 100644
index 224d43f..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateFormat.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.date;
-
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.DateTimeFormat;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.Locale;
-import java.util.Optional;
-import java.util.function.Function;
-
-enum DateFormat {
-    Iso8601 {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return ISODateTimeFormat.dateTimeParser().withZone(timezone)::parseDateTime;
-        }
-    },
-    Unix {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime((long)(Float.parseFloat(date) * 1000), timezone);
-        }
-    },
-    UnixMs {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime(Long.parseLong(date), timezone);
-        }
-
-        @Override
-        public String toString() {
-            return "UNIX_MS";
-        }
-    },
-    Tai64n {
-        @Override
-        Function<String, DateTime> getFunction(DateTimeZone timezone) {
-            return (date) -> new DateTime(parseMillis(date), timezone);
-        }
-
-        private long parseMillis(String date) {
-            if (date.startsWith("@")) {
-                date = date.substring(1);
-            }
-            long base = Long.parseLong(date.substring(1, 16), 16);
-            // 1356138046000
-            long rest = Long.parseLong(date.substring(16, 24), 16);
-            return ((base * 1000) - 10000) + (rest/1000000);
-        }
-    };
-
-    abstract Function<String, DateTime> getFunction(DateTimeZone timezone);
-
-    static Optional<DateFormat> fromString(String format) {
-        switch (format) {
-            case "ISO8601":
-                return Optional.of(Iso8601);
-            case "UNIX":
-                return Optional.of(Unix);
-            case "UNIX_MS":
-                return Optional.of(UnixMs);
-            case "TAI64N":
-                return Optional.of(Tai64n);
-            default:
-                return Optional.empty();
-        }
-    }
-
-    static Function<String, DateTime> getJodaFunction(String matchFormat, DateTimeZone timezone, Locale locale) {
-        return DateTimeFormat.forPattern(matchFormat)
-                .withDefaultYear((new DateTime(DateTimeZone.UTC)).getYear())
-                .withZone(timezone).withLocale(locale)::parseDateTime;
-    }
-
-    @Override
-    public String toString() {
-        return name().toUpperCase(Locale.ROOT);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateProcessor.java
deleted file mode 100644
index 35fcc04..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/date/DateProcessor.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.date;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.*;
-import java.util.function.Function;
-
-public final class DateProcessor implements Processor {
-
-    public static final String TYPE = "date";
-    static final String DEFAULT_TARGET_FIELD = "@timestamp";
-
-    private final DateTimeZone timezone;
-    private final Locale locale;
-    private final String matchField;
-    private final String targetField;
-    private final List<String> matchFormats;
-    private final List<Function<String, DateTime>> dateParsers;
-
-    DateProcessor(DateTimeZone timezone, Locale locale, String matchField, List<String> matchFormats, String targetField) {
-        this.timezone = timezone;
-        this.locale = locale;
-        this.matchField = matchField;
-        this.targetField = targetField;
-        this.matchFormats = matchFormats;
-        this.dateParsers = new ArrayList<>();
-        for (String matchFormat : matchFormats) {
-            Optional<DateFormat> dateFormat = DateFormat.fromString(matchFormat);
-            Function<String, DateTime> stringToDateFunction;
-            if (dateFormat.isPresent()) {
-                stringToDateFunction = dateFormat.get().getFunction(timezone);
-            } else {
-                stringToDateFunction = DateFormat.getJodaFunction(matchFormat, timezone, locale);
-            }
-            dateParsers.add(stringToDateFunction);
-        }
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String value = ingestDocument.getFieldValue(matchField, String.class);
-        // TODO(talevy): handle custom timestamp fields
-
-        DateTime dateTime = null;
-        Exception lastException = null;
-        for (Function<String, DateTime> dateParser : dateParsers) {
-            try {
-                dateTime = dateParser.apply(value);
-            } catch(Exception e) {
-                //try the next parser and keep track of the last exception
-                lastException = e;
-            }
-        }
-
-        if (dateTime == null) {
-            throw new IllegalArgumentException("unable to parse date [" + value + "]", lastException);
-        }
-
-        ingestDocument.setFieldValue(targetField, ISODateTimeFormat.dateTime().print(dateTime));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    DateTimeZone getTimezone() {
-        return timezone;
-    }
-
-    Locale getLocale() {
-        return locale;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    List<String> getMatchFormats() {
-        return matchFormats;
-    }
-
-    public static class Factory implements Processor.Factory<DateProcessor> {
-
-        @SuppressWarnings("unchecked")
-        public DateProcessor create(Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "match_field");
-            String targetField = ConfigurationUtils.readStringProperty(config, "target_field", DEFAULT_TARGET_FIELD);
-            String timezoneString = ConfigurationUtils.readOptionalStringProperty(config, "timezone");
-            DateTimeZone timezone = timezoneString == null ? DateTimeZone.UTC : DateTimeZone.forID(timezoneString);
-            String localeString = ConfigurationUtils.readOptionalStringProperty(config, "locale");
-            Locale locale = localeString == null ? Locale.ENGLISH : Locale.forLanguageTag(localeString);
-            List<String> matchFormats = ConfigurationUtils.readList(config, "match_formats");
-            return new DateProcessor(timezone, locale, matchField, matchFormats, targetField);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessor.java
deleted file mode 100644
index 50ac93a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessor.java
+++ /dev/null
@@ -1,309 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import com.maxmind.geoip2.exception.AddressNotFoundException;
-import com.maxmind.geoip2.model.CityResponse;
-import com.maxmind.geoip2.model.CountryResponse;
-import com.maxmind.geoip2.record.*;
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.SpecialPermission;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.io.InputStream;
-import java.net.InetAddress;
-import java.net.UnknownHostException;
-import java.nio.file.*;
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.util.*;
-import java.util.stream.Stream;
-
-import static org.elasticsearch.ingest.processor.ConfigurationUtils.readOptionalList;
-import static org.elasticsearch.ingest.processor.ConfigurationUtils.readStringProperty;
-
-public final class GeoIpProcessor implements Processor {
-
-    public static final String TYPE = "geoip";
-
-    private final String sourceField;
-    private final String targetField;
-    private final DatabaseReader dbReader;
-    private final Set<Field> fields;
-
-    GeoIpProcessor(String sourceField, DatabaseReader dbReader, String targetField, Set<Field> fields) throws IOException {
-        this.sourceField = sourceField;
-        this.targetField = targetField;
-        this.dbReader = dbReader;
-        this.fields = fields;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String ip = ingestDocument.getFieldValue(sourceField, String.class);
-        final InetAddress ipAddress;
-        try {
-            ipAddress = InetAddress.getByName(ip);
-        } catch (UnknownHostException e) {
-            throw new RuntimeException(e);
-        }
-
-        Map<String, Object> geoData;
-        switch (dbReader.getMetadata().getDatabaseType()) {
-            case "GeoLite2-City":
-                try {
-                    geoData = retrieveCityGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            case "GeoLite2-Country":
-                try {
-                    geoData = retrieveCountryGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            default:
-                throw new IllegalStateException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]");
-        }
-        ingestDocument.setFieldValue(targetField, geoData);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getSourceField() {
-        return sourceField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    DatabaseReader getDbReader() {
-        return dbReader;
-    }
-
-    Set<Field> getFields() {
-        return fields;
-    }
-
-    private Map<String, Object> retrieveCityGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CityResponse response = AccessController.doPrivileged((PrivilegedAction<CityResponse>) () -> {
-            try {
-                return dbReader.city(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        City city = response.getCity();
-        Location location = response.getLocation();
-        Continent continent = response.getContinent();
-        Subdivision subdivision = response.getMostSpecificSubdivision();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-                case REGION_NAME:
-                    geoData.put("region_name", subdivision.getName());
-                    break;
-                case CITY_NAME:
-                    geoData.put("city_name", city.getName());
-                    break;
-                case TIMEZONE:
-                    geoData.put("timezone", location.getTimeZone());
-                    break;
-                case LATITUDE:
-                    geoData.put("latitude", location.getLatitude());
-                    break;
-                case LONGITUDE:
-                    geoData.put("longitude", location.getLongitude());
-                    break;
-                case LOCATION:
-                    if (location.getLatitude() != null && location.getLongitude() != null) {
-                        geoData.put("location", Arrays.asList(location.getLongitude(), location.getLatitude()));
-                    }
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    private Map<String, Object> retrieveCountryGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CountryResponse response = AccessController.doPrivileged((PrivilegedAction<CountryResponse>) () -> {
-            try {
-                return dbReader.country(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        Continent continent = response.getContinent();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    public static class Factory implements Processor.Factory<GeoIpProcessor>, Closeable {
-
-        static final Set<Field> DEFAULT_FIELDS = EnumSet.of(
-                Field.CONTINENT_NAME, Field.COUNTRY_ISO_CODE, Field.REGION_NAME, Field.CITY_NAME, Field.LOCATION
-        );
-
-        private final Map<String, DatabaseReader> databaseReaders;
-
-        public Factory(Path configDirectory) {
-            Path geoIpConfigDirectory = configDirectory.resolve("ingest").resolve("geoip");
-            if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) {
-                throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory  + "] containing databases doesn't exist");
-            }
-
-            try (Stream<Path> databaseFiles = Files.list(geoIpConfigDirectory)) {
-                Map<String, DatabaseReader> databaseReaders = new HashMap<>();
-                PathMatcher pathMatcher = geoIpConfigDirectory.getFileSystem().getPathMatcher("glob:**.mmdb");
-                // Use iterator instead of forEach otherwise IOException needs to be caught twice...
-                Iterator<Path> iterator = databaseFiles.iterator();
-                while (iterator.hasNext()) {
-                    Path databasePath = iterator.next();
-                    if (Files.isRegularFile(databasePath) && pathMatcher.matches(databasePath)) {
-                        try (InputStream inputStream = Files.newInputStream(databasePath, StandardOpenOption.READ)) {
-                            databaseReaders.put(databasePath.getFileName().toString(), new DatabaseReader.Builder(inputStream).build());
-                        }
-                    }
-                }
-                this.databaseReaders = Collections.unmodifiableMap(databaseReaders);
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public GeoIpProcessor create(Map<String, Object> config) throws Exception {
-            String ipField = readStringProperty(config, "source_field");
-            String targetField = readStringProperty(config, "target_field", "geoip");
-            String databaseFile = readStringProperty(config, "database_file", "GeoLite2-City.mmdb");
-            List<String> fieldNames = readOptionalList(config, "fields");
-
-            final Set<Field> fields;
-            if (fieldNames != null) {
-                fields = EnumSet.noneOf(Field.class);
-                for (String fieldName : fieldNames) {
-                    try {
-                        fields.add(Field.parse(fieldName));
-                    } catch (Exception e) {
-                        throw new IllegalArgumentException("illegal field option [" + fieldName +"]. valid values are [" + Arrays.toString(Field.values()) +"]", e);
-                    }
-                }
-            } else {
-                fields = DEFAULT_FIELDS;
-            }
-
-            DatabaseReader databaseReader = databaseReaders.get(databaseFile);
-            if (databaseReader == null) {
-                throw new IllegalArgumentException("database file [" + databaseFile + "] doesn't exist");
-            }
-            return new GeoIpProcessor(ipField, databaseReader, targetField, fields);
-        }
-
-        @Override
-        public void close() throws IOException {
-            IOUtils.close(databaseReaders.values());
-        }
-    }
-
-    // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code
-    // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow
-    // it with an unchecked exception.
-    private final static class AddressNotFoundRuntimeException extends RuntimeException {
-
-        public AddressNotFoundRuntimeException(Throwable cause) {
-            super(cause);
-        }
-    }
-
-    public enum Field {
-
-        IP,
-        COUNTRY_ISO_CODE,
-        COUNTRY_NAME,
-        CONTINENT_NAME,
-        REGION_NAME,
-        CITY_NAME,
-        TIMEZONE,
-        LATITUDE,
-        LONGITUDE,
-        LOCATION;
-
-        public static Field parse(String value) {
-            return valueOf(value.toUpperCase(Locale.ROOT));
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/Grok.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/Grok.java
deleted file mode 100644
index 94e5d22..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/Grok.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import java.lang.Object;
-import java.lang.String;
-import java.lang.StringIndexOutOfBoundsException;
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Locale;
-import java.util.Map;
-
-import org.jcodings.specific.UTF8Encoding;
-import org.joni.*;
-import org.joni.exception.ValueException;
-
-final class Grok {
-
-    private static final String NAME_GROUP = "name";
-    private static final String SUBNAME_GROUP = "subname";
-    private static final String PATTERN_GROUP = "pattern";
-    private static final String DEFINITION_GROUP = "definition";
-    private static final String GROK_PATTERN =
-            "%\\{" +
-            "(?<name>" +
-            "(?<pattern>[A-z0-9]+)" +
-            "(?::(?<subname>[A-z0-9_:.-]+))?" +
-            ")" +
-            "(?:=(?<definition>" +
-            "(?:" +
-            "(?:[^{}]+|\\.+)+" +
-            ")+" +
-            ")" +
-            ")?" + "\\}";
-    private static final Regex GROK_PATTERN_REGEX = new Regex(GROK_PATTERN.getBytes(StandardCharsets.UTF_8), 0, GROK_PATTERN.getBytes(StandardCharsets.UTF_8).length, Option.NONE, UTF8Encoding.INSTANCE, Syntax.DEFAULT);
-    private final Map<String, String> patternBank;
-    private final boolean namedCaptures;
-    private final Regex compiledExpression;
-    private final String expression;
-
-
-    public Grok(Map<String, String> patternBank, String grokPattern) {
-        this(patternBank, grokPattern, true);
-    }
-
-    @SuppressWarnings("unchecked")
-    public Grok(Map<String, String> patternBank, String grokPattern, boolean namedCaptures) {
-        this.patternBank = patternBank;
-        this.namedCaptures = namedCaptures;
-
-        this.expression = toRegex(grokPattern);
-        byte[] expressionBytes = expression.getBytes(StandardCharsets.UTF_8);
-        this.compiledExpression = new Regex(expressionBytes, 0, expressionBytes.length, Option.DEFAULT, UTF8Encoding.INSTANCE);
-    }
-
-
-    public String groupMatch(String name, Region region, String pattern) {
-        try {
-            int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region);
-            int begin = region.beg[number];
-            int end = region.end[number];
-            return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8);
-        } catch (StringIndexOutOfBoundsException e) {
-            return null;
-        } catch (ValueException e) {
-            return null;
-        }
-    }
-
-    /**
-     * converts a grok expression into a named regex expression
-     *
-     * @return named regex expression
-     */
-    public String toRegex(String grokPattern) {
-        byte[] grokPatternBytes = grokPattern.getBytes(StandardCharsets.UTF_8);
-        Matcher matcher = GROK_PATTERN_REGEX.matcher(grokPatternBytes);
-
-        int result = matcher.search(0, grokPatternBytes.length, Option.NONE);
-        if (result != -1) {
-            Region region = matcher.getEagerRegion();
-            String namedPatternRef = groupMatch(NAME_GROUP, region, grokPattern);
-            String subName = groupMatch(SUBNAME_GROUP, region, grokPattern);
-            // TODO(tal): Support definitions
-            String definition = groupMatch(DEFINITION_GROUP, region, grokPattern);
-            String patternName = groupMatch(PATTERN_GROUP, region, grokPattern);
-            String pattern = patternBank.get(patternName);
-
-            String grokPart;
-            if (namedCaptures && subName != null) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", namedPatternRef, pattern);
-            } else if (!namedCaptures) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", patternName + "_" + String.valueOf(result), pattern);
-            } else {
-                grokPart = String.format(Locale.US, "(?:%s)", pattern);
-            }
-
-            String start = new String(grokPatternBytes, 0, result, StandardCharsets.UTF_8);
-            String rest = new String(grokPatternBytes, region.end[0], grokPatternBytes.length - region.end[0], StandardCharsets.UTF_8);
-            return start + toRegex(grokPart + rest);
-        }
-
-        return grokPattern;
-    }
-
-    public boolean match(String text) {
-        Matcher matcher = compiledExpression.matcher(text.getBytes(StandardCharsets.UTF_8));
-        int result = matcher.search(0, text.length(), Option.DEFAULT);
-        return (result != -1);
-    }
-
-    public Map<String, Object> captures(String text) {
-        byte[] textAsBytes = text.getBytes(StandardCharsets.UTF_8);
-        Map<String, Object> fields = new HashMap<>();
-        Matcher matcher = compiledExpression.matcher(textAsBytes);
-        int result = matcher.search(0, textAsBytes.length, Option.DEFAULT);
-        if (result != -1) {
-            Region region = matcher.getEagerRegion();
-            for (Iterator<NameEntry> entry = compiledExpression.namedBackrefIterator(); entry.hasNext();) {
-                NameEntry e = entry.next();
-                int number = e.getBackRefs()[0];
-
-                String groupName = new String(e.name, e.nameP, e.nameEnd - e.nameP, StandardCharsets.UTF_8);
-                String matchValue = null;
-                if (region.beg[number] >= 0) {
-                    matchValue = new String(textAsBytes, region.beg[number], region.end[number] - region.beg[number], StandardCharsets.UTF_8);
-                }
-                GrokMatchGroup match = new GrokMatchGroup(groupName, matchValue);
-                fields.put(match.getName(), match.getValue());
-            }
-        } else {
-            return null;
-        }
-
-        return fields;
-    }
-}
-
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokMatchGroup.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokMatchGroup.java
deleted file mode 100644
index 86ddd69..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokMatchGroup.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import java.lang.Float;
-import java.lang.Integer;
-import java.lang.Object;
-import java.lang.String;
-
-final class GrokMatchGroup {
-    private static final String DEFAULT_TYPE = "string";
-    private final String patternName;
-    private final String fieldName;
-    private final String type;
-    private final String groupValue;
-
-    public GrokMatchGroup(String groupName, String groupValue) {
-        String[] parts = groupName.split(":");
-        patternName = parts[0];
-        if (parts.length >= 2) {
-            fieldName = parts[1];
-        } else {
-            fieldName = null;
-        }
-
-        if (parts.length == 3) {
-            type = parts[2];
-        } else {
-            type = DEFAULT_TYPE;
-        }
-        this.groupValue = groupValue;
-    }
-
-    public String getName() {
-        return (fieldName == null) ? patternName : fieldName;
-    }
-
-    public Object getValue() {
-        if (groupValue == null) { return null; }
-
-        switch(type) {
-            case "int":
-                return Integer.parseInt(groupValue);
-            case "float":
-                return Float.parseFloat(groupValue);
-            default:
-                return groupValue;
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokProcessor.java
deleted file mode 100644
index 562a86a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/grok/GrokProcessor.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.DirectoryStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardOpenOption;
-import java.util.HashMap;
-import java.util.Map;
-
-public final class GrokProcessor implements Processor {
-
-    public static final String TYPE = "grok";
-
-    private final String matchField;
-    private final Grok grok;
-
-    public GrokProcessor(Grok grok, String matchField) {
-        this.matchField = matchField;
-        this.grok = grok;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        String fieldValue = ingestDocument.getFieldValue(matchField, String.class);
-        Map<String, Object> matches = grok.captures(fieldValue);
-        if (matches != null) {
-            matches.forEach((k, v) -> ingestDocument.setFieldValue(k, v));
-        } else {
-            throw new IllegalArgumentException("Grok expression does not match field value: [" + fieldValue + "]");
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    Grok getGrok() {
-        return grok;
-    }
-
-    public static class Factory implements Processor.Factory<GrokProcessor> {
-
-        private final Path grokConfigDirectory;
-
-        public Factory(Path configDirectory) {
-            this.grokConfigDirectory = configDirectory.resolve("ingest").resolve("grok");
-        }
-
-        static void loadBankFromStream(Map<String, String> patternBank, InputStream inputStream) throws IOException {
-            String line;
-            BufferedReader br = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));
-            while ((line = br.readLine()) != null) {
-                String trimmedLine = line.replaceAll("^\\s+", "");
-                if (trimmedLine.startsWith("#") || trimmedLine.length() == 0) {
-                    continue;
-                }
-
-                String[] parts = trimmedLine.split("\\s+", 2);
-                if (parts.length == 2) {
-                    patternBank.put(parts[0], parts[1]);
-                }
-            }
-        }
-
-        public GrokProcessor create(Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "field");
-            String matchPattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            Map<String, String> customPatternBank = ConfigurationUtils.readOptionalMap(config, "pattern_definitions");
-
-            Map<String, String> patternBank = new HashMap<>();
-
-            Path patternsDirectory = grokConfigDirectory.resolve("patterns");
-            try (DirectoryStream<Path> stream = Files.newDirectoryStream(patternsDirectory)) {
-                for (Path patternFilePath : stream) {
-                    if (Files.isRegularFile(patternFilePath)) {
-                        try(InputStream is = Files.newInputStream(patternFilePath, StandardOpenOption.READ)) {
-                            loadBankFromStream(patternBank, is);
-                        }
-                    }
-                }
-            }
-
-            if (customPatternBank != null) {
-                patternBank.putAll(customPatternBank);
-            }
-
-            Grok grok = new Grok(patternBank, matchPattern);
-            return new GrokProcessor(grok, matchField);
-        }
-
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/gsub/GsubProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/gsub/GsubProcessor.java
deleted file mode 100644
index e15f99d..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/gsub/GsubProcessor.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.gsub;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Map;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-/**
- * Processor that allows to search for patterns in field content and replace them with corresponding string replacement.
- * Support fields of string type only, throws exception if a field is of a different type.
- */
-public class GsubProcessor implements Processor {
-
-    public static final String TYPE = "gsub";
-
-    private final String field;
-    private final Pattern pattern;
-    private final String replacement;
-
-    GsubProcessor(String field, Pattern pattern, String replacement) {
-        this.field = field;
-        this.pattern = pattern;
-        this.replacement = replacement;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Pattern getPattern() {
-        return pattern;
-    }
-
-    String getReplacement() {
-        return replacement;
-    }
-
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot match pattern.");
-        }
-        Matcher matcher = pattern.matcher(oldVal);
-        String newVal = matcher.replaceAll(replacement);
-        document.setFieldValue(field, newVal);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<GsubProcessor> {
-        @Override
-        public GsubProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String pattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            String replacement = ConfigurationUtils.readStringProperty(config, "replacement");
-            Pattern searchPattern = Pattern.compile(pattern);
-            return new GsubProcessor(field, searchPattern, replacement);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/join/JoinProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/join/JoinProcessor.java
deleted file mode 100644
index 85be231..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/join/JoinProcessor.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.join;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Collectors;
-
-/**
- * Processor that joins the different items of an array into a single string value using a separator between each item.
- * Throws exception is the specified field is not an array.
- */
-public class JoinProcessor implements Processor {
-
-    public static final String TYPE = "join";
-
-    private final String field;
-    private final String separator;
-
-    JoinProcessor(String field, String separator) {
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        List<?> list = document.getFieldValue(field, List.class);
-        if (list == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot join.");
-        }
-        String joined = list.stream()
-                .map(Object::toString)
-                .collect(Collectors.joining(separator));
-        document.setFieldValue(field, joined);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<JoinProcessor> {
-        @Override
-        public JoinProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String separator = ConfigurationUtils.readStringProperty(config, "separator");
-            return new JoinProcessor(field, separator);
-        }
-    }
-}
-
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessor.java
deleted file mode 100644
index 6bff622..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessor.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.lowercase;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-
-import java.util.Collection;
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to lowercase.
- * Throws exception is the field is not of type string.
- */
-
-public class LowercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "lowercase";
-
-    LowercaseProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toLowerCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<LowercaseProcessor> {
-        @Override
-        protected LowercaseProcessor newProcessor(String field) {
-            return new LowercaseProcessor(field);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessor.java
deleted file mode 100644
index 4de13f5..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessor.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.elasticsearch.ingest.processor.meta;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheFactory;
-import org.elasticsearch.common.io.FastStringReader;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.IngestDocument.MetaData;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.io.StringWriter;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-
-//TODO this processor needs to be removed, as the set processor allows now to set any field, including metadata ones.
-//The only reason for it to be still here is that it supports templating, we will remove once any processor supports templating.
-public final class MetaDataProcessor implements Processor {
-
-    public final static String TYPE = "meta";
-
-    private final Map<MetaData, Mustache> templates;
-
-    public MetaDataProcessor(Map<MetaData, Mustache> templates) {
-        this.templates = templates;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        Map<String, Object> model = ingestDocument.getSourceAndMetadata();
-        for (Map.Entry<MetaData, Mustache> entry : templates.entrySet()) {
-            StringWriter writer = new StringWriter();
-            entry.getValue().execute(writer, model);
-            ingestDocument.setFieldValue(entry.getKey().getFieldName(), writer.toString());
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    Map<MetaData, Mustache> getTemplates() {
-        return templates;
-    }
-
-    public final static class Factory implements Processor.Factory<MetaDataProcessor> {
-
-        private final MustacheFactory mustacheFactory = new DefaultMustacheFactory();
-
-        @Override
-        public MetaDataProcessor create(Map<String, Object> config) throws Exception {
-            Map<MetaData, Mustache> templates = new HashMap<>();
-            Iterator<Map.Entry<String, Object>> iterator = config.entrySet().iterator();
-            while (iterator.hasNext()) {
-                Map.Entry<String, Object> entry = iterator.next();
-                MetaData metaData = MetaData.fromString(entry.getKey());
-                Mustache mustache = mustacheFactory.compile(new FastStringReader(entry.getValue().toString()), "");
-                templates.put(metaData, mustache);
-                iterator.remove();
-            }
-
-            if (templates.isEmpty()) {
-                throw new IllegalArgumentException("no meta fields specified");
-            }
-
-            return new MetaDataProcessor(Collections.unmodifiableMap(templates));
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/remove/RemoveProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/remove/RemoveProcessor.java
deleted file mode 100644
index 80cd017..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/remove/RemoveProcessor.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.remove;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that removes existing fields. Nothing happens if the field is not present.
- */
-public class RemoveProcessor implements Processor {
-
-    public static final String TYPE = "remove";
-
-    private final String field;
-
-    RemoveProcessor(String field) {
-        this.field = field;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.removeField(field);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<RemoveProcessor> {
-        @Override
-        public RemoveProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new RemoveProcessor(field);
-        }
-    }
-}
-
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/rename/RenameProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/rename/RenameProcessor.java
deleted file mode 100644
index 7e894e5..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/rename/RenameProcessor.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.rename;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * Processor that allows to rename existing fields. Will throw exception if the field is not present.
- */
-public class RenameProcessor implements Processor {
-
-    public static final String TYPE = "rename";
-
-    private final String oldFieldName;
-    private final String newFieldName;
-
-    RenameProcessor(String oldFieldName, String newFieldName) {
-        this.oldFieldName = oldFieldName;
-        this.newFieldName = newFieldName;
-    }
-
-    String getOldFieldName() {
-        return oldFieldName;
-    }
-
-    String getNewFieldName() {
-        return newFieldName;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        if (document.hasField(oldFieldName) == false) {
-            throw new IllegalArgumentException("field [" + oldFieldName + "] doesn't exist");
-        }
-        if (document.hasField(newFieldName)) {
-            throw new IllegalArgumentException("field [" + newFieldName + "] already exists");
-        }
-
-        Object oldValue = document.getFieldValue(oldFieldName, Object.class);
-        document.setFieldValue(newFieldName, oldValue);
-        try {
-            document.removeField(oldFieldName);
-        } catch (Exception e) {
-            //remove the new field if the removal of the old one failed
-            document.removeField(newFieldName);
-            throw e;
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<RenameProcessor> {
-        @Override
-        public RenameProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String newField = ConfigurationUtils.readStringProperty(config, "to");
-            return new RenameProcessor(field, newField);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/set/SetProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/set/SetProcessor.java
deleted file mode 100644
index f14be2a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/set/SetProcessor.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.set;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * Processor that adds new fields with their corresponding values. If the field is already present, its value
- * will be replaced with the provided one.
- */
-public class SetProcessor implements Processor {
-
-    public static final String TYPE = "set";
-
-    private final String field;
-    private final Object value;
-
-    SetProcessor(String field, Object value) {
-        this.field = field;
-        this.value = value;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Object getValue() {
-        return value;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.setFieldValue(field, value);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static final class Factory implements Processor.Factory<SetProcessor> {
-        @Override
-        public SetProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Object value = ConfigurationUtils.readObject(config, "value");
-            return new SetProcessor(field, value);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/split/SplitProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/split/SplitProcessor.java
deleted file mode 100644
index 1895fc1..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/split/SplitProcessor.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.split;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.ingest.processor.Processor;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * Processor that splits fields content into different items based on the occurrence of a specified separator.
- * New field value will be an array containing all of the different extracted items.
- * Throws exception if the field is null or a type other than string.
- */
-public class SplitProcessor implements Processor {
-
-    public static final String TYPE = "split";
-
-    private final String field;
-    private final String separator;
-
-    SplitProcessor(String field, String separator) {
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot split.");
-        }
-        document.setFieldValue(field, Arrays.asList(oldVal.split(separator)));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory implements Processor.Factory<SplitProcessor> {
-        @Override
-        public SplitProcessor create(Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new SplitProcessor(field, ConfigurationUtils.readStringProperty(config, "separator"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/trim/TrimProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/trim/TrimProcessor.java
deleted file mode 100644
index 94b617b..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/trim/TrimProcessor.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.trim;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-
-import java.util.Collection;
-
-/**
- * Processor that trims the content of string fields.
- * Throws exception is the field is not of type string.
- */
-public class TrimProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "trim";
-
-    TrimProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.trim();
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<TrimProcessor> {
-        @Override
-        protected TrimProcessor newProcessor(String field) {
-            return new TrimProcessor(field);
-        }
-    }
-}
-
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessor.java b/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessor.java
deleted file mode 100644
index fe0d029..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessor.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.uppercase;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-
-import java.util.Collection;
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to uppercase.
- * Throws exception is the field is not of type string.
- */
-public class UppercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "uppercase";
-
-    UppercaseProcessor(String field) {
-        super(field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toUpperCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<UppercaseProcessor> {
-        @Override
-        protected UppercaseProcessor newProcessor(String field) {
-            return new UppercaseProcessor(field);
-        }
-    }
-}
-
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestModule.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestModule.java
deleted file mode 100644
index 5c6961b..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestModule.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.multibindings.MapBinder;
-import org.elasticsearch.ingest.processor.set.SetProcessor;
-import org.elasticsearch.ingest.processor.convert.ConvertProcessor;
-import org.elasticsearch.ingest.processor.date.DateProcessor;
-import org.elasticsearch.ingest.processor.geoip.GeoIpProcessor;
-import org.elasticsearch.ingest.processor.grok.GrokProcessor;
-import org.elasticsearch.ingest.processor.gsub.GsubProcessor;
-import org.elasticsearch.ingest.processor.join.JoinProcessor;
-import org.elasticsearch.ingest.processor.lowercase.LowercaseProcessor;
-import org.elasticsearch.ingest.processor.remove.RemoveProcessor;
-import org.elasticsearch.ingest.processor.rename.RenameProcessor;
-import org.elasticsearch.ingest.processor.split.SplitProcessor;
-import org.elasticsearch.ingest.processor.trim.TrimProcessor;
-import org.elasticsearch.ingest.processor.uppercase.UppercaseProcessor;
-import org.elasticsearch.ingest.processor.meta.MetaDataProcessor;
-import org.elasticsearch.plugin.ingest.rest.IngestRestFilter;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulateExecutionService;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.plugin.ingest.PipelineStore.ProcessorFactoryProvider;
-
-public class IngestModule extends AbstractModule {
-
-    private final Map<String, ProcessorFactoryProvider> processorFactoryProviders = new HashMap<>();
-
-    @Override
-    protected void configure() {
-        binder().bind(IngestRestFilter.class).asEagerSingleton();
-        binder().bind(PipelineExecutionService.class).asEagerSingleton();
-        binder().bind(PipelineStore.class).asEagerSingleton();
-        binder().bind(SimulateExecutionService.class).asEagerSingleton();
-
-        addProcessor(GeoIpProcessor.TYPE, environment -> new GeoIpProcessor.Factory(environment.configFile()));
-        addProcessor(GrokProcessor.TYPE, environment -> new GrokProcessor.Factory(environment.configFile()));
-        addProcessor(DateProcessor.TYPE, environment -> new DateProcessor.Factory());
-        addProcessor(SetProcessor.TYPE, environment -> new SetProcessor.Factory());
-        addProcessor(RenameProcessor.TYPE, environment -> new RenameProcessor.Factory());
-        addProcessor(RemoveProcessor.TYPE, environment -> new RemoveProcessor.Factory());
-        addProcessor(SplitProcessor.TYPE, environment -> new SplitProcessor.Factory());
-        addProcessor(JoinProcessor.TYPE, environment -> new JoinProcessor.Factory());
-        addProcessor(UppercaseProcessor.TYPE, environment -> new UppercaseProcessor.Factory());
-        addProcessor(LowercaseProcessor.TYPE, environment -> new LowercaseProcessor.Factory());
-        addProcessor(TrimProcessor.TYPE, environment -> new TrimProcessor.Factory());
-        addProcessor(ConvertProcessor.TYPE, environment -> new ConvertProcessor.Factory());
-        addProcessor(GsubProcessor.TYPE, environment -> new GsubProcessor.Factory());
-        addProcessor(MetaDataProcessor.TYPE, environment -> new MetaDataProcessor.Factory());
-
-        MapBinder<String, ProcessorFactoryProvider> mapBinder = MapBinder.newMapBinder(binder(), String.class, ProcessorFactoryProvider.class);
-        for (Map.Entry<String, ProcessorFactoryProvider> entry : processorFactoryProviders.entrySet()) {
-            mapBinder.addBinding(entry.getKey()).toInstance(entry.getValue());
-        }
-    }
-
-    /**
-     * Adds a processor factory under a specific type name.
-     */
-    public void addProcessor(String type, ProcessorFactoryProvider processorFactoryProvider) {
-        processorFactoryProviders.put(type, processorFactoryProvider);
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestPlugin.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestPlugin.java
deleted file mode 100644
index 5405459..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/IngestPlugin.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.action.ActionModule;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.transport.TransportClient;
-import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.rest.RestDeletePipelineAction;
-import org.elasticsearch.plugin.ingest.rest.RestGetPipelineAction;
-import org.elasticsearch.plugin.ingest.rest.RestPutPipelineAction;
-import org.elasticsearch.plugin.ingest.rest.RestSimulatePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.IngestActionFilter;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineTransportAction;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineTransportAction;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineTransportAction;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineTransportAction;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.rest.RestModule;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-
-public class IngestPlugin extends Plugin {
-
-    public static final String PIPELINE_ID_PARAM_CONTEXT_KEY = "__pipeline_id__";
-    public static final String PIPELINE_ID_PARAM = "pipeline_id";
-    public static final String PIPELINE_ALREADY_PROCESSED = "ingest_already_processed";
-    public static final String NAME = "ingest";
-
-    private final Settings nodeSettings;
-    private final boolean transportClient;
-
-    public IngestPlugin(Settings nodeSettings) {
-        this.nodeSettings = nodeSettings;
-        transportClient = TransportClient.CLIENT_TYPE.equals(nodeSettings.get(Client.CLIENT_TYPE_SETTING));
-    }
-
-    @Override
-    public String name() {
-        return NAME;
-    }
-
-    @Override
-    public String description() {
-        return "Plugin that allows to configure pipelines to preprocess documents before indexing";
-    }
-
-    @Override
-    public Collection<Module> nodeModules() {
-        if (transportClient) {
-            return Collections.emptyList();
-        } else {
-            return Collections.singletonList(new IngestModule());
-        }
-    }
-
-    @Override
-    public Collection<Class<? extends LifecycleComponent>> nodeServices() {
-        if (transportClient) {
-            return Collections.emptyList();
-        } else {
-            return Collections.singletonList(PipelineStore.class);
-        }
-    }
-
-    @Override
-    public Settings additionalSettings() {
-        return settingsBuilder()
-                .put(PipelineExecutionService.additionalSettings(nodeSettings))
-                .build();
-    }
-
-    public void onModule(ActionModule module) {
-        if (transportClient == false) {
-            module.registerFilter(IngestActionFilter.class);
-        }
-        module.registerAction(PutPipelineAction.INSTANCE, PutPipelineTransportAction.class);
-        module.registerAction(GetPipelineAction.INSTANCE, GetPipelineTransportAction.class);
-        module.registerAction(DeletePipelineAction.INSTANCE, DeletePipelineTransportAction.class);
-        module.registerAction(SimulatePipelineAction.INSTANCE, SimulatePipelineTransportAction.class);
-    }
-
-    public void onModule(RestModule restModule) {
-        restModule.addRestAction(RestPutPipelineAction.class);
-        restModule.addRestAction(RestGetPipelineAction.class);
-        restModule.addRestAction(RestDeletePipelineAction.class);
-        restModule.addRestAction(RestSimulatePipelineAction.class);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineDefinition.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineDefinition.java
deleted file mode 100644
index d78274c..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineDefinition.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.ingest.Pipeline;
-
-import java.io.IOException;
-
-public class PipelineDefinition implements Writeable<PipelineDefinition>, ToXContent {
-
-    private static final PipelineDefinition PROTOTYPE = new PipelineDefinition((String) null, -1, null);
-
-    public static PipelineDefinition readPipelineDefinitionFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    private final String id;
-    private final long version;
-    private final BytesReference source;
-
-    private final Pipeline pipeline;
-
-    PipelineDefinition(Pipeline pipeline, long version, BytesReference source) {
-        this.id = pipeline.getId();
-        this.version = version;
-        this.source = source;
-        this.pipeline = pipeline;
-    }
-
-    PipelineDefinition(String id, long version, BytesReference source) {
-        this.id = id;
-        this.version = version;
-        this.source = source;
-        this.pipeline = null;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public long getVersion() {
-        return version;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    Pipeline getPipeline() {
-        return pipeline;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        PipelineDefinition holder = (PipelineDefinition) o;
-        return source.equals(holder.source);
-    }
-
-    @Override
-    public int hashCode() {
-        return source.hashCode();
-    }
-
-    @Override
-    public PipelineDefinition readFrom(StreamInput in) throws IOException {
-        String id = in.readString();
-        long version = in.readLong();
-        BytesReference source = in.readBytesReference();
-        return new PipelineDefinition(id, version, source);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(id);
-        out.writeLong(version);
-        out.writeBytesReference(source);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(id);
-        XContentHelper.writeRawField("_source", source, builder, params);
-        builder.field("_version", version);
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineExecutionService.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineExecutionService.java
deleted file mode 100644
index 18a7910..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineExecutionService.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.concurrent.EsExecutors;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.Map;
-
-public class PipelineExecutionService {
-
-    static final String THREAD_POOL_NAME = IngestPlugin.NAME;
-
-    private final PipelineStore store;
-    private final ThreadPool threadPool;
-
-    @Inject
-    public PipelineExecutionService(PipelineStore store, ThreadPool threadPool) {
-        this.store = store;
-        this.threadPool = threadPool;
-    }
-
-    public void execute(IndexRequest indexRequest, String pipelineId, ActionListener<Void> listener) {
-        Pipeline pipeline = store.get(pipelineId);
-        if (pipeline == null) {
-            listener.onFailure(new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist"));
-            return;
-        }
-
-        threadPool.executor(THREAD_POOL_NAME).execute(() -> {
-            String index = indexRequest.index();
-            String type = indexRequest.type();
-            String id = indexRequest.id();
-            String routing = indexRequest.routing();
-            String parent = indexRequest.parent();
-            String timestamp = indexRequest.timestamp();
-            String ttl = indexRequest.ttl() == null ? null : indexRequest.ttl().toString();
-            Map<String, Object> sourceAsMap = indexRequest.sourceAsMap();
-            IngestDocument ingestDocument = new IngestDocument(index, type, id, routing, parent, timestamp, ttl, sourceAsMap);
-            try {
-                pipeline.execute(ingestDocument);
-                Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-                //it's fine to set all metadata fields all the time, as ingest document holds their starting values
-                //before ingestion, which might also get modified during ingestion.
-                indexRequest.index(metadataMap.get(IngestDocument.MetaData.INDEX));
-                indexRequest.type(metadataMap.get(IngestDocument.MetaData.TYPE));
-                indexRequest.id(metadataMap.get(IngestDocument.MetaData.ID));
-                indexRequest.routing(metadataMap.get(IngestDocument.MetaData.ROUTING));
-                indexRequest.parent(metadataMap.get(IngestDocument.MetaData.PARENT));
-                indexRequest.timestamp(metadataMap.get(IngestDocument.MetaData.TIMESTAMP));
-                indexRequest.ttl(metadataMap.get(IngestDocument.MetaData.TTL));
-                indexRequest.source(ingestDocument.getSourceAndMetadata());
-                listener.onResponse(null);
-            } catch (Throwable e) {
-                listener.onFailure(e);
-            }
-        });
-    }
-
-    public static Settings additionalSettings(Settings nodeSettings) {
-        Settings settings = nodeSettings.getAsSettings("threadpool." + THREAD_POOL_NAME);
-        if (!settings.names().isEmpty()) {
-            // the TP is already configured in the node settings
-            // no need for additional settings
-            return Settings.EMPTY;
-        }
-        int availableProcessors = EsExecutors.boundedNumberOfProcessors(nodeSettings);
-        return Settings.builder()
-                .put("threadpool." + THREAD_POOL_NAME + ".type", "fixed")
-                .put("threadpool." + THREAD_POOL_NAME + ".size", availableProcessors)
-                .put("threadpool." + THREAD_POOL_NAME + ".queue_size", 200)
-                .build();
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineStore.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineStore.java
deleted file mode 100644
index b3e30b5..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/PipelineStore.java
+++ /dev/null
@@ -1,299 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.common.SearchScrollIterator;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.component.LifecycleComponent;
-import org.elasticsearch.common.component.LifecycleListener;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.Provider;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.gateway.GatewayService;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineRequest;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineRequest;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.*;
-
-public class PipelineStore extends AbstractLifecycleComponent {
-
-    public final static String INDEX = ".ingest";
-    public final static String TYPE = "pipeline";
-
-    private final ThreadPool threadPool;
-    private final TimeValue scrollTimeout;
-    private final ClusterService clusterService;
-    private final Provider<Client> clientProvider;
-    private final TimeValue pipelineUpdateInterval;
-    private final Pipeline.Factory factory = new Pipeline.Factory();
-    private final Map<String, Processor.Factory> processorFactoryRegistry;
-
-    private volatile Client client;
-    private volatile Map<String, PipelineDefinition> pipelines = new HashMap<>();
-
-    @Inject
-    public PipelineStore(Settings settings, Provider<Client> clientProvider, ThreadPool threadPool, Environment environment, ClusterService clusterService, Map<String, ProcessorFactoryProvider> processorFactoryProviders) {
-        super(settings);
-        this.threadPool = threadPool;
-        this.clusterService = clusterService;
-        this.clientProvider = clientProvider;
-        this.scrollTimeout = settings.getAsTime("ingest.pipeline.store.scroll.timeout", TimeValue.timeValueSeconds(30));
-        this.pipelineUpdateInterval = settings.getAsTime("ingest.pipeline.store.update.interval", TimeValue.timeValueSeconds(1));
-        Map<String, Processor.Factory> processorFactories = new HashMap<>();
-        for (Map.Entry<String, ProcessorFactoryProvider> entry : processorFactoryProviders.entrySet()) {
-            Processor.Factory processorFactory = entry.getValue().get(environment);
-            processorFactories.put(entry.getKey(), processorFactory);
-        }
-        this.processorFactoryRegistry = Collections.unmodifiableMap(processorFactories);
-        clusterService.add(new PipelineStoreListener());
-    }
-
-    @Override
-    protected void doStart() {
-    }
-
-    @Override
-    protected void doStop() {
-    }
-
-    @Override
-    protected void doClose() {
-        // TODO: When org.elasticsearch.node.Node can close Closable instances we should remove this code
-        List<Closeable> closeables = new ArrayList<>();
-        for (Processor.Factory factory : processorFactoryRegistry.values()) {
-            if (factory instanceof Closeable) {
-                closeables.add((Closeable) factory);
-            }
-        }
-        try {
-            IOUtils.close(closeables);
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /**
-     * Deletes the pipeline specified by id in the request.
-     */
-    public void delete(DeletePipelineRequest request, ActionListener<DeleteResponse> listener) {
-        DeleteRequest deleteRequest = new DeleteRequest(request);
-        deleteRequest.index(PipelineStore.INDEX);
-        deleteRequest.type(PipelineStore.TYPE);
-        deleteRequest.id(request.id());
-        deleteRequest.refresh(true);
-        client().delete(deleteRequest, listener);
-    }
-
-    /**
-     * Stores the specified pipeline definition in the request.
-     *
-     * @throws IllegalArgumentException If the pipeline holds incorrect configuration
-     */
-    public void put(PutPipelineRequest request, ActionListener<IndexResponse> listener) throws IllegalArgumentException {
-        try {
-            // validates the pipeline and processor configuration:
-            Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.source(), false).v2();
-            constructPipeline(request.id(), pipelineConfig);
-        } catch (Exception e) {
-            throw new IllegalArgumentException("Invalid pipeline configuration", e);
-        }
-
-        IndexRequest indexRequest = new IndexRequest(request);
-        indexRequest.index(PipelineStore.INDEX);
-        indexRequest.type(PipelineStore.TYPE);
-        indexRequest.id(request.id());
-        indexRequest.source(request.source());
-        indexRequest.refresh(true);
-        client().index(indexRequest, listener);
-    }
-
-    /**
-     * Returns the pipeline by the specified id
-     */
-    public Pipeline get(String id) {
-        PipelineDefinition ref = pipelines.get(id);
-        if (ref != null) {
-            return ref.getPipeline();
-        } else {
-            return null;
-        }
-    }
-
-    public Map<String, Processor.Factory> getProcessorFactoryRegistry() {
-        return processorFactoryRegistry;
-    }
-
-    public List<PipelineDefinition> getReference(String... ids) {
-        List<PipelineDefinition> result = new ArrayList<>(ids.length);
-        for (String id : ids) {
-            if (Regex.isSimpleMatchPattern(id)) {
-                for (Map.Entry<String, PipelineDefinition> entry : pipelines.entrySet()) {
-                    if (Regex.simpleMatch(id, entry.getKey())) {
-                        result.add(entry.getValue());
-                    }
-                }
-            } else {
-                PipelineDefinition reference = pipelines.get(id);
-                if (reference != null) {
-                    result.add(reference);
-                }
-            }
-        }
-        return result;
-    }
-
-    Pipeline constructPipeline(String id, Map<String, Object> config) throws Exception {
-        return factory.create(id, config, processorFactoryRegistry);
-    }
-
-    synchronized void updatePipelines() throws Exception {
-        // note: this process isn't fast or smart, but the idea is that there will not be many pipelines,
-        // so for that reason the goal is to keep the update logic simple.
-
-        int changed = 0;
-        Map<String, PipelineDefinition> newPipelines = new HashMap<>(pipelines);
-        for (SearchHit hit : readAllPipelines()) {
-            String pipelineId = hit.getId();
-            BytesReference pipelineSource = hit.getSourceRef();
-            PipelineDefinition previous = newPipelines.get(pipelineId);
-            if (previous != null) {
-                if (previous.getSource().equals(pipelineSource)) {
-                    continue;
-                }
-            }
-
-            changed++;
-            Pipeline pipeline = constructPipeline(hit.getId(), hit.sourceAsMap());
-            newPipelines.put(pipelineId, new PipelineDefinition(pipeline, hit.getVersion(), pipelineSource));
-        }
-
-        int removed = 0;
-        for (String existingPipelineId : pipelines.keySet()) {
-            if (!existPipeline(existingPipelineId)) {
-                newPipelines.remove(existingPipelineId);
-                removed++;
-            }
-        }
-
-        if (changed != 0 || removed != 0) {
-            logger.debug("adding or updating [{}] pipelines and [{}] pipelines removed", changed, removed);
-            pipelines = newPipelines;
-        } else {
-            logger.debug("no pipelines changes detected");
-        }
-    }
-
-    void startUpdateWorker() {
-        threadPool.schedule(pipelineUpdateInterval, ThreadPool.Names.GENERIC, new Updater());
-    }
-
-    boolean existPipeline(String pipelineId) {
-        GetRequest request = new GetRequest(PipelineStore.INDEX, PipelineStore.TYPE, pipelineId);
-        GetResponse response = client().get(request).actionGet();
-        return response.isExists();
-    }
-
-    Iterable<SearchHit> readAllPipelines() {
-        // TODO: the search should be replaced with an ingest API when it is available
-        SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();
-        sourceBuilder.version(true);
-        sourceBuilder.sort("_doc", SortOrder.ASC);
-        SearchRequest searchRequest = new SearchRequest(PipelineStore.INDEX);
-        searchRequest.source(sourceBuilder);
-        searchRequest.indicesOptions(IndicesOptions.lenientExpandOpen());
-        return SearchScrollIterator.createIterator(client(), scrollTimeout, searchRequest);
-    }
-
-
-    private Client client() {
-        if (client == null) {
-            client = clientProvider.get();
-        }
-        return client;
-    }
-
-    /**
-     * The ingest framework (pipeline, processor and processor factory) can't rely on ES specific code. However some
-     * processors rely on reading files from the config directory. We can't add Environment as a constructor parameter,
-     * so we need some code that provides the physical location of the configuration directory to the processor factories
-     * that need this and this is what this processor factory provider does.
-     */
-    @FunctionalInterface
-    interface ProcessorFactoryProvider {
-
-        Processor.Factory get(Environment environment);
-
-    }
-
-    class Updater implements Runnable {
-
-        @Override
-        public void run() {
-            try {
-                updatePipelines();
-            } catch (Exception e) {
-                logger.error("pipeline store update failure", e);
-            } finally {
-                startUpdateWorker();
-            }
-        }
-
-    }
-
-    class PipelineStoreListener implements ClusterStateListener {
-
-        @Override
-        public void clusterChanged(ClusterChangedEvent event) {
-            if (event.state().blocks().hasGlobalBlock(GatewayService.STATE_NOT_RECOVERED_BLOCK) == false) {
-                startUpdateWorker();
-                clusterService.remove(this);
-            }
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/IngestRestFilter.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/IngestRestFilter.java
deleted file mode 100644
index fe7af03..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/IngestRestFilter.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.rest;
-
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.rest.*;
-
-import static org.elasticsearch.plugin.ingest.IngestPlugin.*;
-import static org.elasticsearch.plugin.ingest.IngestPlugin.PIPELINE_ID_PARAM_CONTEXT_KEY;
-
-public class IngestRestFilter extends RestFilter {
-
-    @Inject
-    public IngestRestFilter(RestController controller) {
-        controller.registerFilter(this);
-    }
-
-    @Override
-    public void process(RestRequest request, RestChannel channel, RestFilterChain filterChain) throws Exception {
-        if (request.hasParam(PIPELINE_ID_PARAM)) {
-            request.putInContext(PIPELINE_ID_PARAM_CONTEXT_KEY, request.param(PIPELINE_ID_PARAM));
-        }
-        filterChain.continueProcessing(request, channel);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestDeletePipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestDeletePipelineAction.java
deleted file mode 100644
index bf86453..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestDeletePipelineAction.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.rest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestDeletePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestDeletePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.DELETE, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        DeletePipelineRequest request = new DeletePipelineRequest();
-        request.id(restRequest.param("id"));
-        client.execute(DeletePipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestGetPipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestGetPipelineAction.java
deleted file mode 100644
index 6d44473..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestGetPipelineAction.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.rest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestGetPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestGetPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        GetPipelineRequest request = new GetPipelineRequest();
-        request.ids(Strings.splitStringByCommaToArray(restRequest.param("id")));
-        client.execute(GetPipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestPutPipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestPutPipelineAction.java
deleted file mode 100644
index 2fc5508..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestPutPipelineAction.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.rest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestPutPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestPutPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.PUT, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        PutPipelineRequest request = new PutPipelineRequest();
-        request.id(restRequest.param("id"));
-        if (restRequest.hasContent()) {
-            request.source(restRequest.content());
-        }
-        client.execute(PutPipelineAction.INSTANCE, request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestSimulatePipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestSimulatePipelineAction.java
deleted file mode 100644
index 0b86e35..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/rest/RestSimulatePipelineAction.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.rest;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineRequest;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestActions;
-import org.elasticsearch.rest.action.support.RestToXContentListener;
-
-public class RestSimulatePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestSimulatePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/_simulate", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        SimulatePipelineRequest request = new SimulatePipelineRequest();
-        request.setId(restRequest.param("id"));
-        request.setVerbose(restRequest.paramAsBoolean("verbose", false));
-
-        if (RestActions.hasBodyContent(restRequest)) {
-            request.setSource(RestActions.getRestContent(restRequest));
-        }
-
-        client.execute(SimulatePipelineAction.INSTANCE, request, new RestToXContentListener<>(channel));
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilter.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilter.java
deleted file mode 100644
index e552c76..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilter.java
+++ /dev/null
@@ -1,236 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilter;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.IngestPlugin;
-import org.elasticsearch.plugin.ingest.PipelineExecutionService;
-
-import java.util.*;
-
-public final class IngestActionFilter extends AbstractComponent implements ActionFilter {
-
-    private final PipelineExecutionService executionService;
-
-    @Inject
-    public IngestActionFilter(Settings settings, PipelineExecutionService executionService) {
-        super(settings);
-        this.executionService = executionService;
-    }
-
-    @Override
-    public void apply(String action, ActionRequest request, ActionListener listener, ActionFilterChain chain) {
-        String pipelineId = request.getFromContext(IngestPlugin.PIPELINE_ID_PARAM_CONTEXT_KEY);
-        if (pipelineId == null) {
-            pipelineId = request.getHeader(IngestPlugin.PIPELINE_ID_PARAM);
-            if (pipelineId == null) {
-                chain.proceed(action, request, listener);
-                return;
-            }
-        }
-
-        if (request instanceof IndexRequest) {
-            processIndexRequest(action, listener, chain, (IndexRequest) request, pipelineId);
-        } else if (request instanceof BulkRequest) {
-            BulkRequest bulkRequest = (BulkRequest) request;
-            @SuppressWarnings("unchecked")
-            ActionListener<BulkResponse> actionListener = (ActionListener<BulkResponse>) listener;
-            BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(bulkRequest);
-            processBulkIndexRequest(bulkRequestModifier, pipelineId, action, chain, actionListener);
-        } else {
-            chain.proceed(action, request, listener);
-        }
-    }
-
-    @Override
-    public void apply(String action, ActionResponse response, ActionListener listener, ActionFilterChain chain) {
-        chain.proceed(action, response, listener);
-    }
-
-    void processIndexRequest(String action, ActionListener listener, ActionFilterChain chain, IndexRequest indexRequest, String pipelineId) {
-        // The IndexRequest has the same type on the node that receives the request and the node that
-        // processes the primary action. This could lead to a pipeline being executed twice for the same
-        // index request, hence this check
-        if (indexRequest.hasHeader(IngestPlugin.PIPELINE_ALREADY_PROCESSED)) {
-            chain.proceed(action, indexRequest, listener);
-            return;
-        }
-        executionService.execute(indexRequest, pipelineId, new ActionListener<Void>() {
-            @Override
-            public void onResponse(Void aVoid) {
-                indexRequest.putHeader(IngestPlugin.PIPELINE_ALREADY_PROCESSED, true);
-                chain.proceed(action, indexRequest, listener);
-            }
-
-            @Override
-            public void onFailure(Throwable e) {
-                logger.error("failed to execute pipeline [{}]", e, pipelineId);
-                listener.onFailure(e);
-            }
-        });
-    }
-
-    void processBulkIndexRequest(BulkRequestModifier bulkRequestModifier, String pipelineId, String action, ActionFilterChain chain, ActionListener<BulkResponse> listener) {
-        if (!bulkRequestModifier.hasNext()) {
-            BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest();
-            ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(listener);
-            if (bulkRequest.requests().isEmpty()) {
-                // in this stage, the transport bulk action can't deal with a bulk request with no requests,
-                // so we stop and send a empty response back to the client.
-                // (this will happen if all preprocessing all items in the bulk failed)
-                actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0));
-            } else {
-                chain.proceed(action, bulkRequest, actionListener);
-            }
-            return;
-        }
-
-        ActionRequest actionRequest = bulkRequestModifier.next();
-        if (!(actionRequest instanceof IndexRequest)) {
-            processBulkIndexRequest(bulkRequestModifier, pipelineId, action, chain, listener);
-            return;
-        }
-
-        IndexRequest indexRequest = (IndexRequest) actionRequest;
-        executionService.execute(indexRequest, pipelineId, new ActionListener<Void>() {
-            @Override
-            public void onResponse(Void aVoid) {
-                processBulkIndexRequest(bulkRequestModifier, pipelineId, action, chain, listener);
-            }
-
-            @Override
-            public void onFailure(Throwable e) {
-                logger.debug("failed to execute pipeline [{}]", e, pipelineId);
-                bulkRequestModifier.markCurrentItemAsFailed(e);
-                processBulkIndexRequest(bulkRequestModifier, pipelineId, action, chain, listener);
-            }
-        });
-    }
-
-    @Override
-    public int order() {
-        return Integer.MAX_VALUE;
-    }
-
-    final static class BulkRequestModifier implements Iterator<ActionRequest> {
-
-        final BulkRequest bulkRequest;
-        final Set<Integer> failedSlots;
-        final List<BulkItemResponse> itemResponses;
-
-        int currentSlot = -1;
-        int[] originalSlots;
-
-        BulkRequestModifier(BulkRequest bulkRequest) {
-            this.bulkRequest = bulkRequest;
-            this.failedSlots = new HashSet<>();
-            this.itemResponses = new ArrayList<>(bulkRequest.requests().size());
-        }
-
-        @Override
-        public ActionRequest next() {
-            return bulkRequest.requests().get(++currentSlot);
-        }
-
-        @Override
-        public boolean hasNext() {
-            return (currentSlot + 1) < bulkRequest.requests().size();
-        }
-
-        BulkRequest getBulkRequest() {
-            if (itemResponses.isEmpty()) {
-                return bulkRequest;
-            } else {
-                BulkRequest modifiedBulkRequest = new BulkRequest(bulkRequest);
-                modifiedBulkRequest.refresh(bulkRequest.refresh());
-                modifiedBulkRequest.consistencyLevel(bulkRequest.consistencyLevel());
-                modifiedBulkRequest.timeout(bulkRequest.timeout());
-
-                int slot = 0;
-                originalSlots = new int[bulkRequest.requests().size() - failedSlots.size()];
-                for (int i = 0; i < bulkRequest.requests().size(); i++) {
-                    ActionRequest request = bulkRequest.requests().get(i);
-                    if (failedSlots.contains(i) == false) {
-                        modifiedBulkRequest.add(request);
-                        originalSlots[slot++] = i;
-                    }
-                }
-                return modifiedBulkRequest;
-            }
-        }
-
-        ActionListener<BulkResponse> wrapActionListenerIfNeeded(ActionListener<BulkResponse> actionListener) {
-            if (itemResponses.isEmpty()) {
-                return actionListener;
-            } else {
-                return new IngestBulkResponseListener(originalSlots, itemResponses, actionListener);
-            }
-        }
-
-        void markCurrentItemAsFailed(Throwable e) {
-            IndexRequest indexRequest = (IndexRequest) bulkRequest.requests().get(currentSlot);
-            // We hit a error during preprocessing a request, so we:
-            // 1) Remember the request item slot from the bulk, so that we're done processing all requests we know what failed
-            // 2) Add a bulk item failure for this request
-            // 3) Continue with the next request in the bulk.
-            failedSlots.add(currentSlot);
-            BulkItemResponse.Failure failure = new BulkItemResponse.Failure(indexRequest.index(), indexRequest.type(), indexRequest.id(), e);
-            itemResponses.add(new BulkItemResponse(currentSlot, indexRequest.opType().lowercase(), failure));
-        }
-
-    }
-
-    private final static class IngestBulkResponseListener implements ActionListener<BulkResponse> {
-
-        private final int[] originalSlots;
-        private final List<BulkItemResponse> itemResponses;
-        private final ActionListener<BulkResponse> actionListener;
-
-        IngestBulkResponseListener(int[] originalSlots, List<BulkItemResponse> itemResponses, ActionListener<BulkResponse> actionListener) {
-            this.itemResponses = itemResponses;
-            this.actionListener = actionListener;
-            this.originalSlots = originalSlots;
-        }
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            for (int i = 0; i < bulkItemResponses.getItems().length; i++) {
-                itemResponses.add(originalSlots[i], bulkItemResponses.getItems()[i]);
-            }
-            actionListener.onResponse(new BulkResponse(itemResponses.toArray(new BulkItemResponse[itemResponses.size()]), bulkItemResponses.getTookInMillis()));
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-            actionListener.onFailure(e);
-        }
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineAction.java
deleted file mode 100644
index c1fba7f..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.delete;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineAction extends Action<DeletePipelineRequest, DeleteResponse, DeletePipelineRequestBuilder> {
-
-    public static final DeletePipelineAction INSTANCE = new DeletePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/delete";
-
-    public DeletePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public DeletePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new DeletePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public DeleteResponse newResponse() {
-        return new DeleteResponse();
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequest.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequest.java
deleted file mode 100644
index 1b31d5f..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequest.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.delete;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class DeletePipelineRequest extends ActionRequest {
-
-    private String id;
-
-    public void id(String id) {
-        this.id = id;
-    }
-
-    public String id() {
-        return id;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (id == null) {
-            validationException = addValidationError("id is missing", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequestBuilder.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequestBuilder.java
deleted file mode 100644
index ee8089a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineRequestBuilder.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.delete;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineRequestBuilder extends ActionRequestBuilder<DeletePipelineRequest, DeleteResponse, DeletePipelineRequestBuilder> {
-
-    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action) {
-        super(client, action, new DeletePipelineRequest());
-    }
-
-    public DeletePipelineRequestBuilder setId(String id) {
-        request.id(id);
-        return this;
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineTransportAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineTransportAction.java
deleted file mode 100644
index 3b5e72c..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/delete/DeletePipelineTransportAction.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.delete;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class DeletePipelineTransportAction extends HandledTransportAction<DeletePipelineRequest, DeleteResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public DeletePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, PipelineStore pipelineStore) {
-        super(settings, DeletePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, DeletePipelineRequest::new);
-        this.pipelineStore = pipelineStore;
-    }
-
-    @Override
-    protected void doExecute(DeletePipelineRequest request, ActionListener<DeleteResponse> listener) {
-        pipelineStore.delete(request, listener);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineAction.java
deleted file mode 100644
index 0904a8a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.get;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineAction extends Action<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public static final GetPipelineAction INSTANCE = new GetPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/get";
-
-    public GetPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public GetPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new GetPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public GetPipelineResponse newResponse() {
-        return new GetPipelineResponse();
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequest.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequest.java
deleted file mode 100644
index 0ff673a..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequest.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.get;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class GetPipelineRequest extends ActionRequest {
-
-    private String[] ids;
-
-    public void ids(String... ids) {
-        this.ids = ids;
-    }
-
-    public String[] ids() {
-        return ids;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (ids == null || ids.length == 0) {
-            validationException = addValidationError("ids is missing", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        ids = in.readStringArray();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeStringArray(ids);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequestBuilder.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequestBuilder.java
deleted file mode 100644
index 4269b6c..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineRequestBuilder.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.get;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineRequestBuilder extends ActionRequestBuilder<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action) {
-        super(client, action, new GetPipelineRequest());
-    }
-
-    public GetPipelineRequestBuilder setIds(String... ids) {
-        request.ids(ids);
-        return this;
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineResponse.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineResponse.java
deleted file mode 100644
index 8d7bf9c..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineResponse.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.get;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.plugin.ingest.PipelineDefinition;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-public class GetPipelineResponse extends ActionResponse implements StatusToXContent {
-
-    private List<PipelineDefinition> pipelines;
-
-    public GetPipelineResponse() {
-    }
-
-    public GetPipelineResponse(List<PipelineDefinition> pipelines) {
-        this.pipelines = pipelines;
-    }
-
-    public List<PipelineDefinition> pipelines() {
-        return pipelines;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        int size = in.readVInt();
-        pipelines = new ArrayList<>(size);
-        for (int i = 0; i < size; i++) {
-            pipelines.add(PipelineDefinition.readPipelineDefinitionFrom(in));
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeVInt(pipelines.size());
-        for (PipelineDefinition pipeline : pipelines) {
-            pipeline.writeTo(out);
-        }
-    }
-
-    public boolean isFound() {
-        return !pipelines.isEmpty();
-    }
-
-    @Override
-    public RestStatus status() {
-        return isFound() ? RestStatus.OK : RestStatus.NOT_FOUND;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        for (PipelineDefinition definition : pipelines) {
-            definition.toXContent(builder, params);
-        }
-        return builder;
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineTransportAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineTransportAction.java
deleted file mode 100644
index ada112c..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/get/GetPipelineTransportAction.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.get;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.ingest.PipelineDefinition;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-public class GetPipelineTransportAction extends HandledTransportAction<GetPipelineRequest, GetPipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public GetPipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, PipelineStore pipelineStore) {
-        super(settings, GetPipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, GetPipelineRequest::new);
-        this.pipelineStore = pipelineStore;
-    }
-
-    @Override
-    protected void doExecute(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {
-        List<PipelineDefinition> references = pipelineStore.getReference(request.ids());
-        listener.onResponse(new GetPipelineResponse(references));
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineAction.java
deleted file mode 100644
index 1356503..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.put;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class PutPipelineAction extends Action<PutPipelineRequest, IndexResponse, PutPipelineRequestBuilder> {
-
-    public static final PutPipelineAction INSTANCE = new PutPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/put";
-
-    public PutPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public PutPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new PutPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public IndexResponse newResponse() {
-        return new IndexResponse();
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequest.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequest.java
deleted file mode 100644
index b9ef9c1..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.put;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class PutPipelineRequest extends ActionRequest {
-
-    private String id;
-    private BytesReference source;
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (id == null) {
-            validationException = addValidationError("id is missing", validationException);
-        }
-        if (source == null) {
-            validationException = addValidationError("source is missing", validationException);
-        }
-        return validationException;
-    }
-
-    public String id() {
-        return id;
-    }
-
-    public void id(String id) {
-        this.id = id;
-    }
-
-    public BytesReference source() {
-        return source;
-    }
-
-    public void source(BytesReference source) {
-        this.source = source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBytesReference(source);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequestBuilder.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequestBuilder.java
deleted file mode 100644
index cb6a74d..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineRequestBuilder.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.put;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class PutPipelineRequestBuilder extends ActionRequestBuilder<PutPipelineRequest, IndexResponse, PutPipelineRequestBuilder> {
-
-    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action) {
-        super(client, action, new PutPipelineRequest());
-    }
-
-    public PutPipelineRequestBuilder setId(String id) {
-        request.id(id);
-        return this;
-    }
-
-    public PutPipelineRequestBuilder setSource(BytesReference source) {
-        request.source(source);
-        return this;
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineTransportAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineTransportAction.java
deleted file mode 100644
index 476dcf6..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/put/PutPipelineTransportAction.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.put;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.index.TransportIndexAction;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.util.Map;
-
-public class PutPipelineTransportAction extends HandledTransportAction<PutPipelineRequest, IndexResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public PutPipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, PipelineStore pipelineStore) {
-        super(settings, PutPipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, PutPipelineRequest::new);
-        this.pipelineStore = pipelineStore;
-    }
-
-    @Override
-    protected void doExecute(PutPipelineRequest request, ActionListener<IndexResponse> listener) {
-        pipelineStore.put(request, listener);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentResult.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentResult.java
deleted file mode 100644
index ff9ad82..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentResult.java
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-
-public interface SimulateDocumentResult<T extends SimulateDocumentResult> extends Writeable<T>, ToXContent {
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResult.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResult.java
deleted file mode 100644
index eb6170e..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResult.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-
-public class SimulateDocumentSimpleResult implements SimulateDocumentResult<SimulateDocumentSimpleResult> {
-
-    private static final SimulateDocumentSimpleResult PROTOTYPE = new SimulateDocumentSimpleResult(new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
-
-    private WriteableIngestDocument ingestDocument;
-    private Exception failure;
-
-    public SimulateDocumentSimpleResult(IngestDocument ingestDocument) {
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-    }
-
-    private SimulateDocumentSimpleResult(WriteableIngestDocument ingestDocument) {
-        this.ingestDocument = ingestDocument;
-    }
-
-    public SimulateDocumentSimpleResult(Exception failure) {
-        this.failure = failure;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    public static SimulateDocumentSimpleResult readSimulateDocumentSimpleResult(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentSimpleResult readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            Exception exception = in.readThrowable();
-            return new SimulateDocumentSimpleResult(exception);
-        }
-        return new SimulateDocumentSimpleResult(WriteableIngestDocument.readWriteableIngestDocumentFrom(in));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentVerboseResult.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentVerboseResult.java
deleted file mode 100644
index eac308d..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentVerboseResult.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-public class SimulateDocumentVerboseResult implements SimulateDocumentResult<SimulateDocumentVerboseResult> {
-
-    private static final SimulateDocumentVerboseResult PROTOTYPE = new SimulateDocumentVerboseResult(Collections.emptyList());
-
-    private final List<SimulateProcessorResult> processorResults;
-
-    public SimulateDocumentVerboseResult(List<SimulateProcessorResult> processorResults) {
-        this.processorResults = processorResults;
-    }
-
-    public List<SimulateProcessorResult> getProcessorResults() {
-        return processorResults;
-    }
-
-    public static SimulateDocumentVerboseResult readSimulateDocumentVerboseResultFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentVerboseResult readFrom(StreamInput in) throws IOException {
-        int size = in.readVInt();
-        List<SimulateProcessorResult> processorResults = new ArrayList<>();
-        for (int i = 0; i < size; i++) {
-            processorResults.add(SimulateProcessorResult.readSimulateProcessorResultFrom(in));
-        }
-        return new SimulateDocumentVerboseResult(processorResults);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(processorResults.size());
-        for (SimulateProcessorResult result : processorResults) {
-            result.writeTo(out);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.startArray(Fields.PROCESSOR_RESULTS);
-        for (SimulateProcessorResult processorResult : processorResults) {
-            processorResult.toXContent(builder, params);
-        }
-        builder.endArray();
-        builder.endObject();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString PROCESSOR_RESULTS = new XContentBuilderString("processor_results");
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionService.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionService.java
deleted file mode 100644
index fcf6e00..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionService.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class SimulateExecutionService {
-
-    private static final String THREAD_POOL_NAME = ThreadPool.Names.MANAGEMENT;
-
-    private final ThreadPool threadPool;
-
-    @Inject
-    public SimulateExecutionService(ThreadPool threadPool) {
-        this.threadPool = threadPool;
-    }
-
-    SimulateDocumentResult executeDocument(Pipeline pipeline, IngestDocument ingestDocument, boolean verbose) {
-        if (verbose) {
-            List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-            IngestDocument currentIngestDocument = new IngestDocument(ingestDocument);
-            for (int i = 0; i < pipeline.getProcessors().size(); i++) {
-                Processor processor = pipeline.getProcessors().get(i);
-                String processorId = "processor[" + processor.getType() + "]-" + i;
-                try {
-                    processor.execute(currentIngestDocument);
-                    processorResultList.add(new SimulateProcessorResult(processorId, currentIngestDocument));
-                } catch (Exception e) {
-                    processorResultList.add(new SimulateProcessorResult(processorId, e));
-                }
-                currentIngestDocument = new IngestDocument(currentIngestDocument);
-            }
-            return new SimulateDocumentVerboseResult(processorResultList);
-        } else {
-            try {
-                pipeline.execute(ingestDocument);
-                return new SimulateDocumentSimpleResult(ingestDocument);
-            } catch (Exception e) {
-                return new SimulateDocumentSimpleResult(e);
-            }
-        }
-    }
-
-    public void execute(SimulatePipelineRequest.Parsed request, ActionListener<SimulatePipelineResponse> listener) {
-        threadPool.executor(THREAD_POOL_NAME).execute(() -> {
-            List<SimulateDocumentResult> responses = new ArrayList<>();
-            for (IngestDocument ingestDocument : request.getDocuments()) {
-                responses.add(executeDocument(request.getPipeline(), ingestDocument, request.isVerbose()));
-            }
-            listener.onResponse(new SimulatePipelineResponse(request.getPipeline().getId(), request.isVerbose(), responses));
-        });
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineAction.java
deleted file mode 100644
index 7c671a4..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class SimulatePipelineAction extends Action<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public static final SimulatePipelineAction INSTANCE = new SimulatePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/simulate";
-
-    public SimulatePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public SimulatePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new SimulatePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public SimulatePipelineResponse newResponse() {
-        return new SimulatePipelineResponse();
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequest.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequest.java
deleted file mode 100644
index 8e48a5e..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequest.java
+++ /dev/null
@@ -1,162 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.ConfigurationUtils;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-import static org.elasticsearch.ingest.IngestDocument.MetaData;
-
-public class SimulatePipelineRequest extends ActionRequest {
-
-    private String id;
-    private boolean verbose;
-    private BytesReference source;
-
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException validationException = null;
-        if (source == null) {
-            validationException = addValidationError("source is missing", validationException);
-        }
-        return validationException;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public void setId(String id) {
-        this.id = id;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    public void setVerbose(boolean verbose) {
-        this.verbose = verbose;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    public void setSource(BytesReference source) {
-        this.source = source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        verbose = in.readBoolean();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBoolean(verbose);
-        out.writeBytesReference(source);
-    }
-
-    public static final class Fields {
-        static final String PIPELINE = "pipeline";
-        static final String DOCS = "docs";
-        static final String SOURCE = "_source";
-    }
-
-    static class Parsed {
-        private final List<IngestDocument> documents;
-        private final Pipeline pipeline;
-        private final boolean verbose;
-
-        Parsed(Pipeline pipeline, List<IngestDocument> documents, boolean verbose) {
-            this.pipeline = pipeline;
-            this.documents = Collections.unmodifiableList(documents);
-            this.verbose = verbose;
-        }
-
-        public Pipeline getPipeline() {
-            return pipeline;
-        }
-
-        public List<IngestDocument> getDocuments() {
-            return documents;
-        }
-
-        public boolean isVerbose() {
-            return verbose;
-        }
-    }
-
-    private static final Pipeline.Factory PIPELINE_FACTORY = new Pipeline.Factory();
-    static final String SIMULATED_PIPELINE_ID = "_simulate_pipeline";
-
-    static Parsed parseWithPipelineId(String pipelineId, Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) {
-        if (pipelineId == null) {
-            throw new IllegalArgumentException("param [pipeline] is null");
-        }
-        Pipeline pipeline = pipelineStore.get(pipelineId);
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    static Parsed parse(Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) throws Exception {
-        Map<String, Object> pipelineConfig = ConfigurationUtils.readMap(config, Fields.PIPELINE);
-        Pipeline pipeline = PIPELINE_FACTORY.create(SIMULATED_PIPELINE_ID, pipelineConfig, pipelineStore.getProcessorFactoryRegistry());
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    private static List<IngestDocument> parseDocs(Map<String, Object> config) {
-        List<Map<String, Object>> docs = ConfigurationUtils.readList(config, Fields.DOCS);
-        List<IngestDocument> ingestDocumentList = new ArrayList<>();
-        for (Map<String, Object> dataMap : docs) {
-            Map<String, Object> document = ConfigurationUtils.readMap(dataMap, Fields.SOURCE);
-            IngestDocument ingestDocument = new IngestDocument(ConfigurationUtils.readStringProperty(dataMap, MetaData.INDEX.getFieldName()),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.TYPE.getFieldName()),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.ID.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.ROUTING.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.PARENT.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TIMESTAMP.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TTL.getFieldName()),
-                    document);
-            ingestDocumentList.add(ingestDocument);
-        }
-        return ingestDocumentList;
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestBuilder.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestBuilder.java
deleted file mode 100644
index 0799829..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestBuilder.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class SimulatePipelineRequestBuilder extends ActionRequestBuilder<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action) {
-        super(client, action, new SimulatePipelineRequest());
-    }
-
-    public SimulatePipelineRequestBuilder setId(String id) {
-        request.setId(id);
-        return this;
-    }
-
-    public SimulatePipelineRequestBuilder setVerbose(boolean verbose) {
-        request.setVerbose(verbose);
-        return this;
-    }
-
-    public SimulatePipelineRequestBuilder setSource(BytesReference source) {
-        request.setSource(source);
-        return this;
-    }
-
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponse.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponse.java
deleted file mode 100644
index 097595f..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponse.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-public class SimulatePipelineResponse extends ActionResponse implements ToXContent {
-    private String pipelineId;
-    private boolean verbose;
-    private List<SimulateDocumentResult> results;
-
-    public SimulatePipelineResponse() {
-
-    }
-
-    public SimulatePipelineResponse(String pipelineId, boolean verbose, List<SimulateDocumentResult> responses) {
-        this.pipelineId = pipelineId;
-        this.verbose = verbose;
-        this.results = Collections.unmodifiableList(responses);
-    }
-
-    public String getPipelineId() {
-        return pipelineId;
-    }
-
-    public List<SimulateDocumentResult> getResults() {
-        return results;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(pipelineId);
-        out.writeBoolean(verbose);
-        out.writeVInt(results.size());
-        for (SimulateDocumentResult response : results) {
-            response.writeTo(out);
-        }
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        this.pipelineId = in.readString();
-        boolean verbose = in.readBoolean();
-        int responsesLength = in.readVInt();
-        results = new ArrayList<>();
-        for (int i = 0; i < responsesLength; i++) {
-            SimulateDocumentResult<?> simulateDocumentResult;
-            if (verbose) {
-                simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
-            } else {
-                simulateDocumentResult = SimulateDocumentSimpleResult.readSimulateDocumentSimpleResult(in);
-            }
-            results.add(simulateDocumentResult);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(Fields.DOCUMENTS);
-        for (SimulateDocumentResult response : results) {
-            response.toXContent(builder, params);
-        }
-        builder.endArray();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString DOCUMENTS = new XContentBuilderString("docs");
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineTransportAction.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineTransportAction.java
deleted file mode 100644
index b4036c5..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineTransportAction.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.util.Map;
-
-public class SimulatePipelineTransportAction extends HandledTransportAction<SimulatePipelineRequest, SimulatePipelineResponse> {
-    private final PipelineStore pipelineStore;
-    private final SimulateExecutionService executionService;
-
-    @Inject
-    public SimulatePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, PipelineStore pipelineStore, SimulateExecutionService executionService) {
-        super(settings, SimulatePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, SimulatePipelineRequest::new);
-        this.pipelineStore = pipelineStore;
-        this.executionService = executionService;
-    }
-
-    @Override
-    protected void doExecute(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
-        Map<String, Object> source = XContentHelper.convertToMap(request.getSource(), false).v2();
-
-        SimulatePipelineRequest.Parsed simulateRequest;
-        try {
-            if (request.getId() != null) {
-                simulateRequest = SimulatePipelineRequest.parseWithPipelineId(request.getId(), source, request.isVerbose(), pipelineStore);
-            } else {
-                simulateRequest = SimulatePipelineRequest.parse(source, request.isVerbose(), pipelineStore);
-            }
-        } catch (Exception e) {
-            listener.onFailure(e);
-            return;
-        }
-
-        executionService.execute(simulateRequest, listener);
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResult.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResult.java
deleted file mode 100644
index 78eafd5..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResult.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-
-public class SimulateProcessorResult implements Writeable<SimulateProcessorResult>, ToXContent {
-
-    private static final SimulateProcessorResult PROTOTYPE = new SimulateProcessorResult("_na", new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
-
-    private String processorId;
-    private WriteableIngestDocument ingestDocument;
-    private Exception failure;
-
-    public SimulateProcessorResult(String processorId, IngestDocument ingestDocument) {
-        this.processorId = processorId;
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-    }
-
-    private SimulateProcessorResult(String processorId, WriteableIngestDocument ingestDocument) {
-        this.processorId = processorId;
-        this.ingestDocument = ingestDocument;
-    }
-
-    public SimulateProcessorResult(String processorId, Exception failure) {
-        this.processorId = processorId;
-        this.failure = failure;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public String getProcessorId() {
-        return processorId;
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    public static SimulateProcessorResult readSimulateProcessorResultFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateProcessorResult readFrom(StreamInput in) throws IOException {
-        String processorId = in.readString();
-        if (in.readBoolean()) {
-            Exception exception = in.readThrowable();
-            return new SimulateProcessorResult(processorId, exception);
-        }
-        return new SimulateProcessorResult(processorId, WriteableIngestDocument.readWriteableIngestDocumentFrom(in));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(processorId);
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.field(Fields.PROCESSOR_ID, processorId);
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString PROCESSOR_ID = new XContentBuilderString("processor_id");
-    }
-}
diff --git a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocument.java b/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocument.java
deleted file mode 100644
index 2b9ac56..0000000
--- a/plugins/ingest/src/main/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocument.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-import java.util.Objects;
-
-final class WriteableIngestDocument implements Writeable<WriteableIngestDocument>, ToXContent {
-
-    private static final WriteableIngestDocument PROTOTYPE = new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap()));
-
-    private final IngestDocument ingestDocument;
-
-    WriteableIngestDocument(IngestDocument ingestDocument) {
-        assert ingestDocument != null;
-        this.ingestDocument = ingestDocument;
-    }
-
-    IngestDocument getIngestDocument() {
-        return ingestDocument;
-    }
-
-    static WriteableIngestDocument readWriteableIngestDocumentFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public WriteableIngestDocument readFrom(StreamInput in) throws IOException {
-        Map<String, Object> sourceAndMetadata = in.readMap();
-        @SuppressWarnings("unchecked")
-        Map<String, String> ingestMetadata = (Map<String, String>) in.readGenericValue();
-        return new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeMap(ingestDocument.getSourceAndMetadata());
-        out.writeGenericValue(ingestDocument.getIngestMetadata());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(Fields.DOCUMENT);
-        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
-            builder.field(metadata.getKey().getFieldName(), metadata.getValue());
-        }
-        builder.field(Fields.SOURCE, ingestDocument.getSourceAndMetadata());
-        builder.startObject(Fields.INGEST);
-        for (Map.Entry<String, String> ingestMetadata : ingestDocument.getIngestMetadata().entrySet()) {
-            builder.field(ingestMetadata.getKey(), ingestMetadata.getValue());
-        }
-        builder.endObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) {
-            return true;
-        }
-        if (o == null || getClass() != o.getClass()) {
-            return false;
-        }
-        WriteableIngestDocument that = (WriteableIngestDocument) o;
-        return Objects.equals(ingestDocument, that.ingestDocument);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(ingestDocument);
-    }
-
-    @Override
-    public String toString() {
-        return ingestDocument.toString();
-    }
-
-    static final class Fields {
-        static final XContentBuilderString DOCUMENT = new XContentBuilderString("doc");
-        static final XContentBuilderString SOURCE = new XContentBuilderString("_source");
-        static final XContentBuilderString INGEST = new XContentBuilderString("_ingest");
-    }
-}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/aws b/plugins/ingest/src/main/packaging/config/grok/patterns/aws
deleted file mode 100644
index 71edbc9..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/aws
+++ /dev/null
@@ -1,11 +0,0 @@
-S3_REQUEST_LINE (?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-S3_ACCESS_LOG %{WORD:owner} %{NOTSPACE:bucket} \[%{HTTPDATE:timestamp}\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:"%{S3_REQUEST_LINE}"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:"?%{QS:agent}"?|-) (?:-|%{NOTSPACE:version_id})
-
-ELB_URIPATHPARAM %{URIPATH:path}(?:%{URIPARAM:params})?
-
-ELB_URI %{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?
-
-ELB_REQUEST_LINE (?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-ELB_ACCESS_LOG %{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} "%{ELB_REQUEST_LINE}"
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/bacula b/plugins/ingest/src/main/packaging/config/grok/patterns/bacula
deleted file mode 100644
index d80dfe5..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/bacula
+++ /dev/null
@@ -1,50 +0,0 @@
-BACULA_TIMESTAMP %{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}
-BACULA_HOST [a-zA-Z0-9-]+
-BACULA_VOLUME %{USER}
-BACULA_DEVICE %{USER}
-BACULA_DEVICEPATH %{UNIXPATH}
-BACULA_CAPACITY %{INT}{1,3}(,%{INT}{3})*
-BACULA_VERSION %{USER}
-BACULA_JOB %{USER}
-
-BACULA_LOG_MAX_CAPACITY User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_END_VOLUME End of medium on Volume \"%{BACULA_VOLUME:volume}\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NEW_VOLUME Created new Volume \"%{BACULA_VOLUME:volume}\" in catalog.
-BACULA_LOG_NEW_LABEL Labeled new Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\).
-BACULA_LOG_WROTE_LABEL Wrote label to prelabeled Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_NEW_MOUNT New volume \"%{BACULA_VOLUME:volume}\" mounted on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NOOPEN \s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOOPENDIR \s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOSTAT \s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOJOBS There are no more Jobs associated with Volume \"%{BACULA_VOLUME:volume}\". Marking it purged.
-BACULA_LOG_ALL_RECORDS_PRUNED All records pruned from Volume \"%{BACULA_VOLUME:volume}\"; marking it \"Purged\"
-BACULA_LOG_BEGIN_PRUNE_JOBS Begin pruning Jobs older than %{INT} month %{INT} days .
-BACULA_LOG_BEGIN_PRUNE_FILES Begin pruning Files.
-BACULA_LOG_PRUNED_JOBS Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_PRUNED_FILES Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_ENDPRUNE End auto prune.
-BACULA_LOG_STARTJOB Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}
-BACULA_LOG_STARTRESTORE Start Restore Job %{BACULA_JOB:job}
-BACULA_LOG_USEDEVICE Using Device \"%{BACULA_DEVICE:device}\"
-BACULA_LOG_DIFF_FS \s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it.
-BACULA_LOG_JOBEND Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second
-BACULA_LOG_NOPRUNE_JOBS No Jobs found to prune.
-BACULA_LOG_NOPRUNE_FILES No Files found to prune.
-BACULA_LOG_VOLUME_PREVWRITTEN Volume \"%{BACULA_VOLUME:volume}\" previously written, moving to end of data.
-BACULA_LOG_READYAPPEND Ready to append to end of Volume \"%{BACULA_VOLUME:volume}\" size=%{INT}
-BACULA_LOG_CANCELLING Cancelling duplicate JobId=%{INT}.
-BACULA_LOG_MARKCANCEL JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled.
-BACULA_LOG_CLIENT_RBJ shell command: run ClientRunBeforeJob \"%{GREEDYDATA:runjob}\"
-BACULA_LOG_VSS (Generate )?VSS (Writer)?
-BACULA_LOG_MAXSTART Fatal error: Job canceled because max start delay time exceeded.
-BACULA_LOG_DUPLICATE Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed.
-BACULA_LOG_NOJOBSTAT Fatal error: No Job status returned from FD.
-BACULA_LOG_FATAL_CONN Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_CONNECT Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_AUTH Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:
-BACULA_LOG_NOSUIT No prior or suitable Full backup found in catalog. Doing FULL backup.
-BACULA_LOG_NOPRIOR No prior Full backup Job record found.
-
-BACULA_LOG_JOB (Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \(%{BACULA_VERSION}\):
-
-BACULA_LOGLINE %{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/bro b/plugins/ingest/src/main/packaging/config/grok/patterns/bro
deleted file mode 100644
index 31b138b..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/bro
+++ /dev/null
@@ -1,13 +0,0 @@
-# https://www.bro.org/sphinx/script-reference/log-files.html
-
-# http.log
-BRO_HTTP %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{INT:trans_depth}\t%{GREEDYDATA:method}\t%{GREEDYDATA:domain}\t%{GREEDYDATA:uri}\t%{GREEDYDATA:referrer}\t%{GREEDYDATA:user_agent}\t%{NUMBER:request_body_len}\t%{NUMBER:response_body_len}\t%{GREEDYDATA:status_code}\t%{GREEDYDATA:status_msg}\t%{GREEDYDATA:info_code}\t%{GREEDYDATA:info_msg}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:bro_tags}\t%{GREEDYDATA:username}\t%{GREEDYDATA:password}\t%{GREEDYDATA:proxied}\t%{GREEDYDATA:orig_fuids}\t%{GREEDYDATA:orig_mime_types}\t%{GREEDYDATA:resp_fuids}\t%{GREEDYDATA:resp_mime_types}
-
-# dns.log
-BRO_DNS %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{INT:trans_id}\t%{GREEDYDATA:query}\t%{GREEDYDATA:qclass}\t%{GREEDYDATA:qclass_name}\t%{GREEDYDATA:qtype}\t%{GREEDYDATA:qtype_name}\t%{GREEDYDATA:rcode}\t%{GREEDYDATA:rcode_name}\t%{GREEDYDATA:AA}\t%{GREEDYDATA:TC}\t%{GREEDYDATA:RD}\t%{GREEDYDATA:RA}\t%{GREEDYDATA:Z}\t%{GREEDYDATA:answers}\t%{GREEDYDATA:TTLs}\t%{GREEDYDATA:rejected}
-
-# conn.log
-BRO_CONN %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{GREEDYDATA:service}\t%{NUMBER:duration}\t%{NUMBER:orig_bytes}\t%{NUMBER:resp_bytes}\t%{GREEDYDATA:conn_state}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:missed_bytes}\t%{GREEDYDATA:history}\t%{GREEDYDATA:orig_pkts}\t%{GREEDYDATA:orig_ip_bytes}\t%{GREEDYDATA:resp_pkts}\t%{GREEDYDATA:resp_ip_bytes}\t%{GREEDYDATA:tunnel_parents}
-
-# files.log
-BRO_FILES %{NUMBER:ts}\t%{NOTSPACE:fuid}\t%{IP:tx_hosts}\t%{IP:rx_hosts}\t%{NOTSPACE:conn_uids}\t%{GREEDYDATA:source}\t%{GREEDYDATA:depth}\t%{GREEDYDATA:analyzers}\t%{GREEDYDATA:mime_type}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:duration}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:is_orig}\t%{GREEDYDATA:seen_bytes}\t%{GREEDYDATA:total_bytes}\t%{GREEDYDATA:missing_bytes}\t%{GREEDYDATA:overflow_bytes}\t%{GREEDYDATA:timedout}\t%{GREEDYDATA:parent_fuid}\t%{GREEDYDATA:md5}\t%{GREEDYDATA:sha1}\t%{GREEDYDATA:sha256}\t%{GREEDYDATA:extracted}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/exim b/plugins/ingest/src/main/packaging/config/grok/patterns/exim
deleted file mode 100644
index 68c4e5c..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/exim
+++ /dev/null
@@ -1,13 +0,0 @@
-EXIM_MSGID [0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}
-EXIM_FLAGS (<=|[-=>*]>|[*]{2}|==)
-EXIM_DATE %{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}
-EXIM_PID \[%{POSINT}\]
-EXIM_QT ((\d+y)?(\d+w)?(\d+d)?(\d+h)?(\d+m)?(\d+s)?)
-EXIM_EXCLUDE_TERMS (Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)
-EXIM_REMOTE_HOST (H=(%{NOTSPACE:remote_hostname} )?(\(%{NOTSPACE:remote_heloname}\) )?\[%{IP:remote_host}\])
-EXIM_INTERFACE (I=\[%{IP:exim_interface}\](:%{NUMBER:exim_interface_port}))
-EXIM_PROTOCOL (P=%{NOTSPACE:protocol})
-EXIM_MSG_SIZE (S=%{NUMBER:exim_msg_size})
-EXIM_HEADER_ID (id=%{NOTSPACE:exim_header_id})
-EXIM_SUBJECT (T=%{QS:exim_subject})
-
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/firewalls b/plugins/ingest/src/main/packaging/config/grok/patterns/firewalls
deleted file mode 100644
index 03c3e5a..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/firewalls
+++ /dev/null
@@ -1,86 +0,0 @@
-# NetScreen firewall logs
-NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
-
-#== Cisco ASA ==
-CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:
-CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
-CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
-# Common Particles
-CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
-CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
-CISCO_DIRECTION Inbound|inbound|Outbound|outbound
-CISCO_INTERVAL first hit|%{INT}-second interval
-CISCO_XLATE_TYPE static|dynamic
-# ASA-1-104001
-CISCOFW104001 \((?:Primary|Secondary)\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}
-# ASA-1-104002
-CISCOFW104002 \((?:Primary|Secondary)\) Switching to STANDBY - %{GREEDYDATA:switch_reason}
-# ASA-1-104003
-CISCOFW104003 \((?:Primary|Secondary)\) Switching to FAILED\.
-# ASA-1-104004
-CISCOFW104004 \((?:Primary|Secondary)\) Switching to OK\.
-# ASA-1-105003
-CISCOFW105003 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting
-# ASA-1-105004
-CISCOFW105004 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal
-# ASA-1-105005
-CISCOFW105005 \((?:Primary|Secondary)\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105008
-CISCOFW105008 \((?:Primary|Secondary)\) Testing [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105009
-CISCOFW105009 \((?:Primary|Secondary)\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)
-# ASA-2-106001
-CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
-# ASA-2-106006, ASA-2-106007, ASA-2-106010
-CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
-# ASA-3-106014
-CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
-# ASA-6-106015
-CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
-# ASA-1-106021
-CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
-# ASA-4-106023
-CISCOFW106023 %{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group "?%{DATA:policy_id}"? \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-4-106100, ASA-4-106102, ASA-4-106103
-CISCOFW106100_2_3 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\) -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-5-106100
-CISCOFW106100 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-6-110002
-CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-302010
-CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
-# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
-CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
-# ASA-6-302020, ASA-6-302021
-CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
-# ASA-6-305011
-CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
-# ASA-3-313001, ASA-3-313004, ASA-3-313008
-CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
-# ASA-4-313005
-CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
-# ASA-5-321001
-CISCOFW321001 Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system
-# ASA-4-402117
-CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
-# ASA-4-402119
-CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
-# ASA-4-419001
-CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
-# ASA-4-419002
-CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
-# ASA-4-500004
-CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-602303, ASA-6-602304
-CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
-# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
-CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-713172
-CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
-# ASA-4-733100
-CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
-#== End Cisco ASA ==
-
-# Shorewall firewall logs
-SHOREWALL (%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)
-#== End Shorewall
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/grok-patterns b/plugins/ingest/src/main/packaging/config/grok/patterns/grok-patterns
deleted file mode 100644
index cb4c3ff..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/grok-patterns
+++ /dev/null
@@ -1,102 +0,0 @@
-USERNAME [a-zA-Z0-9._-]+
-USER %{USERNAME}
-EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+
-EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME}
-HTTPDUSER %{EMAILADDRESS}|%{USER}
-INT (?:[+-]?(?:[0-9]+))
-BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
-NUMBER (?:%{BASE10NUM})
-BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
-BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
-
-POSINT \b(?:[1-9][0-9]*)\b
-NONNEGINT \b(?:[0-9]+)\b
-WORD \b\w+\b
-NOTSPACE \S+
-SPACE \s*
-DATA .*?
-GREEDYDATA .*
-QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
-UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
-
-# Networking
-MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
-CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
-WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
-COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
-IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
-IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])
-IP (?:%{IPV6}|%{IPV4})
-HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
-IPORHOST (?:%{IP}|%{HOSTNAME})
-HOSTPORT %{IPORHOST}:%{POSINT}
-
-# paths
-PATH (?:%{UNIXPATH}|%{WINPATH})
-UNIXPATH (/([\w_%!$@:.,~-]+|\\.)*)+
-TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
-WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
-URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
-URIHOST %{IPORHOST}(?::%{POSINT:port})?
-# uripath comes loosely from RFC1738, but mostly from what Firefox
-# doesn't turn into %XX
-URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
-#URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
-URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]<>]*
-URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
-URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
-
-# Months: January, Feb, 3, 03, 12, December
-MONTH \b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\b
-MONTHNUM (?:0?[1-9]|1[0-2])
-MONTHNUM2 (?:0[1-9]|1[0-2])
-MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
-
-# Days: Monday, Tue, Thu, etc...
-DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
-
-# Years?
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-# '60' is a leap second in most time standards and thus is valid.
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
-# datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
-DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
-DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
-ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
-ISO8601_SECOND (?:%{SECOND}|60)
-ISO8601_HOUR (?:2[0123]|[01][0-9])
-TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{ISO8601_HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
-DATE %{DATE_US}|%{DATE_EU}
-DATESTAMP %{DATE}[- ]%{TIME}
-TZ (?:[PMCE][SD]T|UTC)
-DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
-DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
-DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
-DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
-HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}
-
-# Syslog Dates: Month Day HH:MM:SS
-SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
-PROG [\x21-\x5a\x5c\x5e-\x7e]+
-SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
-SYSLOGHOST %{IPORHOST}
-SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
-HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
-
-# Shortcuts
-QS %{QUOTEDSTRING}
-
-# Log formats
-SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
-COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
-HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:errormsg}
-HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}:tid %{NUMBER:tid}\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_errormessage}:)?( \[client %{IPORHOST:client}:%{POSINT:clientport}\])? %{DATA:errorcode}: %{GREEDYDATA:message}
-HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}
-
-
-# Log Levels
-LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/haproxy b/plugins/ingest/src/main/packaging/config/grok/patterns/haproxy
deleted file mode 100644
index ddabd19..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/haproxy
+++ /dev/null
@@ -1,39 +0,0 @@
-## These patterns were tested w/ haproxy-1.4.15
-
-## Documentation of the haproxy log formats can be found at the following links:
-## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
-## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
-
-HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
-HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
-
-# Override these default patterns to parse out what is captured in your haproxy.cfg
-HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
-HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
-
-# Example:
-#  These haproxy config lines will add data to the logs that are captured
-#  by the patterns below. Place them in your custom patterns directory to
-#  override the defaults.
-#
-#  capture request header Host len 40
-#  capture request header X-Forwarded-For len 50
-#  capture request header Accept-Language len 50
-#  capture request header Referer len 200
-#  capture request header User-Agent len 200
-#
-#  capture response header Content-Type len 30
-#  capture response header Content-Encoding len 10
-#  capture response header Cache-Control len 200
-#  capture response header Last-Modified len 200
-#
-# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
-# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
-
-# parse a haproxy 'httplog' line
-HAPROXYHTTPBASE %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
-
-HAPROXYHTTP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}
-
-# parse a haproxy 'tcplog' line
-HAPROXYTCP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/java b/plugins/ingest/src/main/packaging/config/grok/patterns/java
deleted file mode 100644
index e968006..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/java
+++ /dev/null
@@ -1,20 +0,0 @@
-JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
-#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
-JAVAFILE (?:[A-Za-z0-9_. -]+)
-#Allow special <init> method
-JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
-#Line number is optional in special cases 'Native method' or 'Unknown source'
-JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
-# Java Logs
-JAVATHREAD (?:[A-Z]{2}-Processor[\d]+)
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$]+
-JAVAFILE (?:[A-Za-z0-9_.-]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
-JAVALOGMESSAGE (.*)
-# MMM dd, yyyy HH:mm:ss eg: Jan 9, 2014 7:13:13 AM
-CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)
-# yyyy-MM-dd HH:mm:ss,SSS ZZZ eg: 2014-01-09 17:32:25,527 -0800
-TOMCAT_DATESTAMP 20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}
-CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}
-# 2014-01-09 20:03:28,269 -0800 | ERROR | com.example.service.ExampleService - something compeletely unexpected happened...
-TOMCATLOG %{TOMCAT_DATESTAMP:timestamp} \| %{LOGLEVEL:level} \| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/junos b/plugins/ingest/src/main/packaging/config/grok/patterns/junos
deleted file mode 100644
index 4eea59d..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/junos
+++ /dev/null
@@ -1,9 +0,0 @@
-# JUNOS 11.4 RT_FLOW patterns
-RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
-
-RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
-
-RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
-
-RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
-
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/linux-syslog b/plugins/ingest/src/main/packaging/config/grok/patterns/linux-syslog
deleted file mode 100644
index dcffb41..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/linux-syslog
+++ /dev/null
@@ -1,16 +0,0 @@
-SYSLOG5424PRINTASCII [!-~]+
-
-SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)
-SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
-
-CRON_ACTION [A-Z ]+
-CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
-
-SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
-
-# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
-SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
-SYSLOG5424SD \[%{DATA}\]+
-SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
-
-SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/mcollective-patterns b/plugins/ingest/src/main/packaging/config/grok/patterns/mcollective-patterns
deleted file mode 100644
index bb2f7f9..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/mcollective-patterns
+++ /dev/null
@@ -1,4 +0,0 @@
-# Remember, these can be multi-line events.
-MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
-
-MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/mongodb b/plugins/ingest/src/main/packaging/config/grok/patterns/mongodb
deleted file mode 100644
index 78a4300..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/mongodb
+++ /dev/null
@@ -1,7 +0,0 @@
-MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
-MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
-MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
-MONGO_WORDDASH \b[\w-]+\b
-MONGO3_SEVERITY \w
-MONGO3_COMPONENT %{WORD}|-
-MONGO3_LOG %{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\[%{DATA:context}\])? %{GREEDYDATA:message}
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/nagios b/plugins/ingest/src/main/packaging/config/grok/patterns/nagios
deleted file mode 100644
index f4a98bf..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/nagios
+++ /dev/null
@@ -1,124 +0,0 @@
-##################################################################################
-##################################################################################
-# Chop Nagios log files to smithereens!
-#
-# A set of GROK filters to process logfiles generated by Nagios.
-# While it does not, this set intends to cover all possible Nagios logs.
-#
-# Some more work needs to be done to cover all External Commands:
-# http://old.nagios.org/developerinfo/externalcommands/commandlist.php
-#
-# If you need some support on these rules please contact:
-# Jelle Smet http://smetj.net
-#
-#################################################################################
-#################################################################################
-
-NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
-
-###############################################
-######## Begin nagios log types
-###############################################
-NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
-NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
-
-NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
-NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
-
-NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
-NAGIOS_TYPE_HOST_ALERT HOST ALERT
-
-NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
-NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
-
-NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
-NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
-
-NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
-NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
-
-NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
-NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
-
-NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
-NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
-###############################################
-######## End nagios log types
-###############################################
-
-###############################################
-######## Begin external check types
-###############################################
-NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
-NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
-NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
-NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
-NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
-NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
-NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
-NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
-NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS DISABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS ENABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS DISABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS ENABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS DISABLE_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS ENABLE_SVC_NOTIFICATIONS
-###############################################
-######## End external check types
-###############################################
-NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
-
-NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-
-NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}
-
-####################
-#### External checks
-####################
-
-#Disable host & service check
-NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Enable host & service check
-NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Process host & service check
-NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-
-#Disable host & service notifications
-NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Enable host & service notifications
-NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Schedule host & service downtime
-NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
-
-#End matching line
-NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/postgresql b/plugins/ingest/src/main/packaging/config/grok/patterns/postgresql
deleted file mode 100644
index c5b3e90..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/postgresql
+++ /dev/null
@@ -1,3 +0,0 @@
-# Default postgresql pg_log format pattern
-POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
-
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/rails b/plugins/ingest/src/main/packaging/config/grok/patterns/rails
deleted file mode 100644
index 68a50c7..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/rails
+++ /dev/null
@@ -1,13 +0,0 @@
-RUUID \h{32}
-# rails controller with action
-RCONTROLLER (?<controller>[^#]+)#(?<action>\w+)
-
-# this will often be the only line:
-RAILS3HEAD (?m)Started %{WORD:verb} "%{URIPATHPARAM:request}" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})
-# for some a strange reason, params are stripped of {} - not sure that's a good idea.
-RPROCESSING \W*Processing by %{RCONTROLLER} as (?<format>\S+)(?:\W*Parameters: {%{DATA:params}}\W*)?
-RAILS3FOOT Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}
-RAILS3PROFILE (?:\(Views: %{NUMBER:viewms}ms \| ActiveRecord: %{NUMBER:activerecordms}ms|\(ActiveRecord: %{NUMBER:activerecordms}ms)?
-
-# putting it all together
-RAILS3 %{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\n)*)(?:%{RAILS3FOOT})?
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/redis b/plugins/ingest/src/main/packaging/config/grok/patterns/redis
deleted file mode 100644
index 8655c4f..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/redis
+++ /dev/null
@@ -1,3 +0,0 @@
-REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
-REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
-
diff --git a/plugins/ingest/src/main/packaging/config/grok/patterns/ruby b/plugins/ingest/src/main/packaging/config/grok/patterns/ruby
deleted file mode 100644
index b1729cd..0000000
--- a/plugins/ingest/src/main/packaging/config/grok/patterns/ruby
+++ /dev/null
@@ -1,2 +0,0 @@
-RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
-RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/plugins/ingest/src/main/plugin-metadata/plugin-security.policy b/plugins/ingest/src/main/plugin-metadata/plugin-security.policy
deleted file mode 100644
index 3faba71..0000000
--- a/plugins/ingest/src/main/plugin-metadata/plugin-security.policy
+++ /dev/null
@@ -1,23 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-grant {
-  // needed because geoip2 is using reflection to deserialize data into its own domain classes
-  permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
-};
diff --git a/plugins/ingest/src/test/java/IngestRunner.java b/plugins/ingest/src/test/java/IngestRunner.java
deleted file mode 100644
index a860cb9..0000000
--- a/plugins/ingest/src/test/java/IngestRunner.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import org.elasticsearch.Version;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugin.ingest.IngestPlugin;
-
-import java.util.Collections;
-import java.util.concurrent.CountDownLatch;
-
-public class IngestRunner {
-
-    public static void main(String[] args) throws Exception {
-        Settings.Builder settings = Settings.builder();
-        settings.put("http.cors.enabled", "true");
-        settings.put("http.cors.allow-origin", "*");
-        settings.put("script.inline", "on");
-        settings.put("cluster.name", IngestRunner.class.getSimpleName());
-
-        final CountDownLatch latch = new CountDownLatch(1);
-        final Node node = new MockNode(settings.build(), Version.CURRENT, Collections.singleton(IngestPlugin.class));
-        Runtime.getRuntime().addShutdownHook(new Thread() {
-
-            @Override
-            public void run() {
-                node.close();
-                latch.countDown();
-            }
-        });
-        node.start();
-        latch.await();
-    }
-
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestClientIT.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
deleted file mode 100644
index 9db406a..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
+++ /dev/null
@@ -1,261 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.action.update.UpdateResponse;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.plugin.ingest.IngestPlugin;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.delete.DeletePipelineRequestBuilder;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineRequestBuilder;
-import org.elasticsearch.plugin.ingest.transport.get.GetPipelineResponse;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineAction;
-import org.elasticsearch.plugin.ingest.transport.put.PutPipelineRequestBuilder;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulateDocumentSimpleResult;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineAction;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineRequestBuilder;
-import org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineResponse;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
-import static org.hamcrest.core.Is.is;
-
-public class IngestClientIT extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestPlugin.class);
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> transportClientPlugins() {
-        return nodePlugins();
-    }
-
-    public void testSimulate() throws Exception {
-        new PutPipelineRequestBuilder(client(), PutPipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .field("description", "my_pipeline")
-                        .startArray("processors")
-                        .startObject()
-                        .startObject("grok")
-                        .field("field", "field1")
-                        .field("pattern", "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-        assertBusy(new Runnable() {
-            @Override
-            public void run() {
-                GetPipelineResponse response = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                        .setIds("_id")
-                        .get();
-                assertThat(response.isFound(), is(true));
-                assertThat(response.pipelines().size(), equalTo(1));
-                assertThat(response.pipelines().get(0).getId(), equalTo("_id"));
-            }
-        });
-
-        SimulatePipelineResponse response = new SimulatePipelineRequestBuilder(client(), SimulatePipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .startArray("docs")
-                        .startObject()
-                        .field("_index", "index")
-                        .field("_type", "type")
-                        .field("_id", "id")
-                        .startObject("_source")
-                        .field("foo", "bar")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-
-        assertThat(response.isVerbose(), equalTo(false));
-        assertThat(response.getPipelineId(), equalTo("_id"));
-        assertThat(response.getResults().size(), equalTo(1));
-        assertThat(response.getResults().get(0), instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) response.getResults().get(0);
-        assertThat(simulateDocumentSimpleResult.getIngestDocument(), nullValue());
-        assertThat(simulateDocumentSimpleResult.getFailure(), notNullValue());
-
-        response = new SimulatePipelineRequestBuilder(client(), SimulatePipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .startArray("docs")
-                        .startObject()
-                        .field("_index", "index")
-                        .field("_type", "type")
-                        .field("_id", "id")
-                        .startObject("_source")
-                        .field("field1", "123.42 400 <foo>")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-
-        assertThat(response.isVerbose(), equalTo(false));
-        assertThat(response.getPipelineId(), equalTo("_id"));
-        assertThat(response.getResults().size(), equalTo(1));
-        assertThat(response.getResults().get(0), instanceOf(SimulateDocumentSimpleResult.class));
-        simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) response.getResults().get(0);
-        Map<String, Object> source = new HashMap<>();
-        source.put("field1", "123.42 400 <foo>");
-        source.put("val", 123.42f);
-        source.put("status", 400);
-        source.put("msg", "foo");
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, source);
-        assertThat(simulateDocumentSimpleResult.getIngestDocument().getSourceAndMetadata(), equalTo(ingestDocument.getSourceAndMetadata()));
-        assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-    }
-
-    public void testBulkWithIngestFailures() {
-        createIndex("index");
-
-        int numRequests = scaledRandomIntBetween(32, 128);
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_none_existing_id");
-        for (int i = 0; i < numRequests; i++) {
-            if (i % 2 == 0) {
-                UpdateRequest updateRequest = new UpdateRequest("index", "type", Integer.toString(i));
-                updateRequest.upsert("field", "value");
-                updateRequest.doc(new HashMap());
-                bulkRequest.add(updateRequest);
-            } else {
-                IndexRequest indexRequest = new IndexRequest("index", "type", Integer.toString(i));
-                indexRequest.source("field1", "value1");
-                bulkRequest.add(indexRequest);
-            }
-        }
-
-        BulkResponse response = client().bulk(bulkRequest).actionGet();
-        assertThat(response.getItems().length, equalTo(bulkRequest.requests().size()));
-        for (int i = 0; i < bulkRequest.requests().size(); i++) {
-            ActionRequest request = bulkRequest.requests().get(i);
-            BulkItemResponse itemResponse = response.getItems()[i];
-            if (request instanceof IndexRequest) {
-                BulkItemResponse.Failure failure = itemResponse.getFailure();
-                assertThat(failure.getMessage(), equalTo("java.lang.IllegalArgumentException: pipeline with id [_none_existing_id] does not exist"));
-            } else if (request instanceof UpdateRequest) {
-                UpdateResponse updateResponse = itemResponse.getResponse();
-                assertThat(updateResponse.getId(), equalTo(Integer.toString(i)));
-                assertThat(updateResponse.isCreated(), is(true));
-            } else {
-                fail("unexpected request item [" + request + "]");
-            }
-        }
-    }
-
-    public void test() throws Exception {
-        new PutPipelineRequestBuilder(client(), PutPipelineAction.INSTANCE)
-                .setId("_id")
-                .setSource(jsonBuilder().startObject()
-                        .field("description", "my_pipeline")
-                        .startArray("processors")
-                        .startObject()
-                        .startObject("grok")
-                        .field("field", "field1")
-                        .field("pattern", "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>")
-                        .endObject()
-                        .endObject()
-                        .endArray()
-                        .endObject().bytes())
-                .get();
-        assertBusy(() -> {
-            GetPipelineResponse response = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                    .setIds("_id")
-                    .get();
-            assertThat(response.isFound(), is(true));
-            assertThat(response.pipelines().size(), equalTo(1));
-            assertThat(response.pipelines().get(0).getId(), equalTo("_id"));
-        });
-
-        createIndex("test");
-        XContentBuilder updateMappingBuilder = jsonBuilder().startObject().startObject("properties")
-                .startObject("status").field("type", "integer").endObject()
-                .startObject("val").field("type", "float").endObject()
-                .endObject();
-        PutMappingResponse putMappingResponse = client().admin().indices()
-                .preparePutMapping("test").setType("type").setSource(updateMappingBuilder).get();
-        assertAcked(putMappingResponse);
-
-        client().prepareIndex("test", "type", "1").setSource("field1", "123.42 400 <foo>")
-                .putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id")
-                .get();
-
-        assertBusy(() -> {
-            Map<String, Object> doc = client().prepareGet("test", "type", "1")
-                    .get().getSourceAsMap();
-            assertThat(doc.get("val"), equalTo(123.42));
-            assertThat(doc.get("status"), equalTo(400));
-            assertThat(doc.get("msg"), equalTo("foo"));
-        });
-
-        client().prepareBulk().add(
-                client().prepareIndex("test", "type", "2").setSource("field1", "123.42 400 <foo>")
-        ).putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id").get();
-        assertBusy(() -> {
-            Map<String, Object> doc = client().prepareGet("test", "type", "2").get().getSourceAsMap();
-            assertThat(doc.get("val"), equalTo(123.42));
-            assertThat(doc.get("status"), equalTo(400));
-            assertThat(doc.get("msg"), equalTo("foo"));
-        });
-
-        DeleteResponse response = new DeletePipelineRequestBuilder(client(), DeletePipelineAction.INSTANCE)
-                .setId("_id")
-                .get();
-        assertThat(response.isFound(), is(true));
-        assertThat(response.getId(), equalTo("_id"));
-
-        assertBusy(() -> {
-            GetPipelineResponse response1 = new GetPipelineRequestBuilder(client(), GetPipelineAction.INSTANCE)
-                    .setIds("_id")
-                    .get();
-            assertThat(response1.isFound(), is(false));
-            assertThat(response1.pipelines().size(), equalTo(0));
-        });
-    }
-
-    @Override
-    protected boolean enableMockModules() {
-        return false;
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestDocumentTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestDocumentTests.java
deleted file mode 100644
index 11ac560..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestDocumentTests.java
+++ /dev/null
@@ -1,733 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class IngestDocumentTests extends ESTestCase {
-
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setIngestDocument() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        document.put("foo", "bar");
-        document.put("int", 123);
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("buzz", "hello world");
-        innerObject.put("foo_null", null);
-        innerObject.put("1", "bar");
-        document.put("fizz", innerObject);
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-        document.put("list", list);
-        ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-    }
-
-    public void testSimpleGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_source.foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("_source.int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_index", String.class), equalTo("index"));
-        assertThat(ingestDocument.getFieldValue("_type", String.class), equalTo("type"));
-        assertThat(ingestDocument.getFieldValue("_id", String.class), equalTo("id"));
-        assertThat(ingestDocument.getFieldValue("_ingest.timestamp", String.class), both(notNullValue()).and(not(equalTo("bogus_timestamp"))));
-        assertThat(ingestDocument.getFieldValue("_source._ingest.timestamp", String.class), equalTo("bogus_timestamp"));
-    }
-
-    public void testGetSourceObject() {
-        try {
-            ingestDocument.getFieldValue("_source", Object.class);
-            fail("get field value should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testGetIngestObject() {
-        assertThat(ingestDocument.getFieldValue("_ingest", Map.class), notNullValue());
-    }
-
-    public void testGetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.getFieldValue("_source.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("_ingest.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testGetFieldValueNullValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.foo_null", Object.class), nullValue());
-    }
-
-    public void testSimpleGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("int", String.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [int] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("foo", Integer.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [foo] of type [java.lang.String] cannot be cast to [java.lang.Integer]"));
-        }
-    }
-
-    public void testNestedGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.buzz", String.class), equalTo("hello world"));
-        assertThat(ingestDocument.getFieldValue("fizz.1", String.class), equalTo("bar"));
-    }
-
-    public void testNestedGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("foo.foo.bar", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testListGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("list.0.field", String.class), equalTo("value"));
-    }
-
-    public void testListGetFieldValueNull() {
-        assertThat(ingestDocument.getFieldValue("list.1", String.class), nullValue());
-    }
-
-    public void testListGetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.getFieldValue("list.test.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListGetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.getFieldValue("list.10.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testGetFieldValueNotFound() {
-        try {
-            ingestDocument.getFieldValue("not.here", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [not] not present as part of path [not.here]"));
-        }
-    }
-
-    public void testGetFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.getFieldValue("fizz.foo_null.not_there", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testGetFieldValueNull() {
-        try {
-            ingestDocument.getFieldValue(null, String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testGetFieldValueEmpty() {
-        try {
-            ingestDocument.getFieldValue("", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasField() {
-        assertTrue(ingestDocument.hasField("fizz"));
-        assertTrue(ingestDocument.hasField("_index"));
-        assertTrue(ingestDocument.hasField("_type"));
-        assertTrue(ingestDocument.hasField("_id"));
-        assertTrue(ingestDocument.hasField("_source.fizz"));
-        assertTrue(ingestDocument.hasField("_ingest.timestamp"));
-    }
-
-    public void testHasFieldNested() {
-        assertTrue(ingestDocument.hasField("fizz.buzz"));
-        assertTrue(ingestDocument.hasField("_source._ingest.timestamp"));
-    }
-
-    public void testListHasField() {
-        assertTrue(ingestDocument.hasField("list.0.field"));
-    }
-
-    public void testListHasFieldNull() {
-        assertTrue(ingestDocument.hasField("list.1"));
-    }
-
-    public void testListHasFieldIndexOutOfBounds() {
-        assertFalse(ingestDocument.hasField("list.10"));
-    }
-
-    public void testListHasFieldIndexNotNumeric() {
-        assertFalse(ingestDocument.hasField("list.test"));
-    }
-
-    public void testNestedHasFieldTypeMismatch() {
-        assertFalse(ingestDocument.hasField("foo.foo.bar"));
-    }
-
-    public void testHasFieldNotFound() {
-        assertFalse(ingestDocument.hasField("not.here"));
-    }
-
-    public void testHasFieldNotFoundNullParent() {
-        assertFalse(ingestDocument.hasField("fizz.foo_null.not_there"));
-    }
-
-    public void testHasFieldNestedNotFound() {
-        assertFalse(ingestDocument.hasField("fizz.doesnotexist"));
-    }
-
-    public void testHasFieldNull() {
-        try {
-            ingestDocument.hasField(null);
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldNullValue() {
-        assertTrue(ingestDocument.hasField("fizz.foo_null"));
-    }
-
-    public void testHasFieldEmpty() {
-        try {
-            ingestDocument.hasField("");
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldSourceObject() {
-        assertThat(ingestDocument.hasField("_source"), equalTo(false));
-    }
-
-    public void testHasFieldIngestObject() {
-        assertThat(ingestDocument.hasField("_ingest"), equalTo(true));
-    }
-
-    public void testHasFieldEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.hasField("_source.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.hasField("_ingest.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testSimpleSetFieldValue() {
-        ingestDocument.setFieldValue("new_field", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), equalTo("foo"));
-        ingestDocument.setFieldValue("_ttl", "ttl");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ttl"), equalTo("ttl"));
-        ingestDocument.setFieldValue("_source.another_field", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("another_field"), equalTo("bar"));
-        ingestDocument.setFieldValue("_ingest.new_field", "new_value");
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(2));
-        assertThat(ingestDocument.getIngestMetadata().get("new_field"), equalTo("new_value"));
-        ingestDocument.setFieldValue("_ingest.timestamp", "timestamp");
-        assertThat(ingestDocument.getIngestMetadata().get("timestamp"), equalTo("timestamp"));
-    }
-
-    public void testSetFieldValueNullValue() {
-        ingestDocument.setFieldValue("new_field", null);
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(true));
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), nullValue());
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testNestedSetFieldValue() {
-        ingestDocument.setFieldValue("a.b.c.d", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("a"), instanceOf(Map.class));
-        Map<String, Object> a = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("a");
-        assertThat(a.get("b"), instanceOf(Map.class));
-        Map<String, Object> b = (Map<String, Object>) a.get("b");
-        assertThat(b.get("c"), instanceOf(Map.class));
-        Map<String, Object> c = (Map<String, Object>) b.get("c");
-        assertThat(c.get("d"), instanceOf(String.class));
-        String d = (String) c.get("d");
-        assertThat(d, equalTo("foo"));
-    }
-
-    public void testSetFieldValueOnExistingField() {
-        ingestDocument.setFieldValue("foo", "newbar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("foo"), equalTo("newbar"));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testSetFieldValueOnExistingParent() {
-        ingestDocument.setFieldValue("fizz.new", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        Map<String, Object> innerMap = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(innerMap.get("new"), instanceOf(String.class));
-        String value = (String) innerMap.get("new");
-        assertThat(value, equalTo("bar"));
-    }
-
-    public void testSetFieldValueOnExistingParentTypeMismatch() {
-        try {
-            ingestDocument.setFieldValue("fizz.buzz.new", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [new] with parent object of type [java.lang.String] as part of path [fizz.buzz.new]"));
-        }
-    }
-
-    public void testSetFieldValueOnExistingNullParent() {
-        try {
-            ingestDocument.setFieldValue("fizz.foo_null.test", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [test] with null parent as part of path [fizz.foo_null.test]"));
-        }
-    }
-
-    public void testSetFieldValueNullName() {
-        try {
-            ingestDocument.setFieldValue(null, "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testSetSourceObject() {
-        ingestDocument.setFieldValue("_source", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetIngestObject() {
-        ingestDocument.setFieldValue("_ingest", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ingest"), equalTo("value"));
-    }
-
-    public void testSetIngestSourceObject() {
-        //test that we don't strip out the _source prefix when _ingest is used
-        ingestDocument.setFieldValue("_ingest._source", "value");
-        assertThat(ingestDocument.getIngestMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.setFieldValue("_source.", "value");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("_ingest.", Object.class);
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListSetFieldValueNoIndexProvided() {
-        ingestDocument.setFieldValue("list", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(String.class));
-        assertThat(object, equalTo("value"));
-    }
-
-    public void testListAppendFieldValue() {
-        ingestDocument.appendFieldValue("list", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo("new_value"));
-    }
-
-    public void testListSetFieldValueIndexProvided() {
-        ingestDocument.setFieldValue("list.1", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), equalTo("value"));
-    }
-
-    public void testSetFieldValueListAsPartOfPath() {
-        ingestDocument.setFieldValue("list.0.field", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "new_value")));
-        assertThat(list.get(1), nullValue());
-    }
-
-    public void testListSetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.setFieldValue("list.test", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.test.field", "new_value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListSetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.setFieldValue("list.10", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.10.field", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testSetFieldValueEmptyName() {
-        try {
-            ingestDocument.setFieldValue("", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveField() {
-        ingestDocument.removeField("foo");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("foo"), equalTo(false));
-        ingestDocument.removeField("_index");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(6));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_index"), equalTo(false));
-        ingestDocument.removeField("_source.fizz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(false));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(1));
-        ingestDocument.removeField("_ingest.timestamp");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(0));
-    }
-
-    public void testRemoveInnerField() {
-        ingestDocument.removeField("fizz.buzz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(map.size(), equalTo(2));
-        assertThat(map.containsKey("buzz"), equalTo(false));
-
-        ingestDocument.removeField("fizz.foo_null");
-        assertThat(map.size(), equalTo(1));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-
-        ingestDocument.removeField("fizz.1");
-        assertThat(map.size(), equalTo(0));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-    }
-
-    public void testRemoveNonExistingField() {
-        try {
-            ingestDocument.removeField("does_not_exist");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [does_not_exist] not present as part of path [does_not_exist]"));
-        }
-    }
-
-    public void testRemoveExistingParentTypeMismatch() {
-        try {
-            ingestDocument.removeField("foo.foo.bar");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testRemoveSourceObject() {
-        try {
-            ingestDocument.removeField("_source");
-            fail("remove field should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testRemoveIngestObject() {
-        ingestDocument.removeField("_ingest");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_ingest"), equalTo(false));
-    }
-
-    public void testRemoveEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.removeField("_source.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.removeField("_ingest.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListRemoveField() {
-        ingestDocument.removeField("list.0.field");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        object = list.get(0);
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        assertThat(map.size(), equalTo(0));
-        ingestDocument.removeField("list.0");
-        assertThat(list.size(), equalTo(1));
-        assertThat(list.get(0), nullValue());
-    }
-
-    public void testRemoveFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.removeField("fizz.foo_null.not_there");
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testNestedRemoveFieldTypeMismatch() {
-        try {
-            ingestDocument.removeField("fizz.1.bar");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [bar] from object of type [java.lang.String] as part of path [fizz.1.bar]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexNotNumeric() {
-        try {
-            ingestDocument.removeField("list.test");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexOutOfBounds() {
-        try {
-            ingestDocument.removeField("list.10");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-    }
-
-    public void testRemoveNullField() {
-        try {
-            ingestDocument.removeField(null);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveEmptyField() {
-        try {
-            ingestDocument.removeField("");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        IngestDocument ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        IngestDocument otherIngestDocument = new IngestDocument(otherSourceAndMetadata, otherIngestMetadata);
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            IngestDocument thirdIngestDocument = new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testDeepCopy() {
-        int iterations = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < iterations; i++) {
-            Map<String, Object> map = RandomDocumentPicks.randomSource(random());
-            Object copy = IngestDocument.deepCopy(map);
-            assertThat("iteration: " + i, copy, equalTo(map));
-            assertThat("iteration: " + i, copy, not(sameInstance(map)));
-        }
-    }
-
-    public void testDeepCopyDoesNotChangeProvidedMap() {
-        Map<String, Object> myPreciousMap = new HashMap<>();
-        myPreciousMap.put("field2", "value2");
-
-        IngestDocument ingestDocument = new IngestDocument("_index", "_type", "_id", null, null, null, null, new HashMap<>());
-        ingestDocument.setFieldValue("field1", myPreciousMap);
-        ingestDocument.removeField("field1.field2");
-
-        assertThat(myPreciousMap.size(), equalTo(1));
-        assertThat(myPreciousMap.get("field2"), equalTo("value2"));
-    }
-
-    public void testDeepCopyDoesNotChangeProvidedList() {
-        List<String> myPreciousList = new ArrayList<>();
-        myPreciousList.add("value");
-
-        IngestDocument ingestDocument = new IngestDocument("_index", "_type", "_id", null, null, null, null, new HashMap<>());
-        ingestDocument.setFieldValue("field1", myPreciousList);
-        ingestDocument.removeField("field1.0");
-
-        assertThat(myPreciousList.size(), equalTo(1));
-        assertThat(myPreciousList.get(0), equalTo("value"));
-    }
-
-    public void testIngestMetadataTimestamp() throws Exception {
-        long before = System.currentTimeMillis();
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        long after = System.currentTimeMillis();
-        String timestampString = ingestDocument.getIngestMetadata().get("timestamp");
-        assertThat(timestampString, notNullValue());
-        assertThat(timestampString, endsWith("+0000"));
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        Date timestamp = df.parse(timestampString);
-        assertThat(timestamp.getTime(), greaterThanOrEqualTo(before));
-        assertThat(timestamp.getTime(), lessThanOrEqualTo(after));
-    }
-
-    public void testCopyConstructor() {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        IngestDocument copy = new IngestDocument(ingestDocument);
-        assertThat(ingestDocument.getSourceAndMetadata(), not(sameInstance(copy.getSourceAndMetadata())));
-        assertThat(ingestDocument.getSourceAndMetadata(), equalTo(copy.getSourceAndMetadata()));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestRestIT.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestRestIT.java
deleted file mode 100644
index f6da5b5..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/IngestRestIT.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugin.ingest.IngestPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class IngestRestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestPlugin.class);
-    }
-
-    public IngestRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java
deleted file mode 100644
index 459f7a6..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/PipelineFactoryTests.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class PipelineFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
-        Processor processor = mock(Processor.class);
-        when(processor.getType()).thenReturn("test-processor");
-        Processor.Factory processorFactory = mock(Processor.Factory.class);
-        when(processorFactory.create(processorConfig)).thenReturn(processor);
-        processorRegistry.put("test", processorFactory);
-
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
-    }
-
-    public void testCreateUnusedProcessorOptions() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("unused", "value");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
-        Processor processor = mock(Processor.class);
-        when(processor.getType()).thenReturn("test-processor");
-        Processor.Factory processorFactory = mock(Processor.Factory.class);
-        when(processorFactory.create(processorConfig)).thenReturn(processor);
-        processorRegistry.put("test", processorFactory);
-        try {
-            factory.create("_id", pipelineConfig, processorRegistry);
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/RandomDocumentPicks.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/RandomDocumentPicks.java
deleted file mode 100644
index 73a5395..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/RandomDocumentPicks.java
+++ /dev/null
@@ -1,232 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.common.Strings;
-
-import java.util.*;
-
-public final class RandomDocumentPicks {
-
-    private RandomDocumentPicks() {
-
-    }
-
-    /**
-     * Returns a random field name. Can be a leaf field name or the
-     * path to refer to a field name using the dot notation.
-     */
-    public static String randomFieldName(Random random) {
-        int numLevels = RandomInts.randomIntBetween(random, 1, 5);
-        String fieldName = "";
-        for (int i = 0; i < numLevels; i++) {
-            if (i > 0) {
-                fieldName += ".";
-            }
-            fieldName += randomString(random);
-        }
-        return fieldName;
-    }
-
-    /**
-     * Returns a random leaf field name.
-     */
-    public static String randomLeafFieldName(Random random) {
-        String fieldName;
-        do {
-            fieldName = randomString(random);
-        } while (fieldName.contains("."));
-        return fieldName;
-    }
-
-    /**
-     * Returns a randomly selected existing field name out of the fields that are contained
-     * in the document provided as an argument.
-     */
-    public static String randomExistingFieldName(Random random, IngestDocument ingestDocument) {
-        Map<String, Object> source = new TreeMap<>(ingestDocument.getSourceAndMetadata());
-        Map.Entry<String, Object> randomEntry = RandomPicks.randomFrom(random, source.entrySet());
-        String key = randomEntry.getKey();
-        while (randomEntry.getValue() instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) randomEntry.getValue();
-            Map<String, Object> treeMap = new TreeMap<>(map);
-            randomEntry = RandomPicks.randomFrom(random, treeMap.entrySet());
-            key += "." + randomEntry.getKey();
-        }
-        assert ingestDocument.getFieldValue(key, Object.class) != null;
-        return key;
-    }
-
-    /**
-     * Adds a random non existing field to the provided document and associates it
-     * with the provided value. The field will be added at a random position within the document,
-     * not necessarily at the top level using a leaf field name.
-     */
-    public static String addRandomField(Random random, IngestDocument ingestDocument, Object value) {
-        String fieldName;
-        do {
-            fieldName = randomFieldName(random);
-        } while (canAddField(fieldName, ingestDocument) == false);
-        ingestDocument.setFieldValue(fieldName, value);
-        return fieldName;
-    }
-
-    /**
-     * Checks whether the provided field name can be safely added to the provided document.
-     * When the provided field name holds the path using the dot notation, we have to make sure
-     * that each node of the tree either doesn't exist or is a map, otherwise new fields cannot be added.
-     */
-    public static boolean canAddField(String path, IngestDocument ingestDocument) {
-        String[] pathElements = Strings.splitStringToArray(path, '.');
-        Map<String, Object> innerMap = ingestDocument.getSourceAndMetadata();
-        if (pathElements.length > 1) {
-            for (int i = 0; i < pathElements.length - 1; i++) {
-                Object currentLevel = innerMap.get(pathElements[i]);
-                if (currentLevel == null) {
-                    return true;
-                }
-                if (currentLevel instanceof Map == false) {
-                    return false;
-                }
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) currentLevel;
-                innerMap = map;
-            }
-        }
-        String leafKey = pathElements[pathElements.length - 1];
-        return innerMap.containsKey(leafKey) == false;
-    }
-
-    /**
-     * Generates a random document and random metadata
-     */
-    public static IngestDocument randomIngestDocument(Random random) {
-        return randomIngestDocument(random, randomSource(random));
-    }
-
-    /**
-     * Generates a document that holds random metadata and the document provided as a map argument
-     */
-    public static IngestDocument randomIngestDocument(Random random, Map<String, Object> source) {
-        String index = randomString(random);
-        String type = randomString(random);
-        String id = randomString(random);
-        String routing = null;
-        if (random.nextBoolean()) {
-            routing = randomString(random);
-        }
-        String parent = null;
-        if (random.nextBoolean()) {
-            parent = randomString(random);
-        }
-        String timestamp = null;
-        if (random.nextBoolean()) {
-            timestamp = randomString(random);
-        }
-        String ttl = null;
-        if (random.nextBoolean()) {
-            ttl = randomString(random);
-        }
-        return new IngestDocument(index, type, id, routing, parent, timestamp, ttl, source);
-    }
-
-    public static Map<String, Object> randomSource(Random random) {
-        Map<String, Object> document = new HashMap<>();
-        addRandomFields(random, document, 0);
-        return document;
-    }
-
-    /**
-     * Generates a random field value, can be a string, a number, a list of an object itself.
-     */
-    public static Object randomFieldValue(Random random) {
-        return randomFieldValue(random, 0);
-    }
-
-    private static Object randomFieldValue(Random random, int currentDepth) {
-        switch(RandomInts.randomIntBetween(random, 0, 8)) {
-            case 0:
-                return randomString(random);
-            case 1:
-                return random.nextInt();
-            case 2:
-                return random.nextBoolean();
-            case 3:
-                return random.nextDouble();
-            case 4:
-                List<String> stringList = new ArrayList<>();
-                int numStringItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numStringItems; j++) {
-                    stringList.add(randomString(random));
-                }
-                return stringList;
-            case 5:
-                List<Integer> intList = new ArrayList<>();
-                int numIntItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numIntItems; j++) {
-                    intList.add(random.nextInt());
-                }
-                return intList;
-            case 6:
-                List<Boolean> booleanList = new ArrayList<>();
-                int numBooleanItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numBooleanItems; j++) {
-                    booleanList.add(random.nextBoolean());
-                }
-                return booleanList;
-            case 7:
-                List<Double> doubleList = new ArrayList<>();
-                int numDoubleItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numDoubleItems; j++) {
-                    doubleList.add(random.nextDouble());
-                }
-                return doubleList;
-            case 8:
-                Map<String, Object> newNode = new HashMap<>();
-                addRandomFields(random, newNode, ++currentDepth);
-                return newNode;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    public static String randomString(Random random) {
-        if (random.nextBoolean()) {
-            return RandomStrings.randomAsciiOfLengthBetween(random, 1, 10);
-        }
-        return RandomStrings.randomUnicodeOfCodepointLengthBetween(random, 1, 10);
-    }
-
-    private static void addRandomFields(Random random, Map<String, Object> parentNode, int currentDepth) {
-        if (currentDepth > 5) {
-            return;
-        }
-        int numFields = RandomInts.randomIntBetween(random, 1, 10);
-        for (int i = 0; i < numFields; i++) {
-            String fieldName = randomLeafFieldName(random);
-            Object fieldValue = randomFieldValue(random, currentDepth);
-            parentNode.put(fieldName, fieldValue);
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
deleted file mode 100644
index 6bb2f9d..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class AbstractStringProcessorTestCase extends ESTestCase {
-
-    protected abstract AbstractStringProcessor newProcessor(String field);
-
-    protected String modifyInput(String input) {
-        return input;
-    }
-
-    protected abstract String expectedResult(String input);
-
-    public void testProcessor() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldValue = RandomDocumentPicks.randomString(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, modifyInput(fieldValue));
-        Processor processor = newProcessor(fieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult(fieldValue)));
-    }
-
-    public void testFieldNotFound() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testNullValue() throws Exception {
-        Processor processor = newProcessor("field");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot process it."));
-        }
-    }
-
-    public void testNonStringValue() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/ConfigurationUtilsTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/ConfigurationUtilsTests.java
deleted file mode 100644
index 38c4e24..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/ConfigurationUtilsTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-
-public class ConfigurationUtilsTests extends ESTestCase {
-    private Map<String, Object> config;
-
-    @Before
-    public void setConfig() {
-        config = new HashMap<>();
-        config.put("foo", "bar");
-        config.put("arr", Arrays.asList("1", "2", "3"));
-        List<Integer> list = new ArrayList<>();
-        list.add(2);
-        config.put("int", list);
-        config.put("ip", "127.0.0.1");
-        Map<String, Object> fizz = new HashMap<>();
-        fizz.put("buzz", "hello world");
-        config.put("fizz", fizz);
-    }
-
-    public void testReadStringProperty() {
-        String val = ConfigurationUtils.readStringProperty(config, "foo");
-        assertThat(val, equalTo("bar"));
-    }
-
-    public void testReadStringProperty_InvalidType() {
-        try {
-            ConfigurationUtils.readStringProperty(config, "arr");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [arr] isn't a string, but of type [java.util.Arrays$ArrayList]"));
-        }
-    }
-
-    // TODO(talevy): Issue with generics. This test should fail, "int" is of type List<Integer>
-    public void testOptional_InvalidType() {
-        List<String> val = ConfigurationUtils.readList(config, "int");
-        assertThat(val, equalTo(Arrays.asList(2)));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorFactoryTests.java
deleted file mode 100644
index 369e4d4..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorFactoryTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.convert;
-
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class ConvertProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        ConvertProcessor.Type type = randomFrom(ConvertProcessor.Type.values());
-        config.put("field", "field1");
-        config.put("type", type.toString());
-        ConvertProcessor convertProcessor = factory.create(config);
-        assertThat(convertProcessor.getField(), equalTo("field1"));
-        assertThat(convertProcessor.getConvertType(), equalTo(type));
-    }
-
-    public void testCreateUnsupportedType() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("field", "field1");
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("type [" + type + "] not supported, cannot convert field."));
-        }
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoTypePresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [type] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorTests.java
deleted file mode 100644
index fe56065..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/convert/ConvertProcessorTests.java
+++ /dev/null
@@ -1,265 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.convert;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.elasticsearch.ingest.processor.convert.ConvertProcessor.Type;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ConvertProcessorTests extends ESTestCase {
-
-    public void testConvertInt() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int randomInt = randomInt();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomInt);
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Integer.class), equalTo(randomInt));
-    }
-
-    public void testConvertIntList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Integer> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            int randomInt = randomInt();
-            fieldValue.add(Integer.toString(randomInt));
-            expectedList.add(randomInt);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertIntError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.INTEGER);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to integer"));
-        }
-    }
-
-    public void testConvertFloat() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Map<String, Float> expectedResult = new HashMap<>();
-        float randomFloat = randomFloat();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomFloat);
-        expectedResult.put(fieldName, randomFloat);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Float.class), equalTo(randomFloat));
-    }
-
-    public void testConvertFloatList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Float> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            float randomFloat = randomFloat();
-            fieldValue.add(Float.toString(randomFloat));
-            expectedList.add(randomFloat);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertFloatError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.FLOAT);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to float"));
-        }
-    }
-
-    public void testConvertBoolean() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Map<String, Type> fields = new HashMap<>();
-        Map<String, Boolean> expectedResult = new HashMap<>();
-        boolean randomBoolean = randomBoolean();
-        String booleanString = Boolean.toString(randomBoolean);
-        if (randomBoolean) {
-            booleanString = booleanString.toUpperCase(Locale.ROOT);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, booleanString);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Boolean.class), equalTo(randomBoolean));
-    }
-
-    public void testConvertBooleanList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Boolean> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            boolean randomBoolean = randomBoolean();
-            String booleanString = Boolean.toString(randomBoolean);
-            if (randomBoolean) {
-                booleanString = booleanString.toUpperCase(Locale.ROOT);
-            }
-            fieldValue.add(booleanString);
-            expectedList.add(randomBoolean);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertBooleanError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String fieldValue;
-        if (randomBoolean()) {
-            fieldValue = "string-" + randomAsciiOfLengthBetween(1, 10);
-        } else {
-            //verify that only proper boolean values are supported and we are strict about it
-            fieldValue = randomFrom("on", "off", "yes", "no", "0", "1");
-        }
-        ingestDocument.setFieldValue(fieldName, fieldValue);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.BOOLEAN);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(Exception e) {
-            assertThat(e.getMessage(), equalTo("[" + fieldValue + "] is not a boolean value, cannot convert to boolean"));
-        }
-    }
-
-    public void testConvertString() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Object fieldValue;
-        String expectedFieldValue;
-        switch(randomIntBetween(0, 2)) {
-            case 0:
-                float randomFloat = randomFloat();
-                fieldValue = randomFloat;
-                expectedFieldValue = Float.toString(randomFloat);
-                break;
-            case 1:
-                int randomInt = randomInt();
-                fieldValue = randomInt;
-                expectedFieldValue = Integer.toString(randomInt);
-                break;
-            case 2:
-                boolean randomBoolean = randomBoolean();
-                fieldValue = randomBoolean;
-                expectedFieldValue = Boolean.toString(randomBoolean);
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-
-        Processor processor = new ConvertProcessor(fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedFieldValue));
-    }
-
-    public void testConvertStringList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<Object> fieldValue = new ArrayList<>();
-        List<String> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            Object randomValue;
-            String randomValueString;
-            switch(randomIntBetween(0, 2)) {
-                case 0:
-                    float randomFloat = randomFloat();
-                    randomValue = randomFloat;
-                    randomValueString = Float.toString(randomFloat);
-                    break;
-                case 1:
-                    int randomInt = randomInt();
-                    randomValue = randomInt;
-                    randomValueString = Integer.toString(randomInt);
-                    break;
-                case 2:
-                    boolean randomBoolean = randomBoolean();
-                    randomValue = randomBoolean;
-                    randomValueString = Boolean.toString(randomBoolean);
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-            fieldValue.add(randomValue);
-            expectedList.add(randomValueString);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor(fieldName, type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testConvertNullField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor("field", type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Field [field] is null, cannot be converted to type [" + type + "]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateFormatTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateFormatTests.java
deleted file mode 100644
index a3f5c33..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateFormatTests.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.date;
-
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.time.Instant;
-import java.time.ZoneId;
-import java.time.format.DateTimeFormatter;
-import java.util.Locale;
-import java.util.Optional;
-import java.util.function.Function;
-
-import static org.hamcrest.core.IsEqual.equalTo;
-
-public class DateFormatTests extends ESTestCase {
-
-    public void testParseJoda() {
-        Function<String, DateTime> jodaFunction = DateFormat.getJodaFunction("MMM dd HH:mm:ss Z", DateTimeZone.forOffsetHours(-8), Locale.ENGLISH);
-        assertThat(Instant.ofEpochMilli(jodaFunction.apply("Nov 24 01:29:01 -0800").getMillis())
-                        .atZone(ZoneId.of("GMT-8"))
-                        .format(DateTimeFormatter.ofPattern("MM dd HH:mm:ss", Locale.ENGLISH)),
-                equalTo("11 24 01:29:01"));
-    }
-
-    public void testParseUnixMs() {
-        assertThat(DateFormat.UnixMs.getFunction(DateTimeZone.UTC).apply("1000500").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseUnix() {
-        assertThat(DateFormat.Unix.getFunction(DateTimeZone.UTC).apply("1000.5").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseISO8601() {
-        assertThat(DateFormat.Iso8601.getFunction(DateTimeZone.UTC).apply("2001-01-01T00:00:00-0800").getMillis(), equalTo(978336000000L));
-    }
-
-    public void testParseISO8601Failure() {
-        Function<String, DateTime> function = DateFormat.Iso8601.getFunction(DateTimeZone.UTC);
-        try {
-            function.apply("2001-01-0:00-0800");
-            fail("parse should have failed");
-        } catch(IllegalArgumentException e) {
-            //all good
-        }
-    }
-
-    public void testTAI64NParse() {
-        String input = "4000000050d506482dbdf024";
-        String expected = "2012-12-22T03:00:46.767+02:00";
-        assertThat(DateFormat.Tai64n.getFunction(DateTimeZone.forOffsetHours(2)).apply((randomBoolean() ? "@" : "") + input).toString(), equalTo(expected));
-    }
-
-    public void testFromString() {
-        assertThat(DateFormat.fromString("UNIX_MS"), equalTo(Optional.of(DateFormat.UnixMs)));
-        assertThat(DateFormat.fromString("unix_ms"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("UNIX"), equalTo(Optional.of(DateFormat.Unix)));
-        assertThat(DateFormat.fromString("unix"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("ISO8601"), equalTo(Optional.of(DateFormat.Iso8601)));
-        assertThat(DateFormat.fromString("iso8601"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("TAI64N"), equalTo(Optional.of(DateFormat.Tai64n)));
-        assertThat(DateFormat.fromString("tai64n"), equalTo(Optional.empty()));
-        assertThat(DateFormat.fromString("prefix-" + randomAsciiOfLengthBetween(1, 10)), equalTo(Optional.empty()));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorFactoryTests.java
deleted file mode 100644
index 62f726c..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorFactoryTests.java
+++ /dev/null
@@ -1,150 +0,0 @@
-/*
- * Licensed to ElasticSearch and Shay Banon under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. ElasticSearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.date;
-
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTimeZone;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class DateProcessorFactoryTests extends ESTestCase {
-
-    public void testBuildDefaults() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo(sourceField));
-        assertThat(processor.getTargetField(), equalTo(DateProcessor.DEFAULT_TARGET_FIELD));
-        assertThat(processor.getMatchFormats(), equalTo(Collections.singletonList("dd/MM/yyyyy")));
-        assertThat(processor.getLocale(), equalTo(Locale.ENGLISH));
-        assertThat(processor.getTimezone(), equalTo(DateTimeZone.UTC));
-    }
-
-    public void testMatchFieldIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("target_field", targetField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_field] is missing"));
-        }
-    }
-
-    public void testMatchFormatsIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_formats] is missing"));
-        }
-    }
-
-    public void testParseLocale() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        Locale locale = randomLocale(random());
-        config.put("locale", locale.toLanguageTag());
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getLocale().toLanguageTag(), equalTo(locale.toLanguageTag()));
-    }
-
-    public void testParseTimezone() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        DateTimeZone timezone = randomTimezone();
-        config.put("timezone", timezone.getID());
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTimezone(), equalTo(timezone));
-    }
-
-    //we generate a timezone out of the available ones in joda, some available in the jdk are not available in joda by default
-    private static DateTimeZone randomTimezone() {
-        List<String> ids = new ArrayList<>(DateTimeZone.getAvailableIDs());
-        Collections.sort(ids);
-        return DateTimeZone.forID(randomFrom(ids));
-    }
-
-
-    public void testParseMatchFormats() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getMatchFormats(), equalTo(Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy")));
-    }
-
-    public void testParseMatchFormatsFailure() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", "dd/MM/yyyy");
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("property [match_formats] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-
-    public void testParseTargetField() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTargetField(), equalTo(targetField));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorTests.java
deleted file mode 100644
index 86e4017..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/date/DateProcessorTests.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/*
- * Licensed to ElasticSearch and Shay Banon under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. ElasticSearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.date;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.util.*;
-
-import static org.hamcrest.CoreMatchers.containsString;
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class DateProcessorTests extends ESTestCase {
-
-    public void testJodaPattern() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("yyyy dd MM hh:mm:ss"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06 11:05:15");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T11:05:15.000+02:00"));
-    }
-
-    public void testJodaPatternMultipleFormats() {
-        List<String> matchFormats = new ArrayList<>();
-        matchFormats.add("yyyy dd MM");
-        matchFormats.add("dd/MM/yyyy");
-        matchFormats.add("dd-MM-yyyy");
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", matchFormats, "date_as_date");
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12/06/2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12-06-2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        try {
-            dateProcessor.execute(ingestDocument);
-            fail("processor should have failed due to not supported date format");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("unable to parse date [2010]"));
-        }
-    }
-
-    public void testJodaPatternLocale() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ITALIAN,
-                "date_as_string", Collections.singletonList("yyyy dd MMM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 giugno");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testJodaPatternDefaultYear() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("dd/MM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "12/06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo(DateTime.now().getYear() + "-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testTAI64N() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.forOffsetHours(2), randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.Tai64n.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        String dateAsString = (randomBoolean() ? "@" : "") + "4000000050d506482dbdf024";
-        document.put("date_as_string", dateAsString);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2012-12-22T03:00:46.767+02:00"));
-    }
-
-    public void testUnixMs() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.UnixMs.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000500");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-
-    public void testUnix() {
-        DateProcessor dateProcessor = new DateProcessor(DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList(DateFormat.Unix.toString()), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000.5");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorFactoryTests.java
deleted file mode 100644
index d42f87d..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorFactoryTests.java
+++ /dev/null
@@ -1,138 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.geoip;
-
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.StreamsUtils;
-import org.junit.Before;
-
-import java.io.ByteArrayInputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.sameInstance;
-import static org.hamcrest.Matchers.startsWith;
-
-public class GeoIpProcessorFactoryTests extends ESTestCase {
-
-    private Path configDir;
-
-    @Before
-    public void prepareConfigDirectory() throws Exception {
-        this.configDir = createTempDir();
-        Path geoIpConfigDir = configDir.resolve("ingest").resolve("geoip");
-        Files.createDirectories(geoIpConfigDir);
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-City.mmdb")), geoIpConfigDir.resolve("GeoLite2-City.mmdb"));
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-Country.mmdb")), geoIpConfigDir.resolve("GeoLite2-Country.mmdb"));
-    }
-
-    public void testBuild_defaults() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-City"));
-        assertThat(processor.getFields(), sameInstance(GeoIpProcessor.Factory.DEFAULT_FIELDS));
-    }
-
-    public void testBuild_targetField() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("target_field", "_field");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("_field"));
-    }
-
-    public void testBuild_dbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-Country"));
-    }
-
-    public void testBuild_nonExistingDbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "does-not-exist.mmdb");
-        try {
-            factory.create(config);
-            fail("Exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("database file [does-not-exist.mmdb] doesn't exist"));
-        }
-    }
-
-    public void testBuild_fields() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Set<GeoIpProcessor.Field> fields = EnumSet.noneOf(GeoIpProcessor.Field.class);
-        List<String> fieldNames = new ArrayList<>();
-        int numFields = scaledRandomIntBetween(1, GeoIpProcessor.Field.values().length);
-        for (int i = 0; i < numFields; i++) {
-            GeoIpProcessor.Field field = GeoIpProcessor.Field.values()[i];
-            fields.add(field);
-            fieldNames.add(field.name().toLowerCase(Locale.ROOT));
-        }
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", fieldNames);
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getFields(), equalTo(fields));
-    }
-
-    public void testBuild_illegalFieldOption() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", Collections.singletonList("invalid"));
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("illegal field option [invalid]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]"));
-        }
-
-        config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", "invalid");
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [fields] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorTests.java
deleted file mode 100644
index 479541b..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/geoip/GeoIpProcessorTests.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.InputStream;
-import java.util.Arrays;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class GeoIpProcessorTests extends ESTestCase {
-
-    public void testCity() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(10));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-        assertThat(geoData.get("region_name"), equalTo("North Holland"));
-        assertThat(geoData.get("city_name"), equalTo("Amsterdam"));
-        assertThat(geoData.get("timezone"), equalTo("Europe/Amsterdam"));
-        assertThat(geoData.get("latitude"), equalTo(52.374));
-        assertThat(geoData.get("longitude"), equalTo(4.8897));
-        assertThat(geoData.get("location"), equalTo(Arrays.asList(4.8897d, 52.374d)));
-    }
-
-    public void testCountry() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(4));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-    }
-
-    public void testAddressIsNotInTheDatabase() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor("source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "202.45.11.11");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(0));
-    }
-
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorFactoryTests.java
deleted file mode 100644
index e8d44d3..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorFactoryTests.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class GrokProcessorFactoryTests extends ESTestCase {
-
-    private Path configDir;
-
-    @Before
-    public void prepareConfigDirectory() throws Exception {
-        this.configDir = createTempDir();
-        Path grokDir = configDir.resolve("ingest").resolve("grok");
-        Path patternsDir = grokDir.resolve("patterns");
-        Files.createDirectories(patternsDir);
-    }
-
-    public void testBuild() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "(?<foo>\\w+)");
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-    }
-
-    public void testCreateWithCustomPatterns() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory(configDir);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "%{MY_PATTERN:name}!");
-        config.put("pattern_definitions", Collections.singletonMap("MY_PATTERN", "foo"));
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-        assertThat(processor.getGrok().match("foo!"), equalTo(true));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorTests.java
deleted file mode 100644
index cb20c58..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokProcessorTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-
-public class GrokProcessorTests extends ESTestCase {
-
-    public void testMatch() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "1");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        processor.execute(doc);
-        assertThat(doc.getFieldValue("one", String.class), equalTo("1"));
-    }
-
-    public void testNoMatch() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "23");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("Grok expression does not match field value: [23]"));
-        }
-    }
-
-    public void testNotStringField() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, 1);
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testMissingField() {
-        String fieldName = "foo.bar";
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [foo] not present as part of path [foo.bar]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokTests.java
deleted file mode 100644
index b73a8e0..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/grok/GrokTests.java
+++ /dev/null
@@ -1,291 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.grok;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.lang.Object;
-import java.lang.String;
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-
-
-public class GrokTests extends ESTestCase {
-    private Map<String, String> basePatterns;
-
-    private Map<String, String> newBankFromStreams(InputStream... inputStreams) throws IOException {
-        Map<String, String> patternBank = new HashMap<>();
-
-        for (InputStream is : inputStreams) {
-            GrokProcessor.Factory.loadBankFromStream(patternBank, is);
-        }
-
-        return patternBank;
-    }
-
-    @Before
-    public void setup() throws IOException {
-        basePatterns = newBankFromStreams(
-                getClass().getResourceAsStream("/grok/patterns/grok-patterns"),
-                getClass().getResourceAsStream("/grok/patterns/linux-syslog")
-        );
-    }
-
-    public void testSimpleSyslogLine() {
-        String line = "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]";
-        Grok grok = new Grok(basePatterns, "%{SYSLOGLINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("evita", matches.get("logsource"));
-        assertEquals("Mar 16 00:01:25", matches.get("timestamp"));
-        assertEquals("connect from camomile.cloud9.net[168.100.1.3]", matches.get("message"));
-        assertEquals("postfix/smtpd", matches.get("program"));
-        assertEquals("1713", matches.get("pid"));
-    }
-
-    public void testSyslog5424Line() {
-        String line = "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"] Hello, syslog.";
-        Grok grok = new Grok(basePatterns, "%{SYSLOG5424LINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("191", matches.get("syslog5424_pri"));
-        assertEquals("1", matches.get("syslog5424_ver"));
-        assertEquals("2009-06-30T18:30:00+02:00", matches.get("syslog5424_ts"));
-        assertEquals("paxton.local", matches.get("syslog5424_host"));
-        assertEquals("grokdebug", matches.get("syslog5424_app"));
-        assertEquals("4123", matches.get("syslog5424_proc"));
-        assertEquals(null, matches.get("syslog5424_msgid"));
-        assertEquals("[id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"]", matches.get("syslog5424_sd"));
-        assertEquals("Hello, syslog.", matches.get("syslog5424_msg"));
-    }
-
-    public void testDatePattern() {
-        String line = "fancy 12-12-12 12:12:12";
-        Grok grok = new Grok(basePatterns, "(?<timestamp>%{DATE_EU} %{TIME})");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("12-12-12 12:12:12", matches.get("timestamp"));
-    }
-
-    public void testNilCoercedValues() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration:float}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals(28.4f, matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testNilWithNoCoercion() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals("28.4", matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testUnicodeSyslog() {
-        Grok grok = new Grok(basePatterns, "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}");
-        Map<String, Object> matches = grok.captures("<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 �all'): Envelope-from: email@domain.no");
-        assertThat(matches.get("syslog_pri"), equalTo("22"));
-        assertThat(matches.get("syslog_program"), equalTo("postfix/policy-spf"));
-        assertThat(matches.get("tags"), nullValue());
-    }
-
-    public void testNamedFieldsWithWholeTextMatch() {
-        Grok grok = new Grok(basePatterns, "%{DATE_EU:stimestamp}");
-        Map<String, Object> matches = grok.captures("11/01/01");
-        assertThat(matches.get("stimestamp"), equalTo("11/01/01"));
-    }
-
-    public void testWithOniguramaNamedCaptures() {
-        Grok grok = new Grok(basePatterns, "(?<foo>\\w+)");
-        Map<String, Object> matches = grok.captures("hello world");
-        assertThat(matches.get("foo"), equalTo("hello"));
-    }
-
-    public void testISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-01-01T00:00:00",
-                "1974-03-02T04:09:09",
-                "2010-05-03T08:18:18+00:00",
-                "2004-07-04T12:27:27-00:00",
-                "2001-09-05T16:36:36+0000",
-                "2001-11-06T20:45:45-0000",
-                "2001-12-07T23:54:54Z",
-                "2001-01-01T00:00:00.123456",
-                "1974-03-02T04:09:09.123456",
-                "2010-05-03T08:18:18.123456+00:00",
-                "2004-07-04T12:27:27.123456-00:00",
-                "2001-09-05T16:36:36.123456+0000",
-                "2001-11-06T20:45:45.123456-0000",
-                "2001-12-07T23:54:54.123456Z",
-                "2001-12-07T23:54:60.123456Z" // '60' second is a leap second.
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(true));
-        }
-    }
-
-    public void testNotISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-13-01T00:00:00", // invalid month
-                "2001-00-01T00:00:00", // invalid month
-                "2001-01-00T00:00:00", // invalid day
-                "2001-01-32T00:00:00", // invalid day
-                "2001-01-aT00:00:00", // invalid day
-                "2001-01-1aT00:00:00", // invalid day
-                "2001-01-01Ta0:00:00", // invalid hour
-                "2001-01-01T0:00:00", // invalid hour
-                "2001-01-01T25:00:00", // invalid hour
-                "2001-01-01T01:60:00", // invalid minute
-                "2001-01-01T00:aa:00", // invalid minute
-                "2001-01-01T00:00:aa", // invalid second
-                "2001-01-01T00:00:-1", // invalid second
-                "2001-01-01T00:00:61", // invalid second
-                "2001-01-01T00:00:00A", // invalid timezone
-                "2001-01-01T00:00:00+", // invalid timezone
-                "2001-01-01T00:00:00+25", // invalid timezone
-                "2001-01-01T00:00:00+2500", // invalid timezone
-                "2001-01-01T00:00:00+25:00", // invalid timezone
-                "2001-01-01T00:00:00-25", // invalid timezone
-                "2001-01-01T00:00:00-2500", // invalid timezone
-                "2001-01-01T00:00:00-00:61" // invalid timezone
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(false));
-        }
-    }
-
-    public void testNoNamedCaptures() {
-        Map<String, String> bank = new HashMap<>();
-
-        bank.put("NAME", "Tal");
-        bank.put("EXCITED_NAME", "!!!%{NAME:name}!!!");
-        bank.put("TEST", "hello world");
-
-        String text = "wowza !!!Tal!!! - Tal";
-        String pattern = "%{EXCITED_NAME} - %{NAME}";
-        Grok g = new Grok(bank, pattern, false);
-
-        assertEquals("(?<EXCITED_NAME_0>!!!(?<NAME_21>Tal)!!!) - (?<NAME_22>Tal)", g.toRegex(pattern));
-        assertEquals(true, g.match(text));
-
-        Object actual = g.captures(text);
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("EXCITED_NAME_0", "!!!Tal!!!");
-        expected.put("NAME_21", "Tal");
-        expected.put("NAME_22", "Tal");
-        assertEquals(expected, actual);
-    }
-
-    public void testNumericCapturesCoercion() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-
-        String pattern = "%{NUMBER:bytes:float} %{NUMBER:status} %{NUMBER}";
-        Grok g = new Grok(bank, pattern);
-
-        String text = "12009.34 200 9032";
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("bytes", 12009.34f);
-        expected.put("status", "200");
-        Map<String, Object> actual = g.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testApacheLog() {
-        String logLine = "31.184.238.164 - - [24/Jul/2014:05:35:37 +0530] \"GET /logs/access.log HTTP/1.0\" 200 69849 \"http://8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\" \"www.dlwindianrailways.com\"";
-        Grok grok = new Grok(basePatterns, "%{COMBINEDAPACHELOG}");
-        Map<String, Object> matches = grok.captures(logLine);
-
-        assertEquals("31.184.238.164", matches.get("clientip"));
-        assertEquals("-", matches.get("ident"));
-        assertEquals("-", matches.get("auth"));
-        assertEquals("24/Jul/2014:05:35:37 +0530", matches.get("timestamp"));
-        assertEquals("GET", matches.get("verb"));
-        assertEquals("/logs/access.log", matches.get("request"));
-        assertEquals("1.0", matches.get("httpversion"));
-        assertEquals("200", matches.get("response"));
-        assertEquals("69849", matches.get("bytes"));
-        assertEquals("\"http://8rursodiol.enjin.com\"", matches.get("referrer"));
-        assertEquals(null, matches.get("port"));
-        assertEquals("\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\"", matches.get("agent"));
-    }
-
-    public void testComplete() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        bank.put("MONTH", "\\b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\\b");
-        bank.put("MINUTE", "(?:[0-5][0-9])");
-        bank.put("YEAR", "(?>\\d\\d){1,2}");
-        bank.put("HOUR", "(?:2[0123]|[01]?[0-9])");
-        bank.put("SECOND", "(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)");
-        bank.put("TIME", "(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])");
-        bank.put("INT", "(?:[+-]?(?:[0-9]+))");
-        bank.put("HTTPDATE", "%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}");
-        bank.put("WORD", "\\b\\w+\\b");
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-        bank.put("IPV6", "((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?");
-        bank.put("IPV4", "(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])");
-        bank.put("IP", "(?:%{IPV6}|%{IPV4})");
-        bank.put("HOSTNAME", "\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)");
-        bank.put("IPORHOST", "(?:%{IP}|%{HOSTNAME})");
-        bank.put("USER", "[a-zA-Z0-9._-]+");
-        bank.put("DATA", ".*?");
-        bank.put("QS", "(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))");
-
-        String text = "83.149.9.216 - - [19/Jul/2015:08:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1\" 200 171717 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"";
-        String pattern = "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}";
-
-        Grok grok = new Grok(bank, pattern);
-
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("clientip", "83.149.9.216");
-        expected.put("ident", "-");
-        expected.put("auth", "-");
-        expected.put("timestamp", "19/Jul/2015:08:13:42 +0000");
-        expected.put("verb", "GET");
-        expected.put("request", "/presentations/logstash-monitorama-2013/images/kibana-dashboard3.png");
-        expected.put("httpversion", "1.1");
-        expected.put("response", 200);
-        expected.put("bytes", 171717);
-        expected.put("referrer", "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"");
-        expected.put("agent", "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"");
-
-        Map<String, Object> actual = grok.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testNoMatch() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        Grok grok = new Grok(bank, "%{MONTHDAY:greatday}");
-        assertThat(grok.captures("nomatch"), nullValue());
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorFactoryTests.java
deleted file mode 100644
index e1e085d..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorFactoryTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.gsub;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class GsubProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        GsubProcessor gsubProcessor = factory.create(config);
-        assertThat(gsubProcessor.getField(), equalTo("field1"));
-        assertThat(gsubProcessor.getPattern().toString(), equalTo("\\."));
-        assertThat(gsubProcessor.getReplacement(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoPatternPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [pattern] is missing"));
-        }
-    }
-
-    public void testCreateNoReplacementPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [replacement] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorTests.java
deleted file mode 100644
index 8eb5be7..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/gsub/GsubProcessorTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.gsub;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.regex.Pattern;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class GsubProcessorTests extends ESTestCase {
-
-    public void testGsub() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo("127-0-0-1"));
-    }
-
-    public void testGsubNotAStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, 123);
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testGsubFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new GsubProcessor(fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testGsubNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new GsubProcessor("field", Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot match pattern."));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorFactoryTests.java
deleted file mode 100644
index deebe50..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorFactoryTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.join;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class JoinProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "-");
-        JoinProcessor joinProcessor = factory.create(config);
-        assertThat(joinProcessor.getField(), equalTo("field1"));
-        assertThat(joinProcessor.getSeparator(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorTests.java
deleted file mode 100644
index df6c835..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/join/JoinProcessorTests.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.join;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class JoinProcessorTests extends ESTestCase {
-
-    private static final String[] SEPARATORS = new String[]{"-", "_", "."};
-
-    public void testJoinStrings() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<String> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            String value = randomAsciiOfLengthBetween(1, 10);
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinIntegers() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<Integer> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            int value = randomInt();
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinNonListField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomAsciiOfLengthBetween(1, 10));
-        Processor processor = new JoinProcessor(fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.String] cannot be cast to [java.util.List]"));
-        }
-    }
-
-    public void testJoinNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new JoinProcessor(fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testJoinNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new JoinProcessor("field", "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot join."));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorFactoryTests.java
deleted file mode 100644
index 34864e3..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.lowercase;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class LowercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        LowercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorTests.java
deleted file mode 100644
index 6e85b33..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/lowercase/LowercaseProcessorTests.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.lowercase;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-import org.elasticsearch.ingest.processor.AbstractStringProcessorTestCase;
-
-import java.util.Collection;
-import java.util.Locale;
-
-public class LowercaseProcessorTests extends AbstractStringProcessorTestCase {
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new LowercaseProcessor(field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toLowerCase(Locale.ROOT);
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorFactoryTests.java
deleted file mode 100644
index ee4cb02..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorFactoryTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package org.elasticsearch.ingest.processor.meta;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheException;
-import org.elasticsearch.common.io.FastStringReader;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.ingest.IngestDocument.MetaData;
-
-public class MetaDataProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        MetaDataProcessor.Factory factory = new MetaDataProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        for (MetaData metaData : MetaData.values()) {
-            config.put(metaData.getFieldName(), randomBoolean() ? "static text" : "{{expression}}");
-        }
-        MetaDataProcessor processor = factory.create(config);
-        assertThat(processor.getTemplates().size(), Matchers.equalTo(7));
-        assertThat(processor.getTemplates().get(MetaData.INDEX), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.TIMESTAMP), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.ID), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.ROUTING), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.PARENT), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.TIMESTAMP), Matchers.notNullValue());
-        assertThat(processor.getTemplates().get(MetaData.TTL), Matchers.notNullValue());
-    }
-
-    public void testCreateIllegalMetaData() throws Exception {
-        MetaDataProcessor.Factory factory = new MetaDataProcessor.Factory();
-        try {
-            factory.create(Collections.singletonMap("_field", "text {{expression}}"));
-            fail("exception should have been thrown");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("no valid metadata field name [_field]"));
-        }
-    }
-
-    public void testCreateIllegalEmpty() throws Exception {
-        MetaDataProcessor.Factory factory = new MetaDataProcessor.Factory();
-        try {
-            factory.create(Collections.emptyMap());
-            fail("exception should have been thrown");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("no meta fields specified"));
-        }
-    }
-
-    public void testIlegalMustacheExpression() throws Exception {
-        try {
-            new MetaDataProcessor.Factory().create(Collections.singletonMap("_index", "text {{var"));
-            fail("exception expected");
-        } catch (MustacheException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("Improperly closed variable in :1"));
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorTests.java
deleted file mode 100644
index c102849..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/meta/MetaDataProcessorTests.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.elasticsearch.ingest.processor.meta;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import org.elasticsearch.common.io.FastStringReader;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.ingest.IngestDocument.*;
-
-public class MetaDataProcessorTests extends ESTestCase {
-
-    public void testExecute() throws Exception {
-        Map<IngestDocument.MetaData, Mustache> templates = new HashMap<>();
-        for (MetaData metaData : MetaData.values()) {
-            templates.put(metaData, new DefaultMustacheFactory().compile(new FastStringReader("some {{field}}"), "noname"));
-        }
-
-        MetaDataProcessor processor = new MetaDataProcessor(templates);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", "value"));
-        processor.execute(ingestDocument);
-
-        Map<MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        for (MetaData metaData : MetaData.values()) {
-            assertThat(metadataMap.get(metaData), Matchers.equalTo("some value"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorFactoryTests.java
deleted file mode 100644
index f45f3bc..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorFactoryTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.remove;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RemoveProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        RemoveProcessor.Factory factory = new RemoveProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        RemoveProcessor removeProcessor = factory.create(config);
-        assertThat(removeProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        RemoveProcessor.Factory factory = new RemoveProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorTests.java
deleted file mode 100644
index 2ccfd5a..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/remove/RemoveProcessorTests.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.remove;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class RemoveProcessorTests extends ESTestCase {
-
-    public void testRemoveFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String field = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RemoveProcessor(field);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(field), equalTo(false));
-    }
-
-    public void testRemoveNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RemoveProcessor(fieldName);
-        try {
-            processor.execute(ingestDocument);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorFactoryTests.java
deleted file mode 100644
index eba08ad..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorFactoryTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.rename;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RenameProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        config.put("to", "new_field");
-        RenameProcessor renameProcessor = factory.create(config);
-        assertThat(renameProcessor.getOldFieldName(), equalTo("old_field"));
-        assertThat(renameProcessor.getNewFieldName(), equalTo("new_field"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("to", "new_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoToPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [to] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorTests.java
deleted file mode 100644
index 3968a2e..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/rename/RenameProcessorTests.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.rename;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-
-public class RenameProcessorTests extends ESTestCase {
-
-    public void testRename() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = ingestDocument.getFieldValue(fieldName, Object.class);
-        String newFieldName;
-        do {
-            newFieldName = RandomDocumentPicks.randomFieldName(random());
-        } while (RandomDocumentPicks.canAddField(newFieldName, ingestDocument) == false || newFieldName.equals(fieldName));
-        Processor processor = new RenameProcessor(fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testRenameArrayElement() throws Exception {
-        Map<String, Object> document = new HashMap<>();
-        List<String> list = new ArrayList<>();
-        list.add("item1");
-        list.add("item2");
-        list.add("item3");
-        document.put("list", list);
-        List<Map<String, String>> one = new ArrayList<>();
-        one.add(Collections.singletonMap("one", "one"));
-        one.add(Collections.singletonMap("two", "two"));
-        document.put("one", one);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-
-        Processor processor = new RenameProcessor("list.0", "item");
-        processor.execute(ingestDocument);
-        Object actualObject = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(actualObject, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> actualList = (List<String>) actualObject;
-        assertThat(actualList.size(), equalTo(2));
-        assertThat(actualList.get(0), equalTo("item2"));
-        assertThat(actualList.get(1), equalTo("item3"));
-        actualObject = ingestDocument.getSourceAndMetadata().get("item");
-        assertThat(actualObject, instanceOf(String.class));
-        assertThat(actualObject, equalTo("item1"));
-
-        processor = new RenameProcessor("list.0", "list.3");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[3] is out of bounds for array with length [2] as part of path [list.3]"));
-            assertThat(actualList.size(), equalTo(2));
-            assertThat(actualList.get(0), equalTo("item2"));
-            assertThat(actualList.get(1), equalTo("item3"));
-        }
-    }
-
-    public void testRenameNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(fieldName, RandomDocumentPicks.randomFieldName(random()));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] doesn't exist"));
-        }
-    }
-
-    public void testRenameNewFieldAlreadyExists() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RenameProcessor(RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument), fieldName);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] already exists"));
-        }
-    }
-
-    public void testRenameExistingFieldNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, null);
-        String newFieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(false));
-        assertThat(ingestDocument.hasField(newFieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), nullValue());
-    }
-
-    public void testRenameAtomicOperationSetFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            private static final long serialVersionUID = 362498820763181265L;
-            @Override
-            public Object put(String key, Object value) {
-                if (key.equals("new_field")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.put(key, value);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor("list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-
-    public void testRenameAtomicOperationRemoveFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            private static final long serialVersionUID = 362498820763181265L;
-            @Override
-            public Object remove(Object key) {
-                if (key.equals("list")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.remove(key);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor("list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorFactoryTests.java
deleted file mode 100644
index 9eb6b2a..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorFactoryTests.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.set;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SetProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", "value1");
-        SetProcessor setProcessor = factory.create(config);
-        assertThat(setProcessor.getField(), equalTo("field1"));
-        assertThat(setProcessor.getValue(), equalTo("value1"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("value", "value1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoValuePresent() throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-    public void testCreateNullValue() throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", null);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorTests.java
deleted file mode 100644
index 7d69306..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/set/SetProcessorTests.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.set;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class SetProcessorTests extends ESTestCase {
-
-    public void testSetExistingFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        Processor processor = new SetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetNewFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        //used to verify that there are no conflicts between subsequent fields going to be added
-        IngestDocument testIngestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), testIngestDocument, fieldValue);
-        Processor processor = new SetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetFieldsTypeMismatch() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue("field", "value");
-        Processor processor = new SetProcessor("field.inner", "value");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [inner] with parent object of type [java.lang.String] as part of path [field.inner]"));
-        }
-    }
-}
\ No newline at end of file
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorFactoryTests.java
deleted file mode 100644
index 4d6634b..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorFactoryTests.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.split;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SplitProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "\\.");
-        SplitProcessor splitProcessor = factory.create(config);
-        assertThat(splitProcessor.getField(), equalTo("field1"));
-        assertThat(splitProcessor.getSeparator(), equalTo("\\."));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorTests.java
deleted file mode 100644
index 594ba3b..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/split/SplitProcessorTests.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.split;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SplitProcessorTests extends ESTestCase {
-
-    public void testSplit() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(Arrays.asList("127", "0", "0", "1")));
-    }
-
-    public void testSplitFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testSplitNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new SplitProcessor("field", "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot split."));
-        }
-    }
-
-    public void testSplitNonStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        Processor processor = new SplitProcessor(fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorFactoryTests.java
deleted file mode 100644
index 169ebda..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorFactoryTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.trim;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class TrimProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        TrimProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorTests.java
deleted file mode 100644
index eea867e..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/trim/TrimProcessorTests.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.trim;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-import org.elasticsearch.ingest.processor.AbstractStringProcessorTestCase;
-
-public class TrimProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new TrimProcessor(field);
-    }
-
-    @Override
-    protected String modifyInput(String input) {
-        String updatedFieldValue = "";
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        updatedFieldValue += input;
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        return updatedFieldValue;
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.trim();
-    }
-
-    private static String addWhitespaces(String input) {
-        int prefixLength = randomIntBetween(0, 10);
-        for (int i = 0; i < prefixLength; i++) {
-            input += ' ';
-        }
-        return input;
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorFactoryTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorFactoryTests.java
deleted file mode 100644
index a8e048b..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.uppercase;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class UppercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        UppercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorTests.java b/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorTests.java
deleted file mode 100644
index 00e4d18..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/ingest/processor/uppercase/UppercaseProcessorTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor.uppercase;
-
-import org.elasticsearch.ingest.processor.AbstractStringProcessor;
-import org.elasticsearch.ingest.processor.AbstractStringProcessorTestCase;
-
-import java.util.Collection;
-import java.util.Locale;
-
-public class UppercaseProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new UppercaseProcessor(field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toUpperCase(Locale.ROOT);
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineExecutionServiceTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineExecutionServiceTests.java
deleted file mode 100644
index 8f8a3a3..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineExecutionServiceTests.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.ingest.processor.meta.MetaDataProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.Before;
-import org.mockito.ArgumentMatcher;
-import org.mockito.Matchers;
-import org.mockito.invocation.InvocationOnMock;
-
-import java.util.*;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.mockito.Matchers.any;
-import static org.mockito.Mockito.*;
-
-public class PipelineExecutionServiceTests extends ESTestCase {
-
-    private PipelineStore store;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        store = mock(PipelineStore.class);
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(anyString())).thenReturn(Runnable::run);
-        executionService = new PipelineExecutionService(store, threadPool);
-    }
-
-    public void testExecutePipelineDoesNotExist() {
-        when(store.get("_id")).thenReturn(null);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        @SuppressWarnings("unchecked")
-        ActionListener<Void> listener = (ActionListener<Void>)mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-        verify(listener).onFailure(any(IllegalArgumentException.class));
-        verify(listener, times(0)).onResponse(any());
-    }
-
-    public void testExecuteSuccess() throws Exception {
-        Processor processor = mock(Processor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        @SuppressWarnings("unchecked")
-        ActionListener<Void> listener = (ActionListener<Void>)mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-        //TODO we remove metadata, this check is not valid anymore, what do we replace it with?
-        //verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(listener).onResponse(null);
-        verify(listener, times(0)).onFailure(any(Exception.class));
-    }
-
-    public void testExecutePropagateAllMetaDataUpdates() throws Exception {
-        Processor processor = mock(Processor.class);
-        doAnswer((InvocationOnMock invocationOnMock) -> {
-            IngestDocument ingestDocument = (IngestDocument) invocationOnMock.getArguments()[0];
-            for (IngestDocument.MetaData metaData : IngestDocument.MetaData.values()) {
-                if (metaData == IngestDocument.MetaData.TTL) {
-                    ingestDocument.setFieldValue(IngestDocument.MetaData.TTL.getFieldName(), "5w");
-                } else {
-                    ingestDocument.setFieldValue(metaData.getFieldName(), "update" + metaData.getFieldName());
-                }
-
-            }
-            return null;
-        }).when(processor).execute(any());
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        @SuppressWarnings("unchecked")
-        ActionListener<Void> listener = (ActionListener<Void>)mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-        verify(processor).execute(any());
-        verify(listener).onResponse(any());
-        verify(listener, times(0)).onFailure(any(Exception.class));
-
-        assertThat(indexRequest.index(), equalTo("update_index"));
-        assertThat(indexRequest.type(), equalTo("update_type"));
-        assertThat(indexRequest.id(), equalTo("update_id"));
-        assertThat(indexRequest.routing(), equalTo("update_routing"));
-        assertThat(indexRequest.parent(), equalTo("update_parent"));
-        assertThat(indexRequest.timestamp(), equalTo("update_timestamp"));
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(3024000000L)));
-    }
-
-    public void testExecuteFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        ActionListener<Void> listener = (ActionListener<Void>)mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(listener, times(0)).onResponse(null);
-        verify(listener).onFailure(any(RuntimeException.class));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testExecuteTTL() throws Exception {
-        // test with valid ttl
-        MetaDataProcessor.Factory metaProcessorFactory = new MetaDataProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("_ttl", "5d");
-        MetaDataProcessor processor = metaProcessorFactory.create(config);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        ActionListener<Void> listener = (ActionListener<Void>)mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-
-        assertThat(indexRequest.ttl(), equalTo(TimeValue.parseTimeValue("5d", null, "ttl")));
-        verify(listener, times(1)).onResponse(any());
-        verify(listener, never()).onFailure(any());
-
-        // test with invalid ttl
-        metaProcessorFactory = new MetaDataProcessor.Factory();
-        config = new HashMap<>();
-        config.put("_ttl", "abc");
-        processor = metaProcessorFactory.create(config);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-
-        indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap());
-        listener = mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-
-        verify(listener, never()).onResponse(any());
-        verify(listener, times(1)).onFailure(any(ElasticsearchParseException.class));
-
-        // test with provided ttl
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.emptyList()));
-
-        indexRequest = new IndexRequest("_index", "_type", "_id")
-                .source(Collections.emptyMap())
-                .ttl(1000L);
-        listener = mock(ActionListener.class);
-        executionService.execute(indexRequest, "_id", listener);
-
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(1000L)));
-        verify(listener, times(1)).onResponse(any());
-        verify(listener, never()).onFailure(any(Throwable.class));
-    }
-
-    private IngestDocument eqID(String index, String type, String id, Map<String, Object> source) {
-        return Matchers.argThat(new IngestDocumentMatcher(index, type, id, source));
-    }
-
-    private class IngestDocumentMatcher extends ArgumentMatcher<IngestDocument> {
-
-        private final IngestDocument ingestDocument;
-
-        public IngestDocumentMatcher(String index, String type, String id, Map<String, Object> source) {
-            this.ingestDocument = new IngestDocument(index, type, id, null, null, null, null, source);
-        }
-
-        @Override
-        public boolean matches(Object o) {
-            if (o.getClass() == IngestDocument.class) {
-                IngestDocument otherIngestDocument = (IngestDocument) o;
-                //ingest metadata will not be the same (timestamp differs every time)
-                return Objects.equals(ingestDocument.getSourceAndMetadata(), otherIngestDocument.getSourceAndMetadata());
-            }
-            return false;
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineStoreTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineStoreTests.java
deleted file mode 100644
index e67262b..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/PipelineStoreTests.java
+++ /dev/null
@@ -1,229 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest;
-
-import org.elasticsearch.action.ActionFuture;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.support.PlainActionFuture;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.Provider;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.text.StringText;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.internal.InternalSearchHit;
-import org.elasticsearch.search.internal.InternalSearchHits;
-import org.elasticsearch.search.internal.InternalSearchResponse;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.After;
-import org.junit.Before;
-import org.mockito.ArgumentMatcher;
-import org.mockito.Matchers;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.*;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-import static org.mockito.Matchers.any;
-
-public class PipelineStoreTests extends ESTestCase {
-
-    private ThreadPool threadPool;
-    private PipelineStore store;
-    private Client client;
-
-    @Before
-    public void init() {
-        threadPool = new ThreadPool("test");
-        client = mock(Client.class);
-
-        ClusterService clusterService = mock(ClusterService.class);
-        when(client.searchScroll(any())).thenReturn(expectedSearchReponse(Collections.emptyList()));
-        Environment environment = mock(Environment.class);
-        store = new PipelineStore(Settings.EMPTY, () -> client, threadPool, environment, clusterService, Collections.emptyMap());
-    }
-
-    @After
-    public void cleanup() {
-        threadPool.shutdown();
-    }
-
-    public void testUpdatePipeline() throws Exception {
-        List<SearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "1", new StringText("type"), Collections.emptyMap())
-                .sourceRef(new BytesArray("{\"description\": \"_description1\"}"))
-        );
-
-        when(client.search(any())).thenReturn(expectedSearchReponse(hits));
-        when(client.get(any())).thenReturn(expectedGetResponse(true));
-        assertThat(store.get("1"), nullValue());
-
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-
-        when(client.get(any())).thenReturn(expectedGetResponse(true));
-        hits.add(new InternalSearchHit(0, "2", new StringText("type"), Collections.emptyMap())
-                        .sourceRef(new BytesArray("{\"description\": \"_description2\"}"))
-        );
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        assertThat(store.get("2").getId(), equalTo("2"));
-        assertThat(store.get("2").getDescription(), equalTo("_description2"));
-
-        hits.remove(1);
-        when(client.get(eqGetRequest(PipelineStore.INDEX, PipelineStore.TYPE, "2"))).thenReturn(expectedGetResponse(false));
-        store.updatePipelines();
-        assertThat(store.get("1").getId(), equalTo("1"));
-        assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        assertThat(store.get("2"), nullValue());
-    }
-
-    public void testPipelineUpdater() throws Exception {
-        List<SearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "1", new StringText("type"), Collections.emptyMap())
-                        .sourceRef(new BytesArray("{\"description\": \"_description1\"}"))
-        );
-        when(client.search(any())).thenReturn(expectedSearchReponse(hits));
-        when(client.get(any())).thenReturn(expectedGetResponse(true));
-        assertThat(store.get("1"), nullValue());
-
-        store.startUpdateWorker();
-        assertBusy(() -> {
-            assertThat(store.get("1"), notNullValue());
-            assertThat(store.get("1").getId(), equalTo("1"));
-            assertThat(store.get("1").getDescription(), equalTo("_description1"));
-        });
-
-        hits.add(new InternalSearchHit(0, "2", new StringText("type"), Collections.emptyMap())
-                        .sourceRef(new BytesArray("{\"description\": \"_description2\"}"))
-        );
-        assertBusy(() -> {
-            assertThat(store.get("1"), notNullValue());
-            assertThat(store.get("1").getId(), equalTo("1"));
-            assertThat(store.get("1").getDescription(), equalTo("_description1"));
-            assertThat(store.get("2"), notNullValue());
-            assertThat(store.get("2").getId(), equalTo("2"));
-            assertThat(store.get("2").getDescription(), equalTo("_description2"));
-        });
-    }
-
-    public void testGetReference() throws Exception {
-        // fill the store up for the test:
-        List<SearchHit> hits = new ArrayList<>();
-        hits.add(new InternalSearchHit(0, "foo", new StringText("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        hits.add(new InternalSearchHit(0, "bar", new StringText("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        hits.add(new InternalSearchHit(0, "foobar", new StringText("type"), Collections.emptyMap()).sourceRef(new BytesArray("{\"description\": \"_description\"}")));
-        when(client.search(any())).thenReturn(expectedSearchReponse(hits));
-        store.updatePipelines();
-
-        List<PipelineDefinition> result = store.getReference("foo");
-        assertThat(result.size(), equalTo(1));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-
-        result = store.getReference("foo*");
-        // to make sure the order is consistent in the test:
-        result.sort((first, second) -> {
-            return first.getPipeline().getId().compareTo(second.getPipeline().getId());
-        });
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("foobar"));
-
-        result = store.getReference("bar*");
-        assertThat(result.size(), equalTo(1));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("bar"));
-
-        result = store.getReference("*");
-        // to make sure the order is consistent in the test:
-        result.sort((first, second) -> {
-            return first.getPipeline().getId().compareTo(second.getPipeline().getId());
-        });
-        assertThat(result.size(), equalTo(3));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("bar"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(2).getPipeline().getId(), equalTo("foobar"));
-
-        result = store.getReference("foo", "bar");
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0).getPipeline().getId(), equalTo("foo"));
-        assertThat(result.get(1).getPipeline().getId(), equalTo("bar"));
-    }
-
-    ActionFuture<SearchResponse> expectedSearchReponse(List<SearchHit> hits) {
-        return new PlainActionFuture<SearchResponse>() {
-
-            @Override
-            public SearchResponse get(long timeout, TimeUnit unit) {
-                InternalSearchHits hits1 = new InternalSearchHits(hits.toArray(new InternalSearchHit[0]), hits.size(), 1f);
-                return new SearchResponse(new InternalSearchResponse(hits1, null, null, false, null), "_scrollId", 1, 1, 1, null);
-            }
-        };
-    }
-
-    ActionFuture<GetResponse> expectedGetResponse(boolean exists) {
-        return new PlainActionFuture<GetResponse>() {
-            @Override
-            public GetResponse get() throws InterruptedException, ExecutionException {
-                return new GetResponse(new GetResult("_index", "_type", "_id", 1, exists, null, null));
-            }
-        };
-    }
-
-    GetRequest eqGetRequest(String index, String type, String id) {
-        return Matchers.argThat(new GetRequestMatcher(index, type, id));
-    }
-
-    static class GetRequestMatcher extends ArgumentMatcher<GetRequest> {
-
-        private final String index;
-        private final String type;
-        private final String id;
-
-        public GetRequestMatcher(String index, String type, String id) {
-            this.index = index;
-            this.type = type;
-            this.id = id;
-        }
-
-        @Override
-        public boolean matches(Object o) {
-            GetRequest getRequest = (GetRequest) o;
-            return Objects.equals(getRequest.index(), index) &&
-                    Objects.equals(getRequest.type(), type) &&
-                    Objects.equals(getRequest.id(), id);
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilterTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilterTests.java
deleted file mode 100644
index 569cf32..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/IngestActionFilterTests.java
+++ /dev/null
@@ -1,386 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.plugin.ingest.IngestPlugin;
-import org.elasticsearch.plugin.ingest.PipelineExecutionService;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.Before;
-import org.mockito.invocation.InvocationOnMock;
-import org.mockito.stubbing.Answer;
-
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Set;
-
-import static org.elasticsearch.plugin.ingest.transport.IngestActionFilter.*;
-import static org.hamcrest.Matchers.*;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.eq;
-import static org.mockito.Mockito.*;
-
-public class IngestActionFilterTests extends ESTestCase {
-
-    private IngestActionFilter filter;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        executionService = mock(PipelineExecutionService.class);
-        filter = new IngestActionFilter(Settings.EMPTY, executionService);
-    }
-
-    public void testApplyNoIngestId() throws Exception {
-        IndexRequest indexRequest = new IndexRequest();
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed("_action", indexRequest, actionListener);
-        verifyZeroInteractions(executionService, actionFilterChain);
-    }
-
-    public void testApplyIngestIdViaRequestParam() throws Exception {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-        indexRequest.source("field", "value");
-        indexRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    public void testApplyIngestIdViaContext() throws Exception {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-        indexRequest.source("field", "value");
-        indexRequest.putInContext(IngestPlugin.PIPELINE_ID_PARAM_CONTEXT_KEY, "_id");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    public void testApplyAlreadyProcessed() throws Exception {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-        indexRequest.source("field", "value");
-        indexRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        indexRequest.putHeader(IngestPlugin.PIPELINE_ALREADY_PROCESSED, true);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed("_action", indexRequest, actionListener);
-        verifyZeroInteractions(executionService, actionListener);
-    }
-
-    public void testApplyExecuted() throws Exception {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-        indexRequest.source("field", "value");
-        indexRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        Answer answer = invocationOnMock -> {
-            @SuppressWarnings("unchecked")
-            ActionListener<Void> listener = (ActionListener<Void>) invocationOnMock.getArguments()[2];
-            listener.onResponse(null);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        verify(actionFilterChain).proceed("_action", indexRequest, actionListener);
-        verifyZeroInteractions(actionListener);
-    }
-
-    public void testApplyFailed() throws Exception {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-        indexRequest.source("field", "value");
-        indexRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        RuntimeException exception = new RuntimeException();
-        Answer answer = new Answer() {
-            @Override
-            public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
-                ActionListener listener = (ActionListener) invocationOnMock.getArguments()[2];
-                listener.onFailure(exception);
-                return null;
-            }
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        filter.apply("_action", indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-        verify(actionListener).onFailure(exception);
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    public void testApplyWithBulkRequest() throws Exception {
-        ThreadPool threadPool = new ThreadPool(
-                Settings.builder()
-                        .put("name", "_name")
-                        .put(PipelineExecutionService.additionalSettings(Settings.EMPTY))
-                        .build()
-        );
-        PipelineStore store = mock(PipelineStore.class);
-
-        Processor processor = new Processor() {
-            @Override
-            public void execute(IngestDocument ingestDocument) {
-                ingestDocument.setFieldValue("field2", "value2");
-            }
-
-            @Override
-            public String getType() {
-                return null;
-            }
-        };
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", Collections.singletonList(processor)));
-        executionService = new PipelineExecutionService(store, threadPool);
-        filter = new IngestActionFilter(Settings.EMPTY, executionService);
-
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        int numRequest = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < numRequest; i++) {
-            if (rarely()) {
-                ActionRequest request;
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-                bulkRequest.add(request);
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-                indexRequest.source("field1", "value1");
-                bulkRequest.add(indexRequest);
-            }
-        }
-
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply("_action", bulkRequest, actionListener, actionFilterChain);
-
-        assertBusy(new Runnable() {
-            @Override
-            public void run() {
-                verify(actionFilterChain).proceed("_action", bulkRequest, actionListener);
-                verifyZeroInteractions(actionListener);
-
-                int assertedRequests = 0;
-                for (ActionRequest actionRequest : bulkRequest.requests()) {
-                    if (actionRequest instanceof IndexRequest) {
-                        IndexRequest indexRequest = (IndexRequest) actionRequest;
-                        assertThat(indexRequest.sourceAsMap().size(), equalTo(2));
-                        assertThat(indexRequest.sourceAsMap().get("field1"), equalTo("value1"));
-                        assertThat(indexRequest.sourceAsMap().get("field2"), equalTo("value2"));
-                    }
-                    assertedRequests++;
-                }
-                assertThat(assertedRequests, equalTo(numRequest));
-            }
-        });
-
-        threadPool.shutdown();
-    }
-
-
-    public void testApplyWithBulkRequestWithFailureAllFailed() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        int numRequest = scaledRandomIntBetween(0, 8);
-        for (int i = 0; i < numRequest; i++) {
-            IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-            indexRequest.source("field1", "value1");
-            bulkRequest.add(indexRequest);
-        }
-
-        RuntimeException exception = new RuntimeException();
-        Answer answer = (invocationOnMock) -> {
-            ActionListener listener = (ActionListener) invocationOnMock.getArguments()[2];
-            listener.onFailure(exception);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-
-        CaptureActionListener actionListener = new CaptureActionListener();
-        RecordRequestAFC actionFilterChain = new RecordRequestAFC();
-
-        filter.apply("_action", bulkRequest, actionListener, actionFilterChain);
-
-        assertThat(actionFilterChain.request, nullValue());
-        ActionResponse response = actionListener.response;
-        assertThat(response, instanceOf(BulkResponse.class));
-        BulkResponse bulkResponse = (BulkResponse) response;
-        assertThat(bulkResponse.getItems().length, equalTo(numRequest));
-        for (BulkItemResponse bulkItemResponse : bulkResponse) {
-            assertThat(bulkItemResponse.isFailed(), equalTo(true));
-        }
-    }
-
-    public void testApplyWithBulkRequestWithFailure() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.putHeader(IngestPlugin.PIPELINE_ID_PARAM, "_id");
-        int numRequest = scaledRandomIntBetween(8, 64);
-        int numNonIndexRequests = 0;
-        for (int i = 0; i < numRequest; i++) {
-            ActionRequest request;
-            if (randomBoolean()) {
-                numNonIndexRequests++;
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id");
-                indexRequest.source("field1", "value1");
-                request = indexRequest;
-            }
-            bulkRequest.add(request);
-        }
-
-        RuntimeException exception = new RuntimeException();
-        Answer answer = (invocationOnMock) -> {
-            ActionListener listener = (ActionListener) invocationOnMock.getArguments()[2];
-            listener.onFailure(exception);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), eq("_id"), any(ActionListener.class));
-
-        ActionListener actionListener = mock(ActionListener.class);
-        RecordRequestAFC actionFilterChain = new RecordRequestAFC();
-
-        filter.apply("_action", bulkRequest, actionListener, actionFilterChain);
-
-        BulkRequest interceptedRequests = actionFilterChain.getRequest();
-        assertThat(interceptedRequests.requests().size(), equalTo(numNonIndexRequests));
-
-        verifyZeroInteractions(actionListener);
-    }
-
-    public void testBulkRequestModifier() {
-        int numRequests = scaledRandomIntBetween(8, 64);
-        BulkRequest bulkRequest = new BulkRequest();
-        for (int i = 0; i < numRequests; i++) {
-            bulkRequest.add(new IndexRequest("_index", "_type", String.valueOf(i)).source("{}"));
-        }
-        CaptureActionListener actionListener = new CaptureActionListener();
-        BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(bulkRequest);
-
-        int i = 0;
-        Set<Integer> failedSlots = new HashSet<>();
-        while (bulkRequestModifier.hasNext()) {
-            bulkRequestModifier.next();
-            if (randomBoolean()) {
-                bulkRequestModifier.markCurrentItemAsFailed(new RuntimeException());
-                failedSlots.add(i);
-            }
-            i++;
-        }
-
-        assertThat(bulkRequestModifier.getBulkRequest().requests().size(), equalTo(numRequests - failedSlots.size()));
-        // simulate that we actually executed the modified bulk request:
-        ActionListener<BulkResponse> result = bulkRequestModifier.wrapActionListenerIfNeeded(actionListener);
-        result.onResponse(new BulkResponse(new BulkItemResponse[numRequests - failedSlots.size()], 0));
-
-        BulkResponse bulkResponse = actionListener.getResponse();
-        for (int j = 0; j < bulkResponse.getItems().length; j++) {
-            if (failedSlots.contains(j)) {
-                BulkItemResponse item =  bulkResponse.getItems()[j];
-                assertThat(item.isFailed(), is(true));
-                assertThat(item.getFailure().getIndex(), equalTo("_index"));
-                assertThat(item.getFailure().getType(), equalTo("_type"));
-                assertThat(item.getFailure().getId(), equalTo(String.valueOf(j)));
-                assertThat(item.getFailure().getMessage(), equalTo("java.lang.RuntimeException"));
-            } else {
-                assertThat(bulkResponse.getItems()[j], nullValue());
-            }
-        }
-    }
-
-    private final static class RecordRequestAFC implements ActionFilterChain {
-
-        private ActionRequest request;
-
-        @Override
-        public void proceed(String action, ActionRequest request, ActionListener listener) {
-            this.request = request;
-        }
-
-        @Override
-        public void proceed(String action, ActionResponse response, ActionListener listener) {
-
-        }
-
-        @SuppressWarnings("unchecked")
-        public <T extends ActionRequest<T>> T getRequest() {
-            return (T) request;
-        }
-    }
-
-    private final static class CaptureActionListener implements ActionListener<BulkResponse> {
-
-        private BulkResponse response;
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            this.response = bulkItemResponses ;
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-        }
-
-        public BulkResponse getResponse() {
-            return response;
-        }
-    }
-
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResultTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResultTests.java
deleted file mode 100644
index 38c1e88..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateDocumentSimpleResultTests.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SimulateDocumentSimpleResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isFailure = randomBoolean();
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult;
-        if (isFailure) {
-            simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateDocumentSimpleResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateDocumentSimpleResult otherSimulateDocumentSimpleResult = SimulateDocumentSimpleResult.readSimulateDocumentSimpleResult(streamInput);
-
-        assertThat(otherSimulateDocumentSimpleResult.getIngestDocument(), equalTo(simulateDocumentSimpleResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateDocumentSimpleResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateDocumentSimpleResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionServiceTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionServiceTests.java
deleted file mode 100644
index 973e8c1..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateExecutionServiceTests.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.After;
-import org.junit.Before;
-
-import java.util.Arrays;
-
-import static org.hamcrest.Matchers.*;
-import static org.mockito.Mockito.*;
-
-public class SimulateExecutionServiceTests extends ESTestCase {
-
-    private ThreadPool threadPool;
-    private SimulateExecutionService executionService;
-    private Pipeline pipeline;
-    private Processor processor;
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setup() {
-        threadPool = new ThreadPool(
-                Settings.builder()
-                        .put("name", getClass().getName())
-                        .build()
-        );
-        executionService = new SimulateExecutionService(threadPool);
-        processor = mock(Processor.class);
-        when(processor.getType()).thenReturn("mock");
-        pipeline = new Pipeline("_id", "_description", Arrays.asList(processor, processor));
-        //ingestDocument = new IngestDocument("_index", "_type", "_id", Collections.singletonMap("foo", "bar"));
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-    }
-
-    @After
-    public void destroy() {
-        threadPool.shutdown();
-    }
-
-    public void testExecuteVerboseItem() throws Exception {
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        verify(processor, times(2)).execute(ingestDocument);
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorId(), equalTo("processor[mock]-0"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorId(), equalTo("processor[mock]-1"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(),
-            not(sameInstance(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteItem() throws Exception {
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        verify(processor, times(2)).execute(ingestDocument);
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) actualItemResponse;
-        assertThat(simulateDocumentSimpleResult.getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-    }
-
-    public void testExecuteVerboseItemWithFailure() throws Exception {
-        Exception e = new RuntimeException("processor failed");
-        doThrow(e).doNothing().when(processor).execute(ingestDocument);
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        verify(processor, times(2)).execute(ingestDocument);
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorId(), equalTo("processor[mock]-0"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorId(), equalTo("processor[mock]-1"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-        runtimeException = (RuntimeException) simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-    }
-
-    public void testExecuteItemWithFailure() throws Exception {
-        Exception e = new RuntimeException("processor failed");
-        doThrow(e).when(processor).execute(ingestDocument);
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        verify(processor, times(1)).execute(ingestDocument);
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentSimpleResult.class));
-        SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) actualItemResponse;
-        assertThat(simulateDocumentSimpleResult.getIngestDocument(), nullValue());
-        assertThat(simulateDocumentSimpleResult.getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentSimpleResult.getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestParsingTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestParsingTests.java
deleted file mode 100644
index 9484a62..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineRequestParsingTests.java
+++ /dev/null
@@ -1,151 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.Pipeline;
-import org.elasticsearch.ingest.processor.Processor;
-import org.elasticsearch.plugin.ingest.PipelineStore;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.elasticsearch.ingest.IngestDocument.MetaData.*;
-import static org.elasticsearch.plugin.ingest.transport.simulate.SimulatePipelineRequest.Fields;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class SimulatePipelineRequestParsingTests extends ESTestCase {
-
-    private PipelineStore store;
-
-    @Before
-    public void init() throws IOException {
-        Pipeline pipeline = new Pipeline(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, null, Collections.singletonList(mock(Processor.class)));
-        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
-        processorRegistry.put("mock_processor", mock(Processor.Factory.class));
-        store = mock(PipelineStore.class);
-        when(store.get(SimulatePipelineRequest.SIMULATED_PIPELINE_ID)).thenReturn(pipeline);
-        when(store.getProcessorFactoryRegistry()).thenReturn(processorRegistry);
-    }
-
-    public void testParseUsingPipelineStore() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parseWithPipelineId(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(1));
-    }
-
-    public void testParseWithProvidedPipeline() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        List<Map<String, Object>> processors = new ArrayList<>();
-        int numProcessors = randomIntBetween(1, 10);
-        for (int i = 0; i < numProcessors; i++) {
-            processors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
-        }
-        pipelineConfig.put("processors", processors);
-        requestContent.put(Fields.PIPELINE, pipelineConfig);
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parse(requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(numProcessors));
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponseTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponseTests.java
deleted file mode 100644
index 1b3b35e..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulatePipelineResponseTests.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class SimulatePipelineResponseTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isVerbose = randomBoolean();
-        int numResults = randomIntBetween(1, 10);
-        List<SimulateDocumentResult> results = new ArrayList<>(numResults);
-        for (int i = 0; i < numResults; i++) {
-            boolean isFailure = randomBoolean();
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            if (isVerbose) {
-                int numProcessors = randomIntBetween(1, 10);
-                List<SimulateProcessorResult> processorResults = new ArrayList<>(numProcessors);
-                for (int j = 0; j < numProcessors; j++) {
-                    String processorId = randomAsciiOfLengthBetween(1, 10);
-                    SimulateProcessorResult processorResult;
-                    if (isFailure) {
-                        processorResult = new SimulateProcessorResult(processorId, new IllegalArgumentException("test"));
-                    } else {
-                        processorResult = new SimulateProcessorResult(processorId, ingestDocument);
-                    }
-                    processorResults.add(processorResult);
-                }
-                results.add(new SimulateDocumentVerboseResult(processorResults));
-            } else {
-                results.add(new SimulateDocumentSimpleResult(ingestDocument));
-                SimulateDocumentSimpleResult simulateDocumentSimpleResult;
-                if (isFailure) {
-                    simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(new IllegalArgumentException("test"));
-                } else {
-                    simulateDocumentSimpleResult = new SimulateDocumentSimpleResult(ingestDocument);
-                }
-                results.add(simulateDocumentSimpleResult);
-            }
-        }
-
-        SimulatePipelineResponse response = new SimulatePipelineResponse(randomAsciiOfLengthBetween(1, 10), isVerbose, results);
-        BytesStreamOutput out = new BytesStreamOutput();
-        response.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulatePipelineResponse otherResponse = new SimulatePipelineResponse();
-        otherResponse.readFrom(streamInput);
-
-        assertThat(otherResponse.getPipelineId(), equalTo(response.getPipelineId()));
-        assertThat(otherResponse.getResults().size(), equalTo(response.getResults().size()));
-
-        Iterator<SimulateDocumentResult> expectedResultIterator = response.getResults().iterator();
-        for (SimulateDocumentResult result : otherResponse.getResults()) {
-            if (isVerbose) {
-                SimulateDocumentVerboseResult expectedSimulateDocumentVerboseResult = (SimulateDocumentVerboseResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentVerboseResult.class));
-                SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) result;
-                assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(expectedSimulateDocumentVerboseResult.getProcessorResults().size()));
-                Iterator<SimulateProcessorResult> expectedProcessorResultIterator = expectedSimulateDocumentVerboseResult.getProcessorResults().iterator();
-                for (SimulateProcessorResult simulateProcessorResult : simulateDocumentVerboseResult.getProcessorResults()) {
-                    SimulateProcessorResult expectedProcessorResult = expectedProcessorResultIterator.next();
-                    assertThat(simulateProcessorResult.getProcessorId(), equalTo(expectedProcessorResult.getProcessorId()));
-                    assertThat(simulateProcessorResult.getIngestDocument(), equalTo(expectedProcessorResult.getIngestDocument()));
-                    if (expectedProcessorResult.getFailure() == null) {
-                        assertThat(simulateProcessorResult.getFailure(), nullValue());
-                    } else {
-                        assertThat(simulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                        IllegalArgumentException e = (IllegalArgumentException) simulateProcessorResult.getFailure();
-                        assertThat(e.getMessage(), equalTo("test"));
-                    }
-                }
-            } else {
-                SimulateDocumentSimpleResult expectedSimulateDocumentSimpleResult = (SimulateDocumentSimpleResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentSimpleResult.class));
-                SimulateDocumentSimpleResult simulateDocumentSimpleResult = (SimulateDocumentSimpleResult) result;
-                assertThat(simulateDocumentSimpleResult.getIngestDocument(), equalTo(expectedSimulateDocumentSimpleResult.getIngestDocument()));
-                if (expectedSimulateDocumentSimpleResult.getFailure() == null) {
-                    assertThat(simulateDocumentSimpleResult.getFailure(), nullValue());
-                } else {
-                    assertThat(simulateDocumentSimpleResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                    IllegalArgumentException e = (IllegalArgumentException) simulateDocumentSimpleResult.getFailure();
-                    assertThat(e.getMessage(), equalTo("test"));
-                }
-            }
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResultTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResultTests.java
deleted file mode 100644
index a2af605..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/SimulateProcessorResultTests.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class SimulateProcessorResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        String processorId = randomAsciiOfLengthBetween(1, 10);
-        boolean isFailure = randomBoolean();
-        SimulateProcessorResult simulateProcessorResult;
-        if (isFailure) {
-            simulateProcessorResult = new SimulateProcessorResult(processorId, new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateProcessorResult = new SimulateProcessorResult(processorId, ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateProcessorResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateProcessorResult otherSimulateProcessorResult = SimulateProcessorResult.readSimulateProcessorResultFrom(streamInput);
-        assertThat(otherSimulateProcessorResult.getProcessorId(), equalTo(simulateProcessorResult.getProcessorId()));
-        assertThat(otherSimulateProcessorResult.getIngestDocument(), equalTo(simulateProcessorResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateProcessorResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocumentTests.java b/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocumentTests.java
deleted file mode 100644
index b153cce..0000000
--- a/plugins/ingest/src/test/java/org/elasticsearch/plugin/ingest/transport/simulate/WriteableIngestDocumentTests.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.ingest.transport.simulate;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-
-public class WriteableIngestDocumentTests extends ESTestCase {
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        WriteableIngestDocument ingestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        WriteableIngestDocument otherIngestDocument = new WriteableIngestDocument(new IngestDocument(otherSourceAndMetadata, otherIngestMetadata));
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            WriteableIngestDocument thirdIngestDocument = new WriteableIngestDocument(new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata)));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testSerialization() throws IOException {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, Object> document = RandomDocumentPicks.randomSource(random());
-        WriteableIngestDocument writeableIngestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        writeableIngestDocument.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        WriteableIngestDocument otherWriteableIngestDocument = WriteableIngestDocument.readWriteableIngestDocumentFrom(streamInput);
-        assertThat(otherWriteableIngestDocument, equalTo(writeableIngestDocument));
-    }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.bulk.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.bulk.json
deleted file mode 100644
index 5595c20..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.bulk.json
+++ /dev/null
@@ -1,56 +0,0 @@
-{
-  "ingest.bulk": {
-    "documentation": "Copied from bulk in core to add the pipeline parameter to rest spec",
-    "methods": ["POST", "PUT"],
-    "url": {
-      "path": "/_bulk",
-      "paths": ["/_bulk", "/{index}/_bulk", "/{index}/{type}/_bulk"],
-      "parts": {
-        "index": {
-          "type" : "string",
-          "description" : "Default index for items which don't provide one"
-        },
-        "type": {
-          "type" : "string",
-          "description" : "Default document type for items which don't provide one"
-        }
-      },
-      "params": {
-        "consistency": {
-          "type" : "enum",
-          "options" : ["one", "quorum", "all"],
-          "description" : "Explicit write consistency setting for the operation"
-        },
-        "refresh": {
-          "type" : "boolean",
-          "description" : "Refresh the index after performing the operation"
-        },
-        "routing": {
-          "type" : "string",
-          "description" : "Specific routing value"
-        },
-        "timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout"
-        },
-        "type": {
-          "type" : "string",
-          "description" : "Default document type for items which don't provide one"
-        },
-        "fields": {
-          "type": "list",
-          "description" : "Default comma-separated list of fields to return in the response for updates"
-        },
-        "pipeline_id" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
-        }
-      }
-    },
-    "body": {
-      "description" : "The operation definition and data (action-data pairs), separated by newlines",
-      "required" : true,
-      "serialize" : "bulk"
-    }
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.delete_pipeline.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.delete_pipeline.json
deleted file mode 100644
index 69b8f53..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.delete_pipeline.json
+++ /dev/null
@@ -1,20 +0,0 @@
-{
-  "ingest.delete_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "DELETE" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": null
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.get_pipeline.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.get_pipeline.json
deleted file mode 100644
index 71772a2..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.get_pipeline.json
+++ /dev/null
@@ -1,20 +0,0 @@
-{
-  "ingest.get_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Comma separated list of pipeline ids. Wildcards supported",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": null
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.index.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.index.json
deleted file mode 100644
index 02dc30b..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.index.json
+++ /dev/null
@@ -1,80 +0,0 @@
-{
-  "ingest.index": {
-    "documentation": "Copied from index in core to add support for the pipeline parameter to rest spec",
-    "methods": ["POST", "PUT"],
-    "url": {
-      "path": "/{index}/{type}",
-      "paths": ["/{index}/{type}", "/{index}/{type}/{id}"],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Document ID"
-        },
-        "index": {
-          "type" : "string",
-          "required" : true,
-          "description" : "The name of the index"
-        },
-        "type": {
-          "type" : "string",
-          "required" : true,
-          "description" : "The type of the document"
-        }
-      },
-      "params": {
-        "consistency": {
-          "type" : "enum",
-          "options" : ["one", "quorum", "all"],
-          "description" : "Explicit write consistency setting for the operation"
-        },
-        "op_type": {
-          "type" : "enum",
-          "options" : ["index", "create"],
-          "default" : "index",
-          "description" : "Explicit operation type"
-        },
-        "parent": {
-          "type" : "string",
-          "description" : "ID of the parent document"
-        },
-        "refresh": {
-          "type" : "boolean",
-          "description" : "Refresh the index after performing the operation"
-        },
-        "routing": {
-          "type" : "string",
-          "description" : "Specific routing value"
-        },
-        "timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout"
-        },
-        "timestamp": {
-          "type" : "time",
-          "description" : "Explicit timestamp for the document"
-        },
-        "ttl": {
-          "type" : "duration",
-          "description" : "Expiration time for the document"
-        },
-        "version" : {
-          "type" : "number",
-          "description" : "Explicit version number for concurrency control"
-        },
-        "version_type": {
-          "type" : "enum",
-          "options" : ["internal", "external", "external_gte", "force"],
-          "description" : "Specific version type"
-        },
-        "pipeline_id" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
-        }
-      }
-    },
-    "body": {
-      "description" : "The document",
-      "required" : true
-    }
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.put_pipeline.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.put_pipeline.json
deleted file mode 100644
index fd88d35..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.put_pipeline.json
+++ /dev/null
@@ -1,23 +0,0 @@
-{
-  "ingest.put_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "PUT" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-      }
-    },
-    "body": {
-      "description" : "The ingest definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.simulate.json b/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.simulate.json
deleted file mode 100644
index a4904ce..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/api/ingest.simulate.json
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "ingest.simulate": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET", "POST" ],
-    "url": {
-      "path": "/_ingest/pipeline/_simulate",
-      "paths": [ "/_ingest/pipeline/_simulate", "/_ingest/pipeline/{id}/_simulate/" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : false
-        }
-      },
-      "params": {
-        "verbose": {
-          "type" : "boolean",
-          "description" : "Verbose mode. Display data output for each processor in executed pipeline",
-          "default" : false
-        }
-      }
-    },
-    "body": {
-      "description" : "The simulate definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/10_basic.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/10_basic.yaml
deleted file mode 100644
index ad10d9b..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/10_basic.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-"Ingest plugin installed":
-    - do:
-        cluster.stats: {}
-
-    - match:  { nodes.plugins.0.name: ingest  }
-    - match:  { nodes.plugins.0.jvm: true  }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/20_crud.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/20_crud.yaml
deleted file mode 100644
index 9cf5d50..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/20_crud.yaml
+++ /dev/null
@@ -1,98 +0,0 @@
----
-"Test basic pipeline crud":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 1 }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { my_pipeline._source.description: "_description" }
-  - match: { my_pipeline._version: 1 }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { _index: ".ingest" }
-  - match: { _type: "pipeline" }
-  - match: { _version: 2 }
-  - match: { _id: "my_pipeline" }
-  - match: { found: true }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      catch: missing
-      ingest.get_pipeline:
-        id: "my_pipeline"
-
----
-"Test invalid config":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      catch: param
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                }
-              }
-            ]
-          }
-
-  - do:
-      catch: param
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "ip_field" : 1234
-                }
-              }
-            ]
-          }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/30_grok.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/30_grok.yaml
deleted file mode 100644
index e0f97b6..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/30_grok.yaml
+++ /dev/null
@@ -1,145 +0,0 @@
----
-"Test Grok Pipeline":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "123.42 400 <foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.val: 123.42 }
-  - match: { _source.status: 400 }
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{MY_PATTERN:msg}>",
-                  "pattern_definitions" : {
-                    "MY_PATTERN" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern Sharing Same Name As Another":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{NUMBER:msg}>",
-                  "pattern_definitions" : {
-                    "NUMBER" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/40_geoip_processor.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/40_geoip_processor.yaml
deleted file mode 100644
index 3c0efc1..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/40_geoip_processor.yaml
+++ /dev/null
@@ -1,156 +0,0 @@
----
-"Test geoip processor with defaults":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 5 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.location: [-93.2166, 44.9759] }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with fields":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "fields" : ["city_name", "country_iso_code", "ip", "latitude", "longitude", "location", "timezone", "country_name", "region_name", "continent_name"]
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 10 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.ip: "128.101.101.101" }
-  - match: { _source.geoip.latitude: 44.9759 }
-  - match: { _source.geoip.longitude: -93.2166 }
-  - match: { _source.geoip.location: [-93.2166, 44.9759] }
-  - match: { _source.geoip.timezone: "America/Chicago" }
-  - match: { _source.geoip.country_name: "United States" }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with different database file":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "database_file" : "GeoLite2-Country.mmdb"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 2 }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.continent_name: "North America" }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/50_date_processor.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/50_date_processor.yaml
deleted file mode 100644
index f2c6d2c..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/50_date_processor.yaml
+++ /dev/null
@@ -1,49 +0,0 @@
----
-"Test date processor":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "date" : {
-                  "match_field" : "date_source_field",
-                  "target_field" : "date_target_field",
-                  "match_formats" : ["dd/MM/yyyy"],
-                  "timezone" : "Europe/Amsterdam"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {date_source_field: "12/06/2010"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.date_source_field: "12/06/2010" }
-  - match: { _source.date_target_field: "2010-06-12T00:00:00.000+02:00" }
-
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/60_mutate.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/60_mutate.yaml
deleted file mode 100644
index eb59cad..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/60_mutate.yaml
+++ /dev/null
@@ -1,115 +0,0 @@
----
-"Test mutate processors":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "new_field",
-                  "value": "new_value"
-                }
-              },
-              {
-                "rename" : {
-                  "field" : "field_to_rename",
-                  "to": "renamed_field"
-                }
-              },
-              {
-                "remove" : {
-                  "field" : "field_to_remove"
-                }
-              },
-              {
-                "lowercase" : {
-                  "field" : "field_to_lowercase"
-                }
-              },
-              {
-                "uppercase" : {
-                  "field" : "field_to_uppercase"
-                }
-              },
-              {
-                "trim" : {
-                  "field" : "field_to_trim"
-                }
-              },
-              {
-                "split" : {
-                  "field" : "field_to_split",
-                  "separator": "-"
-                }
-              },
-              {
-                "join" : {
-                  "field" : "field_to_join",
-                  "separator": "-"
-                }
-              },
-              {
-                "convert" : {
-                  "field" : "field_to_convert",
-                  "type": "integer"
-                }
-              },
-              {
-                "gsub" : {
-                  "field": "field_to_gsub",
-                  "pattern" : "-",
-                  "replacement" : "."
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {
-          field_to_rename: "value",
-          field_to_remove: "old_value",
-          field_to_lowercase: "LOWERCASE",
-          field_to_uppercase: "uppercase",
-          field_to_trim: "   trimmed   ",
-          field_to_split: "127-0-0-1",
-          field_to_join: ["127","0","0","1"],
-          field_to_convert: ["127","0","0","1"],
-          field_to_gsub: "127-0-0-1"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - is_false: _source.field_to_rename
-  - is_false: _source.field_to_remove
-  - match: { _source.renamed_field: "value" }
-  - match: { _source.field_to_lowercase: "lowercase" }
-  - match: { _source.field_to_uppercase: "UPPERCASE" }
-  - match: { _source.field_to_trim: "trimmed" }
-  - match: { _source.field_to_split: ["127","0","0","1"] }
-  - match: { _source.field_to_join: "127-0-0-1" }
-  - match: { _source.field_to_convert: [127,0,0,1] }
-  - match: { _source.field_to_gsub: "127.0.0.1" }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/70_meta_processor.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/70_meta_processor.yaml
deleted file mode 100644
index be13146..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/70_meta_processor.yaml
+++ /dev/null
@@ -1,45 +0,0 @@
----
-"Test meta processor":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "meta" : {
-                  "_index" : "surprise"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.index:
-        index: test
-        type: test
-        id: 1
-        pipeline_id: "my_pipeline"
-        body: {field: "value"}
-
-  - do:
-      get:
-        index: surprise
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field: "value" }
diff --git a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/80_simulate.yaml b/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/80_simulate.yaml
deleted file mode 100644
index edbd949..0000000
--- a/plugins/ingest/src/test/resources/rest-api-spec/test/ingest/80_simulate.yaml
+++ /dev/null
@@ -1,280 +0,0 @@
----
-"Test simulate with stored ingest pipeline":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { _id: "my_pipeline" }
-
-  # Simulate a Thread.sleep(), because pipeline are updated in the background
-  - do:
-      catch: request_timeout
-      cluster.health:
-        wait_for_nodes: 99
-        timeout: 2s
-  - match: { "timed_out": true }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test simulate with provided pipeline definition":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-
----
-"Test simulate with no provided pipeline or pipeline_id":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      catch: request
-      ingest.simulate:
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { error: 3 }
-  - match: { status: 400 }
-  - match: { error.type: "illegal_argument_exception" }
-  - match: { error.reason: "required property [pipeline] is missing" }
-
----
-"Test simulate with verbose flag":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                },
-                {
-                  "set" : {
-                    "field" : "field3",
-                    "value" : "third_val"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - length: { docs.0.processor_results: 2 }
-  - match: { docs.0.processor_results.0.processor_id: "processor[set]-0" }
-  - length: { docs.0.processor_results.0.doc._source: 2 }
-  - match: { docs.0.processor_results.0.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.0.doc._source.field2: "_value" }
-  - length: { docs.0.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.0.doc._ingest.timestamp
-  - length: { docs.0.processor_results.1.doc._source: 3 }
-  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.1.doc._source.field2: "_value" }
-  - match: { docs.0.processor_results.1.doc._source..field3: "third_val" }
-  - length: { docs.0.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
-
----
-"Test simulate with exception thrown":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "uppercase" : {
-                    "field" : "foo"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "not_foo": "bar"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - match: { docs.0.error.type: "illegal_argument_exception" }
-  - match: { docs.1.doc._source.foo: "BAR" }
-  - length: { docs.1.doc._ingest: 1 }
-  - is_true: docs.1.doc._ingest.timestamp
-
----
-"Test verbose simulate with exception thrown":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "convert" : {
-                    "field" : "foo",
-                    "type" : "integer"
-                  }
-                },
-                {
-                  "uppercase" : {
-                    "field" : "bar"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar",
-                  "bar": "hello"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "5",
-                  "bar": "hello"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - length: { docs.0.processor_results: 2 }
-  - match: { docs.0.processor_results.0.error.type: "illegal_argument_exception" }
-  - match: { docs.0.processor_results.1.doc._index: "index" }
-  - match: { docs.0.processor_results.1.doc._type: "type" }
-  - match: { docs.0.processor_results.1.doc._id: "id" }
-  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.1.doc._source.bar: "HELLO" }
-  - length: { docs.0.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
-  - match: { docs.1.processor_results.0.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.0.doc._source.bar: "hello" }
-  - length: { docs.1.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.0.doc._ingest.timestamp
-  - match: { docs.1.processor_results.1.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.1.doc._source.bar: "HELLO" }
-  - length: { docs.1.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.1.doc._ingest.timestamp
-
diff --git a/plugins/lang-plan-a/ant.xml b/plugins/lang-plan-a/ant.xml
new file mode 100644
index 0000000..bf1c9b9
--- /dev/null
+++ b/plugins/lang-plan-a/ant.xml
@@ -0,0 +1,145 @@
+<?xml version="1.0"?>
+<project name="ant-stuff">
+
+<!-- 
+ grammar regeneration logic
+ we do this with ant for several reasons:
+ * remove generated tabs for forbidden-apis
+ * remove generated timestamps/filenames for reproducible build
+ * fix CRLF line endings for windows consistency
+ * ability to make classes package-private
+ * keeping in source code control is easier on IDEs
+ * regeneration should be rare, no reason to be religious about generated files 
+ * all logic already written and battle tested in lucene build
+-->
+  <target name="regenerate" description="Regenerate antlr lexer and parser" depends="run-antlr"/>
+
+  <target name="run-antlr">
+    <regen-delete grammar="PlanA"/>
+    <regen-lexer grammar="PlanA"/>
+    <regen-parser grammar="PlanA"/>
+    <regen-fix grammar="PlanA"/>
+  </target>
+
+  <macrodef name="replace-value">
+    <attribute name="value" />
+    <attribute name="property" />
+    <attribute name="from" />
+    <attribute name="to" />
+    <sequential>
+      <loadresource property="@{property}">
+        <string value="@{value}"/>
+        <filterchain>
+          <tokenfilter>
+            <filetokenizer/>
+            <replacestring from="@{from}" to="@{to}"/>
+          </tokenfilter>
+        </filterchain>
+      </loadresource>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-delete">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="output.path"/>
+      <patternset id="grammar.@{grammar}.patternset">
+        <include name="@{grammar}Lexer.java" />
+        <include name="@{grammar}Parser.java" />
+        <include name="@{grammar}ParserVisitor.java" />
+        <include name="@{grammar}ParserBaseVisitor.java" />
+      </patternset>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- delete parser and lexer so files will be generated -->
+      <delete dir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </delete>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-lexer">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- invoke ANTLR4 -->
+      <java classname="org.antlr.v4.Tool" fork="true" failonerror="true" classpathref="regenerate.classpath" taskname="antlr">
+        <sysproperty key="file.encoding" value="UTF-8"/>
+        <sysproperty key="user.language" value="en"/>
+        <sysproperty key="user.country" value="US"/>
+        <sysproperty key="user.variant" value=""/>
+        <arg value="-package"/>
+        <arg value="org.elasticsearch.plan.a"/>
+        <arg value="-o"/>
+        <arg path="${output.path}"/>
+        <arg path="${grammar.path}/@{grammar}Lexer.g4"/>
+      </java>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-parser">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- invoke ANTLR4 -->
+      <java classname="org.antlr.v4.Tool" fork="true" failonerror="true" classpathref="regenerate.classpath" taskname="antlr">
+        <sysproperty key="file.encoding" value="UTF-8"/>
+        <sysproperty key="user.language" value="en"/>
+        <sysproperty key="user.country" value="US"/>
+        <sysproperty key="user.variant" value=""/>
+        <arg value="-package"/>
+        <arg value="org.elasticsearch.plan.a"/>
+        <arg value="-no-listener"/>
+        <arg value="-visitor"/>
+        <!-- <arg value="-Xlog"/> -->
+        <arg value="-o"/>
+        <arg path="${output.path}"/>
+        <arg path="${grammar.path}/@{grammar}Parser.g4"/>
+      </java>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-fix">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <patternset id="grammar.@{grammar}.patternset">
+        <include name="@{grammar}Lexer.java" />
+        <include name="@{grammar}Parser.java" />
+        <include name="@{grammar}ParserVisitor.java" />
+        <include name="@{grammar}ParserBaseVisitor.java" />
+      </patternset>
+      <!-- fileset with files to edit -->
+      <fileset id="grammar.fileset" dir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </fileset>
+      <!-- remove files that are not needed to compile or at runtime -->
+      <delete dir="${grammar.path}" includes="@{grammar}*.tokens"/>
+      <delete dir="${output.path}" includes="@{grammar}*.tokens"/>
+      <!-- make the generated classes package private -->
+      <replaceregexp match="public ((interface|class) \Q@{grammar}\E\w+)" replace="\1" encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- nuke timestamps/filenames in generated files -->
+      <replaceregexp match="\Q// Generated from \E.*" replace="\/\/ ANTLR GENERATED CODE: DO NOT EDIT" encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- remove tabs in antlr generated files -->
+      <replaceregexp match="\t" flags="g" replace="  " encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- fix line endings -->
+      <fixcrlf srcdir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </fixcrlf>
+    </sequential>
+  </macrodef>
+</project>
diff --git a/plugins/lang-plan-a/build.gradle b/plugins/lang-plan-a/build.gradle
new file mode 100644
index 0000000..618c094
--- /dev/null
+++ b/plugins/lang-plan-a/build.gradle
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.apache.tools.ant.types.Path
+
+esplugin {
+  description 'An easy, safe and fast scripting language for Elasticsearch'
+  classname 'org.elasticsearch.plan.a.PlanAPlugin'
+}
+
+dependencies {
+  compile 'org.antlr:antlr4-runtime:4.5.1-1'
+  compile 'org.ow2.asm:asm:5.0.4'
+  compile 'org.ow2.asm:asm-commons:5.0.4'
+}
+
+compileJava.options.compilerArgs << '-Xlint:-cast,-fallthrough,-rawtypes'
+compileTestJava.options.compilerArgs << '-Xlint:-unchecked'
+
+// regeneration logic, comes in via ant right now
+// don't port it to gradle, it works fine.
+
+configurations {
+  regenerate
+}
+
+dependencies {
+  regenerate 'org.antlr:antlr4:4.5.1-1'
+}
+
+ant.references['regenerate.classpath'] = new Path(ant.project, configurations.regenerate.asPath)
+ant.importBuild 'ant.xml'
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1 b/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1
new file mode 100644
index 0000000..37f80b9
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1
@@ -0,0 +1 @@
+66144204f9d6d7d3f3f775622c2dd7e9bd511d97
\ No newline at end of file
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt b/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt
new file mode 100644
index 0000000..95d0a25
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt
@@ -0,0 +1,26 @@
+[The "BSD license"]
+Copyright (c) 2015 Terence Parr, Sam Harwell
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+ 1. Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright
+    notice, this list of conditions and the following disclaimer in the
+    documentation and/or other materials provided with the distribution.
+ 3. The name of the author may not be used to endorse or promote products
+    derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-NOTICE.txt b/plugins/lang-plan-a/licenses/antlr4-runtime-NOTICE.txt
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1 b/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1
new file mode 100644
index 0000000..9223dba
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1
@@ -0,0 +1 @@
+0da08b8cce7bbf903602a25a3a163ae252435795
diff --git a/plugins/lang-plan-a/licenses/asm-LICENSE.txt b/plugins/lang-plan-a/licenses/asm-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Télécom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/asm-NOTICE.txt b/plugins/lang-plan-a/licenses/asm-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1 b/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1
new file mode 100644
index 0000000..94fe0cd
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1
@@ -0,0 +1 @@
+5a556786086c23cd689a0328f8519db93821c04c
diff --git a/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt b/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Télécom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt b/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4 b/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4
new file mode 100644
index 0000000..5110a73
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+lexer grammar PlanALexer;
+
+@header {
+    import java.util.Set;
+}
+
+@members {
+    private Set<String> types = null;
+
+    void setTypes(Set<String> types) {
+        this.types = types;
+    }
+}
+
+WS: [ \t\n\r]+ -> skip;
+COMMENT: ( '//' .*? [\n\r] | '/*' .*? '*/' ) -> skip;
+
+LBRACK:    '{';
+RBRACK:    '}';
+LBRACE:    '[';
+RBRACE:    ']';
+LP:        '(';
+RP:        ')';
+DOT:       '.' -> mode(EXT);
+COMMA:     ',';
+SEMICOLON: ';';
+IF:        'if';
+ELSE:      'else';
+WHILE:     'while';
+DO:        'do';
+FOR:       'for';
+CONTINUE:  'continue';
+BREAK:     'break';
+RETURN:    'return';
+NEW:       'new';
+TRY:       'try';
+CATCH:     'catch';
+THROW:     'throw';
+
+BOOLNOT: '!';
+BWNOT:   '~';
+MUL:     '*';
+DIV:     '/';
+REM:     '%';
+ADD:     '+';
+SUB:     '-';
+LSH:     '<<';
+RSH:     '>>';
+USH:     '>>>';
+LT:      '<';
+LTE:     '<=';
+GT:      '>';
+GTE:     '>=';
+EQ:      '==';
+EQR:     '===';
+NE:      '!=';
+NER:     '!==';
+BWAND:   '&';
+BWXOR:   '^';
+BWOR:    '|';
+BOOLAND: '&&';
+BOOLOR:  '||';
+COND:    '?';
+COLON:   ':';
+INCR:    '++';
+DECR:    '--';
+
+ASSIGN: '=';
+AADD:   '+=';
+ASUB:   '-=';
+AMUL:   '*=';
+ADIV:   '/=';
+AREM:   '%=';
+AAND:   '&=';
+AXOR:   '^=';
+AOR:    '|=';
+ALSH:   '<<=';
+ARSH:   '>>=';
+AUSH:   '>>>=';
+ACAT:   '..=';
+
+OCTAL: '0' [0-7]+ [lL]?;
+HEX: '0' [xX] [0-9a-fA-F]+ [lL]?;
+INTEGER: ( '0' | [1-9] [0-9]* ) [lLfFdD]?;
+DECIMAL: ( '0' | [1-9] [0-9]* ) DOT [0-9]* ( [eE] [+\-]? [0-9]+ )? [fF]?;
+
+STRING: '"' ( '\\"' | '\\\\' | ~[\\"] )*? '"' {setText(getText().substring(1, getText().length() - 1));};
+CHAR: '\'' . '\''                             {setText(getText().substring(1, getText().length() - 1));};
+
+TRUE:  'true';
+FALSE: 'false';
+
+NULL: 'null';
+
+TYPE: ID GENERIC? {types.contains(getText().replace(" ", ""))}? {setText(getText().replace(" ", ""));};
+fragment GENERIC: ' '* '<' ' '* ( ID GENERIC? ) ' '* ( COMMA ' '* ( ID GENERIC? ) ' '* )* '>';
+ID: [_a-zA-Z] [_a-zA-Z0-9]*;
+
+mode EXT;
+EXTINTEGER: ( '0' | [1-9] [0-9]* ) -> mode(DEFAULT_MODE);
+EXTID: [_a-zA-Z] [_a-zA-Z0-9]* -> mode(DEFAULT_MODE);
diff --git a/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4 b/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4
new file mode 100644
index 0000000..1b177a4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4
@@ -0,0 +1,127 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+parser grammar PlanAParser;
+
+options { tokenVocab=PlanALexer; }
+
+source
+    : statement+ EOF
+    ;
+
+statement
+    : IF LP expression RP block ( ELSE block )?                                              # if
+    | WHILE LP expression RP ( block | empty )                                               # while
+    | DO block WHILE LP expression RP SEMICOLON?                                             # do
+    | FOR LP initializer? SEMICOLON expression? SEMICOLON afterthought? RP ( block | empty ) # for
+    | declaration SEMICOLON?                                                                 # decl
+    | CONTINUE SEMICOLON?                                                                    # continue
+    | BREAK SEMICOLON?                                                                       # break
+    | RETURN expression SEMICOLON?                                                           # return
+    | TRY block ( CATCH LP ( TYPE ID ) RP block )+                                           # try
+    | THROW expression SEMICOLON?                                                            # throw
+    | expression SEMICOLON?                                                                  # expr
+    ;
+
+block
+    : LBRACK statement* RBRACK                 # multiple
+    | statement                                # single
+    ;
+
+empty
+    : SEMICOLON
+    ;
+
+initializer
+    : declaration
+    | expression
+    ;
+
+afterthought
+    : expression
+    ;
+
+declaration
+    : decltype declvar ( COMMA declvar )*
+    ;
+
+decltype
+    : TYPE (LBRACE RBRACE)*
+    ;
+
+declvar
+    : ID ( ASSIGN expression )?
+    ;
+
+expression
+    :               LP expression RP                                    # precedence
+    |               ( OCTAL | HEX | INTEGER | DECIMAL )                 # numeric
+    |               CHAR                                                # char
+    |               TRUE                                                # true
+    |               FALSE                                               # false
+    |               NULL                                                # null
+    | <assoc=right> extstart increment                                  # postinc
+    | <assoc=right> increment extstart                                  # preinc
+    |               extstart                                            # external
+    | <assoc=right> ( BOOLNOT | BWNOT | ADD | SUB ) expression          # unary
+    | <assoc=right> LP decltype RP expression                           # cast
+    |               expression ( MUL | DIV | REM ) expression           # binary
+    |               expression ( ADD | SUB ) expression                 # binary
+    |               expression ( LSH | RSH | USH ) expression           # binary
+    |               expression ( LT | LTE | GT | GTE ) expression       # comp
+    |               expression ( EQ | EQR | NE | NER ) expression       # comp
+    |               expression BWAND expression                         # binary
+    |               expression BWXOR expression                         # binary
+    |               expression BWOR expression                          # binary
+    |               expression BOOLAND expression                       # bool
+    |               expression BOOLOR expression                        # bool
+    | <assoc=right> expression COND expression COLON expression         # conditional
+    | <assoc=right> extstart ( ASSIGN | AADD | ASUB | AMUL | ADIV
+                                      | AREM | AAND | AXOR | AOR
+                                      | ALSH | ARSH | AUSH ) expression # assignment
+    ;
+
+extstart
+    : extprec
+    | extcast
+    | exttype
+    | extvar
+    | extnew
+    | extstring
+    ;
+
+extprec:   LP ( extprec | extcast | exttype | extvar | extnew | extstring ) RP ( extdot | extbrace )?;
+extcast:   LP decltype RP ( extprec | extcast | exttype | extvar | extnew | extstring );
+extbrace:  LBRACE expression RBRACE ( extdot | extbrace )?;
+extdot:    DOT ( extcall | extfield );
+exttype:   TYPE extdot;
+extcall:   EXTID arguments ( extdot | extbrace )?;
+extvar:    ID ( extdot | extbrace )?;
+extfield:  ( EXTID | EXTINTEGER ) ( extdot | extbrace )?;
+extnew:    NEW TYPE ( ( arguments ( extdot | extbrace)? ) | ( ( LBRACE expression RBRACE )+ extdot? ) );
+extstring: STRING (extdot | extbrace )?;
+
+arguments
+    : ( LP ( expression ( COMMA expression )* )? RP )
+    ;
+
+increment
+    : INCR
+    | DECR
+    ;
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java
new file mode 100644
index 0000000..baa06f4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.antlr.v4.runtime.tree.ParseTree;
+
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Adapter {
+    static class StatementMetadata {
+        final ParserRuleContext source;
+
+        boolean last;
+
+        boolean allExit;
+        boolean allReturn;
+        boolean anyReturn;
+        boolean allBreak;
+        boolean anyBreak;
+        boolean allContinue;
+        boolean anyContinue;
+
+        private StatementMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            last = false;
+
+            allExit = false;
+            allReturn = false;
+            anyReturn = false;
+            allBreak = false;
+            anyBreak = false;
+            allContinue = false;
+            anyContinue = false;
+        }
+    }
+
+    static class ExpressionMetadata {
+        final ParserRuleContext source;
+
+        boolean read;
+        boolean statement;
+
+        Object preConst;
+        Object postConst;
+        boolean isNull;
+
+        Type to;
+        Type from;
+        boolean explicit;
+        boolean typesafe;
+
+        Cast cast;
+
+        private ExpressionMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            read = true;
+            statement = false;
+
+            preConst = null;
+            postConst = null;
+            isNull = false;
+
+            to = null;
+            from = null;
+            explicit = false;
+            typesafe = true;
+
+            cast = null;
+        }
+    }
+
+    static class ExternalMetadata {
+        final ParserRuleContext source;
+
+        boolean read;
+        ParserRuleContext storeExpr;
+        int token;
+        boolean pre;
+        boolean post;
+
+        int scope;
+        Type current;
+        boolean statik;
+        boolean statement;
+        Object constant;
+
+        private ExternalMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            read = false;
+            storeExpr = null;
+            token = 0;
+            pre = false;
+            post = false;
+
+            scope = 0;
+            current = null;
+            statik = false;
+            statement = false;
+            constant = null;
+        }
+    }
+
+    static class ExtNodeMetadata {
+        final ParserRuleContext parent;
+        final ParserRuleContext source;
+
+        Object target;
+        boolean last;
+
+        Type type;
+        Type promote;
+
+        Cast castFrom;
+        Cast castTo;
+
+        private ExtNodeMetadata(final ParserRuleContext parent, final ParserRuleContext source) {
+            this.parent = parent;
+            this.source = source;
+
+            target = null;
+            last = false;
+
+            type = null;
+            promote = null;
+
+            castFrom = null;
+            castTo = null;
+        }
+    }
+
+    static String error(final ParserRuleContext ctx) {
+        return "Error [" + ctx.getStart().getLine() + ":" + ctx.getStart().getCharPositionInLine() + "]: ";
+    }
+
+    final Definition definition;
+    final String source;
+    final ParserRuleContext root;
+    final CompilerSettings settings;
+
+    private final Map<ParserRuleContext, StatementMetadata> statementMetadata;
+    private final Map<ParserRuleContext, ExpressionMetadata> expressionMetadata;
+    private final Map<ParserRuleContext, ExternalMetadata> externalMetadata;
+    private final Map<ParserRuleContext, ExtNodeMetadata> extNodeMetadata;
+
+    Adapter(final Definition definition, final String source, final ParserRuleContext root, final CompilerSettings settings) {
+        this.definition = definition;
+        this.source = source;
+        this.root = root;
+        this.settings = settings;
+
+        statementMetadata = new HashMap<>();
+        expressionMetadata = new HashMap<>();
+        externalMetadata = new HashMap<>();
+        extNodeMetadata = new HashMap<>();
+    }
+
+    StatementMetadata createStatementMetadata(final ParserRuleContext source) {
+        final StatementMetadata sourcesmd = new StatementMetadata(source);
+        statementMetadata.put(source, sourcesmd);
+
+        return sourcesmd;
+    }
+
+    StatementMetadata getStatementMetadata(final ParserRuleContext source) {
+        final StatementMetadata sourcesmd = statementMetadata.get(source);
+
+        if (sourcesmd == null) {
+            throw new IllegalStateException(error(source) + "Statement metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourcesmd;
+    }
+
+    ExpressionContext updateExpressionTree(ExpressionContext source) {
+        if (source instanceof PrecedenceContext) {
+            final ParserRuleContext parent = source.getParent();
+            int index = 0;
+
+            for (final ParseTree child : parent.children) {
+                if (child == source) {
+                    break;
+                }
+
+                ++index;
+            }
+
+            while (source instanceof PrecedenceContext) {
+                source = ((PrecedenceContext)source).expression();
+            }
+
+            parent.children.set(index, source);
+        }
+
+        return source;
+    }
+
+    ExpressionMetadata createExpressionMetadata(ParserRuleContext source) {
+        final ExpressionMetadata sourceemd = new ExpressionMetadata(source);
+        expressionMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+    
+    ExpressionMetadata getExpressionMetadata(final ParserRuleContext source) {
+        final ExpressionMetadata sourceemd = expressionMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "Expression metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+
+    ExternalMetadata createExternalMetadata(final ParserRuleContext source) {
+        final ExternalMetadata sourceemd = new ExternalMetadata(source);
+        externalMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+
+    ExternalMetadata getExternalMetadata(final ParserRuleContext source) {
+        final ExternalMetadata sourceemd = externalMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "External metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+
+    ExtNodeMetadata createExtNodeMetadata(final ParserRuleContext parent, final ParserRuleContext source) {
+        final ExtNodeMetadata sourceemd = new ExtNodeMetadata(parent, source);
+        extNodeMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+
+    ExtNodeMetadata getExtNodeMetadata(final ParserRuleContext source) {
+        final ExtNodeMetadata sourceemd = extNodeMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "External metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java
new file mode 100644
index 0000000..a7e2986
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java
@@ -0,0 +1,2983 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.ArrayDeque;
+import java.util.Arrays;
+import java.util.Deque;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+
+import static org.elasticsearch.plan.a.Adapter.*;
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Analyzer extends PlanAParserBaseVisitor<Void> {
+    private static class Variable {
+        final String name;
+        final Type type;
+        final int slot;
+
+        private Variable(final String name, final Type type, final int slot) {
+            this.name = name;
+            this.type = type;
+            this.slot = slot;
+        }
+    }
+
+    static void analyze(final Adapter adapter) {
+        new Analyzer(adapter);
+    }
+
+    private final Adapter adapter;
+    private final Definition definition;
+    private final CompilerSettings settings;
+
+    private final Deque<Integer> scopes;
+    private final Deque<Variable> variables;
+
+    private Analyzer(final Adapter adapter) {
+        this.adapter = adapter;
+        definition = adapter.definition;
+        settings = adapter.settings;
+
+        scopes = new ArrayDeque<>();
+        variables = new ArrayDeque<>();
+
+        incrementScope();
+        addVariable(null, "this", definition.execType);
+        addVariable(null, "input", definition.smapType);
+
+        adapter.createStatementMetadata(adapter.root);
+        visit(adapter.root);
+
+        decrementScope();
+    }
+
+    void incrementScope() {
+        scopes.push(0);
+    }
+
+    void decrementScope() {
+        int remove = scopes.pop();
+
+        while (remove > 0) {
+            variables.pop();
+            --remove;
+        }
+    }
+
+    Variable getVariable(final String name) {
+        final Iterator<Variable> itr = variables.iterator();
+
+        while (itr.hasNext()) {
+            final Variable variable = itr.next();
+
+            if (variable.name.equals(name)) {
+                return variable;
+            }
+        }
+
+        return null;
+    }
+
+    Variable addVariable(final ParserRuleContext source, final String name, final Type type) {
+        if (getVariable(name) != null) {
+            if (source == null) {
+                throw new IllegalArgumentException("Argument name [" + name + "] already defined within the scope.");
+            } else {
+                throw new IllegalArgumentException(
+                        error(source) + "Variable name [" + name + "] already defined within the scope.");
+            }
+        }
+
+        final Variable previous = variables.peekFirst();
+        int slot = 0;
+
+        if (previous != null) {
+            slot += previous.slot + previous.type.type.getSize();
+        }
+
+        final Variable variable = new Variable(name, type, slot);
+        variables.push(variable);
+
+        final int update = scopes.pop() + 1;
+        scopes.push(update);
+
+        return variable;
+    }
+
+    @Override
+    public Void visitSource(final SourceContext ctx) {
+        final StatementMetadata sourcesmd = adapter.getStatementMetadata(ctx);
+        final List<StatementContext> statectxs = ctx.statement();
+        final StatementContext lastctx = statectxs.get(statectxs.size() - 1);
+
+        incrementScope();
+
+        for (final StatementContext statectx : statectxs) {
+            if (sourcesmd.allExit) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Statement will never be executed because all prior paths exit.");
+            }
+
+            final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+            statesmd.last = statectx == lastctx;
+            visit(statectx);
+
+            if (statesmd.anyContinue) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Cannot have a continue statement outside of a loop.");
+            }
+
+            if (statesmd.anyBreak) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Cannot have a break statement outside of a loop.");
+            }
+
+            sourcesmd.allExit = statesmd.allExit;
+            sourcesmd.allReturn = statesmd.allReturn;
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitIf(final IfContext ctx) {
+        final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            throw new IllegalArgumentException(error(ctx) + "If statement is not necessary.");
+        }
+
+        final BlockContext blockctx0 = ctx.block(0);
+        final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0);
+        blocksmd0.last = ifsmd.last;
+        visit(blockctx0);
+
+        ifsmd.anyReturn = blocksmd0.anyReturn;
+        ifsmd.anyBreak = blocksmd0.anyBreak;
+        ifsmd.anyContinue = blocksmd0.anyContinue;
+
+        if (ctx.ELSE() != null) {
+            final BlockContext blockctx1 = ctx.block(1);
+            final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1);
+            blocksmd1.last = ifsmd.last;
+            visit(blockctx1);
+
+            ifsmd.allExit = blocksmd0.allExit && blocksmd1.allExit;
+            ifsmd.allReturn = blocksmd0.allReturn && blocksmd1.allReturn;
+            ifsmd.anyReturn |= blocksmd1.anyReturn;
+            ifsmd.allBreak = blocksmd0.allBreak && blocksmd1.allBreak;
+            ifsmd.anyBreak |= blocksmd1.anyBreak;
+            ifsmd.allContinue = blocksmd0.allContinue && blocksmd1.allContinue;
+            ifsmd.anyContinue |= blocksmd1.anyContinue;
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitWhile(final WhileContext ctx) {
+        final StatementMetadata whilesmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        boolean exitrequired = false;
+
+        if (expremd.postConst != null) {
+            boolean constant = (boolean)expremd.postConst;
+
+            if (!constant) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never be executed.");
+            }
+
+            exitrequired = true;
+        }
+
+        final BlockContext blockctx = ctx.block();
+
+        if (blockctx != null) {
+            final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+            visit(blockctx);
+
+            if (blocksmd.allReturn) {
+                throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+            }
+
+            if (blocksmd.allBreak) {
+                throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+            }
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                whilesmd.allExit = true;
+                whilesmd.allReturn = true;
+            }
+        } else if (exitrequired) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDo(final DoContext ctx) {
+        final StatementMetadata dosmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final BlockContext blockctx = ctx.block();
+        final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+        visit(blockctx);
+
+        if (blocksmd.allReturn) {
+            throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+        }
+
+        if (blocksmd.allBreak) {
+            throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+        }
+
+        if (blocksmd.allContinue) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            final boolean exitrequired = (boolean)expremd.postConst;
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                dosmd.allExit = true;
+                dosmd.allReturn = true;
+            }
+
+            if (!exitrequired && !blocksmd.anyContinue) {
+                throw new IllegalArgumentException(error(ctx) + "All paths exit so the loop is not necessary.");
+            }
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitFor(final ForContext ctx) {
+        final StatementMetadata forsmd = adapter.getStatementMetadata(ctx);
+        boolean exitrequired = false;
+
+        incrementScope();
+
+        final InitializerContext initctx = ctx.initializer();
+
+        if (initctx != null) {
+            adapter.createStatementMetadata(initctx);
+            visit(initctx);
+        }
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = definition.booleanType;
+            visit(exprctx);
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                boolean constant = (boolean)expremd.postConst;
+
+                if (!constant) {
+                    throw new IllegalArgumentException(error(ctx) + "The loop will never be executed.");
+                }
+
+                exitrequired = true;
+            }
+        } else {
+            exitrequired = true;
+        }
+
+        final AfterthoughtContext atctx = ctx.afterthought();
+
+        if (atctx != null) {
+            adapter.createStatementMetadata(atctx);
+            visit(atctx);
+        }
+
+        final BlockContext blockctx = ctx.block();
+
+        if (blockctx != null) {
+            final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+            visit(blockctx);
+
+            if (blocksmd.allReturn) {
+                throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+            }
+
+            if (blocksmd.allBreak) {
+                throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+            }
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                forsmd.allExit = true;
+                forsmd.allReturn = true;
+            }
+        } else if (exitrequired) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecl(final DeclContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        adapter.createStatementMetadata(declctx);
+        visit(declctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitContinue(final ContinueContext ctx) {
+        final StatementMetadata continuesmd = adapter.getStatementMetadata(ctx);
+
+        continuesmd.allExit = true;
+        continuesmd.allContinue = true;
+        continuesmd.anyContinue = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBreak(final BreakContext ctx) {
+        final StatementMetadata breaksmd = adapter.getStatementMetadata(ctx);
+
+        breaksmd.allExit = true;
+        breaksmd.allBreak = true;
+        breaksmd.anyBreak = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitReturn(final ReturnContext ctx) {
+        final StatementMetadata returnsmd = adapter.getStatementMetadata(ctx);
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.objectType;
+        visit(exprctx);
+        markCast(expremd);
+
+        returnsmd.allExit = true;
+        returnsmd.allReturn = true;
+        returnsmd.anyReturn = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExpr(final ExprContext ctx) {
+        final StatementMetadata exprsmd = adapter.getStatementMetadata(ctx);
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.read = exprsmd.last;
+        visit(exprctx);
+
+        if (!expremd.statement && !exprsmd.last) {
+            throw new IllegalArgumentException(error(ctx) + "Not a statement.");
+        }
+
+        final boolean rtn = exprsmd.last && expremd.from.sort != Sort.VOID;
+        exprsmd.allExit = rtn;
+        exprsmd.allReturn = rtn;
+        exprsmd.anyReturn = rtn;
+        expremd.to = rtn ? definition.objectType : expremd.from;
+        markCast(expremd);
+
+        return null;
+    }
+
+    @Override
+    public Void visitMultiple(final MultipleContext ctx) {
+        final StatementMetadata multiplesmd = adapter.getStatementMetadata(ctx);
+        final List<StatementContext> statectxs = ctx.statement();
+        final StatementContext lastctx = statectxs.get(statectxs.size() - 1);
+
+        for (StatementContext statectx : statectxs) {
+            if (multiplesmd.allExit) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Statement will never be executed because all prior paths exit.");
+            }
+
+            final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+            statesmd.last = multiplesmd.last && statectx == lastctx;
+            visit(statectx);
+
+            multiplesmd.allExit = statesmd.allExit;
+            multiplesmd.allReturn = statesmd.allReturn && !statesmd.anyBreak && !statesmd.anyContinue;
+            multiplesmd.anyReturn |= statesmd.anyReturn;
+            multiplesmd.allBreak = !statesmd.anyReturn && statesmd.allBreak && !statesmd.anyContinue;
+            multiplesmd.anyBreak |= statesmd.anyBreak;
+            multiplesmd.allContinue = !statesmd.anyReturn && !statesmd.anyBreak && statesmd.allContinue;
+            multiplesmd.anyContinue |= statesmd.anyContinue;
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitSingle(final SingleContext ctx) {
+        final StatementMetadata singlesmd = adapter.getStatementMetadata(ctx);
+
+        final StatementContext statectx = ctx.statement();
+        final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+        statesmd.last = singlesmd.last;
+        visit(statectx);
+
+        singlesmd.allExit = statesmd.allExit;
+        singlesmd.allReturn = statesmd.allReturn;
+        singlesmd.anyReturn = statesmd.anyReturn;
+        singlesmd.allBreak = statesmd.allBreak;
+        singlesmd.anyBreak = statesmd.anyBreak;
+        singlesmd.allContinue = statesmd.allContinue;
+        singlesmd.anyContinue = statesmd.anyContinue;
+
+        return null;
+    }
+
+    @Override
+    public Void visitEmpty(final EmptyContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitInitializer(InitializerContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (declctx != null) {
+            adapter.createStatementMetadata(declctx);
+            visit(declctx);
+        } else if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.read = false;
+            visit(exprctx);
+
+            expremd.to = expremd.from;
+            markCast(expremd);
+
+            if (!expremd.statement) {
+                throw new IllegalArgumentException(error(exprctx) +
+                        "The intializer of a for loop must be a statement.");
+            }
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAfterthought(AfterthoughtContext ctx) {
+        ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.read = false;
+            visit(exprctx);
+
+            expremd.to = expremd.from;
+            markCast(expremd);
+
+            if (!expremd.statement) {
+                throw new IllegalArgumentException(error(exprctx) +
+                        "The afterthought of a for loop must be a statement.");
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclaration(final DeclarationContext ctx) {
+        final DecltypeContext decltypectx = ctx.decltype();
+        final ExpressionMetadata decltypeemd = adapter.createExpressionMetadata(decltypectx);
+        visit(decltypectx);
+
+        for (final DeclvarContext declvarctx : ctx.declvar()) {
+            final ExpressionMetadata declvaremd = adapter.createExpressionMetadata(declvarctx);
+            declvaremd.to = decltypeemd.from;
+            visit(declvarctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecltype(final DecltypeContext ctx) {
+        final ExpressionMetadata decltypeemd = adapter.getExpressionMetadata(ctx);
+
+        final String name = ctx.getText();
+        decltypeemd.from = definition.getType(name);
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclvar(final DeclvarContext ctx) {
+        final ExpressionMetadata declvaremd = adapter.getExpressionMetadata(ctx);
+
+        final String name = ctx.ID().getText();
+        declvaremd.postConst = addVariable(ctx, name, declvaremd.to).slot;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = declvaremd.to;
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitPrecedence(final PrecedenceContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitNumeric(final NumericContext ctx) {
+        final ExpressionMetadata numericemd = adapter.getExpressionMetadata(ctx);
+        final boolean negate = ctx.parent instanceof UnaryContext && ((UnaryContext)ctx.parent).SUB() != null;
+
+        if (ctx.DECIMAL() != null) {
+            final String svalue = (negate ? "-" : "") + ctx.DECIMAL().getText();
+
+            if (svalue.endsWith("f") || svalue.endsWith("F")) {
+                try {
+                    numericemd.from = definition.floatType;
+                    numericemd.preConst = Float.parseFloat(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else {
+                try {
+                    numericemd.from = definition.doubleType;
+                    numericemd.preConst = Double.parseDouble(svalue);
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid double constant [" + svalue + "].");
+                }
+            }
+        } else {
+            String svalue = negate ? "-" : "";
+            int radix;
+
+            if (ctx.OCTAL() != null) {
+                svalue += ctx.OCTAL().getText();
+                radix = 8;
+            } else if (ctx.INTEGER() != null) {
+                svalue += ctx.INTEGER().getText();
+                radix = 10;
+            } else if (ctx.HEX() != null) {
+                svalue += ctx.HEX().getText();
+                radix = 16;
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+
+            if (svalue.endsWith("d") || svalue.endsWith("D")) {
+                try {
+                    numericemd.from = definition.doubleType;
+                    numericemd.preConst = Double.parseDouble(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else if (svalue.endsWith("f") || svalue.endsWith("F")) {
+                try {
+                    numericemd.from = definition.floatType;
+                    numericemd.preConst = Float.parseFloat(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else if (svalue.endsWith("l") || svalue.endsWith("L")) {
+                try {
+                    numericemd.from = definition.longType;
+                    numericemd.preConst = Long.parseLong(svalue.substring(0, svalue.length() - 1), radix);
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid long constant [" + svalue + "].");
+                }
+            } else {
+                try {
+                    final Type type = numericemd.to;
+                    final Sort sort = type == null ? Sort.INT : type.sort;
+                    final int value = Integer.parseInt(svalue, radix);
+
+                    if (sort == Sort.BYTE && value >= Byte.MIN_VALUE && value <= Byte.MAX_VALUE) {
+                        numericemd.from = definition.byteType;
+                        numericemd.preConst = (byte)value;
+                    } else if (sort == Sort.CHAR && value >= Character.MIN_VALUE && value <= Character.MAX_VALUE) {
+                        numericemd.from = definition.charType;
+                        numericemd.preConst = (char)value;
+                    } else if (sort == Sort.SHORT && value >= Short.MIN_VALUE && value <= Short.MAX_VALUE) {
+                        numericemd.from = definition.shortType;
+                        numericemd.preConst = (short)value;
+                    } else {
+                        numericemd.from = definition.intType;
+                        numericemd.preConst = value;
+                    }
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid int constant [" + svalue + "].");
+                }
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitChar(final CharContext ctx) {
+        final ExpressionMetadata charemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.CHAR() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        charemd.preConst = ctx.CHAR().getText().charAt(0);
+        charemd.from = definition.charType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitTrue(final TrueContext ctx) {
+        final ExpressionMetadata trueemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.TRUE() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        trueemd.preConst = true;
+        trueemd.from = definition.booleanType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitFalse(final FalseContext ctx) {
+        final ExpressionMetadata falseemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.FALSE() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        falseemd.preConst = false;
+        falseemd.from = definition.booleanType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitNull(final NullContext ctx) {
+        final ExpressionMetadata nullemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.NULL() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        nullemd.isNull = true;
+
+        if (nullemd.to != null) {
+            if (nullemd.to.sort.primitive) {
+                throw new IllegalArgumentException("Cannot cast null to a primitive type [" + nullemd.to.name + "].");
+            }
+
+            nullemd.from = nullemd.to;
+        } else {
+            nullemd.from = definition.objectType;
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExternal(final ExternalContext ctx) {
+        final ExpressionMetadata extemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = extemd.read;
+        visit(extstartctx);
+
+        extemd.statement = extstartemd.statement;
+        extemd.preConst = extstartemd.constant;
+        extemd.from = extstartemd.current;
+        extemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitPostinc(final PostincContext ctx) {
+        final ExpressionMetadata postincemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = postincemd.read;
+        extstartemd.storeExpr = ctx.increment();
+        extstartemd.token = ADD;
+        extstartemd.post = true;
+        visit(extstartctx);
+
+        postincemd.statement = true;
+        postincemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        postincemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitPreinc(final PreincContext ctx) {
+        final ExpressionMetadata preincemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = preincemd.read;
+        extstartemd.storeExpr = ctx.increment();
+        extstartemd.token = ADD;
+        extstartemd.pre = true;
+        visit(extstartctx);
+
+        preincemd.statement = true;
+        preincemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        preincemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitUnary(final UnaryContext ctx) {
+        final ExpressionMetadata unaryemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+
+        if (ctx.BOOLNOT() != null) {
+            expremd.to = definition.booleanType;
+            visit(exprctx);
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                unaryemd.preConst = !(boolean)expremd.postConst;
+            }
+
+            unaryemd.from = definition.booleanType;
+        } else if (ctx.BWNOT() != null || ctx.ADD() != null || ctx.SUB() != null) {
+            visit(exprctx);
+
+            final Type promote = promoteNumeric(expremd.from, ctx.BWNOT() == null, true);
+
+            if (promote == null) {
+                throw new ClassCastException("Cannot apply [" + ctx.getChild(0).getText() + "] " +
+                        "operation to type [" + expremd.from.name + "].");
+            }
+
+            expremd.to = promote;
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                final Sort sort = promote.sort;
+
+                if (ctx.BWNOT() != null) {
+                    if (sort == Sort.INT) {
+                        unaryemd.preConst = ~(int)expremd.postConst;
+                    } else if (sort == Sort.LONG) {
+                        unaryemd.preConst = ~(long)expremd.postConst;
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                } else if (ctx.SUB() != null) {
+                    if (exprctx instanceof NumericContext) {
+                        unaryemd.preConst = expremd.postConst;
+                    } else {
+                        if (sort == Sort.INT) {
+                            if (settings.getNumericOverflow()) {
+                                unaryemd.preConst = -(int)expremd.postConst;
+                            } else {
+                                unaryemd.preConst = Math.negateExact((int)expremd.postConst);
+                            }
+                        } else if (sort == Sort.LONG) {
+                            if (settings.getNumericOverflow()) {
+                                unaryemd.preConst = -(long)expremd.postConst;
+                            } else {
+                                unaryemd.preConst = Math.negateExact((long)expremd.postConst);
+                            }
+                        } else if (sort == Sort.FLOAT) {
+                            unaryemd.preConst = -(float)expremd.postConst;
+                        } else if (sort == Sort.DOUBLE) {
+                            unaryemd.preConst = -(double)expremd.postConst;
+                        } else {
+                            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                        }
+                    }
+                } else if (ctx.ADD() != null) {
+                    if (sort == Sort.INT) {
+                        unaryemd.preConst = +(int)expremd.postConst;
+                    } else if (sort == Sort.LONG) {
+                        unaryemd.preConst = +(long)expremd.postConst;
+                    } else if (sort == Sort.FLOAT) {
+                        unaryemd.preConst = +(float)expremd.postConst;
+                    } else if (sort == Sort.DOUBLE) {
+                        unaryemd.preConst = +(double)expremd.postConst;
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            }
+
+            unaryemd.from = promote;
+            unaryemd.typesafe = expremd.typesafe;
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitCast(final CastContext ctx) {
+        final ExpressionMetadata castemd = adapter.getExpressionMetadata(ctx);
+
+        final DecltypeContext decltypectx = ctx.decltype();
+        final ExpressionMetadata decltypemd = adapter.createExpressionMetadata(decltypectx);
+        visit(decltypectx);
+
+        final Type type = decltypemd.from;
+        castemd.from = type;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = type;
+        expremd.explicit = true;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            castemd.preConst = expremd.postConst;
+        }
+
+        castemd.typesafe = expremd.typesafe && castemd.from.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBinary(final BinaryContext ctx) {
+        final ExpressionMetadata binaryemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        visit(exprctx0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        visit(exprctx1);
+
+        final boolean decimal = ctx.MUL() != null || ctx.DIV() != null || ctx.REM() != null || ctx.SUB() != null;
+        final boolean add = ctx.ADD() != null;
+        final boolean xor = ctx.BWXOR() != null;
+        final Type promote = add ? promoteAdd(expremd0.from, expremd1.from) :
+                             xor ? promoteXor(expremd0.from, expremd1.from) :
+                                   promoteNumeric(expremd0.from, expremd1.from, decimal, true);
+
+        if (promote == null) {
+            throw new ClassCastException("Cannot apply [" + ctx.getChild(1).getText() + "] " +
+                    "operation to types [" + expremd0.from.name + "] and [" + expremd1.from.name + "].");
+        }
+
+        final Sort sort = promote.sort;
+        expremd0.to = add && sort == Sort.STRING ? expremd0.from : promote;
+        expremd1.to = add && sort == Sort.STRING ? expremd1.from : promote;
+        markCast(expremd0);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            if (ctx.MUL() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst * (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.multiplyExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst * (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.multiplyExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst * (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.multiplyWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst * (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.multiplyWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.DIV() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst / (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst / (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst / (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst / (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.REM() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst % (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst % (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst % (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.remainderWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst % (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.remainderWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.ADD() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst + (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.addExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst + (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.addExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst + (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.addWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst + (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.addWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else if (sort == Sort.STRING) {
+                    binaryemd.preConst = "" + expremd0.postConst + expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.SUB() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst - (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.subtractExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst - (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.subtractExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst - (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.subtractWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst - (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.subtractWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.LSH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst << (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst << (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.RSH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst >> (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst >> (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.USH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst >>> (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst >>> (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWAND() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst & (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst & (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWXOR() != null) {
+                if (sort == Sort.BOOL) {
+                    binaryemd.preConst = (boolean)expremd0.postConst ^ (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst ^ (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst ^ (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWOR() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst | (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst | (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        binaryemd.from = promote;
+        binaryemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitComp(final CompContext ctx) {
+        final ExpressionMetadata compemd = adapter.getExpressionMetadata(ctx);
+        final boolean equality = ctx.EQ() != null || ctx.NE() != null;
+        final boolean reference = ctx.EQR() != null || ctx.NER() != null;
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        visit(exprctx0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        visit(exprctx1);
+
+        if (expremd0.isNull && expremd1.isNull) {
+            throw new IllegalArgumentException(error(ctx) + "Unnecessary comparison of null constants.");
+        }
+
+        final Type promote = equality ? promoteEquality(expremd0.from, expremd1.from) :
+                reference ? promoteReference(expremd0.from, expremd1.from) :
+                            promoteNumeric(expremd0.from, expremd1.from, true, true);
+
+        if (promote == null) {
+            throw new ClassCastException("Cannot apply [" + ctx.getChild(1).getText() + "] " +
+                    "operation to types [" + expremd0.from.name + "] and [" + expremd1.from.name + "].");
+        }
+
+        expremd0.to = promote;
+        expremd1.to = promote;
+        markCast(expremd0);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            final Sort sort = promote.sort;
+
+            if (ctx.EQ() != null || ctx.EQR() != null) {
+                if (sort == Sort.BOOL) {
+                    compemd.preConst = (boolean)expremd0.postConst == (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst == (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst == (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst == (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst == (double)expremd1.postConst;
+                } else {
+                    if (ctx.EQ() != null && !expremd0.isNull && !expremd1.isNull) {
+                        compemd.preConst = expremd0.postConst.equals(expremd1.postConst);
+                    } else if (ctx.EQR() != null) {
+                        compemd.preConst = expremd0.postConst == expremd1.postConst;
+                    }
+                }
+            } else if (ctx.NE() != null || ctx.NER() != null) {
+                if (sort == Sort.BOOL) {
+                    compemd.preConst = (boolean)expremd0.postConst != (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst != (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst != (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst != (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst != (double)expremd1.postConst;
+                } else {
+                    if (ctx.NE() != null && !expremd0.isNull && !expremd1.isNull) {
+                        compemd.preConst = expremd0.postConst.equals(expremd1.postConst);
+                    } else if (ctx.NER() != null) {
+                        compemd.preConst = expremd0.postConst == expremd1.postConst;
+                    }
+                }
+            } else if (ctx.GTE() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst >= (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst >= (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst >= (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst >= (double)expremd1.postConst;
+                }
+            } else if (ctx.GT() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst > (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst > (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst > (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst > (double)expremd1.postConst;
+                }
+            } else if (ctx.LTE() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst <= (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst <= (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst <= (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst <= (double)expremd1.postConst;
+                }
+            } else if (ctx.LT() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst < (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst < (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst < (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst < (double)expremd1.postConst;
+                }
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        compemd.from = definition.booleanType;
+        compemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBool(final BoolContext ctx) {
+        final ExpressionMetadata boolemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        expremd0.to = definition.booleanType;
+        visit(exprctx0);
+        markCast(expremd0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        expremd1.to = definition.booleanType;
+        visit(exprctx1);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            if (ctx.BOOLAND() != null) {
+                boolemd.preConst = (boolean)expremd0.postConst && (boolean)expremd1.postConst;
+            } else if (ctx.BOOLOR() != null) {
+                boolemd.preConst = (boolean)expremd0.postConst || (boolean)expremd1.postConst;
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        boolemd.from = definition.booleanType;
+        boolemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitConditional(final ConditionalContext ctx) {
+        final ExpressionMetadata condemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        expremd0.to = definition.booleanType;
+        visit(exprctx0);
+        markCast(expremd0);
+
+        if (expremd0.postConst != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unnecessary conditional statement.");
+        }
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        expremd1.to = condemd.to;
+        expremd1.explicit = condemd.explicit;
+        visit(exprctx1);
+
+        final ExpressionContext exprctx2 = adapter.updateExpressionTree(ctx.expression(2));
+        final ExpressionMetadata expremd2 = adapter.createExpressionMetadata(exprctx2);
+        expremd2.to = condemd.to;
+        expremd2.explicit = condemd.explicit;
+        visit(exprctx2);
+
+        if (condemd.to == null) {
+            final Type promote = promoteConditional(expremd1.from, expremd2.from, expremd1.preConst, expremd2.preConst);
+
+            expremd1.to = promote;
+            expremd2.to = promote;
+            condemd.from = promote;
+        } else {
+            condemd.from = condemd.to;
+        }
+
+        markCast(expremd1);
+        markCast(expremd2);
+
+        condemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitAssignment(final AssignmentContext ctx) {
+        final ExpressionMetadata assignemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+
+        extstartemd.read = assignemd.read;
+        extstartemd.storeExpr = adapter.updateExpressionTree(ctx.expression());
+
+        if (ctx.AMUL() != null) {
+            extstartemd.token = MUL;
+        } else if (ctx.ADIV() != null) {
+            extstartemd.token = DIV;
+        } else if (ctx.AREM() != null) {
+            extstartemd.token = REM;
+        } else if (ctx.AADD() != null) {
+            extstartemd.token = ADD;
+        } else if (ctx.ASUB() != null) {
+            extstartemd.token = SUB;
+        } else if (ctx.ALSH() != null) {
+            extstartemd.token = LSH;
+        } else if (ctx.AUSH() != null) {
+            extstartemd.token = USH;
+        } else if (ctx.ARSH() != null) {
+            extstartemd.token = RSH;
+        } else if (ctx.AAND() != null) {
+            extstartemd.token = BWAND;
+        } else if (ctx.AXOR() != null) {
+            extstartemd.token = BWXOR;
+        } else if (ctx.AOR() != null) {
+            extstartemd.token = BWOR;
+        }
+
+        visit(extstartctx);
+
+        assignemd.statement = true;
+        assignemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        assignemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstart(final ExtstartContext ctx) {
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(ctx, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(ctx, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(ctx, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(ctx, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(ctx, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtprec(final ExtprecContext ctx) {
+        final ExtNodeMetadata precenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = precenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null || bracectx != null) {
+            ++parentemd.scope;
+        }
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(parent, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(parent, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(parent, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(parent, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(parent, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        parentemd.statement = false;
+
+        if (dotctx != null) {
+            --parentemd.scope;
+
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            --parentemd.scope;
+
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcast(final ExtcastContext ctx) {
+        final ExtNodeMetadata castenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = castenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(parent, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(parent, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(parent, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(parent, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(parent, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        final DecltypeContext declctx = ctx.decltype();
+        final ExpressionMetadata declemd = adapter.createExpressionMetadata(declctx);
+        visit(declctx);
+
+        castenmd.castTo = getLegalCast(ctx, parentemd.current, declemd.from, true);
+        castenmd.type = declemd.from;
+        parentemd.current = declemd.from;
+        parentemd.statement = false;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtbrace(final ExtbraceContext ctx) {
+        final ExtNodeMetadata braceenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = braceenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final boolean array = parentemd.current.sort == Sort.ARRAY;
+        final boolean def = parentemd.current.sort == Sort.DEF;
+        boolean map = false;
+        boolean list = false;
+
+        try {
+            parentemd.current.clazz.asSubclass(Map.class);
+            map = true;
+        } catch (ClassCastException exception) {
+            // Do nothing.
+        }
+
+        try {
+            parentemd.current.clazz.asSubclass(List.class);
+            list = true;
+        } catch (ClassCastException exception) {
+            // Do nothing.
+        }
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        braceenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+
+        if (array || def) {
+            expremd.to = array ? definition.intType : definition.objectType;
+            visit(exprctx);
+            markCast(expremd);
+
+            braceenmd.target = "#brace";
+            braceenmd.type = def ? definition.defType :
+                    definition.getType(parentemd.current.struct, parentemd.current.type.getDimensions() - 1);
+            analyzeLoadStoreExternal(ctx);
+            parentemd.current = braceenmd.type;
+
+            if (dotctx != null) {
+                adapter.createExtNodeMetadata(parent, dotctx);
+                visit(dotctx);
+            } else if (bracectx != null) {
+                adapter.createExtNodeMetadata(parent, bracectx);
+                visit(bracectx);
+            }
+        } else {
+            final boolean store = braceenmd.last && parentemd.storeExpr != null;
+            final boolean get = parentemd.read || parentemd.token > 0 || !braceenmd.last;
+            final boolean set = braceenmd.last && store;
+
+            Method getter;
+            Method setter;
+            Type valuetype;
+            Type settype;
+
+            if (map) {
+                getter = parentemd.current.struct.methods.get("get");
+                setter = parentemd.current.struct.methods.get("put");
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal map get shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (setter != null && setter.arguments.size() != 2) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal map set shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (getter != null && setter != null && (!getter.arguments.get(0).equals(setter.arguments.get(0))
+                        || !getter.rtn.equals(setter.arguments.get(1)))) {
+                    throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                }
+
+                valuetype = setter != null ? setter.arguments.get(0) : getter != null ? getter.arguments.get(0) : null;
+                settype = setter == null ? null : setter.arguments.get(1);
+            } else if (list) {
+                getter = parentemd.current.struct.methods.get("get");
+                setter = parentemd.current.struct.methods.get("add");
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                        getter.arguments.get(0).sort != Sort.INT)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal list get shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (setter != null && (setter.arguments.size() != 2 || setter.arguments.get(0).sort != Sort.INT)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal list set shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (getter != null && setter != null && (!getter.arguments.get(0).equals(setter.arguments.get(0))
+                        || !getter.rtn.equals(setter.arguments.get(1)))) {
+                    throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                }
+
+                valuetype = definition.intType;
+                settype = setter == null ? null : setter.arguments.get(1);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+
+            if ((get || set) && (!get || getter != null) && (!set || setter != null)) {
+                expremd.to = valuetype;
+                visit(exprctx);
+                markCast(expremd);
+
+                braceenmd.target = new Object[] {getter, setter, true, null};
+                braceenmd.type = get ? getter.rtn : settype;
+                analyzeLoadStoreExternal(ctx);
+                parentemd.current = get ? getter.rtn : setter.rtn;
+            }
+        }
+
+        if (braceenmd.target == null) {
+            throw new IllegalArgumentException(error(ctx) +
+                    "Attempting to address a non-array type [" + parentemd.current.name + "] as an array.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtdot(final ExtdotContext ctx) {
+        final ExtNodeMetadata dotemnd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = dotemnd.parent;
+
+        final ExtcallContext callctx = ctx.extcall();
+        final ExtfieldContext fieldctx = ctx.extfield();
+
+        if (callctx != null) {
+            adapter.createExtNodeMetadata(parent, callctx);
+            visit(callctx);
+        } else if (fieldctx != null) {
+            adapter.createExtNodeMetadata(parent, fieldctx);
+            visit(fieldctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExttype(final ExttypeContext ctx) {
+        final ExtNodeMetadata typeenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = typeenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (parentemd.current != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected static type.");
+        }
+
+        final String typestr = ctx.TYPE().getText();
+        typeenmd.type = definition.getType(typestr);
+        parentemd.current = typeenmd.type;
+        parentemd.statik = true;
+
+        final ExtdotContext dotctx = ctx.extdot();
+        adapter.createExtNodeMetadata(parent, dotctx);
+        visit(dotctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcall(final ExtcallContext ctx) {
+        final ExtNodeMetadata callenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = callenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        callenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final String name = ctx.EXTID().getText();
+
+        if (parentemd.current.sort == Sort.ARRAY) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected call [" + name + "] on an array.");
+        } else if (callenmd.last && parentemd.storeExpr != null) {
+            throw new IllegalArgumentException(error(ctx) + "Cannot assign a value to a call [" + name + "].");
+        }
+
+        final Struct struct = parentemd.current.struct;
+        final List<ExpressionContext> arguments = ctx.arguments().expression();
+        final int size = arguments.size();
+        Type[] types;
+
+        final Method method = parentemd.statik ? struct.functions.get(name) : struct.methods.get(name);
+        final boolean def = parentemd.current.sort == Sort.DEF;
+
+        if (method == null && !def) {
+            throw new IllegalArgumentException(
+                    error(ctx) + "Unknown call [" + name + "] on type [" + struct.name + "].");
+        } else if (method != null) {
+            types = new Type[method.arguments.size()];
+            method.arguments.toArray(types);
+
+            callenmd.target = method;
+            callenmd.type = method.rtn;
+            parentemd.statement = !parentemd.read && callenmd.last;
+            parentemd.current = method.rtn;
+
+            if (size != types.length) {
+                throw new IllegalArgumentException(error(ctx) + "When calling [" + name + "] on type " +
+                        "[" + struct.name + "] expected [" + types.length + "] arguments," +
+                        " but found [" + arguments.size() + "].");
+            }
+        } else {
+            types = new Type[arguments.size()];
+            Arrays.fill(types, definition.defType);
+
+            callenmd.target = name;
+            callenmd.type = definition.defType;
+            parentemd.statement = !parentemd.read && callenmd.last;
+            parentemd.current = callenmd.type;
+        }
+
+        for (int argument = 0; argument < size; ++argument) {
+            final ExpressionContext exprctx = adapter.updateExpressionTree(arguments.get(argument));
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = types[argument];
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        parentemd.statik = false;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtvar(final ExtvarContext ctx) {
+        final ExtNodeMetadata varenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = varenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final String name = ctx.ID().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (parentemd.current != null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected variable [" + name + "] load.");
+        }
+
+        varenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final Variable variable = getVariable(name);
+
+        if (variable == null) {
+            throw new IllegalArgumentException(error(ctx) + "Unknown variable [" + name + "].");
+        }
+
+        varenmd.target = variable.slot;
+        varenmd.type = variable.type;
+        analyzeLoadStoreExternal(ctx);
+        parentemd.current = varenmd.type;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtfield(final ExtfieldContext ctx) {
+        final ExtNodeMetadata memberenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = memberenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (ctx.EXTID() == null && ctx.EXTINTEGER() == null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected parser state.");
+        }
+
+        final String value = ctx.EXTID() == null ? ctx.EXTINTEGER().getText() : ctx.EXTID().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        memberenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+        final boolean store = memberenmd.last && parentemd.storeExpr != null;
+
+        if (parentemd.current == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected field [" + value + "] load.");
+        }
+
+        if (parentemd.current.sort == Sort.ARRAY) {
+            if ("length".equals(value)) {
+                if (!parentemd.read) {
+                    throw new IllegalArgumentException(error(ctx) + "Must read array field [length].");
+                } else if (store) {
+                    throw new IllegalArgumentException(
+                            error(ctx) + "Cannot write to read-only array field [length].");
+                }
+
+                memberenmd.target = "#length";
+                memberenmd.type = definition.intType;
+                parentemd.current = definition.intType;
+            } else {
+                throw new IllegalArgumentException(error(ctx) + "Unexpected array field [" + value + "].");
+            }
+        } else if (parentemd.current.sort == Sort.DEF) {
+            memberenmd.target = value;
+            memberenmd.type = definition.defType;
+            analyzeLoadStoreExternal(ctx);
+            parentemd.current = memberenmd.type;
+        } else {
+            final Struct struct = parentemd.current.struct;
+            final Field field = parentemd.statik ? struct.statics.get(value) : struct.members.get(value);
+
+            if (field != null) {
+                if (store && java.lang.reflect.Modifier.isFinal(field.reflect.getModifiers())) {
+                    throw new IllegalArgumentException(error(ctx) + "Cannot write to read-only" +
+                            " field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                memberenmd.target = field;
+                memberenmd.type = field.type;
+                analyzeLoadStoreExternal(ctx);
+                parentemd.current = memberenmd.type;
+            } else {
+                final boolean get = parentemd.read || parentemd.token > 0 || !memberenmd.last;
+                final boolean set = memberenmd.last && store;
+
+                Method getter = struct.methods.get("get" + Character.toUpperCase(value.charAt(0)) + value.substring(1));
+                Method setter = struct.methods.get("set" + Character.toUpperCase(value.charAt(0)) + value.substring(1));
+                Object constant = null;
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || !getter.arguments.isEmpty())) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal get shortcut on field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                if (setter != null && (setter.rtn.sort != Sort.VOID || setter.arguments.size() != 1)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal set shortcut on field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                Type settype = setter == null ? null : setter.arguments.get(0);
+
+                if (getter == null && setter == null) {
+                    if (ctx.EXTID() != null) {
+                        try {
+                            parentemd.current.clazz.asSubclass(Map.class);
+
+                            getter = parentemd.current.struct.methods.get("get");
+                            setter = parentemd.current.struct.methods.get("put");
+
+                            if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                                getter.arguments.get(0).sort != Sort.STRING)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal map get shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (setter != null && (setter.arguments.size() != 2 ||
+                                    setter.arguments.get(0).sort != Sort.STRING)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal map set shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (getter != null && setter != null && !getter.rtn.equals(setter.arguments.get(1))) {
+                                throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                            }
+
+                            settype = setter == null ? null : setter.arguments.get(1);
+                            constant = value;
+                        } catch (ClassCastException exception) {
+                            //Do nothing.
+                        }
+                    } else if (ctx.EXTINTEGER() != null) {
+                        try {
+                            parentemd.current.clazz.asSubclass(List.class);
+
+                            getter = parentemd.current.struct.methods.get("get");
+                            setter = parentemd.current.struct.methods.get("add");
+
+                            if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                                    getter.arguments.get(0).sort != Sort.INT)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list get shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (setter != null && (setter.rtn.sort != Sort.VOID || setter.arguments.size() != 2 ||
+                                    setter.arguments.get(0).sort != Sort.INT)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list add shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (getter != null && setter != null && !getter.rtn.equals(setter.arguments.get(1))) {
+                                throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                            }
+
+                            settype = setter == null ? null : setter.arguments.get(1);
+
+                            try {
+                                constant = Integer.parseInt(value);
+                            } catch (NumberFormatException exception) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list shortcut value [" + value + "].");
+                            }
+                        } catch (ClassCastException exception) {
+                            //Do nothing.
+                        }
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                }
+
+                if ((get || set) && (!get || getter != null) && (!set || setter != null)) {
+                    memberenmd.target = new Object[] {getter, setter, constant != null, constant};
+                    memberenmd.type = get ? getter.rtn : settype;
+                    analyzeLoadStoreExternal(ctx);
+                    parentemd.current = get ? getter.rtn : setter.rtn;
+                }
+            }
+
+            if (memberenmd.target == null) {
+                throw new IllegalArgumentException(
+                        error(ctx) + "Unknown field [" + value + "] for type [" + struct.name + "].");
+            }
+        }
+
+        parentemd.statik = false;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtnew(ExtnewContext ctx) {
+        final ExtNodeMetadata newenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = newenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        newenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final String name = ctx.TYPE().getText();
+        final Struct struct = definition.structs.get(name);
+
+        if (parentemd.current != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected new call.");
+        } else if (struct == null) {
+            throw new IllegalArgumentException(error(ctx) + "Specified type [" + name + "] not found.");
+        } else if (newenmd.last && parentemd.storeExpr != null) {
+            throw new IllegalArgumentException(error(ctx) + "Cannot assign a value to a new call.");
+        }
+
+        final boolean newclass = ctx.arguments() != null;
+        final boolean newarray = !ctx.expression().isEmpty();
+
+        final List<ExpressionContext> arguments = newclass ? ctx.arguments().expression() : ctx.expression();
+        final int size = arguments.size();
+
+        Type[] types;
+
+        if (newarray) {
+            if (!parentemd.read) {
+                throw new IllegalArgumentException(error(ctx) + "A newly created array must be assigned.");
+            }
+
+            types = new Type[size];
+            Arrays.fill(types, definition.intType);
+
+            newenmd.target = "#makearray";
+
+            if (size > 1) {
+                newenmd.type = definition.getType(struct, size);
+                parentemd.current = newenmd.type;
+            } else if (size == 1) {
+                newenmd.type = definition.getType(struct, 0);
+                parentemd.current = definition.getType(struct, 1);
+            } else {
+                throw new IllegalArgumentException(error(ctx) + "A newly created array cannot have zero dimensions.");
+            }
+        } else if (newclass) {
+            final Constructor constructor = struct.constructors.get("new");
+
+            if (constructor != null) {
+                types = new Type[constructor.arguments.size()];
+                constructor.arguments.toArray(types);
+
+                newenmd.target = constructor;
+                newenmd.type = definition.getType(struct, 0);
+                parentemd.statement = !parentemd.read && newenmd.last;
+                parentemd.current = newenmd.type;
+            } else {
+                throw new IllegalArgumentException(
+                        error(ctx) + "Unknown new call on type [" + struct.name + "].");
+            }
+        } else {
+            throw new IllegalArgumentException(error(ctx) + "Unknown parser state.");
+        }
+
+        if (size != types.length) {
+            throw new IllegalArgumentException(error(ctx) + "When calling [" + name + "] on type " +
+                    "[" + struct.name + "] expected [" + types.length + "] arguments," +
+                    " but found [" + arguments.size() + "].");
+        }
+
+        for (int argument = 0; argument < size; ++argument) {
+            final ExpressionContext exprctx = adapter.updateExpressionTree(arguments.get(argument));
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = types[argument];
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstring(final ExtstringContext ctx) {
+        final ExtNodeMetadata memberenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = memberenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final String string = ctx.STRING().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        memberenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+        final boolean store = memberenmd.last && parentemd.storeExpr != null;
+
+        if (parentemd.current != null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected String constant [" + string + "].");
+        }
+
+        if (!parentemd.read) {
+            throw new IllegalArgumentException(error(ctx) + "Must read String constant [" + string + "].");
+        } else if (store) {
+            throw new IllegalArgumentException(
+                    error(ctx) + "Cannot write to read-only String constant [" + string + "].");
+        }
+
+        memberenmd.target = string;
+        memberenmd.type = definition.stringType;
+        parentemd.current = definition.stringType;
+
+        if (memberenmd.last) {
+            parentemd.constant = string;
+        }
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitArguments(final ArgumentsContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitIncrement(IncrementContext ctx) {
+        final ExpressionMetadata incremd = adapter.getExpressionMetadata(ctx);
+        final Sort sort = incremd.to == null ? null : incremd.to.sort;
+        final boolean positive = ctx.INCR() != null;
+
+        if (incremd.to == null) {
+            incremd.preConst = positive ? 1 : -1;
+            incremd.from = definition.intType;
+        } else {
+            switch (sort) {
+                case LONG:
+                    incremd.preConst = positive ? 1L : -1L;
+                    incremd.from = definition.longType;
+                case FLOAT:
+                    incremd.preConst = positive ? 1.0F : -1.0F;
+                    incremd.from = definition.floatType;
+                case DOUBLE:
+                    incremd.preConst = positive ? 1.0 : -1.0;
+                    incremd.from = definition.doubleType;
+                default:
+                    incremd.preConst = positive ? 1 : -1;
+                    incremd.from = definition.intType;
+            }
+        }
+
+        return null;
+    }
+
+    private void analyzeLoadStoreExternal(final ParserRuleContext source) {
+        final ExtNodeMetadata extenmd = adapter.getExtNodeMetadata(source);
+        final ParserRuleContext parent = extenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (extenmd.last && parentemd.storeExpr != null) {
+            final ParserRuleContext store = parentemd.storeExpr;
+            final ExpressionMetadata storeemd = adapter.createExpressionMetadata(parentemd.storeExpr);
+            final int token = parentemd.token;
+
+            if (token > 0) {
+                visit(store);
+
+                final boolean add = token == ADD;
+                final boolean xor = token == BWAND || token == BWXOR || token == BWOR;
+                final boolean decimal = token == MUL || token == DIV || token == REM || token == SUB;
+
+                extenmd.promote = add ? promoteAdd(extenmd.type, storeemd.from) :
+                                  xor ? promoteXor(extenmd.type, storeemd.from) :
+                                        promoteNumeric(extenmd.type, storeemd.from, decimal, true);
+
+                if (extenmd.promote == null) {
+                    throw new IllegalArgumentException("Cannot apply compound assignment to " +
+                            " types [" + extenmd.type.name + "] and [" + storeemd.from.name + "].");
+                }
+
+                extenmd.castFrom = getLegalCast(source, extenmd.type, extenmd.promote, false);
+                extenmd.castTo = getLegalCast(source, extenmd.promote, extenmd.type, true);
+
+                storeemd.to = add && extenmd.promote.sort == Sort.STRING ? storeemd.from : extenmd.promote;
+                markCast(storeemd);
+            } else {
+                storeemd.to = extenmd.type;
+                visit(store);
+                markCast(storeemd);
+            }
+        }
+    }
+
+    private void markCast(final ExpressionMetadata emd) {
+        if (emd.from == null) {
+            throw new IllegalStateException(error(emd.source) + "From cast type should never be null.");
+        }
+
+        if (emd.to != null) {
+            emd.cast = getLegalCast(emd.source, emd.from, emd.to, emd.explicit || !emd.typesafe);
+
+            if (emd.preConst != null && emd.to.sort.constant) {
+                emd.postConst = constCast(emd.source, emd.preConst, emd.cast);
+            }
+        } else {
+            throw new IllegalStateException(error(emd.source) + "To cast type should never be null.");
+        }
+    }
+
+    private Cast getLegalCast(final ParserRuleContext source, final Type from, final Type to, final boolean explicit) {
+        final Cast cast = new Cast(from, to);
+
+        if (from.equals(to)) {
+            return cast;
+        }
+
+        if (from.sort == Sort.DEF && to.sort != Sort.VOID || from.sort != Sort.VOID && to.sort == Sort.DEF) {
+            final Transform transform = definition.transforms.get(cast);
+
+            if (transform != null) {
+                return transform;
+            }
+
+            return cast;
+        }
+
+        switch (from.sort) {
+            case BOOL:
+                switch (to.sort) {
+                    case OBJECT:
+                    case BOOL_OBJ:
+                        return checkTransform(source, cast);
+                }
+
+                break;
+            case BYTE:
+                switch (to.sort) {
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case SHORT:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case CHAR:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case INT:
+                switch (to.sort) {
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case LONG:
+                switch (to.sort) {
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case FLOAT:
+                switch (to.sort) {
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case DOUBLE:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case OBJECT:
+            case NUMBER:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case BOOL_OBJ:
+                switch (to.sort) {
+                    case BOOL:
+                        return checkTransform(source, cast);
+                }
+
+                break;
+            case BYTE_OBJ:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case CHAR:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case SHORT_OBJ:
+                switch (to.sort) {
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case CHAR:
+                    case BYTE_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case CHAR_OBJ:
+                switch (to.sort) {
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case INT_OBJ:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case LONG_OBJ:
+                switch (to.sort) {
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case FLOAT_OBJ:
+                switch (to.sort) {
+                    case FLOAT:
+                    case DOUBLE:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case DOUBLE_OBJ:
+                switch (to.sort) {
+                    case DOUBLE:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+        }
+
+        try {
+            from.clazz.asSubclass(to.clazz);
+
+            return cast;
+        } catch (ClassCastException cce0) {
+            try {
+                if (explicit) {
+                    to.clazz.asSubclass(from.clazz);
+
+                    return cast;
+                } else {
+                    throw new ClassCastException(
+                            error(source) + "Cannot cast from [" + from.name + "] to [" + to.name + "].");
+                }
+            } catch (ClassCastException cce1) {
+                throw new ClassCastException(
+                        error(source) + "Cannot cast from [" + from.name + "] to [" + to.name + "].");
+            }
+        }
+    }
+
+    private Transform checkTransform(final ParserRuleContext source, final Cast cast) {
+        final Transform transform = definition.transforms.get(cast);
+
+        if (transform == null) {
+            throw new ClassCastException(
+                    error(source) + "Cannot cast from [" + cast.from.name + "] to [" + cast.to.name + "].");
+        }
+
+        return transform;
+    }
+
+    private Object constCast(final ParserRuleContext source, final Object constant, final Cast cast) {
+        if (cast instanceof Transform) {
+            final Transform transform = (Transform)cast;
+            return invokeTransform(source, transform, constant);
+        } else {
+            final Sort fsort = cast.from.sort;
+            final Sort tsort = cast.to.sort;
+
+            if (fsort == tsort) {
+                return constant;
+            } else if (fsort.numeric && tsort.numeric) {
+                Number number;
+
+                if (fsort == Sort.CHAR) {
+                    number = (int)(char)constant;
+                } else {
+                    number = (Number)constant;
+                }
+
+                switch (tsort) {
+                    case BYTE:   return number.byteValue();
+                    case SHORT:  return number.shortValue();
+                    case CHAR:   return (char)number.intValue();
+                    case INT:    return number.intValue();
+                    case LONG:   return number.longValue();
+                    case FLOAT:  return number.floatValue();
+                    case DOUBLE: return number.doubleValue();
+                    default:
+                        throw new IllegalStateException(error(source) + "Expected numeric type for cast.");
+                }
+            } else {
+                throw new IllegalStateException(error(source) + "No valid constant cast from " +
+                        "[" + cast.from.clazz.getCanonicalName() + "] to " +
+                        "[" + cast.to.clazz.getCanonicalName() + "].");
+            }
+        }
+    }
+
+    private Object invokeTransform(final ParserRuleContext source, final Transform transform, final Object object) {
+        final Method method = transform.method;
+        final java.lang.reflect.Method jmethod = method.reflect;
+        final int modifiers = jmethod.getModifiers();
+
+        try {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                return jmethod.invoke(null, object);
+            } else {
+                return jmethod.invoke(object);
+            }
+        } catch (IllegalAccessException | IllegalArgumentException |
+                java.lang.reflect.InvocationTargetException | NullPointerException |
+                ExceptionInInitializerError exception) {
+            throw new IllegalStateException(error(source) + "Unable to invoke transform to cast constant from " +
+                    "[" + transform.from.name + "] to [" + transform.to.name + "].");
+        }
+    }
+
+    private Type promoteNumeric(final Type from, boolean decimal, boolean primitive) {
+        final Sort sort = from.sort;
+
+        if (sort == Sort.DEF) {
+            return definition.defType;
+        } else if ((sort == Sort.DOUBLE || sort == Sort.DOUBLE_OBJ || sort == Sort.NUMBER) && decimal) {
+             return primitive ? definition.doubleType : definition.doubleobjType;
+        } else if ((sort == Sort.FLOAT || sort == Sort.FLOAT_OBJ) && decimal) {
+            return primitive ? definition.floatType : definition.floatobjType;
+        } else if (sort == Sort.LONG || sort == Sort.LONG_OBJ || sort == Sort.NUMBER) {
+            return primitive ? definition.longType : definition.longobjType;
+        } else if (sort.numeric) {
+            return primitive ? definition.intType : definition.intobjType;
+        }
+
+        return null;
+    }
+
+    private Type promoteNumeric(final Type from0, final Type from1, boolean decimal, boolean primitive) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        if (decimal) {
+            if (sort0 == Sort.DOUBLE || sort0 == Sort.DOUBLE_OBJ || sort0 == Sort.NUMBER ||
+                    sort1 == Sort.DOUBLE || sort1 == Sort.DOUBLE_OBJ || sort1 == Sort.NUMBER) {
+                return primitive ? definition.doubleType : definition.doubleobjType;
+            } else if (sort0 == Sort.FLOAT || sort0 == Sort.FLOAT_OBJ || sort1 == Sort.FLOAT || sort1 == Sort.FLOAT_OBJ) {
+                return primitive ? definition.floatType : definition.floatobjType;
+            }
+        }
+
+        if (sort0 == Sort.LONG || sort0 == Sort.LONG_OBJ || sort0 == Sort.NUMBER ||
+                sort1 == Sort.LONG || sort1 == Sort.LONG_OBJ || sort1 == Sort.NUMBER) {
+             return primitive ? definition.longType : definition.longobjType;
+        } else if (sort0.numeric && sort1.numeric) {
+            return primitive ? definition.intType : definition.intobjType;
+        }
+
+        return null;
+    }
+
+    private Type promoteAdd(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.STRING || sort1 == Sort.STRING) {
+            return definition.stringType;
+        }
+
+        return promoteNumeric(from0, from1, true, true);
+    }
+
+    private Type promoteXor(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0.bool || sort1.bool) {
+            return definition.booleanType;
+        }
+
+        return promoteNumeric(from0, from1, false, true);
+    }
+
+    private Type promoteEquality(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        final boolean primitive = sort0.primitive && sort1.primitive;
+
+        if (sort0.bool && sort1.bool) {
+            return primitive ? definition.booleanType : definition.byteobjType;
+        }
+
+        if (sort0.numeric && sort1.numeric) {
+            return promoteNumeric(from0, from1, true, primitive);
+        }
+
+        return definition.objectType;
+    }
+
+    private Type promoteReference(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        if (sort0.primitive && sort1.primitive) {
+            if (sort0.bool && sort1.bool) {
+                return definition.booleanType;
+            }
+
+            if (sort0.numeric && sort1.numeric) {
+                return promoteNumeric(from0, from1, true, true);
+            }
+        }
+
+        return definition.objectType;
+    }
+
+    private Type promoteConditional(final Type from0, final Type from1, final Object const0, final Object const1) {
+        if (from0.equals(from1)) {
+            return from0;
+        }
+
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        final boolean primitive = sort0.primitive && sort1.primitive;
+
+        if (sort0.bool && sort1.bool) {
+            return primitive ? definition.booleanType : definition.booleanobjType;
+        }
+
+        if (sort0.numeric && sort1.numeric) {
+            if (sort0 == Sort.DOUBLE || sort0 == Sort.DOUBLE_OBJ || sort1 == Sort.DOUBLE || sort1 == Sort.DOUBLE_OBJ) {
+                return primitive ? definition.doubleType : definition.doubleobjType;
+            } else if (sort0 == Sort.FLOAT || sort0 == Sort.FLOAT_OBJ || sort1 == Sort.FLOAT || sort1 == Sort.FLOAT_OBJ) {
+                return primitive ? definition.floatType : definition.floatobjType;
+            } else if (sort0 == Sort.LONG || sort0 == Sort.LONG_OBJ || sort1 == Sort.LONG || sort1 == Sort.LONG_OBJ) {
+                return sort0.primitive && sort1.primitive ? definition.longType : definition.longobjType;
+            } else {
+                if (sort0 == Sort.BYTE || sort0 == Sort.BYTE_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        return primitive ? definition.byteType : definition.byteobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        if (const1 != null) {
+                            final short constant = (short)const1;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.SHORT || sort0 == Sort.SHORT_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        if (const0 != null) {
+                            final short constant = (short)const0;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Short.MAX_VALUE && constant >= Short.MIN_VALUE) {
+                                return primitive ? definition.shortType : definition.shortobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.CHAR || sort0 == Sort.CHAR_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.charType : definition.charobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Character.MAX_VALUE && constant >= Character.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.INT || sort0 == Sort.INT_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Short.MAX_VALUE && constant >= Short.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Character.MAX_VALUE && constant >= Character.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                }
+            }
+        }
+
+        final Pair pair = new Pair(from0, from1);
+        final Type bound = definition.bounds.get(pair);
+
+        return bound == null ? definition.objectType : bound;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java
new file mode 100644
index 0000000..6f4a237
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java
@@ -0,0 +1,154 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.security.CodeSource;
+import java.security.SecureClassLoader;
+import java.security.cert.Certificate;
+
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.elasticsearch.bootstrap.BootstrapInfo;
+
+final class Compiler {
+    private static Definition DEFAULT_DEFINITION = new Definition(new Definition());
+
+    /** we define the class with lowest privileges */
+    private static final CodeSource CODESOURCE;
+
+    static {
+        try {
+            CODESOURCE = new CodeSource(new URL("file:" + BootstrapInfo.UNTRUSTED_CODEBASE), (Certificate[]) null);
+        } catch (MalformedURLException impossible) {
+            throw new RuntimeException(impossible);
+        }
+    }
+
+    static class Loader extends SecureClassLoader {
+        Loader(ClassLoader parent) {
+            super(parent);
+        }
+
+        Class<? extends Executable> define(String name, byte[] bytes) {
+            return defineClass(name, bytes, 0, bytes.length, CODESOURCE).asSubclass(Executable.class);
+        }
+    }
+
+    static Executable compile(Loader loader, final String name, final String source, final Definition custom, CompilerSettings settings) {
+        long start = System.currentTimeMillis();
+
+        final Definition definition = custom == null ? DEFAULT_DEFINITION : new Definition(custom);
+
+        //long end = System.currentTimeMillis() - start;
+        //System.out.println("types: " + end);
+        //start = System.currentTimeMillis();
+
+        //final ParserRuleContext root = createParseTree(source, types);
+        final ANTLRInputStream stream = new ANTLRInputStream(source);
+        final ErrorHandlingLexer lexer = new ErrorHandlingLexer(stream);
+        final PlanAParser parser = new PlanAParser(new CommonTokenStream(lexer));
+        final ParserErrorStrategy strategy = new ParserErrorStrategy();
+
+        lexer.removeErrorListeners();
+        lexer.setTypes(definition.structs.keySet());
+
+        //List<? extends Token> tokens = lexer.getAllTokens();
+
+        //for (final Token token : tokens) {
+        //    System.out.println(token.getType() + " " + token.getText());
+        //}
+
+        parser.removeErrorListeners();
+        parser.setErrorHandler(strategy);
+
+        ParserRuleContext root = parser.source();
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("tree: " + end);
+
+        final Adapter adapter = new Adapter(definition, source, root, settings);
+
+        start = System.currentTimeMillis();
+
+        Analyzer.analyze(adapter);
+        //System.out.println(root.toStringTree(parser));
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("analyze: " + end);
+        //start = System.currentTimeMillis();
+
+        final byte[] bytes = Writer.write(adapter);
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("write: " + end);
+        //start = System.currentTimeMillis();
+
+        final Executable executable = createExecutable(loader, definition, name, source, bytes);
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("create: " + end);
+
+        return executable;
+    }
+
+    private static ParserRuleContext createParseTree(String source, Definition definition) {
+        final ANTLRInputStream stream = new ANTLRInputStream(source);
+        final ErrorHandlingLexer lexer = new ErrorHandlingLexer(stream);
+        final PlanAParser parser = new PlanAParser(new CommonTokenStream(lexer));
+        final ParserErrorStrategy strategy = new ParserErrorStrategy();
+
+        lexer.removeErrorListeners();
+        lexer.setTypes(definition.structs.keySet());
+
+        parser.removeErrorListeners();
+        parser.setErrorHandler(strategy);
+
+        ParserRuleContext root = parser.source();
+        // System.out.println(root.toStringTree(parser));
+        return root;
+    }
+
+    private static Executable createExecutable(Loader loader, Definition definition, String name, String source, byte[] bytes) {
+        try {
+            // for debugging:
+             //try {
+             //   FileOutputStream f = new FileOutputStream(new File("/Users/jdconrad/lang/generated/out.class"), false);
+             //   f.write(bytes);
+             //   f.close();
+             //} catch (Exception e) {
+             //   throw new RuntimeException(e);
+             //}
+
+            final Class<? extends Executable> clazz = loader.define(Writer.CLASS_NAME, bytes);
+            final java.lang.reflect.Constructor<? extends Executable> constructor =
+                    clazz.getConstructor(Definition.class, String.class, String.class);
+
+            return constructor.newInstance(definition, name, source);
+        } catch (Exception exception) {
+            throw new IllegalStateException(
+                    "An internal error occurred attempting to define the script [" + name + "].", exception);
+        }
+    }
+
+    private Compiler() {}
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java
new file mode 100644
index 0000000..f66b65d
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** 
+ * Settings to use when compiling a script 
+ */
+final class CompilerSettings {
+
+    private boolean numericOverflow = true;
+
+    /**
+     * Returns {@code true} if numeric operations should overflow, {@code false}
+     * if they should signal an exception.
+     * <p>
+     * If this value is {@code true} (default), then things behave like java:
+     * overflow for integer types can result in unexpected values / unexpected
+     * signs, and overflow for floating point types can result in infinite or
+     * {@code NaN} values.
+     */
+    public boolean getNumericOverflow() {
+        return numericOverflow;
+    }
+
+    /**
+     * Set {@code true} for numerics to overflow, false to deliver exceptions.
+     * @see #getNumericOverflow
+     */
+    public void setNumericOverflow(boolean allow) {
+        this.numericOverflow = allow;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
new file mode 100644
index 0000000..2a1eb13
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
@@ -0,0 +1,1250 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandle;
+import java.lang.reflect.Array;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.plan.a.Definition.*;
+
+public class Def {
+    public static Object methodCall(final Object owner, final String name, final Definition definition,
+                                    final Object[] arguments, final boolean[] typesafe) {
+        final Method method = getMethod(owner, name, definition);
+
+        if (method == null) {
+            throw new IllegalArgumentException("Unable to find dynamic method [" + name + "] " +
+                    "for class [" + owner.getClass().getCanonicalName() + "].");
+        }
+
+        final MethodHandle handle = method.handle;
+        final List<Type> types = method.arguments;
+        final Object[] parameters = new Object[arguments.length + 1];
+
+        parameters[0] = owner;
+
+        if (types.size() != arguments.length) {
+            throw new IllegalArgumentException("When dynamically calling [" + name + "] from class " +
+                    "[" + owner.getClass() + "] expected [" + types.size() + "] arguments," +
+                    " but found [" + arguments.length + "].");
+        }
+
+        try {
+            for (int count = 0; count < arguments.length; ++count) {
+                if (typesafe[count]) {
+                    parameters[count + 1] = arguments[count];
+                } else {
+                    final Transform transform = getTransform(arguments[count].getClass(), types.get(count).clazz, definition);
+                    parameters[count + 1] = transform == null ? arguments[count] : transform.method.handle.invoke(arguments[count]);
+                }
+            }
+
+            return handle.invokeWithArguments(parameters);
+        } catch (Throwable throwable) {
+            throw new IllegalArgumentException("Error invoking method [" + name + "] " +
+                    "with owner class [" + owner.getClass().getCanonicalName() + "].", throwable);
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static void fieldStore(final Object owner, Object value, final String name,
+                                  final Definition definition, final boolean typesafe) {
+        final Field field = getField(owner, name, definition);
+        MethodHandle handle = null;
+
+        if (field == null) {
+            final String set = "set" + Character.toUpperCase(name.charAt(0)) + name.substring(1);
+            final Method method = getMethod(owner, set, definition);
+
+            if (method != null) {
+                handle = method.handle;
+            }
+        } else {
+            handle = field.setter;
+        }
+
+        if (handle != null) {
+            try {
+                if (!typesafe) {
+                    final Transform transform = getTransform(value.getClass(), handle.type().parameterType(1), definition);
+
+                    if (transform != null) {
+                        value = transform.method.handle.invoke(value);
+                    }
+                }
+
+                handle.invoke(owner, value);
+            } catch (Throwable throwable) {
+                throw new IllegalArgumentException("Error storing value [" + value + "] " +
+                        "in field [" + name + "] with owner class [" + owner.getClass() + "].", throwable);
+            }
+        } else if (owner instanceof Map) {
+            ((Map)owner).put(name, value);
+        } else if (owner instanceof List) {
+            try {
+                final int index = Integer.parseInt(name);
+                ((List)owner).add(index, value);
+            } catch (NumberFormatException exception) {
+                throw new IllegalArgumentException( "Illegal list shortcut value [" + name + "].");
+            }
+        } else {
+            throw new IllegalArgumentException("Unable to find dynamic field [" + name + "] " +
+                    "for class [" + owner.getClass().getCanonicalName() + "].");
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static Object fieldLoad(final Object owner, final String name, final Definition definition) {
+        if (owner.getClass().isArray() && "length".equals(name)) {
+            return Array.getLength(owner);
+        } else {
+            final Field field = getField(owner, name, definition);
+            MethodHandle handle;
+
+            if (field == null) {
+                final String get = "get" + Character.toUpperCase(name.charAt(0)) + name.substring(1);
+                final Method method = getMethod(owner, get, definition);
+
+                if (method != null) {
+                    handle = method.handle;
+                } else if (owner instanceof Map) {
+                    return ((Map)owner).get(name);
+                } else if (owner instanceof List) {
+                    try {
+                        final int index = Integer.parseInt(name);
+
+                        return ((List)owner).get(index);
+                    } catch (NumberFormatException exception) {
+                        throw new IllegalArgumentException( "Illegal list shortcut value [" + name + "].");
+                    }
+                } else {
+                    throw new IllegalArgumentException("Unable to find dynamic field [" + name + "] " +
+                            "for class [" + owner.getClass().getCanonicalName() + "].");
+                }
+            } else {
+                handle = field.getter;
+            }
+
+            if (handle == null) {
+                throw new IllegalArgumentException(
+                        "Unable to read from field [" + name + "] with owner class [" + owner.getClass() + "].");
+            } else {
+                try {
+                    return handle.invoke(owner);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error loading value from " +
+                            "field [" + name + "] with owner class [" + owner.getClass() + "].", throwable);
+                }
+            }
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static void arrayStore(final Object array, Object index, Object value, final Definition definition,
+                                  final boolean indexsafe, final boolean valuesafe) {
+        if (array instanceof Map) {
+            ((Map)array).put(index, value);
+        } else {
+            try {
+                if (!indexsafe) {
+                    final Transform transform = getTransform(index.getClass(), Integer.class, definition);
+
+                    if (transform != null) {
+                        index = transform.method.handle.invoke(index);
+                    }
+                }
+            } catch (final Throwable throwable) {
+                throw new IllegalArgumentException(
+                        "Error storing value [" + value + "] in list using index [" + index + "].", throwable);
+            }
+
+            if (array.getClass().isArray()) {
+                try {
+                    if (!valuesafe) {
+                        final Transform transform = getTransform(value.getClass(), array.getClass().getComponentType(), definition);
+
+                        if (transform != null) {
+                            value = transform.method.handle.invoke(value);
+                        }
+                    }
+
+                    Array.set(array, (int)index, value);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error storing value [" + value + "] " +
+                            "in array class [" + array.getClass().getCanonicalName() + "].", throwable);
+                }
+            } else if (array instanceof List) {
+                ((List)array).add((int)index, value);
+            } else {
+                throw new IllegalArgumentException("Attempting to address a non-array type " +
+                        "[" + array.getClass().getCanonicalName() + "] as an array.");
+            }
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static Object arrayLoad(final Object array, Object index,
+                                   final Definition definition, final boolean indexsafe) {
+        if (array instanceof Map) {
+            return ((Map)array).get(index);
+        } else {
+            try {
+                if (!indexsafe) {
+                    final Transform transform = getTransform(index.getClass(), Integer.class, definition);
+
+                    if (transform != null) {
+                        index = transform.method.handle.invoke(index);
+                    }
+                }
+            } catch (final Throwable throwable) {
+                throw new IllegalArgumentException(
+                        "Error loading value using index [" + index + "].", throwable);
+            }
+
+            if (array.getClass().isArray()) {
+                try {
+                    return Array.get(array, (int)index);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error loading value from " +
+                            "array class [" + array.getClass().getCanonicalName() + "].", throwable);
+                }
+            } else if (array instanceof List) {
+                return ((List)array).get((int)index);
+            } else {
+                throw new IllegalArgumentException("Attempting to address a non-array type " +
+                        "[" + array.getClass().getCanonicalName() + "] as an array.");
+            }
+        }
+    }
+
+    public static Method getMethod(final Object owner, final String name, final Definition definition) {
+        Struct struct = null;
+        Class<?> clazz = owner.getClass();
+        Method method = null;
+
+        while (clazz != null) {
+            struct = definition.classes.get(clazz);
+
+            if (struct != null) {
+                method = struct.methods.get(name);
+
+                if (method != null) {
+                    break;
+                }
+            }
+
+            for (final Class iface : clazz.getInterfaces()) {
+                struct = definition.classes.get(iface);
+
+                if (struct != null) {
+                    method = struct.methods.get(name);
+
+                    if (method != null) {
+                        break;
+                    }
+                }
+            }
+
+            if (struct != null) {
+                method = struct.methods.get(name);
+
+                if (method != null) {
+                    break;
+                }
+            }
+
+            clazz = clazz.getSuperclass();
+        }
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Unable to find a dynamic struct for class [" + owner.getClass() + "].");
+        }
+
+        return method;
+    }
+
+    public static Field getField(final Object owner, final String name, final Definition definition) {
+        Struct struct = null;
+        Class<?> clazz = owner.getClass();
+        Field field = null;
+
+        while (clazz != null) {
+            struct = definition.classes.get(clazz);
+
+            if (struct != null) {
+                field = struct.members.get(name);
+
+                if (field != null) {
+                    break;
+                }
+            }
+
+            for (final Class iface : clazz.getInterfaces()) {
+                struct = definition.classes.get(iface);
+
+                if (struct != null) {
+                    field = struct.members.get(name);
+
+                    if (field != null) {
+                        break;
+                    }
+                }
+            }
+
+            if (struct != null) {
+                field = struct.members.get(name);
+
+                if (field != null) {
+                    break;
+                }
+            }
+
+            clazz = clazz.getSuperclass();
+        }
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Unable to find a dynamic struct for class [" + owner.getClass() + "].");
+        }
+
+        return field;
+    }
+
+    public static Transform getTransform(Class<?> fromClass, Class<?> toClass, final Definition definition) {
+        Struct fromStruct = null;
+        Struct toStruct = null;
+
+        if (fromClass.equals(toClass)) {
+            return null;
+        }
+
+        while (fromClass != null) {
+            fromStruct = definition.classes.get(fromClass);
+
+            if (fromStruct != null) {
+                break;
+            }
+
+            for (final Class iface : fromClass.getInterfaces()) {
+                fromStruct = definition.classes.get(iface);
+
+                if (fromStruct != null) {
+                    break;
+                }
+            }
+
+            if (fromStruct != null) {
+                break;
+            }
+
+            fromClass = fromClass.getSuperclass();
+        }
+
+        if (fromStruct != null) {
+            while (toClass != null) {
+                toStruct = definition.classes.get(toClass);
+
+                if (toStruct != null) {
+                    break;
+                }
+
+                for (final Class iface : toClass.getInterfaces()) {
+                    toStruct = definition.classes.get(iface);
+
+                    if (toStruct != null) {
+                        break;
+                    }
+                }
+
+                if (toStruct != null) {
+                    break;
+                }
+
+                toClass = toClass.getSuperclass();
+            }
+        }
+
+        if (toStruct != null) {
+            final Type fromType = definition.getType(fromStruct.name);
+            final Type toType = definition.getType(toStruct.name);
+            final Cast cast = new Cast(fromType, toType);
+
+            return definition.transforms.get(cast);
+        }
+
+        return null;
+    }
+
+    public static Object not(final Object unary) {
+        if (unary instanceof Double || unary instanceof Float || unary instanceof Long) {
+            return ~((Number)unary).longValue();
+        } else if (unary instanceof Number) {
+            return ~((Number)unary).intValue();
+        } else if (unary instanceof Character) {
+            return ~(int)(char)unary;
+        }
+
+        throw new ClassCastException("Cannot apply [~] operation to type " +
+                "[" + unary.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object neg(final Object unary) {
+        if (unary instanceof Double) {
+            return -(double)unary;
+        } else if (unary instanceof Float) {
+            return -(float)unary;
+        } else if (unary instanceof Long) {
+            return -(long)unary;
+        } else if (unary instanceof Number) {
+            return -((Number)unary).intValue();
+        } else if (unary instanceof Character) {
+            return -(char)unary;
+        }
+
+        throw new ClassCastException("Cannot apply [-] operation to type " +
+                "[" + unary.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object mul(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() * ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() * ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() * ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() * ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() * (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() * (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() * (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() * (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left * ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left * ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left * ((Number)right).longValue();
+                } else {
+                    return (int)(char)left * ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left * (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [*] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object div(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() / ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() / ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() / ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() / ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() / (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() / (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() / (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() / (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left / ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left / ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left / ((Number)right).longValue();
+                } else {
+                    return (int)(char)left / ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left / (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [/] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object rem(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() % ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() % ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() % ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() % ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() % (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() % (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() % (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() % (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left % ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left % ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left % ((Number)right).longValue();
+                } else {
+                    return (int)(char)left % ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left % (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [%] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+    
+    public static Object add(final Object left, final Object right) {
+        if (left instanceof String || right instanceof String) {
+            return "" + left + right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() + ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() + ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() + ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() + ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() + (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() + (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() + (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() + (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left + ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left + ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left + ((Number)right).longValue();
+                } else {
+                    return (int)(char)left + ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left + (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [+] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object sub(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() - ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() - ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() - ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() - ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() - (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() - (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() - (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() - (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left - ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left - ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left - ((Number)right).longValue();
+                } else {
+                    return (int)(char)left - ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left - (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [-] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object lsh(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() << ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() << ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() << (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() << (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left << ((Number)right).longValue();
+                } else {
+                    return (int)(char)left << ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left << (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<<] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object rsh(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >> ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() >> (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >> (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left >> ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >> (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object ush(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >>> ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >>> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() >>> (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >>> (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left >>> ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >>> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >>> (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>>>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+    
+    public static Object and(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left && (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() & ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() & ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() & (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() & (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left & ((Number)right).longValue();
+                } else {
+                    return (int)(char)left & ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left & (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [&] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object xor(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left ^ (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() ^ ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() ^ ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() ^ (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() ^ (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left ^ ((Number)right).longValue();
+                } else {
+                    return (int)(char)left ^ ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left ^ (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [^] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object or(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left || (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() | ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() | ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() | (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() | (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left | ((Number)right).longValue();
+                } else {
+                    return (int)(char)left | ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left | (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [|] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean eq(final Object left, final Object right) {
+        if (left != null && right != null) {
+            if (left instanceof Double) {
+                if (right instanceof Number) {
+                    return (double)left == ((Number)right).doubleValue();
+                } else if (right instanceof Character) {
+                    return (double)left == (double)(char)right;
+                }
+            } else if (right instanceof Double) {
+                if (left instanceof Number) {
+                    return ((Number)left).doubleValue() == (double)right;
+                } else if (left instanceof Character) {
+                    return (double)(char)left == ((Number)right).doubleValue();
+                }
+            } else if (left instanceof Float) {
+                if (right instanceof Number) {
+                    return (float)left == ((Number)right).floatValue();
+                } else if (right instanceof Character) {
+                    return (float)left == (float)(char)right;
+                }
+            } else if (right instanceof Float) {
+                if (left instanceof Number) {
+                    return ((Number)left).floatValue() == (float)right;
+                } else if (left instanceof Character) {
+                    return (float)(char)left == ((Number)right).floatValue();
+                }
+            } else if (left instanceof Long) {
+                if (right instanceof Number) {
+                    return (long)left == ((Number)right).longValue();
+                } else if (right instanceof Character) {
+                    return (long)left == (long)(char)right;
+                }
+            } else if (right instanceof Long) {
+                if (left instanceof Number) {
+                    return ((Number)left).longValue() == (long)right;
+                } else if (left instanceof Character) {
+                    return (long)(char)left == ((Number)right).longValue();
+                }
+            } else if (left instanceof Number) {
+                if (right instanceof Number) {
+                    return ((Number)left).intValue() == ((Number)right).intValue();
+                } else if (right instanceof Character) {
+                    return ((Number)left).intValue() == (int)(char)right;
+                }
+            } else if (right instanceof Number && left instanceof Character) {
+                return (int)(char)left == ((Number)right).intValue();
+            } else if (left instanceof Character && right instanceof Character) {
+                return (int)(char)left == (int)(char)right;
+            }
+
+            return left.equals(right);
+        }
+
+        return left == null && right == null;
+    }
+
+    public static boolean lt(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() < ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() < ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() < ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() < ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() < (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() < (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() < (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() < (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left < ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left < ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left < ((Number)right).longValue();
+                } else {
+                    return (int)(char)left < ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left < (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean lte(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() <= ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() <= ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() <= ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() <= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() <= (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() <= (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() <= (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() <= (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left <= ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left <= ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left <= ((Number)right).longValue();
+                } else {
+                    return (int)(char)left <= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left <= (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<=] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean gt(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() > ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() > ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() > ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() > ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() > (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() > (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() > (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() > (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left > ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left > ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left > ((Number)right).longValue();
+                } else {
+                    return (int)(char)left > ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left > (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean gte(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() >= ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() >= ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >= ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() >= (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() >= (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() >= (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >= (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left >= ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left >= ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left >= ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >= (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean DefToboolean(final Object value) {
+        if (value instanceof Boolean) {
+            return (boolean)value;
+        } else if (value instanceof Character) {
+            return ((char)value) != 0;
+        } else {
+            return ((Number)value).intValue() != 0;
+        }
+    }
+
+    public static byte DefTobyte(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (byte)1 : 0;
+        } else if (value instanceof Character) {
+            return (byte)(char)value;
+        } else {
+            return ((Number)value).byteValue();
+        }
+    }
+
+    public static short DefToshort(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (short)1 : 0;
+        } else if (value instanceof Character) {
+            return (short)(char)value;
+        } else {
+            return ((Number)value).shortValue();
+        }
+    }
+
+    public static char DefTochar(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (char)1 : 0;
+        } else if (value instanceof Character) {
+            return ((Character)value);
+        } else {
+            return (char)((Number)value).intValue();
+        }
+    }
+
+    public static int DefToint(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1 : 0;
+        } else if (value instanceof Character) {
+            return (int)(char)value;
+        } else {
+            return ((Number)value).intValue();
+        }
+    }
+
+    public static long DefTolong(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1L : 0;
+        } else if (value instanceof Character) {
+            return (long)(char)value;
+        } else {
+            return ((Number)value).longValue();
+        }
+    }
+
+    public static float DefTofloat(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (float)1 : 0;
+        } else if (value instanceof Character) {
+            return (float)(char)value;
+        } else {
+            return ((Number)value).floatValue();
+        }
+    }
+
+    public static double DefTodouble(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (double)1 : 0;
+        } else if (value instanceof Character) {
+            return (double)(char)value;
+        } else {
+            return ((Number)value).doubleValue();
+        }
+    }
+
+    public static Boolean DefToBoolean(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return (boolean)value;
+        } else if (value instanceof Character) {
+            return ((char)value) != 0;
+        } else {
+            return ((Number)value).intValue() != 0;
+        }
+    }
+
+    public static Byte DefToByte(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (byte)1 : 0;
+        } else if (value instanceof Character) {
+            return (byte)(char)value;
+        } else {
+            return ((Number)value).byteValue();
+        }
+    }
+
+    public static Short DefToShort(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (short)1 : 0;
+        } else if (value instanceof Character) {
+            return (short)(char)value;
+        } else {
+            return ((Number)value).shortValue();
+        }
+    }
+
+    public static Character DefToCharacter(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (char)1 : 0;
+        } else if (value instanceof Character) {
+            return ((Character)value);
+        } else {
+            return (char)((Number)value).intValue();
+        }
+    }
+
+    public static Integer DefToInteger(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1 : 0;
+        } else if (value instanceof Character) {
+            return (int)(char)value;
+        } else {
+            return ((Number)value).intValue();
+        }
+    }
+
+    public static Long DefToLong(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1L : 0;
+        } else if (value instanceof Character) {
+            return (long)(char)value;
+        } else {
+            return ((Number)value).longValue();
+        }
+    }
+
+    public static Float DefToFloat(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (float)1 : 0;
+        } else if (value instanceof Character) {
+            return (float)(char)value;
+        } else {
+            return ((Number)value).floatValue();
+        }
+    }
+
+    public static Double DefToDouble(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (double)1 : 0;
+        } else if (value instanceof Character) {
+            return (double)(char)value;
+        } else {
+            return ((Number)value).doubleValue();
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java
new file mode 100644
index 0000000..5c52a20
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java
@@ -0,0 +1,1809 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandle;
+import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+class Definition {
+    enum Sort {
+        VOID(       void.class      , 0 , true  , false , false , false ),
+        BOOL(       boolean.class   , 1 , true  , true  , false , true  ),
+        BYTE(       byte.class      , 1 , true  , false , true  , true  ),
+        SHORT(      short.class     , 1 , true  , false , true  , true  ),
+        CHAR(       char.class      , 1 , true  , false , true  , true  ),
+        INT(        int.class       , 1 , true  , false , true  , true  ),
+        LONG(       long.class      , 2 , true  , false , true  , true  ),
+        FLOAT(      float.class     , 1 , true  , false , true  , true  ),
+        DOUBLE(     double.class    , 2 , true  , false , true  , true  ),
+
+        VOID_OBJ(   Void.class      , 1 , true  , false , false , false ),
+        BOOL_OBJ(   Boolean.class   , 1 , false , true  , false , false ),
+        BYTE_OBJ(   Byte.class      , 1 , false , false , true  , false ),
+        SHORT_OBJ(  Short.class     , 1 , false , false , true  , false ),
+        CHAR_OBJ(   Character.class , 1 , false , false , true  , false ),
+        INT_OBJ(    Integer.class   , 1 , false , false , true  , false ),
+        LONG_OBJ(   Long.class      , 1 , false , false , true  , false ),
+        FLOAT_OBJ(  Float.class     , 1 , false , false , true  , false ),
+        DOUBLE_OBJ( Double.class    , 1 , false , false , true  , false ),
+
+        NUMBER(     Number.class    , 1 , false , false , true  , false ),
+        STRING(     String.class    , 1 , false , false , false , true  ),
+
+        OBJECT(     null            , 1 , false , false , false , false ),
+        DEF(        null            , 1 , false , false , false , false ),
+        ARRAY(      null            , 1 , false , false , false , false );
+
+        final Class<?> clazz;
+        final int size;
+        final boolean primitive;
+        final boolean bool;
+        final boolean numeric;
+        final boolean constant;
+
+        Sort(final Class<?> clazz, final int size, final boolean primitive,
+             final boolean bool, final boolean numeric, final boolean constant) {
+            this.clazz = clazz;
+            this.size = size;
+            this.bool = bool;
+            this.primitive = primitive;
+            this.numeric = numeric;
+            this.constant = constant;
+        }
+    }
+
+    static class Type {
+        final String name;
+        final Struct struct;
+        final Class<?> clazz;
+        final org.objectweb.asm.Type type;
+        final Sort sort;
+
+        private Type(final String name, final Struct struct, final Class<?> clazz,
+                     final org.objectweb.asm.Type type, final Sort sort) {
+            this.name = name;
+            this.struct = struct;
+            this.clazz = clazz;
+            this.type = type;
+            this.sort = sort;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Type type = (Type)object;
+
+            return this.type.equals(type.type) && struct.equals(type.struct);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = struct.hashCode();
+            result = 31 * result + type.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Constructor {
+        final String name;
+        final Struct owner;
+        final List<Type> arguments;
+        final org.objectweb.asm.commons.Method method;
+        final java.lang.reflect.Constructor<?> reflect;
+
+        private Constructor(final String name, final Struct owner, final List<Type> arguments,
+                            final org.objectweb.asm.commons.Method method, final java.lang.reflect.Constructor<?> reflect) {
+            this.name = name;
+            this.owner = owner;
+            this.arguments = Collections.unmodifiableList(arguments);
+            this.method = method;
+            this.reflect = reflect;
+        }
+    }
+
+    static class Method {
+        final String name;
+        final Struct owner;
+        final Type rtn;
+        final List<Type> arguments;
+        final org.objectweb.asm.commons.Method method;
+        final java.lang.reflect.Method reflect;
+        final MethodHandle handle;
+
+        private Method(final String name, final Struct owner, final Type rtn, final List<Type> arguments,
+                       final org.objectweb.asm.commons.Method method, final java.lang.reflect.Method reflect,
+                       final MethodHandle handle) {
+            this.name = name;
+            this.owner = owner;
+            this.rtn = rtn;
+            this.arguments = Collections.unmodifiableList(arguments);
+            this.method = method;
+            this.reflect = reflect;
+            this.handle = handle;
+        }
+    }
+
+    static class Field {
+        final String name;
+        final Struct owner;
+        final Type generic;
+        final Type type;
+        final java.lang.reflect.Field reflect;
+        final MethodHandle getter;
+        final MethodHandle setter;
+
+        private Field(final String name, final Struct owner, final Type generic, final Type type,
+                      final java.lang.reflect.Field reflect, final MethodHandle getter, final MethodHandle setter) {
+            this.name = name;
+            this.owner = owner;
+            this.generic = generic;
+            this.type = type;
+            this.reflect = reflect;
+            this.getter = getter;
+            this.setter = setter;
+        }
+    }
+
+    static class Struct {
+        final String name;
+        final Class<?> clazz;
+        final org.objectweb.asm.Type type;
+
+        final Map<String, Constructor> constructors;
+        final Map<String, Method> functions;
+        final Map<String, Method> methods;
+
+        final Map<String, Field> statics;
+        final Map<String, Field> members;
+
+        private Struct(final String name, final Class<?> clazz, final org.objectweb.asm.Type type) {
+            this.name = name;
+            this.clazz = clazz;
+            this.type = type;
+
+            constructors = new HashMap<>();
+            functions = new HashMap<>();
+            methods = new HashMap<>();
+
+            statics = new HashMap<>();
+            members = new HashMap<>();
+        }
+
+        private Struct(final Struct struct) {
+            name = struct.name;
+            clazz = struct.clazz;
+            type = struct.type;
+
+            constructors = Collections.unmodifiableMap(struct.constructors);
+            functions = Collections.unmodifiableMap(struct.functions);
+            methods = Collections.unmodifiableMap(struct.methods);
+
+            statics = Collections.unmodifiableMap(struct.statics);
+            members = Collections.unmodifiableMap(struct.members);
+        }
+
+        @Override
+        public boolean equals(Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            Struct struct = (Struct)object;
+
+            return name.equals(struct.name);
+        }
+
+        @Override
+        public int hashCode() {
+            return name.hashCode();
+        }
+    }
+
+    static class Pair {
+        final Type type0;
+        final Type type1;
+
+        Pair(final Type type0, final Type type1) {
+            this.type0 = type0;
+            this.type1 = type1;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Pair pair = (Pair)object;
+
+            return type0.equals(pair.type0) && type1.equals(pair.type1);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = type0.hashCode();
+            result = 31 * result + type1.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Cast {
+        final Type from;
+        final Type to;
+
+        Cast(final Type from, final Type to) {
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Cast cast = (Cast)object;
+
+            return from.equals(cast.from) && to.equals(cast.to);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = from.hashCode();
+            result = 31 * result + to.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Transform extends Cast {
+        final Cast cast;
+        final Method method;
+        final Type upcast;
+        final Type downcast;
+
+        private Transform(final Cast cast, Method method, final Type upcast, final Type downcast) {
+            super(cast.from, cast.to);
+
+            this.cast = cast;
+            this.method = method;
+            this.upcast = upcast;
+            this.downcast = downcast;
+        }
+    }
+
+    final Map<String, Struct> structs;
+    final Map<Class<?>, Struct> classes;
+    final Map<Cast, Transform> transforms;
+    final Map<Pair, Type> bounds;
+
+    final Type voidType;
+    final Type booleanType;
+    final Type byteType;
+    final Type shortType;
+    final Type charType;
+    final Type intType;
+    final Type longType;
+    final Type floatType;
+    final Type doubleType;
+
+    final Type voidobjType;
+    final Type booleanobjType;
+    final Type byteobjType;
+    final Type shortobjType;
+    final Type charobjType;
+    final Type intobjType;
+    final Type longobjType;
+    final Type floatobjType;
+    final Type doubleobjType;
+
+    final Type objectType;
+    final Type defType;
+    final Type numberType;
+    final Type charseqType;
+    final Type stringType;
+    final Type mathType;
+    final Type utilityType;
+    final Type defobjType;
+
+    final Type listType;
+    final Type arraylistType;
+    final Type mapType;
+    final Type hashmapType;
+
+    final Type olistType;
+    final Type oarraylistType;
+    final Type omapType;
+    final Type ohashmapType;
+
+    final Type smapType;
+    final Type shashmapType;
+    final Type somapType;
+    final Type sohashmapType;
+
+    final Type execType;
+
+    public Definition() {
+        structs = new HashMap<>();
+        classes = new HashMap<>();
+        transforms = new HashMap<>();
+        bounds = new HashMap<>();
+
+        addDefaultStructs();
+        addDefaultClasses();
+
+        voidType = getType("void");
+        booleanType = getType("boolean");
+        byteType = getType("byte");
+        shortType = getType("short");
+        charType = getType("char");
+        intType = getType("int");
+        longType = getType("long");
+        floatType = getType("float");
+        doubleType = getType("double");
+
+        voidobjType = getType("Void");
+        booleanobjType = getType("Boolean");
+        byteobjType = getType("Byte");
+        shortobjType = getType("Short");
+        charobjType = getType("Character");
+        intobjType = getType("Integer");
+        longobjType = getType("Long");
+        floatobjType = getType("Float");
+        doubleobjType = getType("Double");
+
+        objectType = getType("Object");
+        defType = getType("def");
+        numberType = getType("Number");
+        charseqType = getType("CharSequence");
+        stringType = getType("String");
+        mathType = getType("Math");
+        utilityType = getType("Utility");
+        defobjType = getType("Def");
+
+        listType = getType("List");
+        arraylistType = getType("ArrayList");
+        mapType = getType("Map");
+        hashmapType = getType("HashMap");
+
+        olistType = getType("List<Object>");
+        oarraylistType = getType("ArrayList<Object>");
+        omapType = getType("Map<Object,Object>");
+        ohashmapType = getType("HashMap<Object,Object>");
+
+        smapType = getType("Map<String,def>");
+        shashmapType = getType("HashMap<String,def>");
+        somapType = getType("Map<String,Object>");
+        sohashmapType = getType("HashMap<String,Object>");
+
+        execType = getType("Executable");
+
+        addDefaultElements();
+        copyDefaultStructs();
+        addDefaultTransforms();
+        addDefaultBounds();
+    }
+
+    Definition(final Definition definition) {
+        final Map<String, Struct> structs = new HashMap<>();
+
+        for (final Struct struct : definition.structs.values()) {
+            structs.put(struct.name, new Struct(struct));
+        }
+
+        this.structs = Collections.unmodifiableMap(structs);
+
+        final Map<Class<?>, Struct> classes = new HashMap<>();
+
+        for (final Struct struct : definition.classes.values()) {
+            classes.put(struct.clazz, this.structs.get(struct.name));
+        }
+
+        this.classes = Collections.unmodifiableMap(classes);
+
+        transforms = Collections.unmodifiableMap(definition.transforms);
+        bounds = Collections.unmodifiableMap(definition.bounds);
+
+        voidType = definition.voidType;
+        booleanType = definition.booleanType;
+        byteType = definition.byteType;
+        shortType = definition.shortType;
+        charType = definition.charType;
+        intType = definition.intType;
+        longType = definition.longType;
+        floatType = definition.floatType;
+        doubleType = definition.doubleType;
+
+        voidobjType = definition.voidobjType;
+        booleanobjType = definition.booleanobjType;
+        byteobjType = definition.byteobjType;
+        shortobjType = definition.shortobjType;
+        charobjType = definition.charobjType;
+        intobjType = definition.intobjType;
+        longobjType = definition.longobjType;
+        floatobjType = definition.floatobjType;
+        doubleobjType = definition.doubleobjType;
+
+        objectType = definition.objectType;
+        defType = definition.defType;
+        numberType = definition.numberType;
+        charseqType = definition.charseqType;
+        stringType = definition.stringType;
+        mathType = definition.mathType;
+        utilityType = definition.utilityType;
+        defobjType = definition.defobjType;
+
+        listType = definition.listType;
+        arraylistType = definition.arraylistType;
+        mapType = definition.mapType;
+        hashmapType = definition.hashmapType;
+
+        olistType = definition.olistType;
+        oarraylistType = definition.oarraylistType;
+        omapType = definition.omapType;
+        ohashmapType = definition.ohashmapType;
+
+        smapType = definition.smapType;
+        shashmapType = definition.shashmapType;
+        somapType = definition.somapType;
+        sohashmapType = definition.sohashmapType;
+
+        execType = definition.execType;
+    }
+
+    private void addDefaultStructs() {
+        addStruct( "void"    , void.class    );
+        addStruct( "boolean" , boolean.class );
+        addStruct( "byte"    , byte.class    );
+        addStruct( "short"   , short.class   );
+        addStruct( "char"    , char.class    );
+        addStruct( "int"     , int.class     );
+        addStruct( "long"    , long.class    );
+        addStruct( "float"   , float.class   );
+        addStruct( "double"  , double.class  );
+
+        addStruct( "Void"      , Void.class      );
+        addStruct( "Boolean"   , Boolean.class   );
+        addStruct( "Byte"      , Byte.class      );
+        addStruct( "Short"     , Short.class     );
+        addStruct( "Character" , Character.class );
+        addStruct( "Integer"   , Integer.class   );
+        addStruct( "Long"      , Long.class      );
+        addStruct( "Float"     , Float.class     );
+        addStruct( "Double"    , Double.class    );
+
+        addStruct( "Object"       , Object.class       );
+        addStruct( "def"          , Object.class       );
+        addStruct( "Number"       , Number.class       );
+        addStruct( "CharSequence" , CharSequence.class );
+        addStruct( "String"       , String.class       );
+        addStruct( "Math"         , Math.class         );
+        addStruct( "Utility"      , Utility.class      );
+        addStruct( "Def"          , Def.class          );
+
+        addStruct( "List"      , List.class      );
+        addStruct( "ArrayList" , ArrayList.class );
+        addStruct( "Map"       , Map.class       );
+        addStruct( "HashMap"   , HashMap.class   );
+
+        addStruct( "List<Object>"           , List.class      );
+        addStruct( "ArrayList<Object>"      , ArrayList.class );
+        addStruct( "Map<Object,Object>"     , Map.class       );
+        addStruct( "HashMap<Object,Object>" , HashMap.class   );
+
+        addStruct( "Map<String,def>"        , Map.class       );
+        addStruct( "HashMap<String,def>"    , HashMap.class   );
+        addStruct( "Map<String,Object>"     , Map.class       );
+        addStruct( "HashMap<String,Object>" , HashMap.class   );
+
+        addStruct( "Executable" , Executable.class );
+    }
+
+    private void addDefaultClasses() {
+        addClass("boolean");
+        addClass("byte");
+        addClass("short");
+        addClass("char");
+        addClass("int");
+        addClass("long");
+        addClass("float");
+        addClass("double");
+
+        addClass("Boolean");
+        addClass("Byte");
+        addClass("Short");
+        addClass("Character");
+        addClass("Integer");
+        addClass("Long");
+        addClass("Float");
+        addClass("Double");
+
+        addClass("Object");
+        addClass("Number");
+        addClass("CharSequence");
+        addClass("String");
+
+        addClass("List");
+        addClass("ArrayList");
+        addClass("Map");
+        addClass("HashMap");
+    }
+
+    private void addDefaultElements() {
+        addMethod("Object", "toString", null, false, stringType, new Type[] {}, null, null);
+        addMethod("Object", "equals", null, false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("Object", "hashCode", null, false, intType, new Type[] {}, null, null);
+
+        addMethod("def", "toString", null, false, stringType, new Type[] {}, null, null);
+        addMethod("def", "equals", null, false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("def", "hashCode", null, false, intType, new Type[] {}, null, null);
+
+        addConstructor("Boolean", "new", new Type[] {booleanType}, null);
+        addMethod("Boolean", "valueOf", null, true, booleanobjType, new Type[] {booleanType}, null, null);
+        addMethod("Boolean", "booleanValue", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("Byte", "new", new Type[]{byteType}, null);
+        addMethod("Byte", "valueOf", null, true, byteobjType, new Type[] {byteType}, null, null);
+        addMethod("Byte", "byteValue", null, false, byteType, new Type[] {}, null, null);
+        addField("Byte", "MIN_VALUE", null, true, byteType, null);
+        addField("Byte", "MAX_VALUE", null, true, byteType, null);
+
+        addConstructor("Short", "new", new Type[]{shortType}, null);
+        addMethod("Short", "valueOf", null, true, shortobjType, new Type[] {shortType}, null, null);
+        addMethod("Short", "shortValue", null, false, shortType, new Type[] {}, null, null);
+        addField("Short", "MIN_VALUE", null, true, shortType, null);
+        addField("Short", "MAX_VALUE", null, true, shortType, null);
+
+        addConstructor("Character", "new", new Type[]{charType}, null);
+        addMethod("Character", "valueOf", null, true, charobjType, new Type[] {charType}, null, null);
+        addMethod("Character", "charValue", null, false, charType, new Type[] {}, null, null);
+        addField("Character", "MIN_VALUE", null, true, charType, null);
+        addField("Character", "MAX_VALUE", null, true, charType, null);
+
+        addConstructor("Integer", "new", new Type[]{intType}, null);
+        addMethod("Integer", "valueOf", null, true, intobjType, new Type[] {intType}, null, null);
+        addMethod("Integer", "intValue", null, false, intType, new Type[] {}, null, null);
+        addField("Integer", "MIN_VALUE", null, true, intType, null);
+        addField("Integer", "MAX_VALUE", null, true, intType, null);
+
+        addConstructor("Long", "new", new Type[]{longType}, null);
+        addMethod("Long", "valueOf", null, true, longobjType, new Type[] {longType}, null, null);
+        addMethod("Long", "longValue", null, false, longType, new Type[] {}, null, null);
+        addField("Long", "MIN_VALUE", null, true, longType, null);
+        addField("Long", "MAX_VALUE", null, true, longType, null);
+
+        addConstructor("Float", "new", new Type[]{floatType}, null);
+        addMethod("Float", "valueOf", null, true, floatobjType, new Type[] {floatType}, null, null);
+        addMethod("Float", "floatValue", null, false, floatType, new Type[] {}, null, null);
+        addField("Float", "MIN_VALUE", null, true, floatType, null);
+        addField("Float", "MAX_VALUE", null, true, floatType, null);
+
+        addConstructor("Double", "new", new Type[]{doubleType}, null);
+        addMethod("Double", "valueOf", null, true, doubleobjType, new Type[] {doubleType}, null, null);
+        addMethod("Double", "doubleValue", null, false, doubleType, new Type[] {}, null, null);
+        addField("Double", "MIN_VALUE", null, true, doubleType, null);
+        addField("Double", "MAX_VALUE", null, true, doubleType, null);
+
+        addMethod("Number", "byteValue", null, false, byteType, new Type[] {}, null, null);
+        addMethod("Number", "shortValue", null, false, shortType, new Type[] {}, null, null);
+        addMethod("Number", "intValue", null, false, intType, new Type[] {}, null, null);
+        addMethod("Number", "longValue", null, false, longType, new Type[] {}, null, null);
+        addMethod("Number", "floatValue", null, false, floatType, new Type[] {}, null, null);
+        addMethod("Number", "doubleValue", null, false, doubleType, new Type[] {}, null, null);
+
+        addMethod("CharSequence", "charAt", null, false, charType, new Type[] {intType}, null, null);
+        addMethod("CharSequence", "length", null, false, intType, new Type[] {}, null, null);
+
+        addConstructor("String", "new", new Type[] {}, null);
+        addMethod("String", "codePointAt", null, false, intType, new Type[] {intType}, null, null);
+        addMethod("String", "compareTo", null, false, intType, new Type[] {stringType}, null, null);
+        addMethod("String", "concat", null, false, stringType, new Type[] {stringType}, null, null);
+        addMethod("String", "endsWith", null, false, booleanType, new Type[] {stringType}, null, null);
+        addMethod("String", "indexOf", null, false, intType, new Type[] {stringType, intType}, null, null);
+        addMethod("String", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+        addMethod("String", "replace", null, false, stringType, new Type[] {charseqType, charseqType}, null, null);
+        addMethod("String", "startsWith", null, false, booleanType, new Type[] {stringType}, null, null);
+        addMethod("String", "substring", null, false, stringType, new Type[] {intType, intType}, null, null);
+        addMethod("String", "toCharArray", null, false, getType(charType.struct, 1), new Type[] {}, null, null);
+        addMethod("String", "trim", null, false, stringType, new Type[] {}, null, null);
+
+        addMethod("Utility", "NumberToboolean", null, true, booleanType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberTochar", null, true, charType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToBoolean", null, true, booleanobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToByte", null, true, byteobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToShort", null, true, shortobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToCharacter", null, true, charobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToInteger", null, true, intobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToLong", null, true, longobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToFloat", null, true, floatobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToDouble", null, true, doubleobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "booleanTobyte", null, true, byteType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToshort", null, true, shortType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTochar", null, true, charType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToint", null, true, intType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTolong", null, true, longType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTofloat", null, true, floatType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTodouble", null, true, doubleType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToInteger", null, true, intobjType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "BooleanTobyte", null, true, byteType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToshort", null, true, shortType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTochar", null, true, charType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToint", null, true, intType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTolong", null, true, longType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTofloat", null, true, floatType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTodouble", null, true, doubleType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToByte", null, true, byteobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToShort", null, true, shortobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToCharacter", null, true, charobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToInteger", null, true, intobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToLong", null, true, longobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToFloat", null, true, floatobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToDouble", null, true, doubleobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "byteToboolean", null, true, booleanType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToShort", null, true, shortobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToCharacter", null, true, charobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToInteger", null, true, intobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToLong", null, true, longobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToFloat", null, true, floatobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToDouble", null, true, doubleobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "ByteToboolean", null, true, booleanType, new Type[] {byteobjType}, null, null);
+        addMethod("Utility", "ByteTochar", null, true, charType, new Type[] {byteobjType}, null, null);
+        addMethod("Utility", "shortToboolean", null, true, booleanType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToByte", null, true, byteobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToCharacter", null, true, charobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToInteger", null, true, intobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToLong", null, true, longobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToFloat", null, true, floatobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToDouble", null, true, doubleobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "ShortToboolean", null, true, booleanType, new Type[] {shortobjType}, null, null);
+        addMethod("Utility", "ShortTochar", null, true, charType, new Type[] {shortobjType}, null, null);
+        addMethod("Utility", "charToboolean", null, true, booleanType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToByte", null, true, byteobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToShort", null, true, shortobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToInteger", null, true, intobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToLong", null, true, longobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToFloat", null, true, floatobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToDouble", null, true, doubleobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "CharacterToboolean", null, true, booleanType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTobyte", null, true, byteType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToshort", null, true, shortType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToint", null, true, intType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTolong", null, true, longType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTofloat", null, true, floatType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTodouble", null, true, doubleType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToBoolean", null, true, booleanobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToByte", null, true, byteobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToShort", null, true, shortobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToInteger", null, true, intobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToLong", null, true, longobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToFloat", null, true, floatobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToDouble", null, true, doubleobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "intToboolean", null, true, booleanType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToByte", null, true, byteobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToShort", null, true, shortobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToCharacter", null, true, charobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToLong", null, true, longobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToFloat", null, true, floatobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToDouble", null, true, doubleobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "IntegerToboolean", null, true, booleanType, new Type[] {intobjType}, null, null);
+        addMethod("Utility", "IntegerTochar", null, true, charType, new Type[] {intobjType}, null, null);
+        addMethod("Utility", "longToboolean", null, true, booleanType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToByte", null, true, byteobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToShort", null, true, shortobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToCharacter", null, true, charobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToInteger", null, true, intobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToFloat", null, true, floatobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToDouble", null, true, doubleobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "LongToboolean", null, true, booleanType, new Type[] {longobjType}, null, null);
+        addMethod("Utility", "LongTochar", null, true, charType, new Type[] {longobjType}, null, null);
+        addMethod("Utility", "floatToboolean", null, true, booleanType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToByte", null, true, byteobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToShort", null, true, shortobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToCharacter", null, true, charobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToInteger", null, true, intobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToLong", null, true, longobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToDouble", null, true, doubleobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "FloatToboolean", null, true, booleanType, new Type[] {floatobjType}, null, null);
+        addMethod("Utility", "FloatTochar", null, true, charType, new Type[] {floatobjType}, null, null);
+        addMethod("Utility", "doubleToboolean", null, true, booleanType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToByte", null, true, byteobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToShort", null, true, shortobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToCharacter", null, true, charobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToInteger", null, true, intobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToLong", null, true, longobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToFloat", null, true, floatobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "DoubleToboolean", null, true, booleanType, new Type[] {doubleobjType}, null, null);
+        addMethod("Utility", "DoubleTochar", null, true, charType, new Type[] {doubleobjType}, null, null);
+
+        addMethod("Math", "dmax", "max", true, doubleType, new Type[] {doubleType, doubleType}, null, null);
+
+        addMethod("Def", "DefToboolean", null, true, booleanType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTobyte", null, true, byteType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToshort", null, true, shortType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTochar", null, true, charType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToint", null, true, intType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTolong", null, true, longType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTofloat", null, true, floatType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTodouble", null, true, doubleType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToBoolean", null, true, booleanobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToByte", null, true, byteobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToShort", null, true, shortobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToCharacter", null, true, charobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToInteger", null, true, intobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToLong", null, true, longobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToFloat", null, true, floatobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToDouble", null, true, doubleobjType, new Type[] {defType}, null, null);
+        
+        addMethod("List", "addLast", "add", false, booleanType, new Type[] {objectType}, null, new Type[] {defType});
+        addMethod("List", "add", null, false, voidType, new Type[] {intType, objectType}, null, new Type[] {intType, defType});
+        addMethod("List", "get", null, false, objectType, new Type[] {intType}, defType, null);
+        addMethod("List", "remove", null, false, objectType, new Type[] {intType}, defType, null);
+        addMethod("List", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("List", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("ArrayList", "new", new Type[] {}, null);
+
+        addMethod("Map", "put", null, false, objectType, new Type[] {objectType, objectType}, defType, new Type[] {defType, defType});
+        addMethod("Map", "get", null, false, objectType, new Type[] {objectType}, defType, new Type[] {defType});
+        addMethod("Map", "remove", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap", "new", new Type[] {}, null);
+
+        addMethod("Map<String,def>", "put", null, false, objectType, new Type[] {objectType, objectType}, defType, new Type[] {stringType, defType});
+        addMethod("Map<String,def>", "get", null, false, objectType, new Type[] {objectType}, defType, new Type[] {stringType});
+        addMethod("Map<String,def>", "remove", null, false, objectType, new Type[] {objectType}, defType, new Type[] {stringType});
+        addMethod("Map<String,def>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<String,def>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<String,def>", "new", new Type[] {}, null);
+
+        addMethod("List<Object>", "addLast", "add", false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("List<Object>", "add", null, false, voidType, new Type[] {intType, objectType}, null, null);
+        addMethod("List<Object>", "get", null, false, objectType, new Type[] {intType}, null, null);
+        addMethod("List<Object>", "remove", null, false, objectType, new Type[] {intType}, null, null);
+        addMethod("List<Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("List<Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("ArrayList<Object>", "new", new Type[] {}, null);
+
+        addMethod("Map<Object,Object>", "put", null, false, objectType, new Type[] {objectType, objectType}, null, null);
+        addMethod("Map<Object,Object>", "get", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map<Object,Object>", "remove", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map<Object,Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<Object,Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<Object,Object>", "new", new Type[] {}, null);
+
+        addMethod("Map<String,Object>", "put", null, false, objectType, new Type[] {objectType, objectType}, null, new Type[] {stringType, objectType});
+        addMethod("Map<String,Object>", "get", null, false, objectType, new Type[] {objectType}, null, new Type[] {stringType});
+        addMethod("Map<String,Object>", "remove", null, false, objectType, new Type[] {objectType}, null, new Type[] {stringType});
+        addMethod("Map<String,Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<String,Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<String,Object>", "new", new Type[] {}, null);
+    }
+
+    private void copyDefaultStructs() {
+        copyStruct("Void", "Object");
+        copyStruct("Boolean", "Object");
+        copyStruct("Byte", "Number", "Object");
+        copyStruct("Short", "Number", "Object");
+        copyStruct("Character", "Object");
+        copyStruct("Integer", "Number", "Object");
+        copyStruct("Long", "Number", "Object");
+        copyStruct("Float", "Number", "Object");
+        copyStruct("Double", "Number", "Object");
+
+        copyStruct("Number", "Object");
+        copyStruct("CharSequence", "Object");
+        copyStruct("String", "CharSequence", "Object");
+
+        copyStruct("List", "Object");
+        copyStruct("ArrayList", "List", "Object");
+        copyStruct("Map", "Object");
+        copyStruct("HashMap", "Map", "Object");
+        copyStruct("Map<String,def>", "Object");
+        copyStruct("HashMap<String,def>", "Map<String,def>", "Object");
+
+        copyStruct("List<Object>", "Object");
+        copyStruct("ArrayList<Object>", "List", "Object");
+        copyStruct("Map<Object,Object>", "Object");
+        copyStruct("HashMap<Object,Object>", "Map<Object,Object>", "Object");
+        copyStruct("Map<String,Object>", "Object");
+        copyStruct("HashMap<String,Object>", "Map<String,Object>", "Object");
+
+        copyStruct("Executable", "Object");
+    }
+
+    private void addDefaultTransforms() {
+        addTransform(booleanType, byteType, "Utility", "booleanTobyte", true);
+        addTransform(booleanType, shortType, "Utility", "booleanToshort", true);
+        addTransform(booleanType, charType, "Utility", "booleanTochar", true);
+        addTransform(booleanType, intType, "Utility", "booleanToint", true);
+        addTransform(booleanType, longType, "Utility", "booleanTolong", true);
+        addTransform(booleanType, floatType, "Utility", "booleanTofloat", true);
+        addTransform(booleanType, doubleType, "Utility", "booleanTodouble", true);
+        addTransform(booleanType, objectType, "Boolean", "valueOf", true);
+        addTransform(booleanType, defType, "Boolean", "valueOf", true);
+        addTransform(booleanType, numberType, "Utility", "booleanToInteger", true);
+        addTransform(booleanType, booleanobjType, "Boolean", "valueOf", true);
+
+        addTransform(byteType, booleanType, "Utility", "byteToboolean", true);
+        addTransform(byteType, objectType, "Byte", "valueOf", true);
+        addTransform(byteType, defType, "Byte", "valueOf", true);
+        addTransform(byteType, numberType, "Byte", "valueOf", true);
+        addTransform(byteType, byteobjType, "Byte", "valueOf", true);
+        addTransform(byteType, shortobjType, "Utility", "byteToShort", true);
+        addTransform(byteType, charobjType, "Utility", "byteToCharacter", true);
+        addTransform(byteType, intobjType, "Utility", "byteToInteger", true);
+        addTransform(byteType, longobjType, "Utility", "byteToLong", true);
+        addTransform(byteType, floatobjType, "Utility", "byteToFloat", true);
+        addTransform(byteType, doubleobjType, "Utility", "byteToDouble", true);
+
+        addTransform(shortType, booleanType, "Utility", "shortToboolean", true);
+        addTransform(shortType, objectType, "Short", "valueOf", true);
+        addTransform(shortType, defType, "Short", "valueOf", true);
+        addTransform(shortType, numberType, "Short", "valueOf", true);
+        addTransform(shortType, byteobjType, "Utility", "shortToByte", true);
+        addTransform(shortType, shortobjType, "Short", "valueOf", true);
+        addTransform(shortType, charobjType, "Utility", "shortToCharacter", true);
+        addTransform(shortType, intobjType, "Utility", "shortToInteger", true);
+        addTransform(shortType, longobjType, "Utility", "shortToLong", true);
+        addTransform(shortType, floatobjType, "Utility", "shortToFloat", true);
+        addTransform(shortType, doubleobjType, "Utility", "shortToDouble", true);
+
+        addTransform(charType, booleanType, "Utility", "charToboolean", true);
+        addTransform(charType, objectType, "Character", "valueOf", true);
+        addTransform(charType, defType, "Character", "valueOf", true);
+        addTransform(charType, numberType, "Utility", "charToInteger", true);
+        addTransform(charType, byteobjType, "Utility", "charToByte", true);
+        addTransform(charType, shortobjType, "Utility", "charToShort", true);
+        addTransform(charType, charobjType, "Character", "valueOf", true);
+        addTransform(charType, intobjType, "Utility", "charToInteger", true);
+        addTransform(charType, longobjType, "Utility", "charToLong", true);
+        addTransform(charType, floatobjType, "Utility", "charToFloat", true);
+        addTransform(charType, doubleobjType, "Utility", "charToDouble", true);
+
+        addTransform(intType, booleanType, "Utility", "intToboolean", true);
+        addTransform(intType, objectType, "Integer", "valueOf", true);
+        addTransform(intType, defType, "Integer", "valueOf", true);
+        addTransform(intType, numberType, "Integer", "valueOf", true);
+        addTransform(intType, byteobjType, "Utility", "intToByte", true);
+        addTransform(intType, shortobjType, "Utility", "intToShort", true);
+        addTransform(intType, charobjType, "Utility", "intToCharacter", true);
+        addTransform(intType, intobjType, "Integer", "valueOf", true);
+        addTransform(intType, longobjType, "Utility", "intToLong", true);
+        addTransform(intType, floatobjType, "Utility", "intToFloat", true);
+        addTransform(intType, doubleobjType, "Utility", "intToDouble", true);
+
+        addTransform(longType, booleanType, "Utility", "longToboolean", true);
+        addTransform(longType, objectType, "Long", "valueOf", true);
+        addTransform(longType, defType, "Long", "valueOf", true);
+        addTransform(longType, numberType, "Long", "valueOf", true);
+        addTransform(longType, byteobjType, "Utility", "longToByte", true);
+        addTransform(longType, shortobjType, "Utility", "longToShort", true);
+        addTransform(longType, charobjType, "Utility", "longToCharacter", true);
+        addTransform(longType, intobjType, "Utility", "longToInteger", true);
+        addTransform(longType, longobjType, "Long", "valueOf", true);
+        addTransform(longType, floatobjType, "Utility", "longToFloat", true);
+        addTransform(longType, doubleobjType, "Utility", "longToDouble", true);
+
+        addTransform(floatType, booleanType, "Utility", "floatToboolean", true);
+        addTransform(floatType, objectType, "Float", "valueOf", true);
+        addTransform(floatType, defType, "Float", "valueOf", true);
+        addTransform(floatType, numberType, "Float", "valueOf", true);
+        addTransform(floatType, byteobjType, "Utility", "floatToByte", true);
+        addTransform(floatType, shortobjType, "Utility", "floatToShort", true);
+        addTransform(floatType, charobjType, "Utility", "floatToCharacter", true);
+        addTransform(floatType, intobjType, "Utility", "floatToInteger", true);
+        addTransform(floatType, longobjType, "Utility", "floatToLong", true);
+        addTransform(floatType, floatobjType, "Float", "valueOf", true);
+        addTransform(floatType, doubleobjType, "Utility", "floatToDouble", true);
+
+        addTransform(doubleType, booleanType, "Utility", "doubleToboolean", true);
+        addTransform(doubleType, objectType, "Double", "valueOf", true);
+        addTransform(doubleType, defType, "Double", "valueOf", true);
+        addTransform(doubleType, numberType, "Double", "valueOf", true);
+        addTransform(doubleType, byteobjType, "Utility", "doubleToByte", true);
+        addTransform(doubleType, shortobjType, "Utility", "doubleToShort", true);
+        addTransform(doubleType, charobjType, "Utility", "doubleToCharacter", true);
+        addTransform(doubleType, intobjType, "Utility", "doubleToInteger", true);
+        addTransform(doubleType, longobjType, "Utility", "doubleToLong", true);
+        addTransform(doubleType, floatobjType, "Utility", "doubleToFloat", true);
+        addTransform(doubleType, doubleobjType, "Double", "valueOf", true);
+
+        addTransform(objectType, booleanType, "Boolean", "booleanValue", false);
+        addTransform(objectType, byteType, "Number", "byteValue", false);
+        addTransform(objectType, shortType, "Number", "shortValue", false);
+        addTransform(objectType, charType, "Character", "charValue", false);
+        addTransform(objectType, intType, "Number", "intValue", false);
+        addTransform(objectType, longType, "Number", "longValue", false);
+        addTransform(objectType, floatType, "Number", "floatValue", false);
+        addTransform(objectType, doubleType, "Number", "doubleValue", false);
+
+        addTransform(defType, booleanType, "Def", "DefToboolean", true);
+        addTransform(defType, byteType, "Def", "DefTobyte", true);
+        addTransform(defType, shortType, "Def", "DefToshort", true);
+        addTransform(defType, charType, "Def", "DefTochar", true);
+        addTransform(defType, intType, "Def", "DefToint", true);
+        addTransform(defType, longType, "Def", "DefTolong", true);
+        addTransform(defType, floatType, "Def", "DefTofloat", true);
+        addTransform(defType, doubleType, "Def", "DefTodouble", true);
+        addTransform(defType, booleanobjType, "Def", "DefToBoolean", true);
+        addTransform(defType, byteobjType, "Def", "DefToByte", true);
+        addTransform(defType, shortobjType, "Def", "DefToShort", true);
+        addTransform(defType, charobjType, "Def", "DefToCharacter", true);
+        addTransform(defType, intobjType, "Def", "DefToInteger", true);
+        addTransform(defType, longobjType, "Def", "DefToLong", true);
+        addTransform(defType, floatobjType, "Def", "DefToFloat", true);
+        addTransform(defType, doubleobjType, "Def", "DefToDouble", true);
+        
+        addTransform(numberType, booleanType, "Utility", "NumberToboolean", true);
+        addTransform(numberType, byteType, "Number", "byteValue", false);
+        addTransform(numberType, shortType, "Number", "shortValue", false);
+        addTransform(numberType, charType, "Utility", "NumberTochar", true);
+        addTransform(numberType, intType, "Number", "intValue", false);
+        addTransform(numberType, longType, "Number", "longValue", false);
+        addTransform(numberType, floatType, "Number", "floatValue", false);
+        addTransform(numberType, doubleType, "Number", "doubleValue", false);
+        addTransform(numberType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(numberType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(numberType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(numberType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(numberType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(numberType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(numberType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(numberType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(booleanobjType, booleanType, "Boolean", "booleanValue", false);
+        addTransform(booleanobjType, byteType, "Utility", "BooleanTobyte", true);
+        addTransform(booleanobjType, shortType, "Utility", "BooleanToshort", true);
+        addTransform(booleanobjType, charType, "Utility", "BooleanTochar", true);
+        addTransform(booleanobjType, intType, "Utility", "BooleanToint", true);
+        addTransform(booleanobjType, longType, "Utility", "BooleanTolong", true);
+        addTransform(booleanobjType, floatType, "Utility", "BooleanTofloat", true);
+        addTransform(booleanobjType, doubleType, "Utility", "BooleanTodouble", true);
+        addTransform(booleanobjType, numberType, "Utility", "BooleanToLong", true);
+        addTransform(booleanobjType, byteobjType, "Utility", "BooleanToByte", true);
+        addTransform(booleanobjType, shortobjType, "Utility", "BooleanToShort", true);
+        addTransform(booleanobjType, charobjType, "Utility", "BooleanToCharacter", true);
+        addTransform(booleanobjType, intobjType, "Utility", "BooleanToInteger", true);
+        addTransform(booleanobjType, longobjType, "Utility", "BooleanToLong", true);
+        addTransform(booleanobjType, floatobjType, "Utility", "BooleanToFloat", true);
+        addTransform(booleanobjType, doubleobjType, "Utility", "BooleanToDouble", true);
+
+        addTransform(byteobjType, booleanType, "Utility", "ByteToboolean", true);
+        addTransform(byteobjType, byteType, "Byte", "byteValue", false);
+        addTransform(byteobjType, shortType, "Byte", "shortValue", false);
+        addTransform(byteobjType, charType, "Utility", "ByteTochar", true);
+        addTransform(byteobjType, intType, "Byte", "intValue", false);
+        addTransform(byteobjType, longType, "Byte", "longValue", false);
+        addTransform(byteobjType, floatType, "Byte", "floatValue", false);
+        addTransform(byteobjType, doubleType, "Byte", "doubleValue", false);
+        addTransform(byteobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(byteobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(byteobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(byteobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(byteobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(byteobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(byteobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(shortobjType, booleanType, "Utility", "ShortToboolean", true);
+        addTransform(shortobjType, byteType, "Short", "byteValue", false);
+        addTransform(shortobjType, shortType, "Short", "shortValue", false);
+        addTransform(shortobjType, charType, "Utility", "ShortTochar", true);
+        addTransform(shortobjType, intType, "Short", "intValue", false);
+        addTransform(shortobjType, longType, "Short", "longValue", false);
+        addTransform(shortobjType, floatType, "Short", "floatValue", false);
+        addTransform(shortobjType, doubleType, "Short", "doubleValue", false);
+        addTransform(shortobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(shortobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(shortobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(shortobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(shortobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(shortobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(shortobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(charobjType, booleanType, "Utility", "CharacterToboolean", true);
+        addTransform(charobjType, byteType, "Utility", "CharacterTobyte", true);
+        addTransform(charobjType, shortType, "Utility", "CharacterToshort", true);
+        addTransform(charobjType, charType, "Character", "charValue", false);
+        addTransform(charobjType, intType, "Utility", "CharacterToint", true);
+        addTransform(charobjType, longType, "Utility", "CharacterTolong", true);
+        addTransform(charobjType, floatType, "Utility", "CharacterTofloat", true);
+        addTransform(charobjType, doubleType, "Utility", "CharacterTodouble", true);
+        addTransform(charobjType, booleanobjType, "Utility", "CharacterToBoolean", true);
+        addTransform(charobjType, byteobjType, "Utility", "CharacterToByte", true);
+        addTransform(charobjType, shortobjType, "Utility", "CharacterToShort", true);
+        addTransform(charobjType, intobjType, "Utility", "CharacterToInteger", true);
+        addTransform(charobjType, longobjType, "Utility", "CharacterToLong", true);
+        addTransform(charobjType, floatobjType, "Utility", "CharacterToFloat", true);
+        addTransform(charobjType, doubleobjType, "Utility", "CharacterToDouble", true);
+
+        addTransform(intobjType, booleanType, "Utility", "IntegerToboolean", true);
+        addTransform(intobjType, byteType, "Integer", "byteValue", false);
+        addTransform(intobjType, shortType, "Integer", "shortValue", false);
+        addTransform(intobjType, charType, "Utility", "IntegerTochar", true);
+        addTransform(intobjType, intType, "Integer", "intValue", false);
+        addTransform(intobjType, longType, "Integer", "longValue", false);
+        addTransform(intobjType, floatType, "Integer", "floatValue", false);
+        addTransform(intobjType, doubleType, "Integer", "doubleValue", false);
+        addTransform(intobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(intobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(intobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(intobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(intobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(intobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(intobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(longobjType, booleanType, "Utility", "LongToboolean", true);
+        addTransform(longobjType, byteType, "Long", "byteValue", false);
+        addTransform(longobjType, shortType, "Long", "shortValue", false);
+        addTransform(longobjType, charType, "Utility", "LongTochar", true);
+        addTransform(longobjType, intType, "Long", "intValue", false);
+        addTransform(longobjType, longType, "Long", "longValue", false);
+        addTransform(longobjType, floatType, "Long", "floatValue", false);
+        addTransform(longobjType, doubleType, "Long", "doubleValue", false);
+        addTransform(longobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(longobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(longobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(longobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(longobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(longobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(longobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(floatobjType, booleanType, "Utility", "FloatToboolean", true);
+        addTransform(floatobjType, byteType, "Float", "byteValue", false);
+        addTransform(floatobjType, shortType, "Float", "shortValue", false);
+        addTransform(floatobjType, charType, "Utility", "FloatTochar", true);
+        addTransform(floatobjType, intType, "Float", "intValue", false);
+        addTransform(floatobjType, longType, "Float", "longValue", false);
+        addTransform(floatobjType, floatType, "Float", "floatValue", false);
+        addTransform(floatobjType, doubleType, "Float", "doubleValue", false);
+        addTransform(floatobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(floatobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(floatobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(floatobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(floatobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(floatobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(floatobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(doubleobjType, booleanType, "Utility", "DoubleToboolean", true);
+        addTransform(doubleobjType, byteType, "Double", "byteValue", false);
+        addTransform(doubleobjType, shortType, "Double", "shortValue", false);
+        addTransform(doubleobjType, charType, "Utility", "DoubleTochar", true);
+        addTransform(doubleobjType, intType, "Double", "intValue", false);
+        addTransform(doubleobjType, longType, "Double", "longValue", false);
+        addTransform(doubleobjType, floatType, "Double", "floatValue", false);
+        addTransform(doubleobjType, doubleType, "Double", "doubleValue", false);
+        addTransform(doubleobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(doubleobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(doubleobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(doubleobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(doubleobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(doubleobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(doubleobjType, floatobjType, "Utility", "NumberToFloat", true);
+    }
+
+    private void addDefaultBounds() {
+        addBound(byteobjType, numberType, numberType);
+
+        addBound(shortobjType, numberType, numberType);
+        addBound(shortobjType, byteobjType, numberType);
+
+        addBound(intobjType, numberType, numberType);
+        addBound(intobjType, byteobjType, numberType);
+        addBound(intobjType, shortobjType, numberType);
+
+        addBound(longobjType, numberType, numberType);
+        addBound(longobjType, byteobjType, numberType);
+        addBound(longobjType, shortobjType, numberType);
+        addBound(longobjType, intobjType, numberType);
+
+        addBound(floatobjType, numberType, numberType);
+        addBound(floatobjType, byteobjType, numberType);
+        addBound(floatobjType, shortobjType, numberType);
+        addBound(floatobjType, intobjType, numberType);
+        addBound(floatobjType, longobjType, numberType);
+
+        addBound(doubleobjType, numberType, numberType);
+        addBound(doubleobjType, byteobjType, numberType);
+        addBound(doubleobjType, shortobjType, numberType);
+        addBound(doubleobjType, intobjType, numberType);
+        addBound(doubleobjType, longobjType, numberType);
+        addBound(doubleobjType, floatobjType, numberType);
+
+        addBound(stringType, charseqType, charseqType);
+
+        addBound(arraylistType, listType, listType);
+        addBound(olistType, listType, listType);
+        addBound(olistType, arraylistType, listType);
+        addBound(oarraylistType, listType, listType);
+        addBound(oarraylistType, olistType, olistType);
+        addBound(oarraylistType, arraylistType, arraylistType);
+
+        addBound(hashmapType, mapType, mapType);
+        addBound(omapType, mapType, mapType);
+        addBound(omapType, hashmapType, mapType);
+        addBound(ohashmapType, mapType, mapType);
+        addBound(ohashmapType, hashmapType, hashmapType);
+        addBound(ohashmapType, omapType, omapType);
+        addBound(smapType, mapType, mapType);
+        addBound(smapType, hashmapType, mapType);
+        addBound(smapType, omapType, omapType);
+        addBound(smapType, ohashmapType, omapType);
+        addBound(shashmapType, mapType, mapType);
+        addBound(shashmapType, hashmapType, hashmapType);
+        addBound(shashmapType, omapType, omapType);
+        addBound(shashmapType, ohashmapType, ohashmapType);
+        addBound(shashmapType, smapType, smapType);
+        addBound(somapType, mapType, mapType);
+        addBound(somapType, hashmapType, mapType);
+        addBound(somapType, omapType, omapType);
+        addBound(somapType, ohashmapType, omapType);
+        addBound(somapType, smapType, smapType);
+        addBound(somapType, shashmapType, smapType);
+        addBound(sohashmapType, mapType, mapType);
+        addBound(sohashmapType, hashmapType, hashmapType);
+        addBound(sohashmapType, omapType, omapType);
+        addBound(sohashmapType, ohashmapType, ohashmapType);
+        addBound(sohashmapType, smapType, smapType);
+        addBound(sohashmapType, shashmapType, shashmapType);
+        addBound(sohashmapType, somapType, somapType);
+    }
+
+    public final void addStruct(final String name, final Class<?> clazz) {
+        if (!name.matches("^[_a-zA-Z][<>,_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid struct name [" + name + "].");
+        }
+
+        if (structs.containsKey(name)) {
+            throw new IllegalArgumentException("Duplicate struct name [" + name + "].");
+        }
+
+        final Struct struct = new Struct(name, clazz, org.objectweb.asm.Type.getType(clazz));
+
+        structs.put(name, struct);
+    }
+
+    public final void addClass(final String name) {
+        final Struct struct = structs.get(name);
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Struct [" + name + "] is not defined.");
+        }
+
+        if (classes.containsKey(struct.clazz)) {
+            throw new IllegalArgumentException("Duplicate struct class [" + struct.clazz + "] when defining dynamic.");
+        }
+
+        classes.put(struct.clazz, struct);
+    }
+
+    public final void addConstructor(final String struct, final String name, final Type[] args, final Type[] genargs) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException(
+                    "Owner struct [" + struct + "] not defined for constructor [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException(
+                    "Invalid constructor name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.constructors.containsKey(name)) {
+            throw new IllegalArgumentException(
+                    "Duplicate constructor name [" + name + "] found within the struct [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and functions may not have the same name" +
+                    " [" + name + "] within the same struct [" + owner.name + "].");
+        }
+
+        if (owner.methods.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and methods may not have the same name" +
+                    " [" + name + "] within the same struct [" + owner.name + "].");
+        }
+
+        final Class[] classes = new Class[args.length];
+
+        for (int count = 0; count < classes.length; ++count) {
+            if (genargs != null) {
+                try {
+                    genargs[count].clazz.asSubclass(args[count].clazz);
+                } catch (ClassCastException exception) {
+                    throw new ClassCastException("Generic argument [" + genargs[count].name + "]" +
+                            " is not a sub class of [" + args[count].name + "] in the constructor" +
+                            " [" + name + " ] from the struct [" + owner.name + "].");
+                }
+            }
+
+            classes[count] = args[count].clazz;
+        }
+
+        final java.lang.reflect.Constructor<?> reflect;
+
+        try {
+            reflect = owner.clazz.getConstructor(classes);
+        } catch (NoSuchMethodException exception) {
+            throw new IllegalArgumentException("Constructor [" + name + "] not found for class" +
+                    " [" + owner.clazz.getName() + "] with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        final org.objectweb.asm.commons.Method asm = org.objectweb.asm.commons.Method.getMethod(reflect);
+        final Constructor constructor =
+                new Constructor(name, owner, Arrays.asList(genargs != null ? genargs : args), asm, reflect);
+
+        owner.constructors.put(name, constructor);
+    }
+
+    public final void addMethod(final String struct, final String name, final String alias, final boolean statik,
+                                final Type rtn, final Type[] args, final Type genrtn, final Type[] genargs) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined" +
+                    " for " + (statik ? "function" : "method") + " [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid " + (statik ? "function" : "method") +
+                    " name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.constructors.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and " + (statik ? "functions" : "methods") +
+                    " may not have the same name [" + name + "] within the same struct" +
+                    " [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException(
+                        "Duplicate function name [" + name + "] found within the struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Functions and methods may not have the same name" +
+                        " [" + name + "] within the same struct [" + owner.name + "].");
+            }
+        }
+
+        if (owner.methods.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Functions and methods may not have the same name" +
+                        " [" + name + "] within the same struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Duplicate method name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            }
+        }
+
+        if (genrtn != null) {
+            try {
+                genrtn.clazz.asSubclass(rtn.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Generic return [" + genrtn.clazz.getCanonicalName() + "]" +
+                        " is not a sub class of [" + rtn.clazz.getCanonicalName() + "] in the method" +
+                        " [" + name + " ] from the struct [" + owner.name + "].");
+            }
+        }
+
+        if (genargs != null && genargs.length != args.length) {
+            throw new IllegalArgumentException("Generic arguments arity [" +  genargs.length + "] is not the same as " +
+                    (statik ? "function" : "method") + " [" + name + "] arguments arity" +
+                    " [" + args.length + "] within the struct [" + owner.name + "].");
+        }
+
+        final Class[] classes = new Class[args.length];
+
+        for (int count = 0; count < classes.length; ++count) {
+            if (genargs != null) {
+                try {
+                    genargs[count].clazz.asSubclass(args[count].clazz);
+                } catch (ClassCastException exception) {
+                    throw new ClassCastException("Generic argument [" + genargs[count].name + "] is not a sub class" +
+                            " of [" + args[count].name + "] in the " + (statik ? "function" : "method") +
+                            " [" + name + " ] from the struct [" + owner.name + "].");
+                }
+            }
+
+            classes[count] = args[count].clazz;
+        }
+
+        final java.lang.reflect.Method reflect;
+
+        try {
+            reflect = owner.clazz.getMethod(alias == null ? name : alias, classes);
+        } catch (NoSuchMethodException exception) {
+            throw new IllegalArgumentException((statik ? "Function" : "Method") +
+                    " [" + (alias == null ? name : alias) + "] not found for class [" + owner.clazz.getName() + "]" +
+                    " with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        if (!reflect.getReturnType().equals(rtn.clazz)) {
+            throw new IllegalArgumentException("Specified return type class [" + rtn.clazz + "]" +
+                    " does not match the found return type class [" + reflect.getReturnType() + "] for the " +
+                    (statik ? "function" : "method") + " [" + name + "]" +
+                    " within the struct [" + owner.name + "].");
+        }
+
+        final org.objectweb.asm.commons.Method asm = org.objectweb.asm.commons.Method.getMethod(reflect);
+
+        MethodHandle handle;
+
+        try {
+            if (statik) {
+                handle = MethodHandles.publicLookup().in(owner.clazz).findStatic(
+                        owner.clazz, alias == null ? name : alias, MethodType.methodType(rtn.clazz, classes));
+            } else {
+                handle = MethodHandles.publicLookup().in(owner.clazz).findVirtual(
+                        owner.clazz, alias == null ? name : alias, MethodType.methodType(rtn.clazz, classes));
+            }
+        } catch (NoSuchMethodException | IllegalAccessException exception) {
+            throw new IllegalArgumentException("Method [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "]" +
+                    " with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        final Method method = new Method(name, owner, genrtn != null ? genrtn : rtn,
+                Arrays.asList(genargs != null ? genargs : args), asm, reflect, handle);
+        final int modifiers = reflect.getModifiers();
+
+        if (statik) {
+            if (!java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Function [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to a static Java method.");
+            }
+
+            owner.functions.put(name, method);
+        } else {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Method [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to a non-static Java method.");
+            }
+
+            owner.methods.put(name, method);
+        }
+    }
+
+    public final void addField(final String struct, final String name, final String alias,
+                               final boolean statik, final Type type, final Type generic) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for " +
+                    (statik ? "static" : "member") + " [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid " + (statik ? "static" : "member") +
+                    " name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Duplicate static name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Statics and members may not have the same name " +
+                        "[" + name + "] within the same struct [" + owner.name + "].");
+            }
+        }
+
+        if (owner.members.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Statics and members may not have the same name " +
+                        "[" + name + "] within the same struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Duplicate member name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            }
+        }
+
+        if (generic != null) {
+            try {
+                generic.clazz.asSubclass(type.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Generic type [" + generic.clazz.getCanonicalName() + "]" +
+                        " is not a sub class of [" + type.clazz.getCanonicalName() + "] for the field" +
+                        " [" + name + " ] from the struct [" + owner.name + "].");
+            }
+        }
+
+        java.lang.reflect.Field reflect;
+
+        try {
+            reflect = owner.clazz.getField(alias == null ? name : alias);
+        } catch (NoSuchFieldException exception) {
+            throw new IllegalArgumentException("Field [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "].");
+        }
+
+        MethodHandle getter = null;
+        MethodHandle setter = null;
+
+        try {
+            if (!statik) {
+                getter = MethodHandles.publicLookup().in(owner.clazz).findGetter(
+                        owner.clazz, alias == null ? name : alias, type.clazz);
+                setter = MethodHandles.publicLookup().in(owner.clazz).findSetter(
+                        owner.clazz, alias == null ? name : alias, type.clazz);
+            }
+        } catch (NoSuchFieldException | IllegalAccessException exception) {
+            throw new IllegalArgumentException("Getter/Setter [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "].");
+        }
+
+        final Field field = new Field(name, owner, generic == null ? type : generic, type, reflect, getter, setter);
+        final int modifiers = reflect.getModifiers();
+
+        if (statik) {
+            if (!java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException();
+            }
+
+            if (!java.lang.reflect.Modifier.isFinal(modifiers)) {
+                throw new IllegalArgumentException("Static [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to static Java field.");
+            }
+
+            owner.statics.put(alias == null ? name : alias, field);
+        } else {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Member [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to non-static Java field.");
+            }
+
+            owner.members.put(alias == null ? name : alias, field);
+        }
+    }
+
+    public final void copyStruct(final String struct, final String... children) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for copy.");
+        }
+
+        for (int count = 0; count < children.length; ++count) {
+            final Struct child = structs.get(children[count]);
+
+            if (struct == null) {
+                throw new IllegalArgumentException("Child struct [" + children[count] + "]" +
+                        " not defined for copy to owner struct [" + owner.name + "].");
+            }
+
+            try {
+                owner.clazz.asSubclass(child.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Child struct [" + child.name + "]" +
+                        " is not a super type of owner struct [" + owner.name + "] in copy.");
+            }
+
+            final boolean object = child.clazz.equals(Object.class) &&
+                    java.lang.reflect.Modifier.isInterface(owner.clazz.getModifiers());
+
+            for (final Method method : child.methods.values()) {
+                if (owner.methods.get(method.name) == null) {
+                    final Class<?> clazz = object ? Object.class : owner.clazz;
+
+                    java.lang.reflect.Method reflect;
+                    MethodHandle handle;
+
+                    try {
+                        reflect = clazz.getMethod(method.method.getName(), method.reflect.getParameterTypes());
+                    } catch (NoSuchMethodException exception) {
+                        throw new IllegalArgumentException("Method [" + method.method.getName() + "] not found for" +
+                                " class [" + owner.clazz.getName() + "] with arguments " +
+                                Arrays.toString(method.reflect.getParameterTypes()) + ".");
+                    }
+
+                    try {
+                        handle = MethodHandles.publicLookup().in(owner.clazz).findVirtual(
+                                owner.clazz, method.method.getName(),
+                                MethodType.methodType(method.reflect.getReturnType(), method.reflect.getParameterTypes()));
+                    } catch (NoSuchMethodException | IllegalAccessException exception) {
+                        throw new IllegalArgumentException("Method [" + method.method.getName() + "] not found for" +
+                                " class [" + owner.clazz.getName() + "] with arguments " +
+                                Arrays.toString(method.reflect.getParameterTypes()) + ".");
+                    }
+
+                    owner.methods.put(method.name,
+                            new Method(method.name, owner, method.rtn, method.arguments, method.method, reflect, handle));
+                }
+            }
+
+            for (final Field field : child.members.values()) {
+                if (owner.members.get(field.name) == null) {
+                    java.lang.reflect.Field reflect;
+                    MethodHandle getter;
+                    MethodHandle setter;
+
+                    try {
+                        reflect = owner.clazz.getField(field.reflect.getName());
+                    } catch (NoSuchFieldException exception) {
+                        throw new IllegalArgumentException("Field [" + field.reflect.getName() + "]" +
+                                " not found for class [" + owner.clazz.getName() + "].");
+                    }
+
+                    try {
+                        getter = MethodHandles.publicLookup().in(owner.clazz).findGetter(
+                                owner.clazz, field.name, field.type.clazz);
+                        setter = MethodHandles.publicLookup().in(owner.clazz).findSetter(
+                                owner.clazz, field.name, field.type.clazz);
+                    } catch (NoSuchFieldException | IllegalAccessException exception) {
+                        throw new IllegalArgumentException("Getter/Setter [" + field.name + "]" +
+                                " not found for class [" + owner.clazz.getName() + "].");
+                    }
+
+                    owner.members.put(field.name,
+                            new Field(field.name, owner, field.type, field.generic, reflect, getter, setter));
+                }
+            }
+        }
+    }
+
+    public final void addTransform(final Type from, final Type to, final String struct,
+                                   final String name, final boolean statik) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for" +
+                    " transform with cast type from [" + from.name + "] and cast type to [" + to.name + "].");
+        }
+
+        if (from.equals(to)) {
+            throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "] cannot" +
+                    " have cast type from [" + from.name + "] be the same as cast type to [" + to.name + "].");
+        }
+
+        final Cast cast = new Cast(from, to);
+
+        if (transforms.containsKey(cast)) {
+            throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                    " and cast type from [" + from.name + "] to cast type to [" + to.name + "] already defined.");
+        }
+
+        Method method;
+        Type upcast = null;
+        Type downcast = null;
+
+        if (statik) {
+            method = owner.functions.get(name);
+
+            if (method == null) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using a function [" + name + "] that is not defined.");
+            }
+
+            if (method.arguments.size() != 1) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using function [" + name + "] does not have a single type argument.");
+            }
+
+            Type argument = method.arguments.get(0);
+
+            try {
+                from.clazz.asSubclass(argument.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    argument.clazz.asSubclass(from.clazz);
+                    upcast = argument;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " function [" + name + "] cannot cast from type to the function input argument type.");
+                }
+            }
+
+            final Type rtn = method.rtn;
+
+            try {
+                rtn.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    to.clazz.asSubclass(rtn.clazz);
+                    downcast = to;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " function [" + name + "] cannot cast to type to the function return argument type.");
+                }
+            }
+        } else {
+            method = owner.methods.get(name);
+
+            if (method == null) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using a method [" + name + "] that is not defined.");
+            }
+
+            if (!method.arguments.isEmpty()) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using method [" + name + "] does not have a single type argument.");
+            }
+
+            try {
+                from.clazz.asSubclass(owner.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    owner.clazz.asSubclass(from.clazz);
+                    upcast = getType(owner.name);
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " method [" + name + "] cannot cast from type to the method input argument type.");
+                }
+            }
+
+            final Type rtn = method.rtn;
+
+            try {
+                rtn.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    to.clazz.asSubclass(rtn.clazz);
+                    downcast = to;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "]" +
+                            " using method [" + name + "] cannot cast to type to the method return argument type.");
+                }
+            }
+        }
+
+        final Transform transform = new Transform(cast, method, upcast, downcast);
+        transforms.put(cast, transform);
+    }
+
+    public final void addBound(final Type type0, final Type type1, final Type bound) {
+        final Pair pair0 = new Pair(type0, type1);
+        final Pair pair1 = new Pair(type1, type0);
+
+        if (bounds.containsKey(pair0)) {
+            throw new IllegalArgumentException(
+                    "Bound already defined for types [" + type0.name + "] and [" + type1.name + "].");
+        }
+
+        if (bounds.containsKey(pair1)) {
+            throw new IllegalArgumentException(
+                    "Bound already defined for types [" + type1.name + "] and [" + type0.name + "].");
+        }
+
+        bounds.put(pair0, bound);
+        bounds.put(pair1, bound);
+    }
+
+    Type getType(final String name) {
+        final int dimensions = getDimensions(name);
+        final String structstr = dimensions == 0 ? name : name.substring(0, name.indexOf('['));
+        final Struct struct = structs.get(structstr);
+
+        if (struct == null) {
+            throw new IllegalArgumentException("The struct with name [" + name + "] has not been defined.");
+        }
+
+        return getType(struct, dimensions);
+    }
+
+    Type getType(final Struct struct, final int dimensions) {
+        String name = struct.name;
+        org.objectweb.asm.Type type = struct.type;
+        Class<?> clazz = struct.clazz;
+        Sort sort;
+
+        if (dimensions > 0) {
+            final StringBuilder builder = new StringBuilder(name);
+            final char[] brackets = new char[dimensions];
+
+            for (int count = 0; count < dimensions; ++count) {
+                builder.append("[]");
+                brackets[count] = '[';
+            }
+
+            final String descriptor = new String(brackets) + struct.type.getDescriptor();
+
+            name = builder.toString();
+            type = org.objectweb.asm.Type.getType(descriptor);
+
+            try {
+                clazz = Class.forName(type.getInternalName().replace('/', '.'));
+            } catch (ClassNotFoundException exception) {
+                throw new IllegalArgumentException("The class [" + type.getInternalName() + "]" +
+                        " could not be found to create type [" + name + "].");
+            }
+
+            sort = Sort.ARRAY;
+        } else if ("def".equals(struct.name)) {
+            sort = Sort.DEF;
+        } else {
+            sort = Sort.OBJECT;
+
+            for (Sort value : Sort.values()) {
+                if (value.clazz == null) {
+                    continue;
+                }
+
+                if (value.clazz.equals(struct.clazz)) {
+                    sort = value;
+
+                    break;
+                }
+            }
+        }
+
+        return new Type(name, struct, clazz, type, sort);
+    }
+
+    private int getDimensions(final String name) {
+        int dimensions = 0;
+        int index = name.indexOf('[');
+
+        if (index != -1) {
+            final int length = name.length();
+
+            while (index < length) {
+                if (name.charAt(index) == '[' && ++index < length && name.charAt(index++) == ']') {
+                    ++dimensions;
+                } else {
+                    throw new IllegalArgumentException("Invalid array braces in canonical name [" + name + "].");
+                }
+            }
+        }
+
+        return dimensions;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java
new file mode 100644
index 0000000..95e3c93
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java
@@ -0,0 +1,45 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.text.ParseException;
+
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.LexerNoViableAltException;
+import org.antlr.v4.runtime.misc.Interval;
+
+class ErrorHandlingLexer extends PlanALexer {
+    public ErrorHandlingLexer(CharStream charStream) {
+        super(charStream);
+    }
+
+    @Override
+    public void recover(LexerNoViableAltException lnvae) {
+        CharStream charStream = lnvae.getInputStream();
+        int startIndex = lnvae.getStartIndex();
+        String text = charStream.getText(Interval.of(startIndex, charStream.index()));
+
+        ParseException parseException = new ParseException("Error [" + _tokenStartLine + ":" +
+                _tokenStartCharPositionInLine + "]: unexpected character [" +
+                getErrorDisplay(text) + "].",  _tokenStartCharIndex);
+        parseException.initCause(lnvae);
+        throw new RuntimeException(parseException);
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java
new file mode 100644
index 0000000..09e28cf
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java
@@ -0,0 +1,50 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.util.Map;
+
+public abstract class Executable {
+    protected final Definition definition;
+
+    private final String name;
+    private final String source;
+
+    public Executable(final Definition definition, final String name, final String source) {
+        this.definition = definition;
+
+        this.name = name;
+        this.source = source;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public String getSource() {
+        return source;
+    }
+
+    public Definition getDefinition() {
+        return definition;
+    }
+
+    public abstract Object execute(Map<String, Object> input);
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java
new file mode 100644
index 0000000..3fe3603
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java
@@ -0,0 +1,74 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.text.ParseException;
+
+import org.antlr.v4.runtime.DefaultErrorStrategy;
+import org.antlr.v4.runtime.InputMismatchException;
+import org.antlr.v4.runtime.NoViableAltException;
+import org.antlr.v4.runtime.Parser;
+import org.antlr.v4.runtime.RecognitionException;
+import org.antlr.v4.runtime.Token;
+
+class ParserErrorStrategy extends DefaultErrorStrategy {
+    @Override
+    public void recover(Parser recognizer, RecognitionException re) {
+        Token token = re.getOffendingToken();
+        String message;
+
+        if (token == null) {
+            message = "Error: no parse token found.";
+        } else if (re instanceof InputMismatchException) {
+            message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                    " unexpected token [" + getTokenErrorDisplay(token) + "]" +
+                    " was expecting one of [" + re.getExpectedTokens().toString(recognizer.getVocabulary()) + "].";
+        } else if (re instanceof NoViableAltException) {
+            if (token.getType() == PlanAParser.EOF) {
+                message = "Error: unexpected end of script.";
+            } else {
+                message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                        "invalid sequence of tokens near [" + getTokenErrorDisplay(token) + "].";
+            }
+        } else {
+            message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                    " unexpected token near [" + getTokenErrorDisplay(token) + "].";
+        }
+
+        ParseException parseException = new ParseException(message, token == null ? -1 : token.getStartIndex());
+        parseException.initCause(re);
+
+        throw new RuntimeException(parseException);
+    }
+
+    @Override
+    public Token recoverInline(Parser recognizer) throws RecognitionException {
+        Token token = recognizer.getCurrentToken();
+        String message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                " unexpected token [" + getTokenErrorDisplay(token) + "]" +
+                " was expecting one of [" + recognizer.getExpectedTokens().toString(recognizer.getVocabulary()) + "].";
+        ParseException parseException = new ParseException(message, token.getStartIndex());
+        throw new RuntimeException(parseException);
+    }
+
+    @Override
+    public void sync(Parser recognizer) {
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java
new file mode 100644
index 0000000..a9e5ff6
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java
@@ -0,0 +1,390 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+
+    import java.util.Set;
+
+import org.antlr.v4.runtime.Lexer;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.Token;
+import org.antlr.v4.runtime.TokenStream;
+import org.antlr.v4.runtime.*;
+import org.antlr.v4.runtime.atn.*;
+import org.antlr.v4.runtime.dfa.DFA;
+import org.antlr.v4.runtime.misc.*;
+
+@SuppressWarnings({"all", "warnings", "unchecked", "unused", "cast"})
+class PlanALexer extends Lexer {
+  static { RuntimeMetaData.checkVersion("4.5.1", RuntimeMetaData.VERSION); }
+
+  protected static final DFA[] _decisionToDFA;
+  protected static final PredictionContextCache _sharedContextCache =
+    new PredictionContextCache();
+  public static final int
+    WS=1, COMMENT=2, LBRACK=3, RBRACK=4, LBRACE=5, RBRACE=6, LP=7, RP=8, DOT=9, 
+    COMMA=10, SEMICOLON=11, IF=12, ELSE=13, WHILE=14, DO=15, FOR=16, CONTINUE=17, 
+    BREAK=18, RETURN=19, NEW=20, TRY=21, CATCH=22, THROW=23, BOOLNOT=24, BWNOT=25, 
+    MUL=26, DIV=27, REM=28, ADD=29, SUB=30, LSH=31, RSH=32, USH=33, LT=34, 
+    LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43, 
+    BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51, 
+    AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59, 
+    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67, 
+    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75, 
+    EXTID=76;
+  public static final int EXT = 1;
+  public static String[] modeNames = {
+    "DEFAULT_MODE", "EXT"
+  };
+
+  public static final String[] ruleNames = {
+    "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", "DOT", 
+    "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "GENERIC", "ID", "EXTINTEGER", "EXTID"
+  };
+
+  private static final String[] _LITERAL_NAMES = {
+    null, null, null, "'{'", "'}'", "'['", "']'", "'('", "')'", "'.'", "','", 
+    "';'", "'if'", "'else'", "'while'", "'do'", "'for'", "'continue'", "'break'", 
+    "'return'", "'new'", "'try'", "'catch'", "'throw'", "'!'", "'~'", "'*'", 
+    "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'", 
+    "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'", 
+    "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='", 
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null, 
+    null, null, null, null, null, "'true'", "'false'", "'null'"
+  };
+  private static final String[] _SYMBOLIC_NAMES = {
+    null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", 
+    "DOT", "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+  };
+  public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
+
+  /**
+   * @deprecated Use {@link #VOCABULARY} instead.
+   */
+  @Deprecated
+  public static final String[] tokenNames;
+  static {
+    tokenNames = new String[_SYMBOLIC_NAMES.length];
+    for (int i = 0; i < tokenNames.length; i++) {
+      tokenNames[i] = VOCABULARY.getLiteralName(i);
+      if (tokenNames[i] == null) {
+        tokenNames[i] = VOCABULARY.getSymbolicName(i);
+      }
+
+      if (tokenNames[i] == null) {
+        tokenNames[i] = "<INVALID>";
+      }
+    }
+  }
+
+  @Override
+  @Deprecated
+  public String[] getTokenNames() {
+    return tokenNames;
+  }
+
+  @Override
+
+  public Vocabulary getVocabulary() {
+    return VOCABULARY;
+  }
+
+
+      private Set<String> types = null;
+
+      void setTypes(Set<String> types) {
+          this.types = types;
+      }
+
+
+  public PlanALexer(CharStream input) {
+    super(input);
+    _interp = new LexerATNSimulator(this,_ATN,_decisionToDFA,_sharedContextCache);
+  }
+
+  @Override
+  public String getGrammarFileName() { return "PlanALexer.g4"; }
+
+  @Override
+  public String[] getRuleNames() { return ruleNames; }
+
+  @Override
+  public String getSerializedATN() { return _serializedATN; }
+
+  @Override
+  public String[] getModeNames() { return modeNames; }
+
+  @Override
+  public ATN getATN() { return _ATN; }
+
+  @Override
+  public void action(RuleContext _localctx, int ruleIndex, int actionIndex) {
+    switch (ruleIndex) {
+    case 67:
+      STRING_action((RuleContext)_localctx, actionIndex);
+      break;
+    case 68:
+      CHAR_action((RuleContext)_localctx, actionIndex);
+      break;
+    case 72:
+      TYPE_action((RuleContext)_localctx, actionIndex);
+      break;
+    }
+  }
+  private void STRING_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 0:
+      setText(getText().substring(1, getText().length() - 1));
+      break;
+    }
+  }
+  private void CHAR_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 1:
+      setText(getText().substring(1, getText().length() - 1));
+      break;
+    }
+  }
+  private void TYPE_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 2:
+      setText(getText().replace(" ", ""));
+      break;
+    }
+  }
+  @Override
+  public boolean sempred(RuleContext _localctx, int ruleIndex, int predIndex) {
+    switch (ruleIndex) {
+    case 72:
+      return TYPE_sempred((RuleContext)_localctx, predIndex);
+    }
+    return true;
+  }
+  private boolean TYPE_sempred(RuleContext _localctx, int predIndex) {
+    switch (predIndex) {
+    case 0:
+      return types.contains(getText().replace(" ", ""));
+    }
+    return true;
+  }
+
+  public static final String _serializedATN =
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\2N\u0236\b\1\b\1\4"+
+    "\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n"+
+    "\4\13\t\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22"+
+    "\t\22\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31"+
+    "\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36\4\37\t\37\4 \t"+
+    " \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t"+
+    "+\4,\t,\4-\t-\4.\t.\4/\t/\4\60\t\60\4\61\t\61\4\62\t\62\4\63\t\63\4\64"+
+    "\t\64\4\65\t\65\4\66\t\66\4\67\t\67\48\t8\49\t9\4:\t:\4;\t;\4<\t<\4=\t"+
+    "=\4>\t>\4?\t?\4@\t@\4A\tA\4B\tB\4C\tC\4D\tD\4E\tE\4F\tF\4G\tG\4H\tH\4"+
+    "I\tI\4J\tJ\4K\tK\4L\tL\4M\tM\4N\tN\3\2\6\2\u00a0\n\2\r\2\16\2\u00a1\3"+
+    "\2\3\2\3\3\3\3\3\3\3\3\7\3\u00aa\n\3\f\3\16\3\u00ad\13\3\3\3\3\3\3\3\3"+
+    "\3\3\3\7\3\u00b4\n\3\f\3\16\3\u00b7\13\3\3\3\3\3\5\3\u00bb\n\3\3\3\3\3"+
+    "\3\4\3\4\3\5\3\5\3\6\3\6\3\7\3\7\3\b\3\b\3\t\3\t\3\n\3\n\3\n\3\n\3\13"+
+    "\3\13\3\f\3\f\3\r\3\r\3\r\3\16\3\16\3\16\3\16\3\16\3\17\3\17\3\17\3\17"+
+    "\3\17\3\17\3\20\3\20\3\20\3\21\3\21\3\21\3\21\3\22\3\22\3\22\3\22\3\22"+
+    "\3\22\3\22\3\22\3\22\3\23\3\23\3\23\3\23\3\23\3\23\3\24\3\24\3\24\3\24"+
+    "\3\24\3\24\3\24\3\25\3\25\3\25\3\25\3\26\3\26\3\26\3\26\3\27\3\27\3\27"+
+    "\3\27\3\27\3\27\3\30\3\30\3\30\3\30\3\30\3\30\3\31\3\31\3\32\3\32\3\33"+
+    "\3\33\3\34\3\34\3\35\3\35\3\36\3\36\3\37\3\37\3 \3 \3 \3!\3!\3!\3\"\3"+
+    "\"\3\"\3\"\3#\3#\3$\3$\3$\3%\3%\3&\3&\3&\3\'\3\'\3\'\3(\3(\3(\3(\3)\3"+
+    ")\3)\3*\3*\3*\3*\3+\3+\3,\3,\3-\3-\3.\3.\3.\3/\3/\3/\3\60\3\60\3\61\3"+
+    "\61\3\62\3\62\3\62\3\63\3\63\3\63\3\64\3\64\3\65\3\65\3\65\3\66\3\66\3"+
+    "\66\3\67\3\67\3\67\38\38\38\39\39\39\3:\3:\3:\3;\3;\3;\3<\3<\3<\3=\3="+
+    "\3=\3=\3>\3>\3>\3>\3?\3?\3?\3?\3?\3@\3@\3@\3@\3A\3A\6A\u0185\nA\rA\16"+
+    "A\u0186\3A\5A\u018a\nA\3B\3B\3B\6B\u018f\nB\rB\16B\u0190\3B\5B\u0194\n"+
+    "B\3C\3C\3C\7C\u0199\nC\fC\16C\u019c\13C\5C\u019e\nC\3C\5C\u01a1\nC\3D"+
+    "\3D\3D\7D\u01a6\nD\fD\16D\u01a9\13D\5D\u01ab\nD\3D\3D\7D\u01af\nD\fD\16"+
+    "D\u01b2\13D\3D\3D\5D\u01b6\nD\3D\6D\u01b9\nD\rD\16D\u01ba\5D\u01bd\nD"+
+    "\3D\5D\u01c0\nD\3E\3E\3E\3E\3E\3E\7E\u01c8\nE\fE\16E\u01cb\13E\3E\3E\3"+
+    "E\3F\3F\3F\3F\3F\3G\3G\3G\3G\3G\3H\3H\3H\3H\3H\3H\3I\3I\3I\3I\3I\3J\3"+
+    "J\5J\u01e7\nJ\3J\3J\3J\3K\7K\u01ed\nK\fK\16K\u01f0\13K\3K\3K\7K\u01f4"+
+    "\nK\fK\16K\u01f7\13K\3K\3K\5K\u01fb\nK\3K\7K\u01fe\nK\fK\16K\u0201\13"+
+    "K\3K\3K\7K\u0205\nK\fK\16K\u0208\13K\3K\3K\5K\u020c\nK\3K\7K\u020f\nK"+
+    "\fK\16K\u0212\13K\7K\u0214\nK\fK\16K\u0217\13K\3K\3K\3L\3L\7L\u021d\n"+
+    "L\fL\16L\u0220\13L\3M\3M\3M\7M\u0225\nM\fM\16M\u0228\13M\5M\u022a\nM\3"+
+    "M\3M\3N\3N\7N\u0230\nN\fN\16N\u0233\13N\3N\3N\5\u00ab\u00b5\u01c9\2O\4"+
+    "\3\6\4\b\5\n\6\f\7\16\b\20\t\22\n\24\13\26\f\30\r\32\16\34\17\36\20 \21"+
+    "\"\22$\23&\24(\25*\26,\27.\30\60\31\62\32\64\33\66\348\35:\36<\37> @!"+
+    "B\"D#F$H%J&L\'N(P)R*T+V,X-Z.\\/^\60`\61b\62d\63f\64h\65j\66l\67n8p9r:"+
+    "t;v<x=z>|?~@\u0080A\u0082B\u0084C\u0086D\u0088E\u008aF\u008cG\u008eH\u0090"+
+    "I\u0092J\u0094K\u0096\2\u0098L\u009aM\u009cN\4\2\3\21\5\2\13\f\17\17\""+
+    "\"\4\2\f\f\17\17\3\2\629\4\2NNnn\4\2ZZzz\5\2\62;CHch\3\2\63;\3\2\62;\b"+
+    "\2FFHHNNffhhnn\4\2GGgg\4\2--//\4\2HHhh\4\2$$^^\5\2C\\aac|\6\2\62;C\\a"+
+    "ac|\u0255\2\4\3\2\2\2\2\6\3\2\2\2\2\b\3\2\2\2\2\n\3\2\2\2\2\f\3\2\2\2"+
+    "\2\16\3\2\2\2\2\20\3\2\2\2\2\22\3\2\2\2\2\24\3\2\2\2\2\26\3\2\2\2\2\30"+
+    "\3\2\2\2\2\32\3\2\2\2\2\34\3\2\2\2\2\36\3\2\2\2\2 \3\2\2\2\2\"\3\2\2\2"+
+    "\2$\3\2\2\2\2&\3\2\2\2\2(\3\2\2\2\2*\3\2\2\2\2,\3\2\2\2\2.\3\2\2\2\2\60"+
+    "\3\2\2\2\2\62\3\2\2\2\2\64\3\2\2\2\2\66\3\2\2\2\28\3\2\2\2\2:\3\2\2\2"+
+    "\2<\3\2\2\2\2>\3\2\2\2\2@\3\2\2\2\2B\3\2\2\2\2D\3\2\2\2\2F\3\2\2\2\2H"+
+    "\3\2\2\2\2J\3\2\2\2\2L\3\2\2\2\2N\3\2\2\2\2P\3\2\2\2\2R\3\2\2\2\2T\3\2"+
+    "\2\2\2V\3\2\2\2\2X\3\2\2\2\2Z\3\2\2\2\2\\\3\2\2\2\2^\3\2\2\2\2`\3\2\2"+
+    "\2\2b\3\2\2\2\2d\3\2\2\2\2f\3\2\2\2\2h\3\2\2\2\2j\3\2\2\2\2l\3\2\2\2\2"+
+    "n\3\2\2\2\2p\3\2\2\2\2r\3\2\2\2\2t\3\2\2\2\2v\3\2\2\2\2x\3\2\2\2\2z\3"+
+    "\2\2\2\2|\3\2\2\2\2~\3\2\2\2\2\u0080\3\2\2\2\2\u0082\3\2\2\2\2\u0084\3"+
+    "\2\2\2\2\u0086\3\2\2\2\2\u0088\3\2\2\2\2\u008a\3\2\2\2\2\u008c\3\2\2\2"+
+    "\2\u008e\3\2\2\2\2\u0090\3\2\2\2\2\u0092\3\2\2\2\2\u0094\3\2\2\2\2\u0098"+
+    "\3\2\2\2\3\u009a\3\2\2\2\3\u009c\3\2\2\2\4\u009f\3\2\2\2\6\u00ba\3\2\2"+
+    "\2\b\u00be\3\2\2\2\n\u00c0\3\2\2\2\f\u00c2\3\2\2\2\16\u00c4\3\2\2\2\20"+
+    "\u00c6\3\2\2\2\22\u00c8\3\2\2\2\24\u00ca\3\2\2\2\26\u00ce\3\2\2\2\30\u00d0"+
+    "\3\2\2\2\32\u00d2\3\2\2\2\34\u00d5\3\2\2\2\36\u00da\3\2\2\2 \u00e0\3\2"+
+    "\2\2\"\u00e3\3\2\2\2$\u00e7\3\2\2\2&\u00f0\3\2\2\2(\u00f6\3\2\2\2*\u00fd"+
+    "\3\2\2\2,\u0101\3\2\2\2.\u0105\3\2\2\2\60\u010b\3\2\2\2\62\u0111\3\2\2"+
+    "\2\64\u0113\3\2\2\2\66\u0115\3\2\2\28\u0117\3\2\2\2:\u0119\3\2\2\2<\u011b"+
+    "\3\2\2\2>\u011d\3\2\2\2@\u011f\3\2\2\2B\u0122\3\2\2\2D\u0125\3\2\2\2F"+
+    "\u0129\3\2\2\2H\u012b\3\2\2\2J\u012e\3\2\2\2L\u0130\3\2\2\2N\u0133\3\2"+
+    "\2\2P\u0136\3\2\2\2R\u013a\3\2\2\2T\u013d\3\2\2\2V\u0141\3\2\2\2X\u0143"+
+    "\3\2\2\2Z\u0145\3\2\2\2\\\u0147\3\2\2\2^\u014a\3\2\2\2`\u014d\3\2\2\2"+
+    "b\u014f\3\2\2\2d\u0151\3\2\2\2f\u0154\3\2\2\2h\u0157\3\2\2\2j\u0159\3"+
+    "\2\2\2l\u015c\3\2\2\2n\u015f\3\2\2\2p\u0162\3\2\2\2r\u0165\3\2\2\2t\u0168"+
+    "\3\2\2\2v\u016b\3\2\2\2x\u016e\3\2\2\2z\u0171\3\2\2\2|\u0175\3\2\2\2~"+
+    "\u0179\3\2\2\2\u0080\u017e\3\2\2\2\u0082\u0182\3\2\2\2\u0084\u018b\3\2"+
+    "\2\2\u0086\u019d\3\2\2\2\u0088\u01aa\3\2\2\2\u008a\u01c1\3\2\2\2\u008c"+
+    "\u01cf\3\2\2\2\u008e\u01d4\3\2\2\2\u0090\u01d9\3\2\2\2\u0092\u01df\3\2"+
+    "\2\2\u0094\u01e4\3\2\2\2\u0096\u01ee\3\2\2\2\u0098\u021a\3\2\2\2\u009a"+
+    "\u0229\3\2\2\2\u009c\u022d\3\2\2\2\u009e\u00a0\t\2\2\2\u009f\u009e\3\2"+
+    "\2\2\u00a0\u00a1\3\2\2\2\u00a1\u009f\3\2\2\2\u00a1\u00a2\3\2\2\2\u00a2"+
+    "\u00a3\3\2\2\2\u00a3\u00a4\b\2\2\2\u00a4\5\3\2\2\2\u00a5\u00a6\7\61\2"+
+    "\2\u00a6\u00a7\7\61\2\2\u00a7\u00ab\3\2\2\2\u00a8\u00aa\13\2\2\2\u00a9"+
+    "\u00a8\3\2\2\2\u00aa\u00ad\3\2\2\2\u00ab\u00ac\3\2\2\2\u00ab\u00a9\3\2"+
+    "\2\2\u00ac\u00ae\3\2\2\2\u00ad\u00ab\3\2\2\2\u00ae\u00bb\t\3\2\2\u00af"+
+    "\u00b0\7\61\2\2\u00b0\u00b1\7,\2\2\u00b1\u00b5\3\2\2\2\u00b2\u00b4\13"+
+    "\2\2\2\u00b3\u00b2\3\2\2\2\u00b4\u00b7\3\2\2\2\u00b5\u00b6\3\2\2\2\u00b5"+
+    "\u00b3\3\2\2\2\u00b6\u00b8\3\2\2\2\u00b7\u00b5\3\2\2\2\u00b8\u00b9\7,"+
+    "\2\2\u00b9\u00bb\7\61\2\2\u00ba\u00a5\3\2\2\2\u00ba\u00af\3\2\2\2\u00bb"+
+    "\u00bc\3\2\2\2\u00bc\u00bd\b\3\2\2\u00bd\7\3\2\2\2\u00be\u00bf\7}\2\2"+
+    "\u00bf\t\3\2\2\2\u00c0\u00c1\7\177\2\2\u00c1\13\3\2\2\2\u00c2\u00c3\7"+
+    "]\2\2\u00c3\r\3\2\2\2\u00c4\u00c5\7_\2\2\u00c5\17\3\2\2\2\u00c6\u00c7"+
+    "\7*\2\2\u00c7\21\3\2\2\2\u00c8\u00c9\7+\2\2\u00c9\23\3\2\2\2\u00ca\u00cb"+
+    "\7\60\2\2\u00cb\u00cc\3\2\2\2\u00cc\u00cd\b\n\3\2\u00cd\25\3\2\2\2\u00ce"+
+    "\u00cf\7.\2\2\u00cf\27\3\2\2\2\u00d0\u00d1\7=\2\2\u00d1\31\3\2\2\2\u00d2"+
+    "\u00d3\7k\2\2\u00d3\u00d4\7h\2\2\u00d4\33\3\2\2\2\u00d5\u00d6\7g\2\2\u00d6"+
+    "\u00d7\7n\2\2\u00d7\u00d8\7u\2\2\u00d8\u00d9\7g\2\2\u00d9\35\3\2\2\2\u00da"+
+    "\u00db\7y\2\2\u00db\u00dc\7j\2\2\u00dc\u00dd\7k\2\2\u00dd\u00de\7n\2\2"+
+    "\u00de\u00df\7g\2\2\u00df\37\3\2\2\2\u00e0\u00e1\7f\2\2\u00e1\u00e2\7"+
+    "q\2\2\u00e2!\3\2\2\2\u00e3\u00e4\7h\2\2\u00e4\u00e5\7q\2\2\u00e5\u00e6"+
+    "\7t\2\2\u00e6#\3\2\2\2\u00e7\u00e8\7e\2\2\u00e8\u00e9\7q\2\2\u00e9\u00ea"+
+    "\7p\2\2\u00ea\u00eb\7v\2\2\u00eb\u00ec\7k\2\2\u00ec\u00ed\7p\2\2\u00ed"+
+    "\u00ee\7w\2\2\u00ee\u00ef\7g\2\2\u00ef%\3\2\2\2\u00f0\u00f1\7d\2\2\u00f1"+
+    "\u00f2\7t\2\2\u00f2\u00f3\7g\2\2\u00f3\u00f4\7c\2\2\u00f4\u00f5\7m\2\2"+
+    "\u00f5\'\3\2\2\2\u00f6\u00f7\7t\2\2\u00f7\u00f8\7g\2\2\u00f8\u00f9\7v"+
+    "\2\2\u00f9\u00fa\7w\2\2\u00fa\u00fb\7t\2\2\u00fb\u00fc\7p\2\2\u00fc)\3"+
+    "\2\2\2\u00fd\u00fe\7p\2\2\u00fe\u00ff\7g\2\2\u00ff\u0100\7y\2\2\u0100"+
+    "+\3\2\2\2\u0101\u0102\7v\2\2\u0102\u0103\7t\2\2\u0103\u0104\7{\2\2\u0104"+
+    "-\3\2\2\2\u0105\u0106\7e\2\2\u0106\u0107\7c\2\2\u0107\u0108\7v\2\2\u0108"+
+    "\u0109\7e\2\2\u0109\u010a\7j\2\2\u010a/\3\2\2\2\u010b\u010c\7v\2\2\u010c"+
+    "\u010d\7j\2\2\u010d\u010e\7t\2\2\u010e\u010f\7q\2\2\u010f\u0110\7y\2\2"+
+    "\u0110\61\3\2\2\2\u0111\u0112\7#\2\2\u0112\63\3\2\2\2\u0113\u0114\7\u0080"+
+    "\2\2\u0114\65\3\2\2\2\u0115\u0116\7,\2\2\u0116\67\3\2\2\2\u0117\u0118"+
+    "\7\61\2\2\u01189\3\2\2\2\u0119\u011a\7\'\2\2\u011a;\3\2\2\2\u011b\u011c"+
+    "\7-\2\2\u011c=\3\2\2\2\u011d\u011e\7/\2\2\u011e?\3\2\2\2\u011f\u0120\7"+
+    ">\2\2\u0120\u0121\7>\2\2\u0121A\3\2\2\2\u0122\u0123\7@\2\2\u0123\u0124"+
+    "\7@\2\2\u0124C\3\2\2\2\u0125\u0126\7@\2\2\u0126\u0127\7@\2\2\u0127\u0128"+
+    "\7@\2\2\u0128E\3\2\2\2\u0129\u012a\7>\2\2\u012aG\3\2\2\2\u012b\u012c\7"+
+    ">\2\2\u012c\u012d\7?\2\2\u012dI\3\2\2\2\u012e\u012f\7@\2\2\u012fK\3\2"+
+    "\2\2\u0130\u0131\7@\2\2\u0131\u0132\7?\2\2\u0132M\3\2\2\2\u0133\u0134"+
+    "\7?\2\2\u0134\u0135\7?\2\2\u0135O\3\2\2\2\u0136\u0137\7?\2\2\u0137\u0138"+
+    "\7?\2\2\u0138\u0139\7?\2\2\u0139Q\3\2\2\2\u013a\u013b\7#\2\2\u013b\u013c"+
+    "\7?\2\2\u013cS\3\2\2\2\u013d\u013e\7#\2\2\u013e\u013f\7?\2\2\u013f\u0140"+
+    "\7?\2\2\u0140U\3\2\2\2\u0141\u0142\7(\2\2\u0142W\3\2\2\2\u0143\u0144\7"+
+    "`\2\2\u0144Y\3\2\2\2\u0145\u0146\7~\2\2\u0146[\3\2\2\2\u0147\u0148\7("+
+    "\2\2\u0148\u0149\7(\2\2\u0149]\3\2\2\2\u014a\u014b\7~\2\2\u014b\u014c"+
+    "\7~\2\2\u014c_\3\2\2\2\u014d\u014e\7A\2\2\u014ea\3\2\2\2\u014f\u0150\7"+
+    "<\2\2\u0150c\3\2\2\2\u0151\u0152\7-\2\2\u0152\u0153\7-\2\2\u0153e\3\2"+
+    "\2\2\u0154\u0155\7/\2\2\u0155\u0156\7/\2\2\u0156g\3\2\2\2\u0157\u0158"+
+    "\7?\2\2\u0158i\3\2\2\2\u0159\u015a\7-\2\2\u015a\u015b\7?\2\2\u015bk\3"+
+    "\2\2\2\u015c\u015d\7/\2\2\u015d\u015e\7?\2\2\u015em\3\2\2\2\u015f\u0160"+
+    "\7,\2\2\u0160\u0161\7?\2\2\u0161o\3\2\2\2\u0162\u0163\7\61\2\2\u0163\u0164"+
+    "\7?\2\2\u0164q\3\2\2\2\u0165\u0166\7\'\2\2\u0166\u0167\7?\2\2\u0167s\3"+
+    "\2\2\2\u0168\u0169\7(\2\2\u0169\u016a\7?\2\2\u016au\3\2\2\2\u016b\u016c"+
+    "\7`\2\2\u016c\u016d\7?\2\2\u016dw\3\2\2\2\u016e\u016f\7~\2\2\u016f\u0170"+
+    "\7?\2\2\u0170y\3\2\2\2\u0171\u0172\7>\2\2\u0172\u0173\7>\2\2\u0173\u0174"+
+    "\7?\2\2\u0174{\3\2\2\2\u0175\u0176\7@\2\2\u0176\u0177\7@\2\2\u0177\u0178"+
+    "\7?\2\2\u0178}\3\2\2\2\u0179\u017a\7@\2\2\u017a\u017b\7@\2\2\u017b\u017c"+
+    "\7@\2\2\u017c\u017d\7?\2\2\u017d\177\3\2\2\2\u017e\u017f\7\60\2\2\u017f"+
+    "\u0180\7\60\2\2\u0180\u0181\7?\2\2\u0181\u0081\3\2\2\2\u0182\u0184\7\62"+
+    "\2\2\u0183\u0185\t\4\2\2\u0184\u0183\3\2\2\2\u0185\u0186\3\2\2\2\u0186"+
+    "\u0184\3\2\2\2\u0186\u0187\3\2\2\2\u0187\u0189\3\2\2\2\u0188\u018a\t\5"+
+    "\2\2\u0189\u0188\3\2\2\2\u0189\u018a\3\2\2\2\u018a\u0083\3\2\2\2\u018b"+
+    "\u018c\7\62\2\2\u018c\u018e\t\6\2\2\u018d\u018f\t\7\2\2\u018e\u018d\3"+
+    "\2\2\2\u018f\u0190\3\2\2\2\u0190\u018e\3\2\2\2\u0190\u0191\3\2\2\2\u0191"+
+    "\u0193\3\2\2\2\u0192\u0194\t\5\2\2\u0193\u0192\3\2\2\2\u0193\u0194\3\2"+
+    "\2\2\u0194\u0085\3\2\2\2\u0195\u019e\7\62\2\2\u0196\u019a\t\b\2\2\u0197"+
+    "\u0199\t\t\2\2\u0198\u0197\3\2\2\2\u0199\u019c\3\2\2\2\u019a\u0198\3\2"+
+    "\2\2\u019a\u019b\3\2\2\2\u019b\u019e\3\2\2\2\u019c\u019a\3\2\2\2\u019d"+
+    "\u0195\3\2\2\2\u019d\u0196\3\2\2\2\u019e\u01a0\3\2\2\2\u019f\u01a1\t\n"+
+    "\2\2\u01a0\u019f\3\2\2\2\u01a0\u01a1\3\2\2\2\u01a1\u0087\3\2\2\2\u01a2"+
+    "\u01ab\7\62\2\2\u01a3\u01a7\t\b\2\2\u01a4\u01a6\t\t\2\2\u01a5\u01a4\3"+
+    "\2\2\2\u01a6\u01a9\3\2\2\2\u01a7\u01a5\3\2\2\2\u01a7\u01a8\3\2\2\2\u01a8"+
+    "\u01ab\3\2\2\2\u01a9\u01a7\3\2\2\2\u01aa\u01a2\3\2\2\2\u01aa\u01a3\3\2"+
+    "\2\2\u01ab\u01ac\3\2\2\2\u01ac\u01b0\5\24\n\2\u01ad\u01af\t\t\2\2\u01ae"+
+    "\u01ad\3\2\2\2\u01af\u01b2\3\2\2\2\u01b0\u01ae\3\2\2\2\u01b0\u01b1\3\2"+
+    "\2\2\u01b1\u01bc\3\2\2\2\u01b2\u01b0\3\2\2\2\u01b3\u01b5\t\13\2\2\u01b4"+
+    "\u01b6\t\f\2\2\u01b5\u01b4\3\2\2\2\u01b5\u01b6\3\2\2\2\u01b6\u01b8\3\2"+
+    "\2\2\u01b7\u01b9\t\t\2\2\u01b8\u01b7\3\2\2\2\u01b9\u01ba\3\2\2\2\u01ba"+
+    "\u01b8\3\2\2\2\u01ba\u01bb\3\2\2\2\u01bb\u01bd\3\2\2\2\u01bc\u01b3\3\2"+
+    "\2\2\u01bc\u01bd\3\2\2\2\u01bd\u01bf\3\2\2\2\u01be\u01c0\t\r\2\2\u01bf"+
+    "\u01be\3\2\2\2\u01bf\u01c0\3\2\2\2\u01c0\u0089\3\2\2\2\u01c1\u01c9\7$"+
+    "\2\2\u01c2\u01c3\7^\2\2\u01c3\u01c8\7$\2\2\u01c4\u01c5\7^\2\2\u01c5\u01c8"+
+    "\7^\2\2\u01c6\u01c8\n\16\2\2\u01c7\u01c2\3\2\2\2\u01c7\u01c4\3\2\2\2\u01c7"+
+    "\u01c6\3\2\2\2\u01c8\u01cb\3\2\2\2\u01c9\u01ca\3\2\2\2\u01c9\u01c7\3\2"+
+    "\2\2\u01ca\u01cc\3\2\2\2\u01cb\u01c9\3\2\2\2\u01cc\u01cd\7$\2\2\u01cd"+
+    "\u01ce\bE\4\2\u01ce\u008b\3\2\2\2\u01cf\u01d0\7)\2\2\u01d0\u01d1\13\2"+
+    "\2\2\u01d1\u01d2\7)\2\2\u01d2\u01d3\bF\5\2\u01d3\u008d\3\2\2\2\u01d4\u01d5"+
+    "\7v\2\2\u01d5\u01d6\7t\2\2\u01d6\u01d7\7w\2\2\u01d7\u01d8\7g\2\2\u01d8"+
+    "\u008f\3\2\2\2\u01d9\u01da\7h\2\2\u01da\u01db\7c\2\2\u01db\u01dc\7n\2"+
+    "\2\u01dc\u01dd\7u\2\2\u01dd\u01de\7g\2\2\u01de\u0091\3\2\2\2\u01df\u01e0"+
+    "\7p\2\2\u01e0\u01e1\7w\2\2\u01e1\u01e2\7n\2\2\u01e2\u01e3\7n\2\2\u01e3"+
+    "\u0093\3\2\2\2\u01e4\u01e6\5\u0098L\2\u01e5\u01e7\5\u0096K\2\u01e6\u01e5"+
+    "\3\2\2\2\u01e6\u01e7\3\2\2\2\u01e7\u01e8\3\2\2\2\u01e8\u01e9\6J\2\2\u01e9"+
+    "\u01ea\bJ\6\2\u01ea\u0095\3\2\2\2\u01eb\u01ed\7\"\2\2\u01ec\u01eb\3\2"+
+    "\2\2\u01ed\u01f0\3\2\2\2\u01ee\u01ec\3\2\2\2\u01ee\u01ef\3\2\2\2\u01ef"+
+    "\u01f1\3\2\2\2\u01f0\u01ee\3\2\2\2\u01f1\u01f5\7>\2\2\u01f2\u01f4\7\""+
+    "\2\2\u01f3\u01f2\3\2\2\2\u01f4\u01f7\3\2\2\2\u01f5\u01f3\3\2\2\2\u01f5"+
+    "\u01f6\3\2\2\2\u01f6\u01f8\3\2\2\2\u01f7\u01f5\3\2\2\2\u01f8\u01fa\5\u0098"+
+    "L\2\u01f9\u01fb\5\u0096K\2\u01fa\u01f9\3\2\2\2\u01fa\u01fb\3\2\2\2\u01fb"+
+    "\u01ff\3\2\2\2\u01fc\u01fe\7\"\2\2\u01fd\u01fc\3\2\2\2\u01fe\u0201\3\2"+
+    "\2\2\u01ff\u01fd\3\2\2\2\u01ff\u0200\3\2\2\2\u0200\u0215\3\2\2\2\u0201"+
+    "\u01ff\3\2\2\2\u0202\u0206\5\26\13\2\u0203\u0205\7\"\2\2\u0204\u0203\3"+
+    "\2\2\2\u0205\u0208\3\2\2\2\u0206\u0204\3\2\2\2\u0206\u0207\3\2\2\2\u0207"+
+    "\u0209\3\2\2\2\u0208\u0206\3\2\2\2\u0209\u020b\5\u0098L\2\u020a\u020c"+
+    "\5\u0096K\2\u020b\u020a\3\2\2\2\u020b\u020c\3\2\2\2\u020c\u0210\3\2\2"+
+    "\2\u020d\u020f\7\"\2\2\u020e\u020d\3\2\2\2\u020f\u0212\3\2\2\2\u0210\u020e"+
+    "\3\2\2\2\u0210\u0211\3\2\2\2\u0211\u0214\3\2\2\2\u0212\u0210\3\2\2\2\u0213"+
+    "\u0202\3\2\2\2\u0214\u0217\3\2\2\2\u0215\u0213\3\2\2\2\u0215\u0216\3\2"+
+    "\2\2\u0216\u0218\3\2\2\2\u0217\u0215\3\2\2\2\u0218\u0219\7@\2\2\u0219"+
+    "\u0097\3\2\2\2\u021a\u021e\t\17\2\2\u021b\u021d\t\20\2\2\u021c\u021b\3"+
+    "\2\2\2\u021d\u0220\3\2\2\2\u021e\u021c\3\2\2\2\u021e\u021f\3\2\2\2\u021f"+
+    "\u0099\3\2\2\2\u0220\u021e\3\2\2\2\u0221\u022a\7\62\2\2\u0222\u0226\t"+
+    "\b\2\2\u0223\u0225\t\t\2\2\u0224\u0223\3\2\2\2\u0225\u0228\3\2\2\2\u0226"+
+    "\u0224\3\2\2\2\u0226\u0227\3\2\2\2\u0227\u022a\3\2\2\2\u0228\u0226\3\2"+
+    "\2\2\u0229\u0221\3\2\2\2\u0229\u0222\3\2\2\2\u022a\u022b\3\2\2\2\u022b"+
+    "\u022c\bM\7\2\u022c\u009b\3\2\2\2\u022d\u0231\t\17\2\2\u022e\u0230\t\20"+
+    "\2\2\u022f\u022e\3\2\2\2\u0230\u0233\3\2\2\2\u0231\u022f\3\2\2\2\u0231"+
+    "\u0232\3\2\2\2\u0232\u0234\3\2\2\2\u0233\u0231\3\2\2\2\u0234\u0235\bN"+
+    "\7\2\u0235\u009d\3\2\2\2%\2\3\u00a1\u00ab\u00b5\u00ba\u0186\u0189\u0190"+
+    "\u0193\u019a\u019d\u01a0\u01a7\u01aa\u01b0\u01b5\u01ba\u01bc\u01bf\u01c7"+
+    "\u01c9\u01e6\u01ee\u01f5\u01fa\u01ff\u0206\u020b\u0210\u0215\u021e\u0226"+
+    "\u0229\u0231\b\b\2\2\4\3\2\3E\2\3F\3\3J\4\4\2\2";
+  public static final ATN _ATN =
+    new ATNDeserializer().deserialize(_serializedATN.toCharArray());
+  static {
+    _decisionToDFA = new DFA[_ATN.getNumberOfDecisions()];
+    for (int i = 0; i < _ATN.getNumberOfDecisions(); i++) {
+      _decisionToDFA[i] = new DFA(_ATN.getDecisionState(i), i);
+    }
+  }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java
new file mode 100644
index 0000000..13f61ac
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java
@@ -0,0 +1,2884 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.atn.*;
+import org.antlr.v4.runtime.dfa.DFA;
+import org.antlr.v4.runtime.*;
+import org.antlr.v4.runtime.misc.*;
+import org.antlr.v4.runtime.tree.*;
+import java.util.List;
+import java.util.Iterator;
+import java.util.ArrayList;
+
+@SuppressWarnings({"all", "warnings", "unchecked", "unused", "cast"})
+class PlanAParser extends Parser {
+  static { RuntimeMetaData.checkVersion("4.5.1", RuntimeMetaData.VERSION); }
+
+  protected static final DFA[] _decisionToDFA;
+  protected static final PredictionContextCache _sharedContextCache =
+    new PredictionContextCache();
+  public static final int
+    WS=1, COMMENT=2, LBRACK=3, RBRACK=4, LBRACE=5, RBRACE=6, LP=7, RP=8, DOT=9, 
+    COMMA=10, SEMICOLON=11, IF=12, ELSE=13, WHILE=14, DO=15, FOR=16, CONTINUE=17, 
+    BREAK=18, RETURN=19, NEW=20, TRY=21, CATCH=22, THROW=23, BOOLNOT=24, BWNOT=25, 
+    MUL=26, DIV=27, REM=28, ADD=29, SUB=30, LSH=31, RSH=32, USH=33, LT=34, 
+    LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43, 
+    BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51, 
+    AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59, 
+    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67, 
+    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75, 
+    EXTID=76;
+  public static final int
+    RULE_source = 0, RULE_statement = 1, RULE_block = 2, RULE_empty = 3, RULE_initializer = 4, 
+    RULE_afterthought = 5, RULE_declaration = 6, RULE_decltype = 7, RULE_declvar = 8, 
+    RULE_expression = 9, RULE_extstart = 10, RULE_extprec = 11, RULE_extcast = 12, 
+    RULE_extbrace = 13, RULE_extdot = 14, RULE_exttype = 15, RULE_extcall = 16, 
+    RULE_extvar = 17, RULE_extfield = 18, RULE_extnew = 19, RULE_extstring = 20, 
+    RULE_arguments = 21, RULE_increment = 22;
+  public static final String[] ruleNames = {
+    "source", "statement", "block", "empty", "initializer", "afterthought", 
+    "declaration", "decltype", "declvar", "expression", "extstart", "extprec", 
+    "extcast", "extbrace", "extdot", "exttype", "extcall", "extvar", "extfield", 
+    "extnew", "extstring", "arguments", "increment"
+  };
+
+  private static final String[] _LITERAL_NAMES = {
+    null, null, null, "'{'", "'}'", "'['", "']'", "'('", "')'", "'.'", "','", 
+    "';'", "'if'", "'else'", "'while'", "'do'", "'for'", "'continue'", "'break'", 
+    "'return'", "'new'", "'try'", "'catch'", "'throw'", "'!'", "'~'", "'*'", 
+    "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'", 
+    "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'", 
+    "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='", 
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null, 
+    null, null, null, null, null, "'true'", "'false'", "'null'"
+  };
+  private static final String[] _SYMBOLIC_NAMES = {
+    null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", 
+    "DOT", "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+  };
+  public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
+
+  /**
+   * @deprecated Use {@link #VOCABULARY} instead.
+   */
+  @Deprecated
+  public static final String[] tokenNames;
+  static {
+    tokenNames = new String[_SYMBOLIC_NAMES.length];
+    for (int i = 0; i < tokenNames.length; i++) {
+      tokenNames[i] = VOCABULARY.getLiteralName(i);
+      if (tokenNames[i] == null) {
+        tokenNames[i] = VOCABULARY.getSymbolicName(i);
+      }
+
+      if (tokenNames[i] == null) {
+        tokenNames[i] = "<INVALID>";
+      }
+    }
+  }
+
+  @Override
+  @Deprecated
+  public String[] getTokenNames() {
+    return tokenNames;
+  }
+
+  @Override
+
+  public Vocabulary getVocabulary() {
+    return VOCABULARY;
+  }
+
+  @Override
+  public String getGrammarFileName() { return "PlanAParser.g4"; }
+
+  @Override
+  public String[] getRuleNames() { return ruleNames; }
+
+  @Override
+  public String getSerializedATN() { return _serializedATN; }
+
+  @Override
+  public ATN getATN() { return _ATN; }
+
+  public PlanAParser(TokenStream input) {
+    super(input);
+    _interp = new ParserATNSimulator(this,_ATN,_decisionToDFA,_sharedContextCache);
+  }
+  public static class SourceContext extends ParserRuleContext {
+    public TerminalNode EOF() { return getToken(PlanAParser.EOF, 0); }
+    public List<StatementContext> statement() {
+      return getRuleContexts(StatementContext.class);
+    }
+    public StatementContext statement(int i) {
+      return getRuleContext(StatementContext.class,i);
+    }
+    public SourceContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_source; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitSource(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final SourceContext source() throws RecognitionException {
+    SourceContext _localctx = new SourceContext(_ctx, getState());
+    enterRule(_localctx, 0, RULE_source);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(47); 
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      do {
+        {
+        {
+        setState(46);
+        statement();
+        }
+        }
+        setState(49); 
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
+      setState(51);
+      match(EOF);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class StatementContext extends ParserRuleContext {
+    public StatementContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_statement; }
+   
+    public StatementContext() { }
+    public void copyFrom(StatementContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class DeclContext extends StatementContext {
+    public DeclarationContext declaration() {
+      return getRuleContext(DeclarationContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public DeclContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDecl(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BreakContext extends StatementContext {
+    public TerminalNode BREAK() { return getToken(PlanAParser.BREAK, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public BreakContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBreak(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ThrowContext extends StatementContext {
+    public TerminalNode THROW() { return getToken(PlanAParser.THROW, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ThrowContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitThrow(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ContinueContext extends StatementContext {
+    public TerminalNode CONTINUE() { return getToken(PlanAParser.CONTINUE, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ContinueContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitContinue(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ForContext extends StatementContext {
+    public TerminalNode FOR() { return getToken(PlanAParser.FOR, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public List<TerminalNode> SEMICOLON() { return getTokens(PlanAParser.SEMICOLON); }
+    public TerminalNode SEMICOLON(int i) {
+      return getToken(PlanAParser.SEMICOLON, i);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public EmptyContext empty() {
+      return getRuleContext(EmptyContext.class,0);
+    }
+    public InitializerContext initializer() {
+      return getRuleContext(InitializerContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public AfterthoughtContext afterthought() {
+      return getRuleContext(AfterthoughtContext.class,0);
+    }
+    public ForContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitFor(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class TryContext extends StatementContext {
+    public TerminalNode TRY() { return getToken(PlanAParser.TRY, 0); }
+    public List<BlockContext> block() {
+      return getRuleContexts(BlockContext.class);
+    }
+    public BlockContext block(int i) {
+      return getRuleContext(BlockContext.class,i);
+    }
+    public List<TerminalNode> CATCH() { return getTokens(PlanAParser.CATCH); }
+    public TerminalNode CATCH(int i) {
+      return getToken(PlanAParser.CATCH, i);
+    }
+    public List<TerminalNode> LP() { return getTokens(PlanAParser.LP); }
+    public TerminalNode LP(int i) {
+      return getToken(PlanAParser.LP, i);
+    }
+    public List<TerminalNode> RP() { return getTokens(PlanAParser.RP); }
+    public TerminalNode RP(int i) {
+      return getToken(PlanAParser.RP, i);
+    }
+    public List<TerminalNode> TYPE() { return getTokens(PlanAParser.TYPE); }
+    public TerminalNode TYPE(int i) {
+      return getToken(PlanAParser.TYPE, i);
+    }
+    public List<TerminalNode> ID() { return getTokens(PlanAParser.ID); }
+    public TerminalNode ID(int i) {
+      return getToken(PlanAParser.ID, i);
+    }
+    public TryContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitTry(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ExprContext extends StatementContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ExprContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExpr(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class DoContext extends StatementContext {
+    public TerminalNode DO() { return getToken(PlanAParser.DO, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public TerminalNode WHILE() { return getToken(PlanAParser.WHILE, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public DoContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDo(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class WhileContext extends StatementContext {
+    public TerminalNode WHILE() { return getToken(PlanAParser.WHILE, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public EmptyContext empty() {
+      return getRuleContext(EmptyContext.class,0);
+    }
+    public WhileContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitWhile(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class IfContext extends StatementContext {
+    public TerminalNode IF() { return getToken(PlanAParser.IF, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public List<BlockContext> block() {
+      return getRuleContexts(BlockContext.class);
+    }
+    public BlockContext block(int i) {
+      return getRuleContext(BlockContext.class,i);
+    }
+    public TerminalNode ELSE() { return getToken(PlanAParser.ELSE, 0); }
+    public IfContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitIf(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ReturnContext extends StatementContext {
+    public TerminalNode RETURN() { return getToken(PlanAParser.RETURN, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ReturnContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitReturn(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final StatementContext statement() throws RecognitionException {
+    StatementContext _localctx = new StatementContext(_ctx, getState());
+    enterRule(_localctx, 2, RULE_statement);
+    int _la;
+    try {
+      int _alt;
+      setState(136);
+      switch ( getInterpreter().adaptivePredict(_input,15,_ctx) ) {
+      case 1:
+        _localctx = new IfContext(_localctx);
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(53);
+        match(IF);
+        setState(54);
+        match(LP);
+        setState(55);
+        expression(0);
+        setState(56);
+        match(RP);
+        setState(57);
+        block();
+        setState(60);
+        switch ( getInterpreter().adaptivePredict(_input,1,_ctx) ) {
+        case 1:
+          {
+          setState(58);
+          match(ELSE);
+          setState(59);
+          block();
+          }
+          break;
+        }
+        }
+        break;
+      case 2:
+        _localctx = new WhileContext(_localctx);
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(62);
+        match(WHILE);
+        setState(63);
+        match(LP);
+        setState(64);
+        expression(0);
+        setState(65);
+        match(RP);
+        setState(68);
+        switch (_input.LA(1)) {
+        case LBRACK:
+        case LP:
+        case IF:
+        case WHILE:
+        case DO:
+        case FOR:
+        case CONTINUE:
+        case BREAK:
+        case RETURN:
+        case NEW:
+        case TRY:
+        case THROW:
+        case BOOLNOT:
+        case BWNOT:
+        case ADD:
+        case SUB:
+        case INCR:
+        case DECR:
+        case OCTAL:
+        case HEX:
+        case INTEGER:
+        case DECIMAL:
+        case STRING:
+        case CHAR:
+        case TRUE:
+        case FALSE:
+        case NULL:
+        case TYPE:
+        case ID:
+          {
+          setState(66);
+          block();
+          }
+          break;
+        case SEMICOLON:
+          {
+          setState(67);
+          empty();
+          }
+          break;
+        default:
+          throw new NoViableAltException(this);
+        }
+        }
+        break;
+      case 3:
+        _localctx = new DoContext(_localctx);
+        enterOuterAlt(_localctx, 3);
+        {
+        setState(70);
+        match(DO);
+        setState(71);
+        block();
+        setState(72);
+        match(WHILE);
+        setState(73);
+        match(LP);
+        setState(74);
+        expression(0);
+        setState(75);
+        match(RP);
+        setState(77);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(76);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 4:
+        _localctx = new ForContext(_localctx);
+        enterOuterAlt(_localctx, 4);
+        {
+        setState(79);
+        match(FOR);
+        setState(80);
+        match(LP);
+        setState(82);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(81);
+          initializer();
+          }
+        }
+
+        setState(84);
+        match(SEMICOLON);
+        setState(86);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(85);
+          expression(0);
+          }
+        }
+
+        setState(88);
+        match(SEMICOLON);
+        setState(90);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(89);
+          afterthought();
+          }
+        }
+
+        setState(92);
+        match(RP);
+        setState(95);
+        switch (_input.LA(1)) {
+        case LBRACK:
+        case LP:
+        case IF:
+        case WHILE:
+        case DO:
+        case FOR:
+        case CONTINUE:
+        case BREAK:
+        case RETURN:
+        case NEW:
+        case TRY:
+        case THROW:
+        case BOOLNOT:
+        case BWNOT:
+        case ADD:
+        case SUB:
+        case INCR:
+        case DECR:
+        case OCTAL:
+        case HEX:
+        case INTEGER:
+        case DECIMAL:
+        case STRING:
+        case CHAR:
+        case TRUE:
+        case FALSE:
+        case NULL:
+        case TYPE:
+        case ID:
+          {
+          setState(93);
+          block();
+          }
+          break;
+        case SEMICOLON:
+          {
+          setState(94);
+          empty();
+          }
+          break;
+        default:
+          throw new NoViableAltException(this);
+        }
+        }
+        break;
+      case 5:
+        _localctx = new DeclContext(_localctx);
+        enterOuterAlt(_localctx, 5);
+        {
+        setState(97);
+        declaration();
+        setState(99);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(98);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 6:
+        _localctx = new ContinueContext(_localctx);
+        enterOuterAlt(_localctx, 6);
+        {
+        setState(101);
+        match(CONTINUE);
+        setState(103);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(102);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 7:
+        _localctx = new BreakContext(_localctx);
+        enterOuterAlt(_localctx, 7);
+        {
+        setState(105);
+        match(BREAK);
+        setState(107);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(106);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 8:
+        _localctx = new ReturnContext(_localctx);
+        enterOuterAlt(_localctx, 8);
+        {
+        setState(109);
+        match(RETURN);
+        setState(110);
+        expression(0);
+        setState(112);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(111);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 9:
+        _localctx = new TryContext(_localctx);
+        enterOuterAlt(_localctx, 9);
+        {
+        setState(114);
+        match(TRY);
+        setState(115);
+        block();
+        setState(123); 
+        _errHandler.sync(this);
+        _alt = 1;
+        do {
+          switch (_alt) {
+          case 1:
+            {
+            {
+            setState(116);
+            match(CATCH);
+            setState(117);
+            match(LP);
+            {
+            setState(118);
+            match(TYPE);
+            setState(119);
+            match(ID);
+            }
+            setState(121);
+            match(RP);
+            setState(122);
+            block();
+            }
+            }
+            break;
+          default:
+            throw new NoViableAltException(this);
+          }
+          setState(125); 
+          _errHandler.sync(this);
+          _alt = getInterpreter().adaptivePredict(_input,12,_ctx);
+        } while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER );
+        }
+        break;
+      case 10:
+        _localctx = new ThrowContext(_localctx);
+        enterOuterAlt(_localctx, 10);
+        {
+        setState(127);
+        match(THROW);
+        setState(128);
+        expression(0);
+        setState(130);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(129);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 11:
+        _localctx = new ExprContext(_localctx);
+        enterOuterAlt(_localctx, 11);
+        {
+        setState(132);
+        expression(0);
+        setState(134);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(133);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class BlockContext extends ParserRuleContext {
+    public BlockContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_block; }
+   
+    public BlockContext() { }
+    public void copyFrom(BlockContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class SingleContext extends BlockContext {
+    public StatementContext statement() {
+      return getRuleContext(StatementContext.class,0);
+    }
+    public SingleContext(BlockContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitSingle(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class MultipleContext extends BlockContext {
+    public TerminalNode LBRACK() { return getToken(PlanAParser.LBRACK, 0); }
+    public TerminalNode RBRACK() { return getToken(PlanAParser.RBRACK, 0); }
+    public List<StatementContext> statement() {
+      return getRuleContexts(StatementContext.class);
+    }
+    public StatementContext statement(int i) {
+      return getRuleContext(StatementContext.class,i);
+    }
+    public MultipleContext(BlockContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitMultiple(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final BlockContext block() throws RecognitionException {
+    BlockContext _localctx = new BlockContext(_ctx, getState());
+    enterRule(_localctx, 4, RULE_block);
+    int _la;
+    try {
+      setState(147);
+      switch (_input.LA(1)) {
+      case LBRACK:
+        _localctx = new MultipleContext(_localctx);
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(138);
+        match(LBRACK);
+        setState(142);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+        while ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          {
+          setState(139);
+          statement();
+          }
+          }
+          setState(144);
+          _errHandler.sync(this);
+          _la = _input.LA(1);
+        }
+        setState(145);
+        match(RBRACK);
+        }
+        break;
+      case LP:
+      case IF:
+      case WHILE:
+      case DO:
+      case FOR:
+      case CONTINUE:
+      case BREAK:
+      case RETURN:
+      case NEW:
+      case TRY:
+      case THROW:
+      case BOOLNOT:
+      case BWNOT:
+      case ADD:
+      case SUB:
+      case INCR:
+      case DECR:
+      case OCTAL:
+      case HEX:
+      case INTEGER:
+      case DECIMAL:
+      case STRING:
+      case CHAR:
+      case TRUE:
+      case FALSE:
+      case NULL:
+      case TYPE:
+      case ID:
+        _localctx = new SingleContext(_localctx);
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(146);
+        statement();
+        }
+        break;
+      default:
+        throw new NoViableAltException(this);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class EmptyContext extends ParserRuleContext {
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public EmptyContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_empty; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitEmpty(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final EmptyContext empty() throws RecognitionException {
+    EmptyContext _localctx = new EmptyContext(_ctx, getState());
+    enterRule(_localctx, 6, RULE_empty);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(149);
+      match(SEMICOLON);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class InitializerContext extends ParserRuleContext {
+    public DeclarationContext declaration() {
+      return getRuleContext(DeclarationContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public InitializerContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_initializer; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitInitializer(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final InitializerContext initializer() throws RecognitionException {
+    InitializerContext _localctx = new InitializerContext(_ctx, getState());
+    enterRule(_localctx, 8, RULE_initializer);
+    try {
+      setState(153);
+      switch ( getInterpreter().adaptivePredict(_input,18,_ctx) ) {
+      case 1:
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(151);
+        declaration();
+        }
+        break;
+      case 2:
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(152);
+        expression(0);
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class AfterthoughtContext extends ParserRuleContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public AfterthoughtContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_afterthought; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitAfterthought(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final AfterthoughtContext afterthought() throws RecognitionException {
+    AfterthoughtContext _localctx = new AfterthoughtContext(_ctx, getState());
+    enterRule(_localctx, 10, RULE_afterthought);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(155);
+      expression(0);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DeclarationContext extends ParserRuleContext {
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public List<DeclvarContext> declvar() {
+      return getRuleContexts(DeclvarContext.class);
+    }
+    public DeclvarContext declvar(int i) {
+      return getRuleContext(DeclvarContext.class,i);
+    }
+    public List<TerminalNode> COMMA() { return getTokens(PlanAParser.COMMA); }
+    public TerminalNode COMMA(int i) {
+      return getToken(PlanAParser.COMMA, i);
+    }
+    public DeclarationContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_declaration; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDeclaration(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DeclarationContext declaration() throws RecognitionException {
+    DeclarationContext _localctx = new DeclarationContext(_ctx, getState());
+    enterRule(_localctx, 12, RULE_declaration);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(157);
+      decltype();
+      setState(158);
+      declvar();
+      setState(163);
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      while (_la==COMMA) {
+        {
+        {
+        setState(159);
+        match(COMMA);
+        setState(160);
+        declvar();
+        }
+        }
+        setState(165);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DecltypeContext extends ParserRuleContext {
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public List<TerminalNode> LBRACE() { return getTokens(PlanAParser.LBRACE); }
+    public TerminalNode LBRACE(int i) {
+      return getToken(PlanAParser.LBRACE, i);
+    }
+    public List<TerminalNode> RBRACE() { return getTokens(PlanAParser.RBRACE); }
+    public TerminalNode RBRACE(int i) {
+      return getToken(PlanAParser.RBRACE, i);
+    }
+    public DecltypeContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_decltype; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDecltype(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DecltypeContext decltype() throws RecognitionException {
+    DecltypeContext _localctx = new DecltypeContext(_ctx, getState());
+    enterRule(_localctx, 14, RULE_decltype);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(166);
+      match(TYPE);
+      setState(171);
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      while (_la==LBRACE) {
+        {
+        {
+        setState(167);
+        match(LBRACE);
+        setState(168);
+        match(RBRACE);
+        }
+        }
+        setState(173);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DeclvarContext extends ParserRuleContext {
+    public TerminalNode ID() { return getToken(PlanAParser.ID, 0); }
+    public TerminalNode ASSIGN() { return getToken(PlanAParser.ASSIGN, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public DeclvarContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_declvar; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDeclvar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DeclvarContext declvar() throws RecognitionException {
+    DeclvarContext _localctx = new DeclvarContext(_ctx, getState());
+    enterRule(_localctx, 16, RULE_declvar);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(174);
+      match(ID);
+      setState(177);
+      _la = _input.LA(1);
+      if (_la==ASSIGN) {
+        {
+        setState(175);
+        match(ASSIGN);
+        setState(176);
+        expression(0);
+        }
+      }
+
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExpressionContext extends ParserRuleContext {
+    public ExpressionContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_expression; }
+   
+    public ExpressionContext() { }
+    public void copyFrom(ExpressionContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class CompContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode LT() { return getToken(PlanAParser.LT, 0); }
+    public TerminalNode LTE() { return getToken(PlanAParser.LTE, 0); }
+    public TerminalNode GT() { return getToken(PlanAParser.GT, 0); }
+    public TerminalNode GTE() { return getToken(PlanAParser.GTE, 0); }
+    public TerminalNode EQ() { return getToken(PlanAParser.EQ, 0); }
+    public TerminalNode EQR() { return getToken(PlanAParser.EQR, 0); }
+    public TerminalNode NE() { return getToken(PlanAParser.NE, 0); }
+    public TerminalNode NER() { return getToken(PlanAParser.NER, 0); }
+    public CompContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitComp(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BoolContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode BOOLAND() { return getToken(PlanAParser.BOOLAND, 0); }
+    public TerminalNode BOOLOR() { return getToken(PlanAParser.BOOLOR, 0); }
+    public BoolContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBool(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ConditionalContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode COND() { return getToken(PlanAParser.COND, 0); }
+    public TerminalNode COLON() { return getToken(PlanAParser.COLON, 0); }
+    public ConditionalContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitConditional(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class AssignmentContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode ASSIGN() { return getToken(PlanAParser.ASSIGN, 0); }
+    public TerminalNode AADD() { return getToken(PlanAParser.AADD, 0); }
+    public TerminalNode ASUB() { return getToken(PlanAParser.ASUB, 0); }
+    public TerminalNode AMUL() { return getToken(PlanAParser.AMUL, 0); }
+    public TerminalNode ADIV() { return getToken(PlanAParser.ADIV, 0); }
+    public TerminalNode AREM() { return getToken(PlanAParser.AREM, 0); }
+    public TerminalNode AAND() { return getToken(PlanAParser.AAND, 0); }
+    public TerminalNode AXOR() { return getToken(PlanAParser.AXOR, 0); }
+    public TerminalNode AOR() { return getToken(PlanAParser.AOR, 0); }
+    public TerminalNode ALSH() { return getToken(PlanAParser.ALSH, 0); }
+    public TerminalNode ARSH() { return getToken(PlanAParser.ARSH, 0); }
+    public TerminalNode AUSH() { return getToken(PlanAParser.AUSH, 0); }
+    public AssignmentContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitAssignment(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class FalseContext extends ExpressionContext {
+    public TerminalNode FALSE() { return getToken(PlanAParser.FALSE, 0); }
+    public FalseContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitFalse(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class NumericContext extends ExpressionContext {
+    public TerminalNode OCTAL() { return getToken(PlanAParser.OCTAL, 0); }
+    public TerminalNode HEX() { return getToken(PlanAParser.HEX, 0); }
+    public TerminalNode INTEGER() { return getToken(PlanAParser.INTEGER, 0); }
+    public TerminalNode DECIMAL() { return getToken(PlanAParser.DECIMAL, 0); }
+    public NumericContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitNumeric(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class UnaryContext extends ExpressionContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode BOOLNOT() { return getToken(PlanAParser.BOOLNOT, 0); }
+    public TerminalNode BWNOT() { return getToken(PlanAParser.BWNOT, 0); }
+    public TerminalNode ADD() { return getToken(PlanAParser.ADD, 0); }
+    public TerminalNode SUB() { return getToken(PlanAParser.SUB, 0); }
+    public UnaryContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitUnary(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PrecedenceContext extends ExpressionContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public PrecedenceContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPrecedence(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PreincContext extends ExpressionContext {
+    public IncrementContext increment() {
+      return getRuleContext(IncrementContext.class,0);
+    }
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public PreincContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPreinc(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PostincContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public IncrementContext increment() {
+      return getRuleContext(IncrementContext.class,0);
+    }
+    public PostincContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPostinc(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class CastContext extends ExpressionContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public CastContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitCast(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ExternalContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public ExternalContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExternal(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class NullContext extends ExpressionContext {
+    public TerminalNode NULL() { return getToken(PlanAParser.NULL, 0); }
+    public NullContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitNull(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BinaryContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode MUL() { return getToken(PlanAParser.MUL, 0); }
+    public TerminalNode DIV() { return getToken(PlanAParser.DIV, 0); }
+    public TerminalNode REM() { return getToken(PlanAParser.REM, 0); }
+    public TerminalNode ADD() { return getToken(PlanAParser.ADD, 0); }
+    public TerminalNode SUB() { return getToken(PlanAParser.SUB, 0); }
+    public TerminalNode LSH() { return getToken(PlanAParser.LSH, 0); }
+    public TerminalNode RSH() { return getToken(PlanAParser.RSH, 0); }
+    public TerminalNode USH() { return getToken(PlanAParser.USH, 0); }
+    public TerminalNode BWAND() { return getToken(PlanAParser.BWAND, 0); }
+    public TerminalNode BWXOR() { return getToken(PlanAParser.BWXOR, 0); }
+    public TerminalNode BWOR() { return getToken(PlanAParser.BWOR, 0); }
+    public BinaryContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBinary(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class CharContext extends ExpressionContext {
+    public TerminalNode CHAR() { return getToken(PlanAParser.CHAR, 0); }
+    public CharContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitChar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class TrueContext extends ExpressionContext {
+    public TerminalNode TRUE() { return getToken(PlanAParser.TRUE, 0); }
+    public TrueContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitTrue(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExpressionContext expression() throws RecognitionException {
+    return expression(0);
+  }
+
+  private ExpressionContext expression(int _p) throws RecognitionException {
+    ParserRuleContext _parentctx = _ctx;
+    int _parentState = getState();
+    ExpressionContext _localctx = new ExpressionContext(_ctx, _parentState);
+    ExpressionContext _prevctx = _localctx;
+    int _startState = 18;
+    enterRecursionRule(_localctx, 18, RULE_expression, _p);
+    int _la;
+    try {
+      int _alt;
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(207);
+      switch ( getInterpreter().adaptivePredict(_input,22,_ctx) ) {
+      case 1:
+        {
+        _localctx = new UnaryContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+
+        setState(180);
+        _la = _input.LA(1);
+        if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        setState(181);
+        expression(14);
+        }
+        break;
+      case 2:
+        {
+        _localctx = new CastContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(182);
+        match(LP);
+        setState(183);
+        decltype();
+        setState(184);
+        match(RP);
+        setState(185);
+        expression(13);
+        }
+        break;
+      case 3:
+        {
+        _localctx = new AssignmentContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(187);
+        extstart();
+        setState(188);
+        _la = _input.LA(1);
+        if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << ASSIGN) | (1L << AADD) | (1L << ASUB) | (1L << AMUL) | (1L << ADIV) | (1L << AREM) | (1L << AAND) | (1L << AXOR) | (1L << AOR) | (1L << ALSH) | (1L << ARSH) | (1L << AUSH))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        setState(189);
+        expression(1);
+        }
+        break;
+      case 4:
+        {
+        _localctx = new PrecedenceContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(191);
+        match(LP);
+        setState(192);
+        expression(0);
+        setState(193);
+        match(RP);
+        }
+        break;
+      case 5:
+        {
+        _localctx = new NumericContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(195);
+        _la = _input.LA(1);
+        if ( !(((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        }
+        break;
+      case 6:
+        {
+        _localctx = new CharContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(196);
+        match(CHAR);
+        }
+        break;
+      case 7:
+        {
+        _localctx = new TrueContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(197);
+        match(TRUE);
+        }
+        break;
+      case 8:
+        {
+        _localctx = new FalseContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(198);
+        match(FALSE);
+        }
+        break;
+      case 9:
+        {
+        _localctx = new NullContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(199);
+        match(NULL);
+        }
+        break;
+      case 10:
+        {
+        _localctx = new PostincContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(200);
+        extstart();
+        setState(201);
+        increment();
+        }
+        break;
+      case 11:
+        {
+        _localctx = new PreincContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(203);
+        increment();
+        setState(204);
+        extstart();
+        }
+        break;
+      case 12:
+        {
+        _localctx = new ExternalContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(206);
+        extstart();
+        }
+        break;
+      }
+      _ctx.stop = _input.LT(-1);
+      setState(247);
+      _errHandler.sync(this);
+      _alt = getInterpreter().adaptivePredict(_input,24,_ctx);
+      while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER ) {
+        if ( _alt==1 ) {
+          if ( _parseListeners!=null ) triggerExitRuleEvent();
+          _prevctx = _localctx;
+          {
+          setState(245);
+          switch ( getInterpreter().adaptivePredict(_input,23,_ctx) ) {
+          case 1:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(209);
+            if (!(precpred(_ctx, 12))) throw new FailedPredicateException(this, "precpred(_ctx, 12)");
+            setState(210);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << MUL) | (1L << DIV) | (1L << REM))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(211);
+            expression(13);
+            }
+            break;
+          case 2:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(212);
+            if (!(precpred(_ctx, 11))) throw new FailedPredicateException(this, "precpred(_ctx, 11)");
+            setState(213);
+            _la = _input.LA(1);
+            if ( !(_la==ADD || _la==SUB) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(214);
+            expression(12);
+            }
+            break;
+          case 3:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(215);
+            if (!(precpred(_ctx, 10))) throw new FailedPredicateException(this, "precpred(_ctx, 10)");
+            setState(216);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LSH) | (1L << RSH) | (1L << USH))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(217);
+            expression(11);
+            }
+            break;
+          case 4:
+            {
+            _localctx = new CompContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(218);
+            if (!(precpred(_ctx, 9))) throw new FailedPredicateException(this, "precpred(_ctx, 9)");
+            setState(219);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LT) | (1L << LTE) | (1L << GT) | (1L << GTE))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(220);
+            expression(10);
+            }
+            break;
+          case 5:
+            {
+            _localctx = new CompContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(221);
+            if (!(precpred(_ctx, 8))) throw new FailedPredicateException(this, "precpred(_ctx, 8)");
+            setState(222);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << EQ) | (1L << EQR) | (1L << NE) | (1L << NER))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(223);
+            expression(9);
+            }
+            break;
+          case 6:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(224);
+            if (!(precpred(_ctx, 7))) throw new FailedPredicateException(this, "precpred(_ctx, 7)");
+            setState(225);
+            match(BWAND);
+            setState(226);
+            expression(8);
+            }
+            break;
+          case 7:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(227);
+            if (!(precpred(_ctx, 6))) throw new FailedPredicateException(this, "precpred(_ctx, 6)");
+            setState(228);
+            match(BWXOR);
+            setState(229);
+            expression(7);
+            }
+            break;
+          case 8:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(230);
+            if (!(precpred(_ctx, 5))) throw new FailedPredicateException(this, "precpred(_ctx, 5)");
+            setState(231);
+            match(BWOR);
+            setState(232);
+            expression(6);
+            }
+            break;
+          case 9:
+            {
+            _localctx = new BoolContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(233);
+            if (!(precpred(_ctx, 4))) throw new FailedPredicateException(this, "precpred(_ctx, 4)");
+            setState(234);
+            match(BOOLAND);
+            setState(235);
+            expression(5);
+            }
+            break;
+          case 10:
+            {
+            _localctx = new BoolContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(236);
+            if (!(precpred(_ctx, 3))) throw new FailedPredicateException(this, "precpred(_ctx, 3)");
+            setState(237);
+            match(BOOLOR);
+            setState(238);
+            expression(4);
+            }
+            break;
+          case 11:
+            {
+            _localctx = new ConditionalContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(239);
+            if (!(precpred(_ctx, 2))) throw new FailedPredicateException(this, "precpred(_ctx, 2)");
+            setState(240);
+            match(COND);
+            setState(241);
+            expression(0);
+            setState(242);
+            match(COLON);
+            setState(243);
+            expression(2);
+            }
+            break;
+          }
+          } 
+        }
+        setState(249);
+        _errHandler.sync(this);
+        _alt = getInterpreter().adaptivePredict(_input,24,_ctx);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      unrollRecursionContexts(_parentctx);
+    }
+    return _localctx;
+  }
+
+  public static class ExtstartContext extends ParserRuleContext {
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtstartContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extstart; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtstart(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtstartContext extstart() throws RecognitionException {
+    ExtstartContext _localctx = new ExtstartContext(_ctx, getState());
+    enterRule(_localctx, 20, RULE_extstart);
+    try {
+      setState(256);
+      switch ( getInterpreter().adaptivePredict(_input,25,_ctx) ) {
+      case 1:
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(250);
+        extprec();
+        }
+        break;
+      case 2:
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(251);
+        extcast();
+        }
+        break;
+      case 3:
+        enterOuterAlt(_localctx, 3);
+        {
+        setState(252);
+        exttype();
+        }
+        break;
+      case 4:
+        enterOuterAlt(_localctx, 4);
+        {
+        setState(253);
+        extvar();
+        }
+        break;
+      case 5:
+        enterOuterAlt(_localctx, 5);
+        {
+        setState(254);
+        extnew();
+        }
+        break;
+      case 6:
+        enterOuterAlt(_localctx, 6);
+        {
+        setState(255);
+        extstring();
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtprecContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtprecContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extprec; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtprec(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtprecContext extprec() throws RecognitionException {
+    ExtprecContext _localctx = new ExtprecContext(_ctx, getState());
+    enterRule(_localctx, 22, RULE_extprec);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(258);
+      match(LP);
+      setState(265);
+      switch ( getInterpreter().adaptivePredict(_input,26,_ctx) ) {
+      case 1:
+        {
+        setState(259);
+        extprec();
+        }
+        break;
+      case 2:
+        {
+        setState(260);
+        extcast();
+        }
+        break;
+      case 3:
+        {
+        setState(261);
+        exttype();
+        }
+        break;
+      case 4:
+        {
+        setState(262);
+        extvar();
+        }
+        break;
+      case 5:
+        {
+        setState(263);
+        extnew();
+        }
+        break;
+      case 6:
+        {
+        setState(264);
+        extstring();
+        }
+        break;
+      }
+      setState(267);
+      match(RP);
+      setState(270);
+      switch ( getInterpreter().adaptivePredict(_input,27,_ctx) ) {
+      case 1:
+        {
+        setState(268);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(269);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtcastContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtcastContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extcast; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtcast(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtcastContext extcast() throws RecognitionException {
+    ExtcastContext _localctx = new ExtcastContext(_ctx, getState());
+    enterRule(_localctx, 24, RULE_extcast);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(272);
+      match(LP);
+      setState(273);
+      decltype();
+      setState(274);
+      match(RP);
+      setState(281);
+      switch ( getInterpreter().adaptivePredict(_input,28,_ctx) ) {
+      case 1:
+        {
+        setState(275);
+        extprec();
+        }
+        break;
+      case 2:
+        {
+        setState(276);
+        extcast();
+        }
+        break;
+      case 3:
+        {
+        setState(277);
+        exttype();
+        }
+        break;
+      case 4:
+        {
+        setState(278);
+        extvar();
+        }
+        break;
+      case 5:
+        {
+        setState(279);
+        extnew();
+        }
+        break;
+      case 6:
+        {
+        setState(280);
+        extstring();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtbraceContext extends ParserRuleContext {
+    public TerminalNode LBRACE() { return getToken(PlanAParser.LBRACE, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RBRACE() { return getToken(PlanAParser.RBRACE, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtbraceContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extbrace; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtbrace(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtbraceContext extbrace() throws RecognitionException {
+    ExtbraceContext _localctx = new ExtbraceContext(_ctx, getState());
+    enterRule(_localctx, 26, RULE_extbrace);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(283);
+      match(LBRACE);
+      setState(284);
+      expression(0);
+      setState(285);
+      match(RBRACE);
+      setState(288);
+      switch ( getInterpreter().adaptivePredict(_input,29,_ctx) ) {
+      case 1:
+        {
+        setState(286);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(287);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtdotContext extends ParserRuleContext {
+    public TerminalNode DOT() { return getToken(PlanAParser.DOT, 0); }
+    public ExtcallContext extcall() {
+      return getRuleContext(ExtcallContext.class,0);
+    }
+    public ExtfieldContext extfield() {
+      return getRuleContext(ExtfieldContext.class,0);
+    }
+    public ExtdotContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extdot; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtdot(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtdotContext extdot() throws RecognitionException {
+    ExtdotContext _localctx = new ExtdotContext(_ctx, getState());
+    enterRule(_localctx, 28, RULE_extdot);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(290);
+      match(DOT);
+      setState(293);
+      switch ( getInterpreter().adaptivePredict(_input,30,_ctx) ) {
+      case 1:
+        {
+        setState(291);
+        extcall();
+        }
+        break;
+      case 2:
+        {
+        setState(292);
+        extfield();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExttypeContext extends ParserRuleContext {
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExttypeContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_exttype; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExttype(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExttypeContext exttype() throws RecognitionException {
+    ExttypeContext _localctx = new ExttypeContext(_ctx, getState());
+    enterRule(_localctx, 30, RULE_exttype);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(295);
+      match(TYPE);
+      setState(296);
+      extdot();
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtcallContext extends ParserRuleContext {
+    public TerminalNode EXTID() { return getToken(PlanAParser.EXTID, 0); }
+    public ArgumentsContext arguments() {
+      return getRuleContext(ArgumentsContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtcallContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extcall; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtcall(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtcallContext extcall() throws RecognitionException {
+    ExtcallContext _localctx = new ExtcallContext(_ctx, getState());
+    enterRule(_localctx, 32, RULE_extcall);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(298);
+      match(EXTID);
+      setState(299);
+      arguments();
+      setState(302);
+      switch ( getInterpreter().adaptivePredict(_input,31,_ctx) ) {
+      case 1:
+        {
+        setState(300);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(301);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtvarContext extends ParserRuleContext {
+    public TerminalNode ID() { return getToken(PlanAParser.ID, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtvarContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extvar; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtvar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtvarContext extvar() throws RecognitionException {
+    ExtvarContext _localctx = new ExtvarContext(_ctx, getState());
+    enterRule(_localctx, 34, RULE_extvar);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(304);
+      match(ID);
+      setState(307);
+      switch ( getInterpreter().adaptivePredict(_input,32,_ctx) ) {
+      case 1:
+        {
+        setState(305);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(306);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtfieldContext extends ParserRuleContext {
+    public TerminalNode EXTID() { return getToken(PlanAParser.EXTID, 0); }
+    public TerminalNode EXTINTEGER() { return getToken(PlanAParser.EXTINTEGER, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtfieldContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extfield; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtfield(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtfieldContext extfield() throws RecognitionException {
+    ExtfieldContext _localctx = new ExtfieldContext(_ctx, getState());
+    enterRule(_localctx, 36, RULE_extfield);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(309);
+      _la = _input.LA(1);
+      if ( !(_la==EXTINTEGER || _la==EXTID) ) {
+      _errHandler.recoverInline(this);
+      } else {
+        consume();
+      }
+      setState(312);
+      switch ( getInterpreter().adaptivePredict(_input,33,_ctx) ) {
+      case 1:
+        {
+        setState(310);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(311);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtnewContext extends ParserRuleContext {
+    public TerminalNode NEW() { return getToken(PlanAParser.NEW, 0); }
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public ArgumentsContext arguments() {
+      return getRuleContext(ArgumentsContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public List<TerminalNode> LBRACE() { return getTokens(PlanAParser.LBRACE); }
+    public TerminalNode LBRACE(int i) {
+      return getToken(PlanAParser.LBRACE, i);
+    }
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public List<TerminalNode> RBRACE() { return getTokens(PlanAParser.RBRACE); }
+    public TerminalNode RBRACE(int i) {
+      return getToken(PlanAParser.RBRACE, i);
+    }
+    public ExtnewContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extnew; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtnew(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtnewContext extnew() throws RecognitionException {
+    ExtnewContext _localctx = new ExtnewContext(_ctx, getState());
+    enterRule(_localctx, 38, RULE_extnew);
+    try {
+      int _alt;
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(314);
+      match(NEW);
+      setState(315);
+      match(TYPE);
+      setState(332);
+      switch (_input.LA(1)) {
+      case LP:
+        {
+        {
+        setState(316);
+        arguments();
+        setState(319);
+        switch ( getInterpreter().adaptivePredict(_input,34,_ctx) ) {
+        case 1:
+          {
+          setState(317);
+          extdot();
+          }
+          break;
+        case 2:
+          {
+          setState(318);
+          extbrace();
+          }
+          break;
+        }
+        }
+        }
+        break;
+      case LBRACE:
+        {
+        {
+        setState(325); 
+        _errHandler.sync(this);
+        _alt = 1;
+        do {
+          switch (_alt) {
+          case 1:
+            {
+            {
+            setState(321);
+            match(LBRACE);
+            setState(322);
+            expression(0);
+            setState(323);
+            match(RBRACE);
+            }
+            }
+            break;
+          default:
+            throw new NoViableAltException(this);
+          }
+          setState(327); 
+          _errHandler.sync(this);
+          _alt = getInterpreter().adaptivePredict(_input,35,_ctx);
+        } while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER );
+        setState(330);
+        switch ( getInterpreter().adaptivePredict(_input,36,_ctx) ) {
+        case 1:
+          {
+          setState(329);
+          extdot();
+          }
+          break;
+        }
+        }
+        }
+        break;
+      default:
+        throw new NoViableAltException(this);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtstringContext extends ParserRuleContext {
+    public TerminalNode STRING() { return getToken(PlanAParser.STRING, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtstringContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extstring; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtstring(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtstringContext extstring() throws RecognitionException {
+    ExtstringContext _localctx = new ExtstringContext(_ctx, getState());
+    enterRule(_localctx, 40, RULE_extstring);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(334);
+      match(STRING);
+      setState(337);
+      switch ( getInterpreter().adaptivePredict(_input,38,_ctx) ) {
+      case 1:
+        {
+        setState(335);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(336);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ArgumentsContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public List<TerminalNode> COMMA() { return getTokens(PlanAParser.COMMA); }
+    public TerminalNode COMMA(int i) {
+      return getToken(PlanAParser.COMMA, i);
+    }
+    public ArgumentsContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_arguments; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitArguments(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ArgumentsContext arguments() throws RecognitionException {
+    ArgumentsContext _localctx = new ArgumentsContext(_ctx, getState());
+    enterRule(_localctx, 42, RULE_arguments);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      {
+      setState(339);
+      match(LP);
+      setState(348);
+      _la = _input.LA(1);
+      if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+        {
+        setState(340);
+        expression(0);
+        setState(345);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+        while (_la==COMMA) {
+          {
+          {
+          setState(341);
+          match(COMMA);
+          setState(342);
+          expression(0);
+          }
+          }
+          setState(347);
+          _errHandler.sync(this);
+          _la = _input.LA(1);
+        }
+        }
+      }
+
+      setState(350);
+      match(RP);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class IncrementContext extends ParserRuleContext {
+    public TerminalNode INCR() { return getToken(PlanAParser.INCR, 0); }
+    public TerminalNode DECR() { return getToken(PlanAParser.DECR, 0); }
+    public IncrementContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_increment; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitIncrement(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final IncrementContext increment() throws RecognitionException {
+    IncrementContext _localctx = new IncrementContext(_ctx, getState());
+    enterRule(_localctx, 44, RULE_increment);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(352);
+      _la = _input.LA(1);
+      if ( !(_la==INCR || _la==DECR) ) {
+      _errHandler.recoverInline(this);
+      } else {
+        consume();
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public boolean sempred(RuleContext _localctx, int ruleIndex, int predIndex) {
+    switch (ruleIndex) {
+    case 9:
+      return expression_sempred((ExpressionContext)_localctx, predIndex);
+    }
+    return true;
+  }
+  private boolean expression_sempred(ExpressionContext _localctx, int predIndex) {
+    switch (predIndex) {
+    case 0:
+      return precpred(_ctx, 12);
+    case 1:
+      return precpred(_ctx, 11);
+    case 2:
+      return precpred(_ctx, 10);
+    case 3:
+      return precpred(_ctx, 9);
+    case 4:
+      return precpred(_ctx, 8);
+    case 5:
+      return precpred(_ctx, 7);
+    case 6:
+      return precpred(_ctx, 6);
+    case 7:
+      return precpred(_ctx, 5);
+    case 8:
+      return precpred(_ctx, 4);
+    case 9:
+      return precpred(_ctx, 3);
+    case 10:
+      return precpred(_ctx, 2);
+    }
+    return true;
+  }
+
+  public static final String _serializedATN =
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3N\u0165\4\2\t\2\4"+
+    "\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t"+
+    "\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22"+
+    "\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\3\2\6\2\62"+
+    "\n\2\r\2\16\2\63\3\2\3\2\3\3\3\3\3\3\3\3\3\3\3\3\3\3\5\3?\n\3\3\3\3\3"+
+    "\3\3\3\3\3\3\3\3\5\3G\n\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\5\3P\n\3\3\3\3\3"+
+    "\3\3\5\3U\n\3\3\3\3\3\5\3Y\n\3\3\3\3\3\5\3]\n\3\3\3\3\3\3\3\5\3b\n\3\3"+
+    "\3\3\3\5\3f\n\3\3\3\3\3\5\3j\n\3\3\3\3\3\5\3n\n\3\3\3\3\3\3\3\5\3s\n\3"+
+    "\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\6\3~\n\3\r\3\16\3\177\3\3\3\3\3\3"+
+    "\5\3\u0085\n\3\3\3\3\3\5\3\u0089\n\3\5\3\u008b\n\3\3\4\3\4\7\4\u008f\n"+
+    "\4\f\4\16\4\u0092\13\4\3\4\3\4\5\4\u0096\n\4\3\5\3\5\3\6\3\6\5\6\u009c"+
+    "\n\6\3\7\3\7\3\b\3\b\3\b\3\b\7\b\u00a4\n\b\f\b\16\b\u00a7\13\b\3\t\3\t"+
+    "\3\t\7\t\u00ac\n\t\f\t\16\t\u00af\13\t\3\n\3\n\3\n\5\n\u00b4\n\n\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\5\13"+
+    "\u00d2\n\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\7\13\u00f8\n\13\f\13"+
+    "\16\13\u00fb\13\13\3\f\3\f\3\f\3\f\3\f\3\f\5\f\u0103\n\f\3\r\3\r\3\r\3"+
+    "\r\3\r\3\r\3\r\5\r\u010c\n\r\3\r\3\r\3\r\5\r\u0111\n\r\3\16\3\16\3\16"+
+    "\3\16\3\16\3\16\3\16\3\16\3\16\5\16\u011c\n\16\3\17\3\17\3\17\3\17\3\17"+
+    "\5\17\u0123\n\17\3\20\3\20\3\20\5\20\u0128\n\20\3\21\3\21\3\21\3\22\3"+
+    "\22\3\22\3\22\5\22\u0131\n\22\3\23\3\23\3\23\5\23\u0136\n\23\3\24\3\24"+
+    "\3\24\5\24\u013b\n\24\3\25\3\25\3\25\3\25\3\25\5\25\u0142\n\25\3\25\3"+
+    "\25\3\25\3\25\6\25\u0148\n\25\r\25\16\25\u0149\3\25\5\25\u014d\n\25\5"+
+    "\25\u014f\n\25\3\26\3\26\3\26\5\26\u0154\n\26\3\27\3\27\3\27\3\27\7\27"+
+    "\u015a\n\27\f\27\16\27\u015d\13\27\5\27\u015f\n\27\3\27\3\27\3\30\3\30"+
+    "\3\30\2\3\24\31\2\4\6\b\n\f\16\20\22\24\26\30\32\34\36 \"$&(*,.\2\f\4"+
+    "\2\32\33\37 \3\2\65@\3\2BE\3\2\34\36\3\2\37 \3\2!#\3\2$\'\3\2(+\3\2MN"+
+    "\3\2\63\64\u01a5\2\61\3\2\2\2\4\u008a\3\2\2\2\6\u0095\3\2\2\2\b\u0097"+
+    "\3\2\2\2\n\u009b\3\2\2\2\f\u009d\3\2\2\2\16\u009f\3\2\2\2\20\u00a8\3\2"+
+    "\2\2\22\u00b0\3\2\2\2\24\u00d1\3\2\2\2\26\u0102\3\2\2\2\30\u0104\3\2\2"+
+    "\2\32\u0112\3\2\2\2\34\u011d\3\2\2\2\36\u0124\3\2\2\2 \u0129\3\2\2\2\""+
+    "\u012c\3\2\2\2$\u0132\3\2\2\2&\u0137\3\2\2\2(\u013c\3\2\2\2*\u0150\3\2"+
+    "\2\2,\u0155\3\2\2\2.\u0162\3\2\2\2\60\62\5\4\3\2\61\60\3\2\2\2\62\63\3"+
+    "\2\2\2\63\61\3\2\2\2\63\64\3\2\2\2\64\65\3\2\2\2\65\66\7\2\2\3\66\3\3"+
+    "\2\2\2\678\7\16\2\289\7\t\2\29:\5\24\13\2:;\7\n\2\2;>\5\6\4\2<=\7\17\2"+
+    "\2=?\5\6\4\2><\3\2\2\2>?\3\2\2\2?\u008b\3\2\2\2@A\7\20\2\2AB\7\t\2\2B"+
+    "C\5\24\13\2CF\7\n\2\2DG\5\6\4\2EG\5\b\5\2FD\3\2\2\2FE\3\2\2\2G\u008b\3"+
+    "\2\2\2HI\7\21\2\2IJ\5\6\4\2JK\7\20\2\2KL\7\t\2\2LM\5\24\13\2MO\7\n\2\2"+
+    "NP\7\r\2\2ON\3\2\2\2OP\3\2\2\2P\u008b\3\2\2\2QR\7\22\2\2RT\7\t\2\2SU\5"+
+    "\n\6\2TS\3\2\2\2TU\3\2\2\2UV\3\2\2\2VX\7\r\2\2WY\5\24\13\2XW\3\2\2\2X"+
+    "Y\3\2\2\2YZ\3\2\2\2Z\\\7\r\2\2[]\5\f\7\2\\[\3\2\2\2\\]\3\2\2\2]^\3\2\2"+
+    "\2^a\7\n\2\2_b\5\6\4\2`b\5\b\5\2a_\3\2\2\2a`\3\2\2\2b\u008b\3\2\2\2ce"+
+    "\5\16\b\2df\7\r\2\2ed\3\2\2\2ef\3\2\2\2f\u008b\3\2\2\2gi\7\23\2\2hj\7"+
+    "\r\2\2ih\3\2\2\2ij\3\2\2\2j\u008b\3\2\2\2km\7\24\2\2ln\7\r\2\2ml\3\2\2"+
+    "\2mn\3\2\2\2n\u008b\3\2\2\2op\7\25\2\2pr\5\24\13\2qs\7\r\2\2rq\3\2\2\2"+
+    "rs\3\2\2\2s\u008b\3\2\2\2tu\7\27\2\2u}\5\6\4\2vw\7\30\2\2wx\7\t\2\2xy"+
+    "\7K\2\2yz\7L\2\2z{\3\2\2\2{|\7\n\2\2|~\5\6\4\2}v\3\2\2\2~\177\3\2\2\2"+
+    "\177}\3\2\2\2\177\u0080\3\2\2\2\u0080\u008b\3\2\2\2\u0081\u0082\7\31\2"+
+    "\2\u0082\u0084\5\24\13\2\u0083\u0085\7\r\2\2\u0084\u0083\3\2\2\2\u0084"+
+    "\u0085\3\2\2\2\u0085\u008b\3\2\2\2\u0086\u0088\5\24\13\2\u0087\u0089\7"+
+    "\r\2\2\u0088\u0087\3\2\2\2\u0088\u0089\3\2\2\2\u0089\u008b\3\2\2\2\u008a"+
+    "\67\3\2\2\2\u008a@\3\2\2\2\u008aH\3\2\2\2\u008aQ\3\2\2\2\u008ac\3\2\2"+
+    "\2\u008ag\3\2\2\2\u008ak\3\2\2\2\u008ao\3\2\2\2\u008at\3\2\2\2\u008a\u0081"+
+    "\3\2\2\2\u008a\u0086\3\2\2\2\u008b\5\3\2\2\2\u008c\u0090\7\5\2\2\u008d"+
+    "\u008f\5\4\3\2\u008e\u008d\3\2\2\2\u008f\u0092\3\2\2\2\u0090\u008e\3\2"+
+    "\2\2\u0090\u0091\3\2\2\2\u0091\u0093\3\2\2\2\u0092\u0090\3\2\2\2\u0093"+
+    "\u0096\7\6\2\2\u0094\u0096\5\4\3\2\u0095\u008c\3\2\2\2\u0095\u0094\3\2"+
+    "\2\2\u0096\7\3\2\2\2\u0097\u0098\7\r\2\2\u0098\t\3\2\2\2\u0099\u009c\5"+
+    "\16\b\2\u009a\u009c\5\24\13\2\u009b\u0099\3\2\2\2\u009b\u009a\3\2\2\2"+
+    "\u009c\13\3\2\2\2\u009d\u009e\5\24\13\2\u009e\r\3\2\2\2\u009f\u00a0\5"+
+    "\20\t\2\u00a0\u00a5\5\22\n\2\u00a1\u00a2\7\f\2\2\u00a2\u00a4\5\22\n\2"+
+    "\u00a3\u00a1\3\2\2\2\u00a4\u00a7\3\2\2\2\u00a5\u00a3\3\2\2\2\u00a5\u00a6"+
+    "\3\2\2\2\u00a6\17\3\2\2\2\u00a7\u00a5\3\2\2\2\u00a8\u00ad\7K\2\2\u00a9"+
+    "\u00aa\7\7\2\2\u00aa\u00ac\7\b\2\2\u00ab\u00a9\3\2\2\2\u00ac\u00af\3\2"+
+    "\2\2\u00ad\u00ab\3\2\2\2\u00ad\u00ae\3\2\2\2\u00ae\21\3\2\2\2\u00af\u00ad"+
+    "\3\2\2\2\u00b0\u00b3\7L\2\2\u00b1\u00b2\7\65\2\2\u00b2\u00b4\5\24\13\2"+
+    "\u00b3\u00b1\3\2\2\2\u00b3\u00b4\3\2\2\2\u00b4\23\3\2\2\2\u00b5\u00b6"+
+    "\b\13\1\2\u00b6\u00b7\t\2\2\2\u00b7\u00d2\5\24\13\20\u00b8\u00b9\7\t\2"+
+    "\2\u00b9\u00ba\5\20\t\2\u00ba\u00bb\7\n\2\2\u00bb\u00bc\5\24\13\17\u00bc"+
+    "\u00d2\3\2\2\2\u00bd\u00be\5\26\f\2\u00be\u00bf\t\3\2\2\u00bf\u00c0\5"+
+    "\24\13\3\u00c0\u00d2\3\2\2\2\u00c1\u00c2\7\t\2\2\u00c2\u00c3\5\24\13\2"+
+    "\u00c3\u00c4\7\n\2\2\u00c4\u00d2\3\2\2\2\u00c5\u00d2\t\4\2\2\u00c6\u00d2"+
+    "\7G\2\2\u00c7\u00d2\7H\2\2\u00c8\u00d2\7I\2\2\u00c9\u00d2\7J\2\2\u00ca"+
+    "\u00cb\5\26\f\2\u00cb\u00cc\5.\30\2\u00cc\u00d2\3\2\2\2\u00cd\u00ce\5"+
+    ".\30\2\u00ce\u00cf\5\26\f\2\u00cf\u00d2\3\2\2\2\u00d0\u00d2\5\26\f\2\u00d1"+
+    "\u00b5\3\2\2\2\u00d1\u00b8\3\2\2\2\u00d1\u00bd\3\2\2\2\u00d1\u00c1\3\2"+
+    "\2\2\u00d1\u00c5\3\2\2\2\u00d1\u00c6\3\2\2\2\u00d1\u00c7\3\2\2\2\u00d1"+
+    "\u00c8\3\2\2\2\u00d1\u00c9\3\2\2\2\u00d1\u00ca\3\2\2\2\u00d1\u00cd\3\2"+
+    "\2\2\u00d1\u00d0\3\2\2\2\u00d2\u00f9\3\2\2\2\u00d3\u00d4\f\16\2\2\u00d4"+
+    "\u00d5\t\5\2\2\u00d5\u00f8\5\24\13\17\u00d6\u00d7\f\r\2\2\u00d7\u00d8"+
+    "\t\6\2\2\u00d8\u00f8\5\24\13\16\u00d9\u00da\f\f\2\2\u00da\u00db\t\7\2"+
+    "\2\u00db\u00f8\5\24\13\r\u00dc\u00dd\f\13\2\2\u00dd\u00de\t\b\2\2\u00de"+
+    "\u00f8\5\24\13\f\u00df\u00e0\f\n\2\2\u00e0\u00e1\t\t\2\2\u00e1\u00f8\5"+
+    "\24\13\13\u00e2\u00e3\f\t\2\2\u00e3\u00e4\7,\2\2\u00e4\u00f8\5\24\13\n"+
+    "\u00e5\u00e6\f\b\2\2\u00e6\u00e7\7-\2\2\u00e7\u00f8\5\24\13\t\u00e8\u00e9"+
+    "\f\7\2\2\u00e9\u00ea\7.\2\2\u00ea\u00f8\5\24\13\b\u00eb\u00ec\f\6\2\2"+
+    "\u00ec\u00ed\7/\2\2\u00ed\u00f8\5\24\13\7\u00ee\u00ef\f\5\2\2\u00ef\u00f0"+
+    "\7\60\2\2\u00f0\u00f8\5\24\13\6\u00f1\u00f2\f\4\2\2\u00f2\u00f3\7\61\2"+
+    "\2\u00f3\u00f4\5\24\13\2\u00f4\u00f5\7\62\2\2\u00f5\u00f6\5\24\13\4\u00f6"+
+    "\u00f8\3\2\2\2\u00f7\u00d3\3\2\2\2\u00f7\u00d6\3\2\2\2\u00f7\u00d9\3\2"+
+    "\2\2\u00f7\u00dc\3\2\2\2\u00f7\u00df\3\2\2\2\u00f7\u00e2\3\2\2\2\u00f7"+
+    "\u00e5\3\2\2\2\u00f7\u00e8\3\2\2\2\u00f7\u00eb\3\2\2\2\u00f7\u00ee\3\2"+
+    "\2\2\u00f7\u00f1\3\2\2\2\u00f8\u00fb\3\2\2\2\u00f9\u00f7\3\2\2\2\u00f9"+
+    "\u00fa\3\2\2\2\u00fa\25\3\2\2\2\u00fb\u00f9\3\2\2\2\u00fc\u0103\5\30\r"+
+    "\2\u00fd\u0103\5\32\16\2\u00fe\u0103\5 \21\2\u00ff\u0103\5$\23\2\u0100"+
+    "\u0103\5(\25\2\u0101\u0103\5*\26\2\u0102\u00fc\3\2\2\2\u0102\u00fd\3\2"+
+    "\2\2\u0102\u00fe\3\2\2\2\u0102\u00ff\3\2\2\2\u0102\u0100\3\2\2\2\u0102"+
+    "\u0101\3\2\2\2\u0103\27\3\2\2\2\u0104\u010b\7\t\2\2\u0105\u010c\5\30\r"+
+    "\2\u0106\u010c\5\32\16\2\u0107\u010c\5 \21\2\u0108\u010c\5$\23\2\u0109"+
+    "\u010c\5(\25\2\u010a\u010c\5*\26\2\u010b\u0105\3\2\2\2\u010b\u0106\3\2"+
+    "\2\2\u010b\u0107\3\2\2\2\u010b\u0108\3\2\2\2\u010b\u0109\3\2\2\2\u010b"+
+    "\u010a\3\2\2\2\u010c\u010d\3\2\2\2\u010d\u0110\7\n\2\2\u010e\u0111\5\36"+
+    "\20\2\u010f\u0111\5\34\17\2\u0110\u010e\3\2\2\2\u0110\u010f\3\2\2\2\u0110"+
+    "\u0111\3\2\2\2\u0111\31\3\2\2\2\u0112\u0113\7\t\2\2\u0113\u0114\5\20\t"+
+    "\2\u0114\u011b\7\n\2\2\u0115\u011c\5\30\r\2\u0116\u011c\5\32\16\2\u0117"+
+    "\u011c\5 \21\2\u0118\u011c\5$\23\2\u0119\u011c\5(\25\2\u011a\u011c\5*"+
+    "\26\2\u011b\u0115\3\2\2\2\u011b\u0116\3\2\2\2\u011b\u0117\3\2\2\2\u011b"+
+    "\u0118\3\2\2\2\u011b\u0119\3\2\2\2\u011b\u011a\3\2\2\2\u011c\33\3\2\2"+
+    "\2\u011d\u011e\7\7\2\2\u011e\u011f\5\24\13\2\u011f\u0122\7\b\2\2\u0120"+
+    "\u0123\5\36\20\2\u0121\u0123\5\34\17\2\u0122\u0120\3\2\2\2\u0122\u0121"+
+    "\3\2\2\2\u0122\u0123\3\2\2\2\u0123\35\3\2\2\2\u0124\u0127\7\13\2\2\u0125"+
+    "\u0128\5\"\22\2\u0126\u0128\5&\24\2\u0127\u0125\3\2\2\2\u0127\u0126\3"+
+    "\2\2\2\u0128\37\3\2\2\2\u0129\u012a\7K\2\2\u012a\u012b\5\36\20\2\u012b"+
+    "!\3\2\2\2\u012c\u012d\7N\2\2\u012d\u0130\5,\27\2\u012e\u0131\5\36\20\2"+
+    "\u012f\u0131\5\34\17\2\u0130\u012e\3\2\2\2\u0130\u012f\3\2\2\2\u0130\u0131"+
+    "\3\2\2\2\u0131#\3\2\2\2\u0132\u0135\7L\2\2\u0133\u0136\5\36\20\2\u0134"+
+    "\u0136\5\34\17\2\u0135\u0133\3\2\2\2\u0135\u0134\3\2\2\2\u0135\u0136\3"+
+    "\2\2\2\u0136%\3\2\2\2\u0137\u013a\t\n\2\2\u0138\u013b\5\36\20\2\u0139"+
+    "\u013b\5\34\17\2\u013a\u0138\3\2\2\2\u013a\u0139\3\2\2\2\u013a\u013b\3"+
+    "\2\2\2\u013b\'\3\2\2\2\u013c\u013d\7\26\2\2\u013d\u014e\7K\2\2\u013e\u0141"+
+    "\5,\27\2\u013f\u0142\5\36\20\2\u0140\u0142\5\34\17\2\u0141\u013f\3\2\2"+
+    "\2\u0141\u0140\3\2\2\2\u0141\u0142\3\2\2\2\u0142\u014f\3\2\2\2\u0143\u0144"+
+    "\7\7\2\2\u0144\u0145\5\24\13\2\u0145\u0146\7\b\2\2\u0146\u0148\3\2\2\2"+
+    "\u0147\u0143\3\2\2\2\u0148\u0149\3\2\2\2\u0149\u0147\3\2\2\2\u0149\u014a"+
+    "\3\2\2\2\u014a\u014c\3\2\2\2\u014b\u014d\5\36\20\2\u014c\u014b\3\2\2\2"+
+    "\u014c\u014d\3\2\2\2\u014d\u014f\3\2\2\2\u014e\u013e\3\2\2\2\u014e\u0147"+
+    "\3\2\2\2\u014f)\3\2\2\2\u0150\u0153\7F\2\2\u0151\u0154\5\36\20\2\u0152"+
+    "\u0154\5\34\17\2\u0153\u0151\3\2\2\2\u0153\u0152\3\2\2\2\u0153\u0154\3"+
+    "\2\2\2\u0154+\3\2\2\2\u0155\u015e\7\t\2\2\u0156\u015b\5\24\13\2\u0157"+
+    "\u0158\7\f\2\2\u0158\u015a\5\24\13\2\u0159\u0157\3\2\2\2\u015a\u015d\3"+
+    "\2\2\2\u015b\u0159\3\2\2\2\u015b\u015c\3\2\2\2\u015c\u015f\3\2\2\2\u015d"+
+    "\u015b\3\2\2\2\u015e\u0156\3\2\2\2\u015e\u015f\3\2\2\2\u015f\u0160\3\2"+
+    "\2\2\u0160\u0161\7\n\2\2\u0161-\3\2\2\2\u0162\u0163\t\13\2\2\u0163/\3"+
+    "\2\2\2+\63>FOTX\\aeimr\177\u0084\u0088\u008a\u0090\u0095\u009b\u00a5\u00ad"+
+    "\u00b3\u00d1\u00f7\u00f9\u0102\u010b\u0110\u011b\u0122\u0127\u0130\u0135"+
+    "\u013a\u0141\u0149\u014c\u014e\u0153\u015b\u015e";
+  public static final ATN _ATN =
+    new ATNDeserializer().deserialize(_serializedATN.toCharArray());
+  static {
+    _decisionToDFA = new DFA[_ATN.getNumberOfDecisions()];
+    for (int i = 0; i < _ATN.getNumberOfDecisions(); i++) {
+      _decisionToDFA[i] = new DFA(_ATN.getDecisionState(i), i);
+    }
+  }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java
new file mode 100644
index 0000000..d731b57
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java
@@ -0,0 +1,357 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.tree.AbstractParseTreeVisitor;
+
+/**
+ * This class provides an empty implementation of {@link PlanAParserVisitor},
+ * which can be extended to create a visitor which only needs to handle a subset
+ * of the available methods.
+ *
+ * @param <T> The return type of the visit operation. Use {@link Void} for
+ * operations with no return type.
+ */
+class PlanAParserBaseVisitor<T> extends AbstractParseTreeVisitor<T> implements PlanAParserVisitor<T> {
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitSource(PlanAParser.SourceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitIf(PlanAParser.IfContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitWhile(PlanAParser.WhileContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDo(PlanAParser.DoContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitFor(PlanAParser.ForContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDecl(PlanAParser.DeclContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitContinue(PlanAParser.ContinueContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBreak(PlanAParser.BreakContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitReturn(PlanAParser.ReturnContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitTry(PlanAParser.TryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitThrow(PlanAParser.ThrowContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExpr(PlanAParser.ExprContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitMultiple(PlanAParser.MultipleContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitSingle(PlanAParser.SingleContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitEmpty(PlanAParser.EmptyContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitInitializer(PlanAParser.InitializerContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitAfterthought(PlanAParser.AfterthoughtContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDeclaration(PlanAParser.DeclarationContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDecltype(PlanAParser.DecltypeContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDeclvar(PlanAParser.DeclvarContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitComp(PlanAParser.CompContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBool(PlanAParser.BoolContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitConditional(PlanAParser.ConditionalContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitAssignment(PlanAParser.AssignmentContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitFalse(PlanAParser.FalseContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitNumeric(PlanAParser.NumericContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitUnary(PlanAParser.UnaryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPrecedence(PlanAParser.PrecedenceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPreinc(PlanAParser.PreincContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPostinc(PlanAParser.PostincContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitCast(PlanAParser.CastContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExternal(PlanAParser.ExternalContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitNull(PlanAParser.NullContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBinary(PlanAParser.BinaryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitChar(PlanAParser.CharContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitTrue(PlanAParser.TrueContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtstart(PlanAParser.ExtstartContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtprec(PlanAParser.ExtprecContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtcast(PlanAParser.ExtcastContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtbrace(PlanAParser.ExtbraceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtdot(PlanAParser.ExtdotContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExttype(PlanAParser.ExttypeContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtcall(PlanAParser.ExtcallContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtvar(PlanAParser.ExtvarContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtfield(PlanAParser.ExtfieldContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtnew(PlanAParser.ExtnewContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtstring(PlanAParser.ExtstringContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitArguments(PlanAParser.ArgumentsContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitIncrement(PlanAParser.IncrementContext ctx) { return visitChildren(ctx); }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java
new file mode 100644
index 0000000..7470f3b
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java
@@ -0,0 +1,336 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.tree.ParseTreeVisitor;
+
+/**
+ * This interface defines a complete generic visitor for a parse tree produced
+ * by {@link PlanAParser}.
+ *
+ * @param <T> The return type of the visit operation. Use {@link Void} for
+ * operations with no return type.
+ */
+interface PlanAParserVisitor<T> extends ParseTreeVisitor<T> {
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#source}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitSource(PlanAParser.SourceContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code if}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitIf(PlanAParser.IfContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code while}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitWhile(PlanAParser.WhileContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code do}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDo(PlanAParser.DoContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code for}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitFor(PlanAParser.ForContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code decl}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDecl(PlanAParser.DeclContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code continue}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitContinue(PlanAParser.ContinueContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code break}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBreak(PlanAParser.BreakContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code return}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitReturn(PlanAParser.ReturnContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code try}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitTry(PlanAParser.TryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code throw}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitThrow(PlanAParser.ThrowContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code expr}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExpr(PlanAParser.ExprContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code multiple}
+   * labeled alternative in {@link PlanAParser#block}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitMultiple(PlanAParser.MultipleContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code single}
+   * labeled alternative in {@link PlanAParser#block}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitSingle(PlanAParser.SingleContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#empty}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitEmpty(PlanAParser.EmptyContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#initializer}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitInitializer(PlanAParser.InitializerContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#afterthought}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitAfterthought(PlanAParser.AfterthoughtContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#declaration}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDeclaration(PlanAParser.DeclarationContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#decltype}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDecltype(PlanAParser.DecltypeContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#declvar}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDeclvar(PlanAParser.DeclvarContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code comp}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitComp(PlanAParser.CompContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code bool}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBool(PlanAParser.BoolContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code conditional}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitConditional(PlanAParser.ConditionalContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code assignment}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitAssignment(PlanAParser.AssignmentContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code false}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitFalse(PlanAParser.FalseContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code numeric}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitNumeric(PlanAParser.NumericContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code unary}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitUnary(PlanAParser.UnaryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code precedence}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPrecedence(PlanAParser.PrecedenceContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code preinc}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPreinc(PlanAParser.PreincContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code postinc}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPostinc(PlanAParser.PostincContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code cast}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitCast(PlanAParser.CastContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code external}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExternal(PlanAParser.ExternalContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code null}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitNull(PlanAParser.NullContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code binary}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBinary(PlanAParser.BinaryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code char}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitChar(PlanAParser.CharContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code true}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitTrue(PlanAParser.TrueContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extstart}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtstart(PlanAParser.ExtstartContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extprec}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtprec(PlanAParser.ExtprecContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extcast}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtcast(PlanAParser.ExtcastContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extbrace}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtbrace(PlanAParser.ExtbraceContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extdot}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtdot(PlanAParser.ExtdotContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#exttype}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExttype(PlanAParser.ExttypeContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extcall}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtcall(PlanAParser.ExtcallContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extvar}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtvar(PlanAParser.ExtvarContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extfield}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtfield(PlanAParser.ExtfieldContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extnew}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtnew(PlanAParser.ExtnewContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extstring}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtstring(PlanAParser.ExtstringContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#arguments}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitArguments(PlanAParser.ArgumentsContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#increment}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitIncrement(PlanAParser.IncrementContext ctx);
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java
new file mode 100644
index 0000000..c893cd3
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptModule;
+
+public final class PlanAPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "lang-plan-a";
+    }
+
+    @Override
+    public String description() {
+        return "Plan A scripting language for Elasticsearch";
+    }
+
+    public void onModule(ScriptModule module) {
+        module.addScriptEngine(PlanAScriptEngineService.class);
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java
new file mode 100644
index 0000000..6b3cd83
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java
@@ -0,0 +1,140 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.LeafSearchScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.io.IOException;
+import java.security.AccessControlContext;
+import java.security.AccessController;
+import java.security.Permissions;
+import java.security.PrivilegedAction;
+import java.security.ProtectionDomain;
+import java.util.Map;
+
+public class PlanAScriptEngineService extends AbstractComponent implements ScriptEngineService {
+
+    public static final String NAME = "plan-a";
+    // TODO: this should really be per-script since scripts do so many different things?
+    private static final CompilerSettings compilerSettings = new CompilerSettings();
+    
+    public static final String NUMERIC_OVERFLOW = "plan-a.numeric_overflow";
+
+    // TODO: how should custom definitions be specified?
+    private Definition definition = null;
+
+    @Inject
+    public PlanAScriptEngineService(Settings settings) {
+        super(settings);
+        compilerSettings.setNumericOverflow(settings.getAsBoolean(NUMERIC_OVERFLOW, compilerSettings.getNumericOverflow()));
+    }
+
+    public void setDefinition(final Definition definition) {
+        this.definition = new Definition(definition);
+    }
+
+    @Override
+    public String[] types() {
+        return new String[] { NAME };
+    }
+
+    @Override
+    public String[] extensions() {
+        return new String[] { NAME };
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    // context used during compilation
+    private static final AccessControlContext COMPILATION_CONTEXT;
+    static {
+        Permissions none = new Permissions();
+        none.setReadOnly();
+        COMPILATION_CONTEXT = new AccessControlContext(new ProtectionDomain[] {
+                new ProtectionDomain(null, none)
+        });
+    }
+
+    @Override
+    public Object compile(String script) {
+        // check we ourselves are not being called by unprivileged code
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        // create our loader (which loads compiled code with no permissions)
+        Compiler.Loader loader = AccessController.doPrivileged(new PrivilegedAction<Compiler.Loader>() {
+            @Override
+            public Compiler.Loader run() {
+                return new Compiler.Loader(getClass().getClassLoader());
+            }
+        });
+        // drop all permissions to actually compile the code itself
+        return AccessController.doPrivileged(new PrivilegedAction<Executable>() {
+            @Override
+            public Executable run() {
+                return Compiler.compile(loader, "something", script, definition, compilerSettings);
+            }
+        }, COMPILATION_CONTEXT);
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, Map<String,Object> vars) {
+        return new ScriptImpl((Executable) compiledScript.compiled(), vars, null);
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String,Object> vars) {
+        return new SearchScript() {
+            @Override
+            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
+                return new ScriptImpl((Executable) compiledScript.compiled(), vars, lookup.getLeafSearchLookup(context));
+            }
+
+            @Override
+            public boolean needsScores() {
+                return true; // TODO: maybe even do these different and more like expressions.
+            }
+        };
+    }
+
+    @Override
+    public void scriptRemoved(CompiledScript script) {
+        // nothing to do
+    }
+
+    @Override
+    public void close() throws IOException {
+        // nothing to do
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java
new file mode 100644
index 0000000..3910cdc
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.apache.lucene.search.Scorer;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.LeafSearchScript;
+import org.elasticsearch.script.ScoreAccessor;
+import org.elasticsearch.search.lookup.LeafSearchLookup;
+
+import java.util.HashMap;
+import java.util.Map;
+
+final class ScriptImpl implements ExecutableScript, LeafSearchScript {
+    final Executable executable;
+    final Map<String,Object> variables;
+    final LeafSearchLookup lookup;
+    
+    ScriptImpl(Executable executable, Map<String,Object> vars, LeafSearchLookup lookup) {
+        this.executable = executable;
+        this.lookup = lookup;
+        this.variables = new HashMap<>();
+        if (vars != null) {
+            variables.putAll(vars);
+        }
+        if (lookup != null) {
+            variables.putAll(lookup.asMap());
+        }
+    }
+    
+    @Override
+    public void setNextVar(String name, Object value) {
+        variables.put(name, value);
+    }
+    
+    @Override
+    public Object run() {
+        return executable.execute(variables);
+    }
+    
+    @Override
+    public float runAsFloat() {
+        return ((Number) run()).floatValue();
+    }
+
+    @Override
+    public long runAsLong() {
+        return ((Number) run()).longValue();
+    }
+
+    @Override
+    public double runAsDouble() {
+        return ((Number) run()).doubleValue();
+    }
+    
+    @Override
+    public Object unwrap(Object value) {
+        return value;
+    }
+
+    @Override
+    public void setScorer(Scorer scorer) {
+        variables.put("_score", new ScoreAccessor(scorer));
+    }
+
+    @Override
+    public void setDocument(int doc) {
+        if (lookup != null) {
+            lookup.setDocument(doc);
+        }
+    }
+
+    @Override
+    public void setSource(Map<String,Object> source) {
+        if (lookup != null) {
+            lookup.source().setSource(source);
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java
new file mode 100644
index 0000000..3bb5ae4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java
@@ -0,0 +1,801 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+public class Utility {
+    public static boolean NumberToboolean(final Number value) {
+        return value.longValue() != 0;
+    }
+
+    public static char NumberTochar(final Number value) {
+        return (char)value.intValue();
+    }
+
+    public static Boolean NumberToBoolean(final Number value) {
+        return value.longValue() != 0;
+    }
+
+    public static Byte NumberToByte(final Number value) {
+        return value == null ? null : value.byteValue();
+    }
+
+    public static Short NumberToShort(final Number value) {
+        return value == null ? null : value.shortValue();
+    }
+
+    public static Character NumberToCharacter(final Number value) {
+        return value == null ? null : (char)value.intValue();
+    }
+
+    public static Integer NumberToInteger(final Number value) {
+        return value == null ? null : value.intValue();
+    }
+
+    public static Long NumberToLong(final Number value) {
+        return value == null ? null : value.longValue();
+    }
+
+    public static Float NumberToFloat(final Number value) {
+        return value == null ? null : value.floatValue();
+    }
+
+    public static Double NumberToDouble(final Number value) {
+        return value == null ? null : value.doubleValue();
+    }
+
+    public static byte booleanTobyte(final boolean value) {
+        return (byte)(value ? 1 : 0);
+    }
+
+    public static short booleanToshort(final boolean value) {
+        return (short)(value ? 1 : 0);
+    }
+
+    public static char booleanTochar(final boolean value) {
+        return (char)(value ? 1 : 0);
+    }
+
+    public static int booleanToint(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static long booleanTolong(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static float booleanTofloat(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static double booleanTodouble(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static Integer booleanToInteger(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static byte BooleanTobyte(final Boolean value) {
+        return (byte)(value ? 1 : 0);
+    }
+
+    public static short BooleanToshort(final Boolean value) {
+        return (short)(value ? 1 : 0);
+    }
+
+    public static char BooleanTochar(final Boolean value) {
+        return (char)(value ? 1 : 0);
+    }
+
+    public static int BooleanToint(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static long BooleanTolong(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static float BooleanTofloat(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static double BooleanTodouble(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static Byte BooleanToByte(final Boolean value) {
+        return value == null ? null : (byte)(value ? 1 : 0);
+    }
+
+    public static Short BooleanToShort(final Boolean value) {
+        return value == null ? null : (short)(value ? 1 : 0);
+    }
+
+    public static Character BooleanToCharacter(final Boolean value) {
+        return value == null ? null : (char)(value ? 1 : 0);
+    }
+
+    public static Integer BooleanToInteger(final Boolean value) {
+        return value == null ? null : value ? 1 : 0;
+    }
+
+    public static Long BooleanToLong(final Boolean value) {
+        return value == null ? null : value ? 1L : 0L;
+    }
+
+    public static Float BooleanToFloat(final Boolean value) {
+        return value == null ? null : value ? 1F : 0F;
+    }
+
+    public static Double BooleanToDouble(final Boolean value) {
+        return value == null ? null : value ? 1D : 0D;
+    }
+
+    public static boolean byteToboolean(final byte value) {
+        return value != 0;
+    }
+
+    public static Short byteToShort(final byte value) {
+        return (short)value;
+    }
+
+    public static Character byteToCharacter(final byte value) {
+        return (char)(byte)value;
+    }
+
+    public static Integer byteToInteger(final byte value) {
+        return (int)value;
+    }
+
+    public static Long byteToLong(final byte value) {
+        return (long)value;
+    }
+
+    public static Float byteToFloat(final byte value) {
+        return (float)value;
+    }
+
+    public static Double byteToDouble(final byte value) {
+        return (double)value;
+    }
+
+    public static boolean ByteToboolean(final Byte value) {
+        return value != 0;
+    }
+
+    public static char ByteTochar(final Byte value) {
+        return (char)value.byteValue();
+    }
+
+    public static boolean shortToboolean(final short value) {
+        return value != 0;
+    }
+
+    public static Byte shortToByte(final short value) {
+        return (byte)value;
+    }
+
+    public static Character shortToCharacter(final short value) {
+        return (char)(short)value;
+    }
+
+    public static Integer shortToInteger(final short value) {
+        return (int)value;
+    }
+
+    public static Long shortToLong(final short value) {
+        return (long)value;
+    }
+
+    public static Float shortToFloat(final short value) {
+        return (float)value;
+    }
+
+    public static Double shortToDouble(final short value) {
+        return (double)value;
+    }
+    
+    public static boolean ShortToboolean(final Short value) {
+        return value != 0;
+    }
+    
+    public static char ShortTochar(final Short value) {
+        return (char)value.shortValue();
+    }
+
+    public static boolean charToboolean(final char value) {
+        return value != 0;
+    }
+
+    public static Byte charToByte(final char value) {
+        return (byte)value;
+    }
+
+    public static Short charToShort(final char value) {
+        return (short)value;
+    }
+
+    public static Integer charToInteger(final char value) {
+        return (int)value;
+    }
+
+    public static Long charToLong(final char value) {
+        return (long)value;
+    }
+
+    public static Float charToFloat(final char value) {
+        return (float)value;
+    }
+
+    public static Double charToDouble(final char value) {
+        return (double)value;
+    }
+
+    public static boolean CharacterToboolean(final Character value) {
+        return value != 0;
+    }
+
+    public static byte CharacterTobyte(final Character value) {
+        return (byte)value.charValue();
+    }
+
+    public static short CharacterToshort(final Character value) {
+        return (short)value.charValue();
+    }
+
+    public static int CharacterToint(final Character value) {
+        return (int)value;
+    }
+
+    public static long CharacterTolong(final Character value) {
+        return (long)value;
+    }
+
+    public static float CharacterTofloat(final Character value) {
+        return (float)value;
+    }
+
+    public static double CharacterTodouble(final Character value) {
+        return (double)value;
+    }
+
+    public static Boolean CharacterToBoolean(final Character value) {
+        return value == null ? null : value != 0;
+    }
+
+    public static Byte CharacterToByte(final Character value) {
+        return value == null ? null : (byte)value.charValue();
+    }
+
+    public static Short CharacterToShort(final Character value) {
+        return value == null ? null : (short)value.charValue();
+    }
+
+    public static Integer CharacterToInteger(final Character value) {
+        return value == null ? null : (int)value;
+    }
+
+    public static Long CharacterToLong(final Character value) {
+        return value == null ? null : (long)value;
+    }
+
+    public static Float CharacterToFloat(final Character value) {
+        return value == null ? null : (float)value;
+    }
+
+    public static Double CharacterToDouble(final Character value) {
+        return value == null ? null : (double)value;
+    }
+
+    public static boolean intToboolean(final int value) {
+        return value != 0;
+    }
+
+    public static Byte intToByte(final int value) {
+        return (byte)value;
+    }
+
+    public static Short intToShort(final int value) {
+        return (short)value;
+    }
+
+    public static Character intToCharacter(final int value) {
+        return (char)(int)value;
+    }
+
+    public static Long intToLong(final int value) {
+        return (long)value;
+    }
+
+    public static Float intToFloat(final int value) {
+        return (float)value;
+    }
+
+    public static Double intToDouble(final int value) {
+        return (double)value;
+    }
+    
+    public static boolean IntegerToboolean(final Integer value) {
+        return value != 0;
+    }
+
+    public static char IntegerTochar(final Integer value) {
+        return (char)value.intValue();
+    }
+
+    public static boolean longToboolean(final long value) {
+        return value != 0;
+    }
+
+    public static Byte longToByte(final long value) {
+        return (byte)value;
+    }
+
+    public static Short longToShort(final long value) {
+        return (short)value;
+    }
+
+    public static Character longToCharacter(final long value) {
+        return (char)(long)value;
+    }
+
+    public static Integer longToInteger(final long value) {
+        return (int)value;
+    }
+
+    public static Float longToFloat(final long value) {
+        return (float)value;
+    }
+
+    public static Double longToDouble(final long value) {
+        return (double)value;
+    }
+    
+    public static boolean LongToboolean(final Long value) {
+        return value != 0;
+    }
+
+    public static char LongTochar(final Long value) {
+        return (char)value.longValue();
+    }
+
+    public static boolean floatToboolean(final float value) {
+        return value != 0;
+    }
+
+    public static Byte floatToByte(final float value) {
+        return (byte)value;
+    }
+
+    public static Short floatToShort(final float value) {
+        return (short)value;
+    }
+
+    public static Character floatToCharacter(final float value) {
+        return (char)(float)value;
+    }
+
+    public static Integer floatToInteger(final float value) {
+        return (int)value;
+    }
+
+    public static Long floatToLong(final float value) {
+        return (long)value;
+    }
+
+    public static Double floatToDouble(final float value) {
+        return (double)value;
+    }
+    
+    public static boolean FloatToboolean(final Float value) {
+        return value != 0;
+    }
+    
+    public static char FloatTochar(final Float value) {
+        return (char)value.floatValue();
+    }
+
+    public static boolean doubleToboolean(final double value) {
+        return value != 0;
+    }
+
+    public static Byte doubleToByte(final double value) {
+        return (byte)value;
+    }
+
+    public static Short doubleToShort(final double value) {
+        return (short)value;
+    }
+
+    public static Character doubleToCharacter(final double value) {
+        return (char)(double)value;
+    }
+
+    public static Integer doubleToInteger(final double value) {
+        return (int)value;
+    }
+
+    public static Long doubleToLong(final double value) {
+        return (long)value;
+    }
+    
+    public static Float doubleToFloat(final double value) {
+        return (float)value;
+    }
+    
+    public static boolean DoubleToboolean(final Double value) {
+        return value != 0;
+    }
+    
+    public static char DoubleTochar(final Double value) {
+        return (char)value.doubleValue();
+    }
+    
+    // although divide by zero is guaranteed, the special overflow case is not caught.
+    // its not needed for remainder because it is not possible there.
+    // see https://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html#jls-15.17.2
+    
+    /**
+     * Integer divide without overflow
+     * @throws ArithmeticException on overflow or divide-by-zero
+     */
+    public static int divideWithoutOverflow(int x, int y) {
+       if (x == Integer.MIN_VALUE && y == -1) {
+           throw new ArithmeticException("integer overflow");
+       }
+       return x / y;
+    }
+    
+    /**
+     * Long divide without overflow
+     * @throws ArithmeticException on overflow or divide-by-zero
+     */
+    public static long divideWithoutOverflow(long x, long y) {
+        if (x == Long.MIN_VALUE && y == -1L) {
+            throw new ArithmeticException("long overflow");
+        }
+        return x / y;
+    }
+
+    // byte, short, and char are promoted to int for normal operations,
+    // so the JDK exact methods are typically used, and the result has a wider range.
+    // but compound assignments and increment/decrement operators (e.g. byte b = Byte.MAX_VALUE; b++;)
+    // implicitly cast back to the original type: so these need to be checked against the original range.
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteExact(int value) {
+        byte s = (byte) value;
+        if (s != value) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteExact(long value) {
+        byte s = (byte) value;
+        if (s != value) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteWithoutOverflow(float value) {
+        if (value < Byte.MIN_VALUE || value > Byte.MAX_VALUE) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return (byte)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteWithoutOverflow(double value) {
+        if (value < Byte.MIN_VALUE || value > Byte.MAX_VALUE) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return (byte)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(int value) {
+        short s = (short) value;
+        if (s != value) {
+            throw new ArithmeticException("short overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(long value) {
+        short s = (short) value;
+        if (s != value) {
+            throw new ArithmeticException("short overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortWithoutOverflow(float value) {
+        if (value < Short.MIN_VALUE || value > Short.MAX_VALUE) {
+            throw new ArithmeticException("short overflow");
+        }
+        return (short)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(double value) {
+        if (value < Short.MIN_VALUE || value > Short.MAX_VALUE) {
+            throw new ArithmeticException("short overflow");
+        }
+        return (short)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharExact(int value) {
+        char s = (char) value;
+        if (s != value) {
+            throw new ArithmeticException("char overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharExact(long value) {
+        char s = (char) value;
+        if (s != value) {
+            throw new ArithmeticException("char overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharWithoutOverflow(float value) {
+        if (value < Character.MIN_VALUE || value > Character.MAX_VALUE) {
+            throw new ArithmeticException("char overflow");
+        }
+        return (char)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharWithoutOverflow(double value) {
+        if (value < Character.MIN_VALUE || value > Character.MAX_VALUE) {
+            throw new ArithmeticException("char overflow");
+        }
+        return (char)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for int range.
+     */
+    public static int toIntWithoutOverflow(float value) {
+        if (value < Integer.MIN_VALUE || value > Integer.MAX_VALUE) {
+            throw new ArithmeticException("int overflow");
+        }
+        return (int)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for int range.
+     */
+    public static int toIntWithoutOverflow(double value) {
+        if (value < Integer.MIN_VALUE || value > Integer.MAX_VALUE) {
+            throw new ArithmeticException("int overflow");
+        }
+        return (int)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for long range.
+     */
+    public static long toLongExactWithoutOverflow(float value) {
+        if (value < Long.MIN_VALUE || value > Long.MAX_VALUE) {
+            throw new ArithmeticException("long overflow");
+        }
+        return (long)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for long range.
+     */
+    public static float toLongExactWithoutOverflow(double value) {
+        if (value < Long.MIN_VALUE || value > Long.MAX_VALUE) {
+            throw new ArithmeticException("long overflow");
+        }
+        return (long)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for float range.
+     */
+    public static float toFloatWithoutOverflow(double value) {
+        if (value < Float.MIN_VALUE || value > Float.MAX_VALUE) {
+            throw new ArithmeticException("float overflow");
+        }
+        return (float)value;
+    }
+
+    /**
+     * Checks for overflow, result is infinite but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static float checkInfFloat(float x, float y, float z) {
+        if (Float.isInfinite(z)) {
+            if (Float.isFinite(x) && Float.isFinite(y)) {
+                throw new ArithmeticException("float overflow");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is NaN but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static float checkNaNFloat(float x, float y, float z) {
+        if (Float.isNaN(z)) {
+            if (Float.isFinite(x) && Float.isFinite(y)) {
+                throw new ArithmeticException("NaN");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is infinite but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static double checkInfDouble(double x, double y, double z) {
+        if (Double.isInfinite(z)) {
+            if (Double.isFinite(x) && Double.isFinite(y)) {
+                throw new ArithmeticException("double overflow");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is NaN but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static double checkNaNDouble(double x, double y, double z) {
+        if (Double.isNaN(z)) {
+            if (Double.isFinite(x) && Double.isFinite(y)) {
+                throw new ArithmeticException("NaN");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Adds two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float addWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x + y);
+    }
+    
+    /**
+     * Adds two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double addWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y, x + y);
+    }
+    
+    /**
+     * Subtracts two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float subtractWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x - y);
+    }
+    
+    /**
+     * Subtracts two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double subtractWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y , x - y);
+    }
+    
+    /**
+     * Multiplies two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float multiplyWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x * y);
+    }
+    
+    /**
+     * Multiplies two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double multiplyWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y, x * y);
+    }
+    
+    /**
+     * Divides two floats but throws {@code ArithmeticException}
+     * if the result overflows, or would create NaN from finite
+     * inputs ({@code x == 0, y == 0})
+     */
+    public static float divideWithoutOverflow(float x, float y) {
+        return checkNaNFloat(x, y, checkInfFloat(x, y, x / y));
+    }
+    
+    /**
+     * Divides two doubles but throws {@code ArithmeticException}
+     * if the result overflows, or would create NaN from finite
+     * inputs ({@code x == 0, y == 0})
+     */
+    public static double divideWithoutOverflow(double x, double y) {
+        return checkNaNDouble(x, y, checkInfDouble(x, y, x / y));
+    }
+    
+    /**
+     * Takes remainder two floats but throws {@code ArithmeticException}
+     * if the result would create NaN from finite inputs ({@code y == 0})
+     */
+    public static float remainderWithoutOverflow(float x, float y) {
+        return checkNaNFloat(x, y, x % y);
+    }
+    
+    /**
+     * Divides two doubles but throws {@code ArithmeticException}
+     * if the result would create NaN from finite inputs ({@code y == 0})
+     */
+    public static double remainderWithoutOverflow(double x, double y) {
+        return checkNaNDouble(x, y, x % y);
+    }
+
+    public static boolean checkEquals(final Object left, final Object right) {
+        if (left != null && right != null) {
+            return left.equals(right);
+        }
+
+        return left == null && right == null;
+    }
+
+    private Utility() {}
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java
new file mode 100644
index 0000000..3756e02
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java
@@ -0,0 +1,2224 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.antlr.v4.runtime.tree.ParseTree;
+import org.objectweb.asm.ClassWriter;
+import org.objectweb.asm.Label;
+import org.objectweb.asm.Opcodes;
+import org.objectweb.asm.commons.GeneratorAdapter;
+
+import java.util.ArrayDeque;
+import java.util.Deque;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import static org.elasticsearch.plan.a.Adapter.*;
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Writer extends PlanAParserBaseVisitor<Void> {
+    private static class Branch {
+        final ParserRuleContext source;
+
+        Label begin;
+        Label end;
+        Label tru;
+        Label fals;
+
+        private Branch(final ParserRuleContext source) {
+            this.source = source;
+
+            begin = null;
+            end = null;
+            tru = null;
+            fals = null;
+        }
+    }
+
+    final static String BASE_CLASS_NAME = Executable.class.getName();
+    final static String CLASS_NAME = BASE_CLASS_NAME + "$CompiledPlanAExecutable";
+    private final static org.objectweb.asm.Type BASE_CLASS_TYPE = org.objectweb.asm.Type.getType(Executable.class);
+    private final static org.objectweb.asm.Type CLASS_TYPE =
+            org.objectweb.asm.Type.getType("L" + CLASS_NAME.replace(".", "/") + ";");
+
+    private final static org.objectweb.asm.commons.Method CONSTRUCTOR = org.objectweb.asm.commons.Method.getMethod(
+            "void <init>(org.elasticsearch.plan.a.Definition, java.lang.String, java.lang.String)");
+    private final static org.objectweb.asm.commons.Method EXECUTE = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object execute(java.util.Map)");
+    private final static String SIGNATURE = "(Ljava/util/Map<Ljava/lang/String;Ljava/lang/Object;>;)Ljava/lang/Object;";
+
+    private final static org.objectweb.asm.Type DEFINITION_TYPE = org.objectweb.asm.Type.getType(Definition.class);
+
+    private final static org.objectweb.asm.commons.Method DEF_METHOD_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object methodCall(java.lang.Object, java.lang.String, " +
+            "org.elasticsearch.plan.a.Definition, java.lang.Object[], boolean[])");
+    private final static org.objectweb.asm.commons.Method DEF_ARRAY_STORE = org.objectweb.asm.commons.Method.getMethod(
+            "void arrayStore(java.lang.Object, java.lang.Object, java.lang.Object, " +
+            "org.elasticsearch.plan.a.Definition, boolean, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_ARRAY_LOAD = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object arrayLoad(java.lang.Object, java.lang.Object, " +
+            "org.elasticsearch.plan.a.Definition, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_FIELD_STORE = org.objectweb.asm.commons.Method.getMethod(
+            "void fieldStore(java.lang.Object, java.lang.Object, java.lang.String, " +
+            "org.elasticsearch.plan.a.Definition, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_FIELD_LOAD = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object fieldLoad(java.lang.Object, java.lang.String, org.elasticsearch.plan.a.Definition)");
+
+    private final static org.objectweb.asm.commons.Method DEF_NOT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object not(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_NEG_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object neg(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_MUL_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object mul(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_DIV_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object div(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_REM_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object rem(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_ADD_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object add(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_SUB_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object sub(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LSH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object lsh(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_RSH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object rsh(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_USH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object ush(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_AND_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object and(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_XOR_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object xor(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_OR_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object or(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_EQ_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean eq(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean lt(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LTE_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean lte(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_GT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean gt(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_GTE_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean gte(java.lang.Object, java.lang.Object)");
+
+    private final static org.objectweb.asm.Type STRINGBUILDER_TYPE = org.objectweb.asm.Type.getType(StringBuilder.class);
+
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_CONSTRUCTOR =
+            org.objectweb.asm.commons.Method.getMethod("void <init>()");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_BOOLEAN =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(boolean)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_CHAR =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(char)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_INT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(int)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_LONG =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(long)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(float)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(double)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_STRING =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(java.lang.String)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_OBJECT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_TOSTRING =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.String toString()");
+
+    private final static org.objectweb.asm.commons.Method TOINTEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("int toIntExact(long)");
+    private final static org.objectweb.asm.commons.Method NEGATEEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int negateExact(int)");
+    private final static org.objectweb.asm.commons.Method NEGATEEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long negateExact(long)");
+    private final static org.objectweb.asm.commons.Method MULEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int multiplyExact(int, int)");
+    private final static org.objectweb.asm.commons.Method MULEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long multiplyExact(long, long)");
+    private final static org.objectweb.asm.commons.Method ADDEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int addExact(int, int)");
+    private final static org.objectweb.asm.commons.Method ADDEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long addExact(long, long)");
+    private final static org.objectweb.asm.commons.Method SUBEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int subtractExact(int, int)");
+    private final static org.objectweb.asm.commons.Method SUBEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long subtractExact(long, long)");
+
+    private final static org.objectweb.asm.commons.Method CHECKEQUALS =
+            org.objectweb.asm.commons.Method.getMethod("boolean checkEquals(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method TOBYTEEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteExact(int)");
+    private final static org.objectweb.asm.commons.Method TOBYTEEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteExact(long)");
+    private final static org.objectweb.asm.commons.Method TOBYTEWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOBYTEWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOSHORTEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("short toShortExact(int)");
+    private final static org.objectweb.asm.commons.Method TOSHORTEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("short toShortExact(long)");
+    private final static org.objectweb.asm.commons.Method TOSHORTWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("short toShortWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOSHORTWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("short toShortWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOCHAREXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("char toCharExact(int)");
+    private final static org.objectweb.asm.commons.Method TOCHAREXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("char toCharExact(long)");
+    private final static org.objectweb.asm.commons.Method TOCHARWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("char toCharWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOCHARWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("char toCharWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOINTWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("int toIntWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOINTWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("int toIntWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOLONGWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("long toLongExactWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOLONGWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("long toLongExactWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOFLOATWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("float toFloatWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method MULWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float multiplyWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method MULWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double multiplyWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_INT =
+            org.objectweb.asm.commons.Method.getMethod("int divideWithoutOverflow(int, int)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long divideWithoutOverflow(long, long)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float divideWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double divideWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method REMWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float remainderWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method REMWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double remainderWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method ADDWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float addWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method ADDWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double addWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method SUBWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float subtractWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method SUBWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double subtractWithoutOverflow(double, double)");
+
+    static byte[] write(Adapter adapter) {
+        Writer writer = new Writer(adapter);
+
+        return writer.getBytes();
+    }
+
+    private final Adapter adapter;
+    private final Definition definition;
+    private final ParseTree root;
+    private final String source;
+    private final CompilerSettings settings;
+
+    private final Map<ParserRuleContext, Branch> branches;
+    private final Deque<Branch> jumps;
+    private final Set<ParserRuleContext> strings;
+
+    private ClassWriter writer;
+    private GeneratorAdapter execute;
+
+    private Writer(final Adapter adapter) {
+        this.adapter = adapter;
+        definition = adapter.definition;
+        root = adapter.root;
+        source = adapter.source;
+        settings = adapter.settings;
+
+        branches = new HashMap<>();
+        jumps = new ArrayDeque<>();
+        strings = new HashSet<>();
+
+        writeBegin();
+        writeConstructor();
+        writeExecute();
+        writeEnd();
+    }
+
+    private Branch markBranch(final ParserRuleContext source, final ParserRuleContext... nodes) {
+        final Branch branch = new Branch(source);
+
+        for (final ParserRuleContext node : nodes) {
+            branches.put(node, branch);
+        }
+
+        return branch;
+    }
+
+    private void copyBranch(final Branch branch, final ParserRuleContext... nodes) {
+        for (final ParserRuleContext node : nodes) {
+            branches.put(node, branch);
+        }
+    }
+
+    private Branch getBranch(final ParserRuleContext source) {
+        return branches.get(source);
+    }
+
+    private void writeBegin() {
+        final int compute = ClassWriter.COMPUTE_FRAMES | ClassWriter.COMPUTE_MAXS;
+        final int version = Opcodes.V1_7;
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SUPER | Opcodes.ACC_FINAL | Opcodes.ACC_SYNTHETIC;
+        final String base = BASE_CLASS_TYPE.getInternalName();
+        final String name = CLASS_TYPE.getInternalName();
+
+        writer = new ClassWriter(compute);
+        writer.visit(version, access, name, null, base, null);
+        writer.visitSource(source, null);
+    }
+
+    private void writeConstructor() {
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SYNTHETIC;
+        final GeneratorAdapter constructor = new GeneratorAdapter(access, CONSTRUCTOR, null, null, writer);
+        constructor.loadThis();
+        constructor.loadArgs();
+        constructor.invokeConstructor(org.objectweb.asm.Type.getType(Executable.class), CONSTRUCTOR);
+        constructor.returnValue();
+        constructor.endMethod();
+    }
+
+    private void writeExecute() {
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SYNTHETIC;
+        execute = new GeneratorAdapter(access, EXECUTE, SIGNATURE, null, writer);
+        visit(root);
+        execute.endMethod();
+    }
+
+    @Override
+    public Void visitSource(final SourceContext ctx) {
+        final StatementMetadata sourcesmd = adapter.getStatementMetadata(ctx);
+
+        for (final StatementContext sctx : ctx.statement()) {
+            visit(sctx);
+        }
+
+        if (!sourcesmd.allReturn) {
+            execute.visitInsn(Opcodes.ACONST_NULL);
+            execute.returnValue();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitIf(final IfContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final boolean els = ctx.ELSE() != null;
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.end = new Label();
+        branch.fals = els ? new Label() : branch.end;
+
+        visit(exprctx);
+
+        final BlockContext blockctx0 = ctx.block(0);
+        final StatementMetadata blockmd0 = adapter.getStatementMetadata(blockctx0);
+        visit(blockctx0);
+
+        if (els) {
+            if (!blockmd0.allExit) {
+                execute.goTo(branch.end);
+            }
+
+            execute.mark(branch.fals);
+            visit(ctx.block(1));
+        }
+
+        execute.mark(branch.end);
+
+        return null;
+    }
+
+    @Override
+    public Void visitWhile(final WhileContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.begin = new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+        execute.mark(branch.begin);
+        visit(exprctx);
+
+        final BlockContext blockctx = ctx.block();
+        boolean allexit = false;
+
+        if (blockctx != null) {
+            StatementMetadata blocksmd = adapter.getStatementMetadata(blockctx);
+            allexit = blocksmd.allExit;
+            visit(blockctx);
+        }
+
+        if (!allexit) {
+            execute.goTo(branch.begin);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDo(final DoContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.begin = new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+        execute.mark(branch.begin);
+
+        final BlockContext bctx = ctx.block();
+        final StatementMetadata blocksmd = adapter.getStatementMetadata(bctx);
+        visit(bctx);
+
+        visit(exprctx);
+
+        if (!blocksmd.allExit) {
+            execute.goTo(branch.begin);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitFor(final ForContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final AfterthoughtContext atctx = ctx.afterthought();
+        final Branch branch = markBranch(ctx, exprctx);
+        final Label start = new Label();
+        branch.begin = atctx == null ? start : new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+
+        if (ctx.initializer() != null) {
+            visit(ctx.initializer());
+        }
+
+        execute.mark(start);
+
+        if (exprctx != null) {
+            visit(exprctx);
+        }
+
+        final BlockContext blockctx = ctx.block();
+        boolean allexit = false;
+
+        if (blockctx != null) {
+            StatementMetadata blocksmd = adapter.getStatementMetadata(blockctx);
+            allexit = blocksmd.allExit;
+            visit(blockctx);
+        }
+
+        if (atctx != null) {
+            execute.mark(branch.begin);
+            visit(atctx);
+        }
+
+        if (atctx != null || !allexit) {
+            execute.goTo(start);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecl(final DeclContext ctx) {
+        visit(ctx.declaration());
+
+        return null;
+    }
+
+    @Override
+    public Void visitContinue(final ContinueContext ctx) {
+        final Branch jump = jumps.peek();
+        execute.goTo(jump.begin);
+
+        return null;
+    }
+
+    @Override
+    public Void visitBreak(final BreakContext ctx) {
+        final Branch jump = jumps.peek();
+        execute.goTo(jump.end);
+
+        return null;
+    }
+
+    @Override
+    public Void visitReturn(final ReturnContext ctx) {
+        visit(ctx.expression());
+        execute.returnValue();
+
+        return null;
+    }
+
+    @Override
+    public Void visitExpr(final ExprContext ctx) {
+        final StatementMetadata exprsmd = adapter.getStatementMetadata(ctx);
+        final ExpressionContext exprctx = ctx.expression();
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(exprctx);
+        visit(exprctx);
+
+        if (exprsmd.allReturn) {
+            execute.returnValue();
+        } else {
+            writePop(expremd.to.type.getSize());
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitMultiple(final MultipleContext ctx) {
+        for (final StatementContext sctx : ctx.statement()) {
+            visit(sctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitSingle(final SingleContext ctx) {
+        visit(ctx.statement());
+
+        return null;
+    }
+
+    @Override
+    public Void visitEmpty(final EmptyContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitInitializer(InitializerContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        final ExpressionContext exprctx = ctx.expression();
+
+        if (declctx != null) {
+            visit(declctx);
+        } else if (exprctx != null) {
+            visit(exprctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAfterthought(AfterthoughtContext ctx) {
+        visit(ctx.expression());
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclaration(DeclarationContext ctx) {
+        for (final DeclvarContext declctx : ctx.declvar()) {
+            visit(declctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecltype(final DecltypeContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitDeclvar(final DeclvarContext ctx) {
+        final ExpressionMetadata declvaremd = adapter.getExpressionMetadata(ctx);
+        final org.objectweb.asm.Type type = declvaremd.to.type;
+        final Sort sort = declvaremd.to.sort;
+        final int slot = (int)declvaremd.postConst;
+
+        final ExpressionContext exprctx = ctx.expression();
+        final boolean initialize = exprctx == null;
+
+        if (!initialize) {
+            visit(exprctx);
+        }
+
+        switch (sort) {
+            case VOID:   throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            case BOOL:
+            case BYTE:
+            case SHORT:
+            case CHAR:
+            case INT:    if (initialize) execute.push(0);    break;
+            case LONG:   if (initialize) execute.push(0L);   break;
+            case FLOAT:  if (initialize) execute.push(0.0F); break;
+            case DOUBLE: if (initialize) execute.push(0.0);  break;
+            default:     if (initialize) execute.visitInsn(Opcodes.ACONST_NULL);
+        }
+
+        execute.visitVarInsn(type.getOpcode(Opcodes.ISTORE), slot);
+
+        return null;
+    }
+
+    @Override
+    public Void visitPrecedence(final PrecedenceContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitNumeric(final NumericContext ctx) {
+        final ExpressionMetadata numericemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = numericemd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, numericemd.preConst);
+            checkWriteCast(numericemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitChar(final CharContext ctx) {
+        final ExpressionMetadata charemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = charemd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, (int)(char)charemd.preConst);
+            checkWriteCast(charemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitTrue(final TrueContext ctx) {
+        final ExpressionMetadata trueemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = trueemd.postConst;
+        final Branch branch = getBranch(ctx);
+
+        if (branch == null) {
+            if (postConst == null) {
+                writeBoolean(ctx, true);
+                checkWriteCast(trueemd);
+            } else {
+                writeConstant(ctx, postConst);
+            }
+        } else if (branch.tru != null) {
+            execute.goTo(branch.tru);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitFalse(final FalseContext ctx) {
+        final ExpressionMetadata falseemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = falseemd.postConst;
+        final Branch branch = getBranch(ctx);
+
+        if (branch == null) {
+            if (postConst == null) {
+                writeBoolean(ctx, false);
+                checkWriteCast(falseemd);
+            } else {
+                writeConstant(ctx, postConst);
+            }
+        } else if (branch.fals != null) {
+            execute.goTo(branch.fals);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitNull(final NullContext ctx) {
+        final ExpressionMetadata nullemd = adapter.getExpressionMetadata(ctx);
+
+        execute.visitInsn(Opcodes.ACONST_NULL);
+        checkWriteCast(nullemd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExternal(final ExternalContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+
+    @Override
+    public Void visitPostinc(final PostincContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitPreinc(final PreincContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitUnary(final UnaryContext ctx) {
+        final ExpressionMetadata unaryemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = unaryemd.postConst;
+        final Object preConst = unaryemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (ctx.BOOLNOT() != null) {
+                if (branch == null) {
+                    writeConstant(ctx, postConst);
+                } else {
+                    if ((boolean)postConst && branch.tru != null) {
+                        execute.goTo(branch.tru);
+                    } else if (!(boolean)postConst && branch.fals != null) {
+                        execute.goTo(branch.fals);
+                    }
+                }
+            } else {
+                writeConstant(ctx, postConst);
+                checkWriteBranch(ctx);
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(unaryemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx = ctx.expression();
+
+            if (ctx.BOOLNOT() != null) {
+                final Branch local = markBranch(ctx, exprctx);
+
+                if (branch == null) {
+                    local.fals = new Label();
+                    final Label aend = new Label();
+
+                    visit(exprctx);
+
+                    execute.push(false);
+                    execute.goTo(aend);
+                    execute.mark(local.fals);
+                    execute.push(true);
+                    execute.mark(aend);
+
+                    checkWriteCast(unaryemd);
+                } else {
+                    local.tru = branch.fals;
+                    local.fals = branch.tru;
+
+                    visit(exprctx);
+                }
+            } else {
+                final org.objectweb.asm.Type type = unaryemd.from.type;
+                final Sort sort = unaryemd.from.sort;
+
+                visit(exprctx);
+
+                if (ctx.BWNOT() != null) {
+                    if (sort == Sort.DEF) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_NOT_CALL);
+                    } else {
+                        if (sort == Sort.INT) {
+                            writeConstant(ctx, -1);
+                        } else if (sort == Sort.LONG) {
+                            writeConstant(ctx, -1L);
+                        } else {
+                            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                        }
+
+                        execute.math(GeneratorAdapter.XOR, type);
+                    }
+                } else if (ctx.SUB() != null) {
+                    if (sort == Sort.DEF) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_NEG_CALL);
+                    } else {
+                        if (settings.getNumericOverflow()) {
+                            execute.math(GeneratorAdapter.NEG, type);
+                        } else {
+                            if (sort == Sort.INT) {
+                                execute.invokeStatic(definition.mathType.type, NEGATEEXACT_INT);
+                            } else if (sort == Sort.LONG) {
+                                execute.invokeStatic(definition.mathType.type, NEGATEEXACT_LONG);
+                            } else {
+                                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                            }
+                        }
+                    }
+                } else if (ctx.ADD() == null) {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+
+                checkWriteCast(unaryemd);
+                checkWriteBranch(ctx);
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitCast(final CastContext ctx) {
+        final ExpressionMetadata castemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = castemd.postConst;
+
+        if (postConst == null) {
+            visit(ctx.expression());
+            checkWriteCast(castemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitBinary(final BinaryContext ctx) {
+        final ExpressionMetadata binaryemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = binaryemd.postConst;
+        final Object preConst = binaryemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            writeConstant(ctx, postConst);
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(binaryemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else if (binaryemd.from.sort == Sort.STRING) {
+            final boolean marked = strings.contains(ctx);
+
+            if (!marked) {
+                writeNewStrings();
+            }
+
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(exprctx0);
+            strings.add(exprctx0);
+            visit(exprctx0);
+
+            if (strings.contains(exprctx0)) {
+                writeAppendStrings(expremd0.from.sort);
+                strings.remove(exprctx0);
+            }
+
+            final ExpressionContext exprctx1 = ctx.expression(1);
+            final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(exprctx1);
+            strings.add(exprctx1);
+            visit(exprctx1);
+
+            if (strings.contains(exprctx1)) {
+                writeAppendStrings(expremd1.from.sort);
+                strings.remove(exprctx1);
+            }
+
+            if (marked) {
+                strings.remove(ctx);
+            } else {
+                writeToStrings();
+            }
+
+            checkWriteCast(binaryemd);
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionContext exprctx1 = ctx.expression(1);
+
+            visit(exprctx0);
+            visit(exprctx1);
+
+            final Type type = binaryemd.from;
+
+            if      (ctx.MUL()   != null) writeBinaryInstruction(ctx, type, MUL);
+            else if (ctx.DIV()   != null) writeBinaryInstruction(ctx, type, DIV);
+            else if (ctx.REM()   != null) writeBinaryInstruction(ctx, type, REM);
+            else if (ctx.ADD()   != null) writeBinaryInstruction(ctx, type, ADD);
+            else if (ctx.SUB()   != null) writeBinaryInstruction(ctx, type, SUB);
+            else if (ctx.LSH()   != null) writeBinaryInstruction(ctx, type, LSH);
+            else if (ctx.USH()   != null) writeBinaryInstruction(ctx, type, USH);
+            else if (ctx.RSH()   != null) writeBinaryInstruction(ctx, type, RSH);
+            else if (ctx.BWAND() != null) writeBinaryInstruction(ctx, type, BWAND);
+            else if (ctx.BWXOR() != null) writeBinaryInstruction(ctx, type, BWXOR);
+            else if (ctx.BWOR()  != null) writeBinaryInstruction(ctx, type, BWOR);
+            else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+
+            checkWriteCast(binaryemd);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitComp(final CompContext ctx) {
+        final ExpressionMetadata compemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = compemd.postConst;
+        final Object preConst = compemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, postConst);
+            } else {
+                if ((boolean)postConst && branch.tru != null) {
+                    execute.mark(branch.tru);
+                } else if (!(boolean)postConst && branch.fals != null) {
+                    execute.mark(branch.fals);
+                }
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(compemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(exprctx0);
+
+            final ExpressionContext exprctx1 = ctx.expression(1);
+            final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(exprctx1);
+            final org.objectweb.asm.Type type = expremd1.to.type;
+            final Sort sort1 = expremd1.to.sort;
+
+            visit(exprctx0);
+
+            if (!expremd1.isNull) {
+                visit(exprctx1);
+            }
+
+            final boolean tru = branch != null && branch.tru != null;
+            final boolean fals = branch != null && branch.fals != null;
+            final Label jump = tru ? branch.tru : fals ? branch.fals : new Label();
+            final Label end = new Label();
+
+            final boolean eq = (ctx.EQ() != null || ctx.EQR() != null) && (tru || !fals) ||
+                    (ctx.NE() != null || ctx.NER() != null) && fals;
+            final boolean ne = (ctx.NE() != null || ctx.NER() != null) && (tru || !fals) ||
+                    (ctx.EQ() != null || ctx.EQR() != null) && fals;
+            final boolean lt  = ctx.LT()  != null && (tru || !fals) || ctx.GTE() != null && fals;
+            final boolean lte = ctx.LTE() != null && (tru || !fals) || ctx.GT()  != null && fals;
+            final boolean gt  = ctx.GT()  != null && (tru || !fals) || ctx.LTE() != null && fals;
+            final boolean gte = ctx.GTE() != null && (tru || !fals) || ctx.LT()  != null && fals;
+
+            boolean writejump = true;
+
+            switch (sort1) {
+                case VOID:
+                case BYTE:
+                case SHORT:
+                case CHAR:
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                case BOOL:
+                    if      (eq) execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                    else if (ne) execute.ifZCmp(GeneratorAdapter.NE, jump);
+                    else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case INT:
+                case LONG:
+                case FLOAT:
+                case DOUBLE:
+                    if      (eq)  execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                    else if (ne)  execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                    else if (lt)  execute.ifCmp(type, GeneratorAdapter.LT, jump);
+                    else if (lte) execute.ifCmp(type, GeneratorAdapter.LE, jump);
+                    else if (gt)  execute.ifCmp(type, GeneratorAdapter.GT, jump);
+                    else if (gte) execute.ifCmp(type, GeneratorAdapter.GE, jump);
+                    else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case DEF:
+                    if (eq) {
+                        if (expremd1.isNull) {
+                            execute.ifNull(jump);
+                        } else if (!expremd0.isNull && ctx.EQ() != null) {
+                            execute.invokeStatic(definition.defobjType.type, DEF_EQ_CALL);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                        }
+                    } else if (ne) {
+                        if (expremd1.isNull) {
+                            execute.ifNonNull(jump);
+                        } else if (!expremd0.isNull && ctx.NE() != null) {
+                            execute.invokeStatic(definition.defobjType.type, DEF_EQ_CALL);
+                            execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                        }
+                    } else if (lt) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_LT_CALL);
+                    } else if (lte) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_LTE_CALL);
+                    } else if (gt) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_GT_CALL);
+                    } else if (gte) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_GTE_CALL);
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    writejump = expremd1.isNull || ne || ctx.EQR() != null;
+
+                    if (branch != null && !writejump) {
+                        execute.ifZCmp(GeneratorAdapter.NE, jump);
+                    }
+
+                    break;
+                default:
+                    if (eq) {
+                        if (expremd1.isNull) {
+                            execute.ifNull(jump);
+                        } else if (ctx.EQ() != null) {
+                            execute.invokeStatic(definition.utilityType.type, CHECKEQUALS);
+
+                            if (branch != null) {
+                                execute.ifZCmp(GeneratorAdapter.NE, jump);
+                            }
+
+                            writejump = false;
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                        }
+                    } else if (ne) {
+                        if (expremd1.isNull) {
+                            execute.ifNonNull(jump);
+                        } else if (ctx.NE() != null) {
+                            execute.invokeStatic(definition.utilityType.type, CHECKEQUALS);
+                            execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                        }
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+            }
+
+            if (branch == null) {
+                if (writejump) {
+                    execute.push(false);
+                    execute.goTo(end);
+                    execute.mark(jump);
+                    execute.push(true);
+                    execute.mark(end);
+                }
+
+                checkWriteCast(compemd);
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitBool(final BoolContext ctx) {
+        final ExpressionMetadata boolemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = boolemd.postConst;
+        final Object preConst = boolemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, postConst);
+            } else {
+                if ((boolean)postConst && branch.tru != null) {
+                    execute.mark(branch.tru);
+                } else if (!(boolean)postConst && branch.fals != null) {
+                    execute.mark(branch.fals);
+                }
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(boolemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionContext exprctx1 = ctx.expression(1);
+
+            if (branch == null) {
+                if (ctx.BOOLAND() != null) {
+                    final Branch local = markBranch(ctx, exprctx0, exprctx1);
+                    local.fals = new Label();
+                    final Label end = new Label();
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    execute.push(true);
+                    execute.goTo(end);
+                    execute.mark(local.fals);
+                    execute.push(false);
+                    execute.mark(end);
+                } else if (ctx.BOOLOR() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.tru = new Label();
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.fals = new Label();
+                    final Label aend = new Label();
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    execute.mark(branch0.tru);
+                    execute.push(true);
+                    execute.goTo(aend);
+                    execute.mark(branch1.fals);
+                    execute.push(false);
+                    execute.mark(aend);
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+
+                checkWriteCast(boolemd);
+            } else {
+                if (ctx.BOOLAND() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.fals = branch.fals == null ? new Label() : branch.fals;
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.tru = branch.tru;
+                    branch1.fals = branch.fals;
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    if (branch.fals == null) {
+                        execute.mark(branch0.fals);
+                    }
+                } else if (ctx.BOOLOR() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.tru = branch.tru == null ? new Label() : branch.tru;
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.tru = branch.tru;
+                    branch1.fals = branch.fals;
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    if (branch.tru == null) {
+                        execute.mark(branch0.tru);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitConditional(final ConditionalContext ctx) {
+        final ExpressionMetadata condemd = adapter.getExpressionMetadata(ctx);
+        final Branch branch = getBranch(ctx);
+
+        final ExpressionContext expr0 = ctx.expression(0);
+        final ExpressionContext expr1 = ctx.expression(1);
+        final ExpressionContext expr2 = ctx.expression(2);
+
+        final Branch local = markBranch(ctx, expr0);
+        local.fals = new Label();
+        local.end = new Label();
+
+        if (branch != null) {
+            copyBranch(branch, expr1, expr2);
+        }
+
+        visit(expr0);
+        visit(expr1);
+        execute.goTo(local.end);
+        execute.mark(local.fals);
+        visit(expr2);
+        execute.mark(local.end);
+
+        if (branch == null) {
+            checkWriteCast(condemd);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAssignment(final AssignmentContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstart(ExtstartContext ctx) {
+        final ExternalMetadata startemd = adapter.getExternalMetadata(ctx);
+
+        if (startemd.token == ADD) {
+            final ExpressionMetadata storeemd = adapter.getExpressionMetadata(startemd.storeExpr);
+
+            if (startemd.current.sort == Sort.STRING || storeemd.from.sort == Sort.STRING) {
+                writeNewStrings();
+                strings.add(startemd.storeExpr);
+            }
+        }
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtprec(final ExtprecContext ctx) {
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcast(final ExtcastContext ctx) {
+        ExtNodeMetadata castenmd = adapter.getExtNodeMetadata(ctx);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        checkWriteCast(ctx, castenmd.castTo);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtbrace(final ExtbraceContext ctx) {
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        visit(exprctx);
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtdot(final ExtdotContext ctx) {
+        final ExtcallContext callctx = ctx.extcall();
+        final ExtfieldContext fieldctx = ctx.extfield();
+
+        if (callctx != null) {
+            visit(callctx);
+        } else if (fieldctx != null) {
+            visit(fieldctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExttype(final ExttypeContext ctx) {
+        visit(ctx.extdot());
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcall(final ExtcallContext ctx) {
+        writeCallExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtvar(final ExtvarContext ctx) {
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtfield(final ExtfieldContext ctx) {
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtnew(ExtnewContext ctx) {
+        writeNewExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstring(ExtstringContext ctx) {
+        final ExtNodeMetadata stringenmd = adapter.getExtNodeMetadata(ctx);
+
+        writeConstant(ctx, stringenmd.target);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitArguments(final ArgumentsContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitIncrement(IncrementContext ctx) {
+        final ExpressionMetadata incremd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = incremd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, incremd.preConst);
+            checkWriteCast(incremd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    private void writeConstant(final ParserRuleContext source, final Object constant) {
+        if (constant instanceof Number) {
+            writeNumeric(source, constant);
+        } else if (constant instanceof Character) {
+            writeNumeric(source, (int)(char)constant);
+        } else if (constant instanceof String) {
+            writeString(source, constant);
+        } else if (constant instanceof Boolean) {
+            writeBoolean(source, constant);
+        } else if (constant != null) {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeNumeric(final ParserRuleContext source, final Object numeric) {
+        if (numeric instanceof Double) {
+            execute.push((double)numeric);
+        } else if (numeric instanceof Float) {
+            execute.push((float)numeric);
+        } else if (numeric instanceof Long) {
+            execute.push((long)numeric);
+        } else if (numeric instanceof Number) {
+            execute.push(((Number)numeric).intValue());
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeString(final ParserRuleContext source, final Object string) {
+        if (string instanceof String) {
+            execute.push((String)string);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeBoolean(final ParserRuleContext source, final Object bool) {
+        if (bool instanceof Boolean) {
+            execute.push((boolean)bool);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeNewStrings() {
+        execute.newInstance(STRINGBUILDER_TYPE);
+        execute.dup();
+        execute.invokeConstructor(STRINGBUILDER_TYPE, STRINGBUILDER_CONSTRUCTOR);
+    }
+
+    private void writeAppendStrings(final Sort sort) {
+        switch (sort) {
+            case BOOL:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_BOOLEAN); break;
+            case CHAR:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_CHAR);    break;
+            case BYTE:
+            case SHORT:
+            case INT:    execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_INT);     break;
+            case LONG:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_LONG);    break;
+            case FLOAT:  execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_FLOAT);   break;
+            case DOUBLE: execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_DOUBLE);  break;
+            case STRING: execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_STRING);  break;
+            default:     execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_OBJECT);
+        }
+    }
+
+    private void writeToStrings() {
+        execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_TOSTRING);
+    }
+
+    private void writeBinaryInstruction(final ParserRuleContext source, final Type type, final int token) {
+        final Sort sort = type.sort;
+        final boolean exact = !settings.getNumericOverflow() &&
+                ((sort == Sort.INT || sort == Sort.LONG) &&
+                (token == MUL || token == DIV || token == ADD || token == SUB) ||
+                (sort == Sort.FLOAT || sort == Sort.DOUBLE) &&
+                (token == MUL || token == DIV || token == REM || token == ADD || token == SUB));
+
+        // if its a 64-bit shift, fixup the last argument to truncate to 32-bits
+        // note unlike java, this means we still do binary promotion of shifts,
+        // but it keeps things simple -- this check works because we promote shifts.
+        if (sort == Sort.LONG && (token == LSH || token == USH || token == RSH)) {
+            execute.cast(org.objectweb.asm.Type.LONG_TYPE, org.objectweb.asm.Type.INT_TYPE);
+        }
+
+        if (exact) {
+            switch (sort) {
+                case INT:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.mathType.type,    MULEXACT_INT);     break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_INT); break;
+                        case ADD: execute.invokeStatic(definition.mathType.type,    ADDEXACT_INT);     break;
+                        case SUB: execute.invokeStatic(definition.mathType.type,    SUBEXACT_INT);     break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case LONG:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.mathType.type,    MULEXACT_LONG);     break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_LONG); break;
+                        case ADD: execute.invokeStatic(definition.mathType.type,    ADDEXACT_LONG);     break;
+                        case SUB: execute.invokeStatic(definition.mathType.type,    SUBEXACT_LONG);     break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case FLOAT:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.utilityType.type, MULWOOVERLOW_FLOAT); break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_FLOAT); break;
+                        case REM: execute.invokeStatic(definition.utilityType.type, REMWOOVERLOW_FLOAT); break;
+                        case ADD: execute.invokeStatic(definition.utilityType.type, ADDWOOVERLOW_FLOAT); break;
+                        case SUB: execute.invokeStatic(definition.utilityType.type, SUBWOOVERLOW_FLOAT); break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case DOUBLE:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.utilityType.type, MULWOOVERLOW_DOUBLE); break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_DOUBLE); break;
+                        case REM: execute.invokeStatic(definition.utilityType.type, REMWOOVERLOW_DOUBLE); break;
+                        case ADD: execute.invokeStatic(definition.utilityType.type, ADDWOOVERLOW_DOUBLE); break;
+                        case SUB: execute.invokeStatic(definition.utilityType.type, SUBWOOVERLOW_DOUBLE); break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                default:
+                    throw new IllegalStateException(error(source) + "Unexpected writer state.");
+            }
+        } else {
+            if ((sort == Sort.FLOAT || sort == Sort.DOUBLE) &&
+                    (token == LSH || token == USH || token == RSH || token == BWAND || token == BWXOR || token == BWOR)) {
+                throw new IllegalStateException(error(source) + "Unexpected writer state.");
+            }
+
+            if (sort == Sort.DEF) {
+                switch (token) {
+                    case MUL:   execute.invokeStatic(definition.defobjType.type, DEF_MUL_CALL); break;
+                    case DIV:   execute.invokeStatic(definition.defobjType.type, DEF_DIV_CALL); break;
+                    case REM:   execute.invokeStatic(definition.defobjType.type, DEF_REM_CALL); break;
+                    case ADD:   execute.invokeStatic(definition.defobjType.type, DEF_ADD_CALL); break;
+                    case SUB:   execute.invokeStatic(definition.defobjType.type, DEF_SUB_CALL); break;
+                    case LSH:   execute.invokeStatic(definition.defobjType.type, DEF_LSH_CALL); break;
+                    case USH:   execute.invokeStatic(definition.defobjType.type, DEF_RSH_CALL); break;
+                    case RSH:   execute.invokeStatic(definition.defobjType.type, DEF_USH_CALL); break;
+                    case BWAND: execute.invokeStatic(definition.defobjType.type, DEF_AND_CALL); break;
+                    case BWXOR: execute.invokeStatic(definition.defobjType.type, DEF_XOR_CALL); break;
+                    case BWOR:  execute.invokeStatic(definition.defobjType.type, DEF_OR_CALL);  break;
+                    default:
+                        throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                }
+            } else {
+                switch (token) {
+                    case MUL:   execute.math(GeneratorAdapter.MUL,  type.type); break;
+                    case DIV:   execute.math(GeneratorAdapter.DIV,  type.type); break;
+                    case REM:   execute.math(GeneratorAdapter.REM,  type.type); break;
+                    case ADD:   execute.math(GeneratorAdapter.ADD,  type.type); break;
+                    case SUB:   execute.math(GeneratorAdapter.SUB,  type.type); break;
+                    case LSH:   execute.math(GeneratorAdapter.SHL,  type.type); break;
+                    case USH:   execute.math(GeneratorAdapter.USHR, type.type); break;
+                    case RSH:   execute.math(GeneratorAdapter.SHR,  type.type); break;
+                    case BWAND: execute.math(GeneratorAdapter.AND,  type.type); break;
+                    case BWXOR: execute.math(GeneratorAdapter.XOR,  type.type); break;
+                    case BWOR:  execute.math(GeneratorAdapter.OR,   type.type); break;
+                    default:
+                        throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                }
+            }
+        }
+    }
+
+    /**
+     * Called for any compound assignment (including increment/decrement instructions).
+     * We have to be stricter than writeBinary, and do overflow checks against the original type's size
+     * instead of the promoted type's size, since the result will be implicitly cast back.
+     *
+     * @return true if an instruction is written, false otherwise
+     */
+    private boolean writeExactInstruction(final Sort osort, final Sort psort) {
+            if (psort == Sort.DOUBLE) {
+                if (osort == Sort.FLOAT) {
+                    execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.FLOAT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.floatobjType.type);
+                } else if (osort == Sort.LONG) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.LONG_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.longobjType.type);
+                } else if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.FLOAT) {
+                if (osort == Sort.LONG) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.LONG_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.longobjType.type);
+                } else if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.LONG) {
+                if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.INT) {
+                if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else {
+                return false;
+            }
+
+        return true;
+    }
+
+    private void writeLoadStoreExternal(final ParserRuleContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceenmd.parent);
+
+        final boolean length = "#length".equals(sourceenmd.target);
+        final boolean array = "#brace".equals(sourceenmd.target);
+        final boolean name = sourceenmd.target instanceof String && !length && !array;
+        final boolean variable = sourceenmd.target instanceof Integer;
+        final boolean field = sourceenmd.target instanceof Field;
+        final boolean shortcut = sourceenmd.target instanceof Object[];
+
+        if (!length && !variable && !field && !array && !name && !shortcut) {
+            throw new IllegalStateException(error(source) + "Target not found for load/store.");
+        }
+
+        final boolean maplist = shortcut && (boolean)((Object[])sourceenmd.target)[2];
+        final Object constant = shortcut ? ((Object[])sourceenmd.target)[3] : null;
+
+        final boolean x1 = field || name || (shortcut && !maplist);
+        final boolean x2 = array || (shortcut && maplist);
+
+        if (length) {
+            execute.arrayLength();
+        } else if (sourceenmd.last && parentemd.storeExpr != null) {
+            final ExpressionMetadata expremd = adapter.getExpressionMetadata(parentemd.storeExpr);
+            final boolean cat = strings.contains(parentemd.storeExpr);
+
+            if (cat) {
+                if (field || name || shortcut) {
+                    execute.dupX1();
+                } else if (array) {
+                    execute.dup2X1();
+                }
+
+                if (maplist) {
+                    if (constant != null) {
+                        writeConstant(source, constant);
+                    }
+
+                    execute.dupX2();
+                }
+
+                writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+                writeAppendStrings(sourceenmd.type.sort);
+                visit(parentemd.storeExpr);
+
+                if (strings.contains(parentemd.storeExpr)) {
+                    writeAppendStrings(expremd.to.sort);
+                    strings.remove(parentemd.storeExpr);
+                }
+
+                writeToStrings();
+                checkWriteCast(source, sourceenmd.castTo);
+
+                if (parentemd.read) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            } else if (parentemd.token > 0) {
+                final int token = parentemd.token;
+
+                if (field || name || shortcut) {
+                    execute.dup();
+                } else if (array) {
+                    execute.dup2();
+                }
+
+                if (maplist) {
+                    if (constant != null) {
+                        writeConstant(source, constant);
+                    }
+
+                    execute.dupX1();
+                }
+
+                writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+
+                if (parentemd.read && parentemd.post) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                checkWriteCast(source, sourceenmd.castFrom);
+                visit(parentemd.storeExpr);
+
+                writeBinaryInstruction(source, sourceenmd.promote, token);
+
+                boolean exact = false;
+
+                if (!settings.getNumericOverflow() && expremd.typesafe && sourceenmd.type.sort != Sort.DEF &&
+                        (token == MUL || token == DIV || token == REM || token == ADD || token == SUB)) {
+                    exact = writeExactInstruction(sourceenmd.type.sort, sourceenmd.promote.sort);
+                }
+
+                if (!exact) {
+                    checkWriteCast(source, sourceenmd.castTo);
+                }
+
+                if (parentemd.read && !parentemd.post) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            } else {
+                if (constant != null) {
+                    writeConstant(source, constant);
+                }
+
+                visit(parentemd.storeExpr);
+
+                if (parentemd.read) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            }
+        } else {
+            if (constant != null) {
+                writeConstant(source, constant);
+            }
+
+            writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+        }
+    }
+
+    private void writeLoadStoreInstruction(final ParserRuleContext source,
+                                           final boolean store, final boolean variable,
+                                           final boolean field, final boolean name,
+                                           final boolean array, final boolean shortcut) {
+        final ExtNodeMetadata sourceemd = adapter.getExtNodeMetadata(source);
+
+        if (variable) {
+            writeLoadStoreVariable(source, store, sourceemd.type, (int)sourceemd.target);
+        } else if (field) {
+            writeLoadStoreField(store, (Field)sourceemd.target);
+        } else if (name) {
+            writeLoadStoreField(source, store, (String)sourceemd.target);
+        } else if (array) {
+            writeLoadStoreArray(source, store, sourceemd.type);
+        } else if (shortcut) {
+            Object[] targets = (Object[])sourceemd.target;
+            writeLoadStoreShortcut(store, (Method)targets[0], (Method)targets[1]);
+        } else {
+            throw new IllegalStateException(error(source) + "Load/Store requires a variable, field, or array.");
+        }
+    }
+
+    private void writeLoadStoreVariable(final ParserRuleContext source, final boolean store,
+                                        final Type type, final int slot) {
+        if (type.sort == Sort.VOID) {
+            throw new IllegalStateException(error(source) + "Cannot load/store void type.");
+        }
+
+        if (store) {
+            execute.visitVarInsn(type.type.getOpcode(Opcodes.ISTORE), slot);
+        } else {
+            execute.visitVarInsn(type.type.getOpcode(Opcodes.ILOAD), slot);
+        }
+    }
+
+    private void writeLoadStoreField(final boolean store, final Field field) {
+        if (java.lang.reflect.Modifier.isStatic(field.reflect.getModifiers())) {
+            if (store) {
+                execute.putStatic(field.owner.type, field.reflect.getName(), field.type.type);
+            } else {
+                execute.getStatic(field.owner.type, field.reflect.getName(), field.type.type);
+
+                if (!field.generic.clazz.equals(field.type.clazz)) {
+                    execute.checkCast(field.generic.type);
+                }
+            }
+        } else {
+            if (store) {
+                execute.putField(field.owner.type, field.reflect.getName(), field.type.type);
+            } else {
+                execute.getField(field.owner.type, field.reflect.getName(), field.type.type);
+
+                if (!field.generic.clazz.equals(field.type.clazz)) {
+                    execute.checkCast(field.generic.type);
+                }
+            }
+        }
+    }
+
+    private void writeLoadStoreField(final ParserRuleContext source, final boolean store, final String name) {
+        if (store) {
+            final ExtNodeMetadata sourceemd = adapter.getExtNodeMetadata(source);
+            final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceemd.parent);
+            final ExpressionMetadata expremd = adapter.getExpressionMetadata(parentemd.storeExpr);
+
+            execute.push(name);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+            execute.push(parentemd.token == 0 && expremd.typesafe);
+            execute.invokeStatic(definition.defobjType.type, DEF_FIELD_STORE);
+        } else {
+            execute.push(name);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+            execute.invokeStatic(definition.defobjType.type, DEF_FIELD_LOAD);
+        }
+    }
+
+    private void writeLoadStoreArray(final ParserRuleContext source, final boolean store, final Type type) {
+        if (type.sort == Sort.VOID) {
+            throw new IllegalStateException(error(source) + "Cannot load/store void type.");
+        }
+
+        if (type.sort == Sort.DEF) {
+            final ExtbraceContext bracectx = (ExtbraceContext)source;
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(bracectx.expression());
+
+            if (store) {
+                final ExtNodeMetadata braceenmd = adapter.getExtNodeMetadata(bracectx);
+                final ExternalMetadata parentemd = adapter.getExternalMetadata(braceenmd.parent);
+                final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(parentemd.storeExpr);
+
+                execute.loadThis();
+                execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+                execute.push(expremd0.typesafe);
+                execute.push(parentemd.token == 0 && expremd1.typesafe);
+                execute.invokeStatic(definition.defobjType.type, DEF_ARRAY_STORE);
+            } else {
+                execute.loadThis();
+                execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+                execute.push(expremd0.typesafe);
+                execute.invokeStatic(definition.defobjType.type, DEF_ARRAY_LOAD);
+            }
+        } else {
+            if (store) {
+                execute.arrayStore(type.type);
+            } else {
+                execute.arrayLoad(type.type);
+            }
+        }
+    }
+
+    private void writeLoadStoreShortcut(final boolean store, final Method getter, final Method setter) {
+        final Method method = store ? setter : getter;
+
+        if (java.lang.reflect.Modifier.isInterface(getter.owner.clazz.getModifiers())) {
+            execute.invokeInterface(method.owner.type, method.method);
+        } else {
+            execute.invokeVirtual(method.owner.type, method.method);
+        }
+
+        if (store) {
+            writePop(method.rtn.type.getSize());
+        } else if (!method.rtn.clazz.equals(method.handle.type().returnType())) {
+            execute.checkCast(method.rtn.type);
+        }
+    }
+
+    private void writeDup(final int size, final boolean x1, final boolean x2) {
+        if (size == 1) {
+            if (x2) {
+                execute.dupX2();
+            } else if (x1) {
+                execute.dupX1();
+            } else {
+                execute.dup();
+            }
+        } else if (size == 2) {
+            if (x2) {
+                execute.dup2X2();
+            } else if (x1) {
+                execute.dup2X1();
+            } else {
+                execute.dup2();
+            }
+        }
+    }
+
+    private void writeNewExternal(final ExtnewContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceenmd.parent);
+
+        final boolean makearray = "#makearray".equals(sourceenmd.target);
+        final boolean constructor = sourceenmd.target instanceof Constructor;
+
+        if (!makearray && !constructor) {
+            throw new IllegalStateException(error(source) + "Target not found for new call.");
+        }
+
+        if (makearray) {
+            for (final ExpressionContext exprctx : source.expression()) {
+                visit(exprctx);
+            }
+
+            if (sourceenmd.type.sort == Sort.ARRAY) {
+                execute.visitMultiANewArrayInsn(sourceenmd.type.type.getDescriptor(), sourceenmd.type.type.getDimensions());
+            } else {
+                execute.newArray(sourceenmd.type.type);
+            }
+        } else {
+            execute.newInstance(sourceenmd.type.type);
+
+            if (parentemd.read) {
+                execute.dup();
+            }
+
+            for (final ExpressionContext exprctx : source.arguments().expression()) {
+                visit(exprctx);
+            }
+
+            final Constructor target = (Constructor)sourceenmd.target;
+            execute.invokeConstructor(target.owner.type, target.method);
+        }
+    }
+
+    private void writeCallExternal(final ExtcallContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+
+        final boolean method = sourceenmd.target instanceof Method;
+        final boolean def = sourceenmd.target instanceof String;
+
+        if (!method && !def) {
+            throw new IllegalStateException(error(source) + "Target not found for call.");
+        }
+
+        final List<ExpressionContext> arguments = source.arguments().expression();
+
+        if (method) {
+            for (final ExpressionContext exprctx : arguments) {
+                visit(exprctx);
+            }
+
+            final Method target = (Method)sourceenmd.target;
+
+            if (java.lang.reflect.Modifier.isStatic(target.reflect.getModifiers())) {
+                execute.invokeStatic(target.owner.type, target.method);
+            } else if (java.lang.reflect.Modifier.isInterface(target.owner.clazz.getModifiers())) {
+                execute.invokeInterface(target.owner.type, target.method);
+            } else {
+                execute.invokeVirtual(target.owner.type, target.method);
+            }
+
+            if (!target.rtn.clazz.equals(target.handle.type().returnType())) {
+                execute.checkCast(target.rtn.type);
+            }
+        } else {
+            execute.push((String)sourceenmd.target);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+
+            execute.push(arguments.size());
+            execute.newArray(definition.defType.type);
+
+            for (int argument = 0; argument < arguments.size(); ++argument) {
+                execute.dup();
+                execute.push(argument);
+                visit(arguments.get(argument));
+                execute.arrayStore(definition.defType.type);
+            }
+
+            execute.push(arguments.size());
+            execute.newArray(definition.booleanType.type);
+
+            for (int argument = 0; argument < arguments.size(); ++argument) {
+                execute.dup();
+                execute.push(argument);
+                execute.push(adapter.getExpressionMetadata(arguments.get(argument)).typesafe);
+                execute.arrayStore(definition.booleanType.type);
+            }
+
+            execute.invokeStatic(definition.defobjType.type, DEF_METHOD_CALL);
+        }
+    }
+
+    private void writePop(final int size) {
+        if (size == 1) {
+            execute.pop();
+        } else if (size == 2) {
+            execute.pop2();
+        }
+    }
+
+    private void checkWriteCast(final ExpressionMetadata sort) {
+        checkWriteCast(sort.source, sort.cast);
+    }
+
+    private void checkWriteCast(final ParserRuleContext source, final Cast cast) {
+        if (cast instanceof Transform) {
+            writeTransform((Transform)cast);
+        } else if (cast != null) {
+            writeCast(cast);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected cast object.");
+        }
+    }
+
+    private void writeCast(final Cast cast) {
+        final Type from = cast.from;
+        final Type to = cast.to;
+
+        if (from.equals(to)) {
+            return;
+        }
+
+        if (from.sort.numeric && from.sort.primitive && to.sort.numeric && to.sort.primitive) {
+            execute.cast(from.type, to.type);
+        } else {
+            try {
+                from.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException exception) {
+                execute.checkCast(to.type);
+            }
+        }
+    }
+
+    private void writeTransform(final Transform transform) {
+        if (transform.upcast != null) {
+            execute.checkCast(transform.upcast.type);
+        }
+
+        if (java.lang.reflect.Modifier.isStatic(transform.method.reflect.getModifiers())) {
+            execute.invokeStatic(transform.method.owner.type, transform.method.method);
+        } else if (java.lang.reflect.Modifier.isInterface(transform.method.owner.clazz.getModifiers())) {
+            execute.invokeInterface(transform.method.owner.type, transform.method.method);
+        } else {
+            execute.invokeVirtual(transform.method.owner.type, transform.method.method);
+        }
+
+        if (transform.downcast != null) {
+            execute.checkCast(transform.downcast.type);
+        }
+    }
+
+    void checkWriteBranch(final ParserRuleContext source) {
+        final Branch branch = getBranch(source);
+
+        if (branch != null) {
+            if (branch.tru != null) {
+                execute.visitJumpInsn(Opcodes.IFNE, branch.tru);
+            } else if (branch.fals != null) {
+                execute.visitJumpInsn(Opcodes.IFEQ, branch.fals);
+            }
+        }
+    }
+
+    private void writeEnd() {
+        writer.visitEnd();
+    }
+
+    private byte[] getBytes() {
+        return writer.toByteArray();
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy b/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy
new file mode 100644
index 0000000..e45c1b8
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy
@@ -0,0 +1,23 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+grant {
+  // needed to generate runtime classes
+  permission java.lang.RuntimePermission "createClassLoader";
+};
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java
new file mode 100644
index 0000000..af7eb25
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java
@@ -0,0 +1,199 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
+import java.util.HashMap;
+import java.util.Map;
+
+/** Tests for addition operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class AdditionTests extends ScriptTestCase {
+
+    public void testInt() throws Exception {
+        assertEquals(1+1, exec("int x = 1; int y = 1; return x+y;"));
+        assertEquals(1+2, exec("int x = 1; int y = 2; return x+y;"));
+        assertEquals(5+10, exec("int x = 5; int y = 10; return x+y;"));
+        assertEquals(1+1+2, exec("int x = 1; int y = 1; int z = 2; return x+y+z;"));
+        assertEquals((1+1)+2, exec("int x = 1; int y = 1; int z = 2; return (x+y)+z;"));
+        assertEquals(1+(1+2), exec("int x = 1; int y = 1; int z = 2; return x+(y+z);"));
+        assertEquals(0+1, exec("int x = 0; int y = 1; return x+y;"));
+        assertEquals(1+0, exec("int x = 1; int y = 0; return x+y;"));
+        assertEquals(0+0, exec("int x = 0; int y = 0; return x+y;"));
+        assertEquals(0+0, exec("int x = 0; int y = 0; return x+y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1+1, exec("return 1+1;"));
+        assertEquals(1+2, exec("return 1+2;"));
+        assertEquals(5+10, exec("return 5+10;"));
+        assertEquals(1+1+2, exec("return 1+1+2;"));
+        assertEquals((1+1)+2, exec("return (1+1)+2;"));
+        assertEquals(1+(1+2), exec("return 1+(1+2);"));
+        assertEquals(0+1, exec("return 0+1;"));
+        assertEquals(1+0, exec("return 1+0;"));
+        assertEquals(0+0, exec("return 0+0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1+(byte)1, exec("byte x = 1; byte y = 1; return x+y;"));
+        assertEquals((byte)1+(byte)2, exec("byte x = 1; byte y = 2; return x+y;"));
+        assertEquals((byte)5+(byte)10, exec("byte x = 5; byte y = 10; return x+y;"));
+        assertEquals((byte)1+(byte)1+(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x+y+z;"));
+        assertEquals(((byte)1+(byte)1)+(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x+y)+z;"));
+        assertEquals((byte)1+((byte)1+(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x+(y+z);"));
+        assertEquals((byte)0+(byte)1, exec("byte x = 0; byte y = 1; return x+y;"));
+        assertEquals((byte)1+(byte)0, exec("byte x = 1; byte y = 0; return x+y;"));
+        assertEquals((byte)0+(byte)0, exec("byte x = 0; byte y = 0; return x+y;"));
+    }
+    
+    public void testByteConst() throws Exception {
+        assertEquals((byte)1+(byte)1, exec("return (byte)1+(byte)1;"));
+        assertEquals((byte)1+(byte)2, exec("return (byte)1+(byte)2;"));
+        assertEquals((byte)5+(byte)10, exec("return (byte)5+(byte)10;"));
+        assertEquals((byte)1+(byte)1+(byte)2, exec("return (byte)1+(byte)1+(byte)2;"));
+        assertEquals(((byte)1+(byte)1)+(byte)2, exec("return ((byte)1+(byte)1)+(byte)2;"));
+        assertEquals((byte)1+((byte)1+(byte)2), exec("return (byte)1+((byte)1+(byte)2);"));
+        assertEquals((byte)0+(byte)1, exec("return (byte)0+(byte)1;"));
+        assertEquals((byte)1+(byte)0, exec("return (byte)1+(byte)0;"));
+        assertEquals((byte)0+(byte)0, exec("return (byte)0+(byte)0;"));
+    }
+    
+    public void testChar() throws Exception {
+        assertEquals((char)1+(char)1, exec("char x = 1; char y = 1; return x+y;"));
+        assertEquals((char)1+(char)2, exec("char x = 1; char y = 2; return x+y;"));
+        assertEquals((char)5+(char)10, exec("char x = 5; char y = 10; return x+y;"));
+        assertEquals((char)1+(char)1+(char)2, exec("char x = 1; char y = 1; char z = 2; return x+y+z;"));
+        assertEquals(((char)1+(char)1)+(char)2, exec("char x = 1; char y = 1; char z = 2; return (x+y)+z;"));
+        assertEquals((char)1+((char)1+(char)2), exec("char x = 1; char y = 1; char z = 2; return x+(y+z);"));
+        assertEquals((char)0+(char)1, exec("char x = 0; char y = 1; return x+y;"));
+        assertEquals((char)1+(char)0, exec("char x = 1; char y = 0; return x+y;"));
+        assertEquals((char)0+(char)0, exec("char x = 0; char y = 0; return x+y;"));
+    }
+    
+    public void testCharConst() throws Exception {
+        assertEquals((char)1+(char)1, exec("return (char)1+(char)1;"));
+        assertEquals((char)1+(char)2, exec("return (char)1+(char)2;"));
+        assertEquals((char)5+(char)10, exec("return (char)5+(char)10;"));
+        assertEquals((char)1+(char)1+(char)2, exec("return (char)1+(char)1+(char)2;"));
+        assertEquals(((char)1+(char)1)+(char)2, exec("return ((char)1+(char)1)+(char)2;"));
+        assertEquals((char)1+((char)1+(char)2), exec("return (char)1+((char)1+(char)2);"));
+        assertEquals((char)0+(char)1, exec("return (char)0+(char)1;"));
+        assertEquals((char)1+(char)0, exec("return (char)1+(char)0;"));
+        assertEquals((char)0+(char)0, exec("return (char)0+(char)0;"));
+    }
+    
+    public void testShort() throws Exception {
+        assertEquals((short)1+(short)1, exec("short x = 1; short y = 1; return x+y;"));
+        assertEquals((short)1+(short)2, exec("short x = 1; short y = 2; return x+y;"));
+        assertEquals((short)5+(short)10, exec("short x = 5; short y = 10; return x+y;"));
+        assertEquals((short)1+(short)1+(short)2, exec("short x = 1; short y = 1; short z = 2; return x+y+z;"));
+        assertEquals(((short)1+(short)1)+(short)2, exec("short x = 1; short y = 1; short z = 2; return (x+y)+z;"));
+        assertEquals((short)1+((short)1+(short)2), exec("short x = 1; short y = 1; short z = 2; return x+(y+z);"));
+        assertEquals((short)0+(short)1, exec("short x = 0; short y = 1; return x+y;"));
+        assertEquals((short)1+(short)0, exec("short x = 1; short y = 0; return x+y;"));
+        assertEquals((short)0+(short)0, exec("short x = 0; short y = 0; return x+y;"));
+    }
+    
+    public void testShortConst() throws Exception {
+        assertEquals((short)1+(short)1, exec("return (short)1+(short)1;"));
+        assertEquals((short)1+(short)2, exec("return (short)1+(short)2;"));
+        assertEquals((short)5+(short)10, exec("return (short)5+(short)10;"));
+        assertEquals((short)1+(short)1+(short)2, exec("return (short)1+(short)1+(short)2;"));
+        assertEquals(((short)1+(short)1)+(short)2, exec("return ((short)1+(short)1)+(short)2;"));
+        assertEquals((short)1+((short)1+(short)2), exec("return (short)1+((short)1+(short)2);"));
+        assertEquals((short)0+(short)1, exec("return (short)0+(short)1;"));
+        assertEquals((short)1+(short)0, exec("return (short)1+(short)0;"));
+        assertEquals((short)0+(short)0, exec("return (short)0+(short)0;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L+1L, exec("long x = 1; long y = 1; return x+y;"));
+        assertEquals(1L+2L, exec("long x = 1; long y = 2; return x+y;"));
+        assertEquals(5L+10L, exec("long x = 5; long y = 10; return x+y;"));
+        assertEquals(1L+1L+2L, exec("long x = 1; long y = 1; long z = 2; return x+y+z;"));
+        assertEquals((1L+1L)+2L, exec("long x = 1; long y = 1; long z = 2; return (x+y)+z;"));
+        assertEquals(1L+(1L+2L), exec("long x = 1; long y = 1; long z = 2; return x+(y+z);"));
+        assertEquals(0L+1L, exec("long x = 0; long y = 1; return x+y;"));
+        assertEquals(1L+0L, exec("long x = 1; long y = 0; return x+y;"));
+        assertEquals(0L+0L, exec("long x = 0; long y = 0; return x+y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L+1L, exec("return 1L+1L;"));
+        assertEquals(1L+2L, exec("return 1L+2L;"));
+        assertEquals(5L+10L, exec("return 5L+10L;"));
+        assertEquals(1L+1L+2L, exec("return 1L+1L+2L;"));
+        assertEquals((1L+1L)+2L, exec("return (1L+1L)+2L;"));
+        assertEquals(1L+(1L+2L), exec("return 1L+(1L+2L);"));
+        assertEquals(0L+1L, exec("return 0L+1L;"));
+        assertEquals(1L+0L, exec("return 1L+0L;"));
+        assertEquals(0L+0L, exec("return 0L+0L;"));
+    }
+
+    public void testFloat() throws Exception {
+        assertEquals(1F+1F, exec("float x = 1F; float y = 1F; return x+y;"));
+        assertEquals(1F+2F, exec("float x = 1F; float y = 2F; return x+y;"));
+        assertEquals(5F+10F, exec("float x = 5F; float y = 10F; return x+y;"));
+        assertEquals(1F+1F+2F, exec("float x = 1F; float y = 1F; float z = 2F; return x+y+z;"));
+        assertEquals((1F+1F)+2F, exec("float x = 1F; float y = 1F; float z = 2F; return (x+y)+z;"));
+        assertEquals((1F+1F)+2F, exec("float x = 1F; float y = 1F; float z = 2F; return x+(y+z);"));
+        assertEquals(0F+1F, exec("float x = 0F; float y = 1F; return x+y;"));
+        assertEquals(1F+0F, exec("float x = 1F; float y = 0F; return x+y;"));
+        assertEquals(0F+0F, exec("float x = 0F; float y = 0F; return x+y;"));
+    }
+
+    public void testFloatConst() throws Exception {
+        assertEquals(1F+1F, exec("return 1F+1F;"));
+        assertEquals(1F+2F, exec("return 1F+2F;"));
+        assertEquals(5F+10F, exec("return 5F+10F;"));
+        assertEquals(1F+1F+2F, exec("return 1F+1F+2F;"));
+        assertEquals((1F+1F)+2F, exec("return (1F+1F)+2F;"));
+        assertEquals(1F+(1F+2F), exec("return 1F+(1F+2F);"));
+        assertEquals(0F+1F, exec("return 0F+1F;"));
+        assertEquals(1F+0F, exec("return 1F+0F;"));
+        assertEquals(0F+0F, exec("return 0F+0F;"));
+    }
+
+    public void testDouble() throws Exception {
+        assertEquals(1.0+1.0, exec("double x = 1.0; double y = 1.0; return x+y;"));
+        assertEquals(1.0+2.0, exec("double x = 1.0; double y = 2.0; return x+y;"));
+        assertEquals(5.0+10.0, exec("double x = 5.0; double y = 10.0; return x+y;"));
+        assertEquals(1.0+1.0+2.0, exec("double x = 1.0; double y = 1.0; double z = 2.0; return x+y+z;"));
+        assertEquals((1.0+1.0)+2.0, exec("double x = 1.0; double y = 1.0; double z = 2.0; return (x+y)+z;"));
+        assertEquals(1.0+(1.0+2.0), exec("double x = 1.0; double y = 1.0; double z = 2.0; return x+(y+z);"));
+        assertEquals(0.0+1.0, exec("double x = 0.0; double y = 1.0; return x+y;"));
+        assertEquals(1.0+0.0, exec("double x = 1.0; double y = 0.0; return x+y;"));
+        assertEquals(0.0+0.0, exec("double x = 0.0; double y = 0.0; return x+y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0+1.0, exec("return 1.0+1.0;"));
+        assertEquals(1.0+2.0, exec("return 1.0+2.0;"));
+        assertEquals(5.0+10.0, exec("return 5.0+10.0;"));
+        assertEquals(1.0+1.0+2.0, exec("return 1.0+1.0+2.0;"));
+        assertEquals((1.0+1.0)+2.0, exec("return (1.0+1.0)+2.0;"));
+        assertEquals(1.0+(1.0+2.0), exec("return 1.0+(1.0+2.0);"));
+        assertEquals(0.0+1.0, exec("return 0.0+1.0;"));
+        assertEquals(1.0+0.0, exec("return 1.0+0.0;"));
+        assertEquals(0.0+0.0, exec("return 0.0+0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java
new file mode 100644
index 0000000..6a41684
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for and operator across all types */
+public class AndTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 & 12, exec("int x = 5; int y = 12; return x & y;"));
+        assertEquals(5 & -12, exec("int x = 5; int y = -12; return x & y;"));
+        assertEquals(7 & 15 & 3, exec("int x = 7; int y = 15; int z = 3; return x & y & z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 & 12, exec("return 5 & 12;"));
+        assertEquals(5 & -12, exec("return 5 & -12;"));
+        assertEquals(7 & 15 & 3, exec("return 7 & 15 & 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L & 12L, exec("long x = 5; long y = 12; return x & y;"));
+        assertEquals(5L & -12L, exec("long x = 5; long y = -12; return x & y;"));
+        assertEquals(7L & 15L & 3L, exec("long x = 7; long y = 15; long z = 3; return x & y & z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L & 12L, exec("return 5L & 12L;"));
+        assertEquals(5L & -12L, exec("return 5L & -12L;"));
+        assertEquals(7L & 15L & 3L, exec("return 7L & 15L & 3L;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java
new file mode 100644
index 0000000..6af8ada
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java
@@ -0,0 +1,126 @@
+package org.elasticsearch.plan.a;
+
+import java.util.Collections;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+public class BasicExpressionTests extends ScriptTestCase {
+
+    /** simple tests returning a constant value */
+    public void testReturnConstant() {
+        assertEquals(5, exec("return 5;"));
+        assertEquals(7L, exec("return 7L;"));
+        assertEquals(7.0, exec("return 7.0;"));
+        assertEquals(32.0F, exec("return 32.0F;"));
+        assertEquals((byte)255, exec("return (byte)255;"));
+        assertEquals((short)5, exec("return (short)5;"));
+        assertEquals("string", exec("return \"string\";"));
+        assertEquals(true, exec("return true;"));
+        assertEquals(false, exec("return false;"));
+        assertNull(exec("return null;"));
+    }
+
+    public void testReturnConstantChar() {
+        assertEquals('x', exec("return 'x';"));
+    }
+
+    public void testConstantCharTruncation() {
+        assertEquals('蚠', exec("return (char)100000;"));
+    }
+
+    /** declaring variables for primitive types */
+    public void testDeclareVariable() {
+        assertEquals(5, exec("int i = 5; return i;"));
+        assertEquals(7L, exec("long l = 7; return l;"));
+        assertEquals(7.0, exec("double d = 7; return d;"));
+        assertEquals(32.0F, exec("float f = 32F; return f;"));
+        assertEquals((byte)255, exec("byte b = (byte)255; return b;"));
+        assertEquals((short)5, exec("short s = (short)5; return s;"));
+        assertEquals("string", exec("String s = \"string\"; return s;"));
+        assertEquals(true, exec("boolean v = true; return v;"));
+        assertEquals(false, exec("boolean v = false; return v;"));
+    }
+
+    public void testCast() {
+        assertEquals(1, exec("return (int)1.0;"));
+        assertEquals((byte)100, exec("double x = 100; return (byte)x;"));
+
+        assertEquals(3, exec(
+                "Map x = new HashMap();\n" +
+                "Object y = x;\n" +
+                "((Map)y).put(2, 3);\n" +
+                "return x.get(2);\n"));
+    }
+
+    public void testCat() {
+        assertEquals("aaabbb", exec("return \"aaa\" + \"bbb\";"));
+        assertEquals("aaabbb", exec("String aaa = \"aaa\", bbb = \"bbb\"; return aaa + bbb;"));
+
+        assertEquals("aaabbbbbbbbb", exec(
+                "String aaa = \"aaa\", bbb = \"bbb\"; int x;\n" +
+                "for (; x < 3; ++x) \n" +
+                "    aaa += bbb;\n" +
+                "return aaa;"));
+    }
+
+    public void testComp() {
+        assertEquals(true, exec("return 2 < 3;"));
+        assertEquals(false, exec("int x = 4; char y = 2; return x < y;"));
+        assertEquals(true, exec("return 3 <= 3;"));
+        assertEquals(true, exec("int x = 3; char y = 3; return x <= y;"));
+        assertEquals(false, exec("return 2 > 3;"));
+        assertEquals(true, exec("int x = 4; long y = 2; return x > y;"));
+        assertEquals(false, exec("return 3 >= 4;"));
+        assertEquals(true, exec("double x = 3; float y = 3; return x >= y;"));
+        assertEquals(false, exec("return 3 == 4;"));
+        assertEquals(true, exec("double x = 3; float y = 3; return x == y;"));
+        assertEquals(true, exec("return 3 != 4;"));
+        assertEquals(false, exec("double x = 3; float y = 3; return x != y;"));
+    }
+    
+    /** 
+     * Test boxed objects in various places
+     */
+    public void testBoxing() {
+        // return
+        assertEquals(4, exec("return input.get(\"x\");", Collections.singletonMap("x", 4)));
+        // assignment
+        assertEquals(4, exec("int y = (Integer)input.get(\"x\"); return y;", Collections.singletonMap("x", 4)));
+        // comparison
+        assertEquals(true, exec("return 5 > (Integer)input.get(\"x\");", Collections.singletonMap("x", 4)));
+    }
+
+    public void testBool() {
+        assertEquals(true, exec("return true && true;"));
+        assertEquals(false, exec("boolean a = true, b = false; return a && b;"));
+        assertEquals(true, exec("return true || true;"));
+        assertEquals(true, exec("boolean a = true, b = false; return a || b;"));
+    }
+
+    public void testConditional() {
+        assertEquals(1, exec("int x = 5; return x > 3 ? 1 : 0;"));
+        assertEquals(0, exec("String a = null; return a != null ? 1 : 0;"));
+    }
+
+    public void testPrecedence() {
+        assertEquals(2, exec("int x = 5; return (x+x)/x;"));
+        assertEquals(true, exec("boolean t = true, f = false; return t && (f || t);"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java
new file mode 100644
index 0000000..07ad32d
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java
@@ -0,0 +1,178 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class BasicStatementTests extends ScriptTestCase {
+
+    public void testIfStatement() {
+        assertEquals(1, exec("int x = 5; if (x == 5) return 1; return 0;"));
+        assertEquals(0, exec("int x = 4; if (x == 5) return 1; else return 0;"));
+        assertEquals(2, exec("int x = 4; if (x == 5) return 1; else if (x == 4) return 2; else return 0;"));
+        assertEquals(1, exec("int x = 4; if (x == 5) return 1; else if (x == 4) return 1; else return 0;"));
+
+        assertEquals(3, exec(
+                "int x = 5;\n" +
+                "if (x == 5) {\n" +
+                "    int y = 2;\n" +
+                "    \n" +
+                "    if (y == 2) {\n" +
+                "        x = 3;\n" +
+                "    }\n" +
+                "    \n" +
+                "}\n" +
+                "\n" +
+                "return x;\n"));
+    }
+
+    public void testWhileStatement() {
+
+        assertEquals("aaaaaa", exec("String c = \"a\"; int x; while (x < 5) { c += \"a\"; ++x; } return c;"));
+
+        Object value = exec(
+                " byte[][] b = new byte[5][5];       \n" +
+                " byte x = 0, y;                     \n" +
+                "                                    \n" +
+                " while (x < 5) {                    \n" +
+                "     y = 0;                         \n" +
+                "                                    \n" +
+                "     while (y < 5) {                \n" +
+                "         b[x][y] = (byte)(x*y);     \n" +
+                "         ++y;                       \n" +
+                "     }                              \n" +
+                "                                    \n" +
+                "     ++x;                           \n" +
+                " }                                  \n" +
+                "                                    \n" +
+                " return b;                          \n");
+
+        byte[][] b = (byte[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDoWhileStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\"; int x; do { c += \"a\"; ++x; } while (x < 5); return c;"));
+
+        Object value = exec(
+                " int[][] b = new int[5][5]; \n" +
+                " int x = 0, y;                    \n" +
+                "                                  \n" +
+                " do {                             \n" +
+                "     y = 0;                       \n" +
+                "                                  \n" +
+                "     do {                         \n" +
+                "         b[x][y] = x*y;           \n" +
+                "         ++y;                     \n" +
+                "     } while (y < 5);             \n" +
+                "                                  \n" +
+                "     ++x;                         \n" +
+                " } while (x < 5);                 \n" +
+                "                                  \n" +
+                " return b;                        \n");
+
+        int[][] b = (int[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testForStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\"; for (int x = 0; x < 5; ++x) c += \"a\"; return c;"));
+
+        Object value = exec(
+                " int[][] b = new int[5][5];  \n" +
+                " for (int x = 0; x < 5; ++x) {     \n" +
+                "     for (int y = 0; y < 5; ++y) { \n" +
+                "         b[x][y] = x*y;            \n" +
+                "     }                             \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return b;                         \n");
+
+        int[][] b = (int[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDeclarationStatement() {
+        assertEquals((byte)2, exec("byte a = 2; return a;"));
+        assertEquals((short)2, exec("short a = 2; return a;"));
+        assertEquals((char)2, exec("char a = 2; return a;"));
+        assertEquals(2, exec("int a = 2; return a;"));
+        assertEquals(2L, exec("long a = 2; return a;"));
+        assertEquals(2F, exec("float a = 2; return a;"));
+        assertEquals(2.0, exec("double a = 2; return a;"));
+        assertEquals(false, exec("boolean a = false; return a;"));
+        assertEquals("string", exec("String a = \"string\"; return a;"));
+        assertEquals(HashMap.class, exec("Map<String,Object> a = new HashMap<String,Object>(); return a;").getClass());
+
+        assertEquals(byte[].class, exec("byte[] a = new byte[1]; return a;").getClass());
+        assertEquals(short[].class, exec("short[] a = new short[1]; return a;").getClass());
+        assertEquals(char[].class, exec("char[] a = new char[1]; return a;").getClass());
+        assertEquals(int[].class, exec("int[] a = new int[1]; return a;").getClass());
+        assertEquals(long[].class, exec("long[] a = new long[1]; return a;").getClass());
+        assertEquals(float[].class, exec("float[] a = new float[1]; return a;").getClass());
+        assertEquals(double[].class, exec("double[] a = new double[1]; return a;").getClass());
+        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1]; return a;").getClass());
+        assertEquals(String[].class, exec("String[] a = new String[1]; return a;").getClass());
+        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1]; return a;").getClass());
+
+        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2]; return a;").getClass());
+        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3]; return a;").getClass());
+        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4]; return a;").getClass());
+        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5]; return a;").getClass());
+        assertEquals(long[][].class, exec("long[][] a = new long[1][2]; return a;").getClass());
+        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3]; return a;").getClass());
+        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4]; return a;").getClass());
+        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5]; return a;").getClass());
+        assertEquals(String[][].class, exec("String[][] a = new String[1][2]; return a;").getClass());
+        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3]; return a;").getClass());
+    }
+
+    public void testContinueStatement() {
+        assertEquals(9, exec("int x = 0, y = 0; while (x < 10) { ++x; if (x == 1) continue; ++y; } return y;"));
+    }
+
+    public void testBreakStatement() {
+        assertEquals(4, exec("int x = 0, y = 0; while (x < 10) { ++x; if (x == 5) break; ++y; } return y;"));
+    }
+
+    public void testReturnStatement() {
+        assertEquals(10, exec("return 10;"));
+        assertEquals(5, exec("int x = 5; return x;"));
+        assertEquals(4, exec("int[] x = new int[2]; x[1] = 4; return x[1];"));
+        assertEquals(5, ((short[])exec("short[] s = new short[3]; s[1] = 5; return s;"))[1]);
+        assertEquals(10, ((Map)exec("Map<String, Object> s = new HashMap< String , Object >(); s.put(\"x\", 10); return s;")).get("x"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java
new file mode 100644
index 0000000..032cdcd
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java
@@ -0,0 +1,294 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** 
+ * Tests binary operators across different types
+ */
+// TODO: NaN/Inf/overflow/...
+public class BinaryOperatorTests extends ScriptTestCase {
+    
+    // TODO: move to per-type tests and test for each type
+    public void testBasics() {
+        assertEquals(2.25F / 1.5F, exec("return 2.25F / 1.5F;"));
+        assertEquals(2.25F % 1.5F, exec("return 2.25F % 1.5F;"));
+        assertEquals(2 - 1, exec("return 2 - 1;"));
+        assertEquals(1 << 2, exec("return 1 << 2;"));
+        assertEquals(4 >> 2, exec("return 4 >> 2;"));
+        assertEquals(-1 >>> 29, exec("return -1 >>> 29;"));
+        assertEquals(5 & 3, exec("return 5 & 3;"));
+        assertEquals(5 & 3L, exec("return 5 & 3L;"));
+        assertEquals(5L & 3, exec("return 5L & 3;"));
+        assertEquals(5 | 3, exec("return 5 | 3;"));
+        assertEquals(5L | 3, exec("return 5L | 3;"));
+        assertEquals(5 | 3L, exec("return 5 | 3L;"));
+        assertEquals(9 ^ 3, exec("return 9 ^ 3;"));
+        assertEquals(9L ^ 3, exec("return 9L ^ 3;"));
+        assertEquals(9 ^ 3L, exec("return 9 ^ 3L;"));
+    }
+    
+    public void testLongShifts() {
+        // note: we always promote the results of shifts too (unlike java)
+        assertEquals(1L << 2, exec("long x = 1L; int y = 2; return x << y;"));
+        assertEquals(1L << 2L, exec("long x = 1L; long y = 2L; return x << y;"));
+        assertEquals(4L >> 2L, exec("long x = 4L; long y = 2L; return x >> y;"));
+        assertEquals(4L >> 2, exec("long x = 4L; int y = 2; return x >> y;"));
+        assertEquals(-1L >>> 29, exec("long x = -1L; int y = 29; return x >>> y;"));
+        assertEquals(-1L >>> 29L, exec("long x = -1L; long y = 29L; return x >>> y;"));
+    }
+    
+    public void testLongShiftsConst() {
+        // note: we always promote the results of shifts too (unlike java)
+        assertEquals(1L << 2, exec("return 1L << 2;"));
+        assertEquals(1L << 2L, exec("return 1 << 2L;"));
+        assertEquals(4L >> 2L, exec("return 4 >> 2L;"));
+        assertEquals(4L >> 2, exec("return 4L >> 2;"));
+        assertEquals(-1L >>> 29, exec("return -1L >>> 29;"));
+        assertEquals(-1L >>> 29L, exec("return -1 >>> 29L;"));
+    }
+    
+    public void testMixedTypes() {
+        assertEquals(8, exec("int x = 4; char y = 2; return x*y;"));
+        assertEquals(0.5, exec("double x = 1; float y = 2; return x / y;"));
+        assertEquals(1, exec("int x = 3; int y = 2; return x % y;"));
+        assertEquals(3.0, exec("double x = 1; byte y = 2; return x + y;"));
+        assertEquals(-1, exec("int x = 1; char y = 2; return x - y;"));
+        assertEquals(4, exec("int x = 1; char y = 2; return x << y;"));
+        assertEquals(-1, exec("int x = -1; char y = 29; return x >> y;"));
+        assertEquals(3, exec("int x = -1; char y = 30; return x >>> y;"));
+        assertEquals(1L, exec("int x = 5; long y = 3; return x & y;"));
+        assertEquals(7, exec("short x = 5; byte y = 3; return x | y;"));
+        assertEquals(10, exec("short x = 9; char y = 3; return x ^ y;"));
+    }
+    
+    public void testBinaryPromotion() throws Exception {
+        // byte/byte
+        assertEquals((byte)1 + (byte)1, exec("byte x = 1; byte y = 1; return x+y;"));
+        // byte/char
+        assertEquals((byte)1 + (char)1, exec("byte x = 1; char y = 1; return x+y;"));
+        // byte/short
+        assertEquals((byte)1 + (short)1, exec("byte x = 1; short y = 1; return x+y;"));
+        // byte/int
+        assertEquals((byte)1 + 1, exec("byte x = 1; int y = 1; return x+y;"));
+        // byte/long
+        assertEquals((byte)1 + 1L, exec("byte x = 1; long y = 1; return x+y;"));
+        // byte/float
+        assertEquals((byte)1 + 1F, exec("byte x = 1; float y = 1; return x+y;"));
+        // byte/double
+        assertEquals((byte)1 + 1.0, exec("byte x = 1; double y = 1; return x+y;"));
+        
+        // char/byte
+        assertEquals((char)1 + (byte)1, exec("char x = 1; byte y = 1; return x+y;"));
+        // char/char
+        assertEquals((char)1 + (char)1, exec("char x = 1; char y = 1; return x+y;"));
+        // char/short
+        assertEquals((char)1 + (short)1, exec("char x = 1; short y = 1; return x+y;"));
+        // char/int
+        assertEquals((char)1 + 1, exec("char x = 1; int y = 1; return x+y;"));
+        // char/long
+        assertEquals((char)1 + 1L, exec("char x = 1; long y = 1; return x+y;"));
+        // char/float
+        assertEquals((char)1 + 1F, exec("char x = 1; float y = 1; return x+y;"));
+        // char/double
+        assertEquals((char)1 + 1.0, exec("char x = 1; double y = 1; return x+y;"));
+        
+        // short/byte
+        assertEquals((short)1 + (byte)1, exec("short x = 1; byte y = 1; return x+y;"));
+        // short/char
+        assertEquals((short)1 + (char)1, exec("short x = 1; char y = 1; return x+y;"));
+        // short/short
+        assertEquals((short)1 + (short)1, exec("short x = 1; short y = 1; return x+y;"));
+        // short/int
+        assertEquals((short)1 + 1, exec("short x = 1; int y = 1; return x+y;"));
+        // short/long
+        assertEquals((short)1 + 1L, exec("short x = 1; long y = 1; return x+y;"));
+        // short/float
+        assertEquals((short)1 + 1F, exec("short x = 1; float y = 1; return x+y;"));
+        // short/double
+        assertEquals((short)1 + 1.0, exec("short x = 1; double y = 1; return x+y;"));
+        
+        // int/byte
+        assertEquals(1 + (byte)1, exec("int x = 1; byte y = 1; return x+y;"));
+        // int/char
+        assertEquals(1 + (char)1, exec("int x = 1; char y = 1; return x+y;"));
+        // int/short
+        assertEquals(1 + (short)1, exec("int x = 1; short y = 1; return x+y;"));
+        // int/int
+        assertEquals(1 + 1, exec("int x = 1; int y = 1; return x+y;"));
+        // int/long
+        assertEquals(1 + 1L, exec("int x = 1; long y = 1; return x+y;"));
+        // int/float
+        assertEquals(1 + 1F, exec("int x = 1; float y = 1; return x+y;"));
+        // int/double
+        assertEquals(1 + 1.0, exec("int x = 1; double y = 1; return x+y;"));
+        
+        // long/byte
+        assertEquals(1L + (byte)1, exec("long x = 1; byte y = 1; return x+y;"));
+        // long/char
+        assertEquals(1L + (char)1, exec("long x = 1; char y = 1; return x+y;"));
+        // long/short
+        assertEquals(1L + (short)1, exec("long x = 1; short y = 1; return x+y;"));
+        // long/int
+        assertEquals(1L + 1, exec("long x = 1; int y = 1; return x+y;"));
+        // long/long
+        assertEquals(1L + 1L, exec("long x = 1; long y = 1; return x+y;"));
+        // long/float
+        assertEquals(1L + 1F, exec("long x = 1; float y = 1; return x+y;"));
+        // long/double
+        assertEquals(1L + 1.0, exec("long x = 1; double y = 1; return x+y;"));
+        
+        // float/byte
+        assertEquals(1F + (byte)1, exec("float x = 1; byte y = 1; return x+y;"));
+        // float/char
+        assertEquals(1F + (char)1, exec("float x = 1; char y = 1; return x+y;"));
+        // float/short
+        assertEquals(1F + (short)1, exec("float x = 1; short y = 1; return x+y;"));
+        // float/int
+        assertEquals(1F + 1, exec("float x = 1; int y = 1; return x+y;"));
+        // float/long
+        assertEquals(1F + 1L, exec("float x = 1; long y = 1; return x+y;"));
+        // float/float
+        assertEquals(1F + 1F, exec("float x = 1; float y = 1; return x+y;"));
+        // float/double
+        assertEquals(1F + 1.0, exec("float x = 1; double y = 1; return x+y;"));
+        
+        // double/byte
+        assertEquals(1.0 + (byte)1, exec("double x = 1; byte y = 1; return x+y;"));
+        // double/char
+        assertEquals(1.0 + (char)1, exec("double x = 1; char y = 1; return x+y;"));
+        // double/short
+        assertEquals(1.0 + (short)1, exec("double x = 1; short y = 1; return x+y;"));
+        // double/int
+        assertEquals(1.0 + 1, exec("double x = 1; int y = 1; return x+y;"));
+        // double/long
+        assertEquals(1.0 + 1L, exec("double x = 1; long y = 1; return x+y;"));
+        // double/float
+        assertEquals(1.0 + 1F, exec("double x = 1; float y = 1; return x+y;"));
+        // double/double
+        assertEquals(1.0 + 1.0, exec("double x = 1; double y = 1; return x+y;"));
+    }
+    
+    public void testBinaryPromotionConst() throws Exception {
+        // byte/byte
+        assertEquals((byte)1 + (byte)1, exec("return (byte)1 + (byte)1;"));
+        // byte/char
+        assertEquals((byte)1 + (char)1, exec("return (byte)1 + (char)1;"));
+        // byte/short
+        assertEquals((byte)1 + (short)1, exec("return (byte)1 + (short)1;"));
+        // byte/int
+        assertEquals((byte)1 + 1, exec("return (byte)1 + 1;"));
+        // byte/long
+        assertEquals((byte)1 + 1L, exec("return (byte)1 + 1L;"));
+        // byte/float
+        assertEquals((byte)1 + 1F, exec("return (byte)1 + 1F;"));
+        // byte/double
+        assertEquals((byte)1 + 1.0, exec("return (byte)1 + 1.0;"));
+        
+        // char/byte
+        assertEquals((char)1 + (byte)1, exec("return (char)1 + (byte)1;"));
+        // char/char
+        assertEquals((char)1 + (char)1, exec("return (char)1 + (char)1;"));
+        // char/short
+        assertEquals((char)1 + (short)1, exec("return (char)1 + (short)1;"));
+        // char/int
+        assertEquals((char)1 + 1, exec("return (char)1 + 1;"));
+        // char/long
+        assertEquals((char)1 + 1L, exec("return (char)1 + 1L;"));
+        // char/float
+        assertEquals((char)1 + 1F, exec("return (char)1 + 1F;"));
+        // char/double
+        assertEquals((char)1 + 1.0, exec("return (char)1 + 1.0;"));
+        
+        // short/byte
+        assertEquals((short)1 + (byte)1, exec("return (short)1 + (byte)1;"));
+        // short/char
+        assertEquals((short)1 + (char)1, exec("return (short)1 + (char)1;"));
+        // short/short
+        assertEquals((short)1 + (short)1, exec("return (short)1 + (short)1;"));
+        // short/int
+        assertEquals((short)1 + 1, exec("return (short)1 + 1;"));
+        // short/long
+        assertEquals((short)1 + 1L, exec("return (short)1 + 1L;"));
+        // short/float
+        assertEquals((short)1 + 1F, exec("return (short)1 + 1F;"));
+        // short/double
+        assertEquals((short)1 + 1.0, exec("return (short)1 + 1.0;"));
+        
+        // int/byte
+        assertEquals(1 + (byte)1, exec("return 1 + (byte)1;"));
+        // int/char
+        assertEquals(1 + (char)1, exec("return 1 + (char)1;"));
+        // int/short
+        assertEquals(1 + (short)1, exec("return 1 + (short)1;"));
+        // int/int
+        assertEquals(1 + 1, exec("return 1 + 1;"));
+        // int/long
+        assertEquals(1 + 1L, exec("return 1 + 1L;"));
+        // int/float
+        assertEquals(1 + 1F, exec("return 1 + 1F;"));
+        // int/double
+        assertEquals(1 + 1.0, exec("return 1 + 1.0;"));
+        
+        // long/byte
+        assertEquals(1L + (byte)1, exec("return 1L + (byte)1;"));
+        // long/char
+        assertEquals(1L + (char)1, exec("return 1L + (char)1;"));
+        // long/short
+        assertEquals(1L + (short)1, exec("return 1L + (short)1;"));
+        // long/int
+        assertEquals(1L + 1, exec("return 1L + 1;"));
+        // long/long
+        assertEquals(1L + 1L, exec("return 1L + 1L;"));
+        // long/float
+        assertEquals(1L + 1F, exec("return 1L + 1F;"));
+        // long/double
+        assertEquals(1L + 1.0, exec("return 1L + 1.0;"));
+        
+        // float/byte
+        assertEquals(1F + (byte)1, exec("return 1F + (byte)1;"));
+        // float/char
+        assertEquals(1F + (char)1, exec("return 1F + (char)1;"));
+        // float/short
+        assertEquals(1F + (short)1, exec("return 1F + (short)1;"));
+        // float/int
+        assertEquals(1F + 1, exec("return 1F + 1;"));
+        // float/long
+        assertEquals(1F + 1L, exec("return 1F + 1L;"));
+        // float/float
+        assertEquals(1F + 1F, exec("return 1F + 1F;"));
+        // float/double
+        assertEquals(1F + 1.0, exec("return 1F + 1.0;"));
+        
+        // double/byte
+        assertEquals(1.0 + (byte)1, exec("return 1.0 + (byte)1;"));
+        // double/char
+        assertEquals(1.0 + (char)1, exec("return 1.0 + (char)1;"));
+        // double/short
+        assertEquals(1.0 + (short)1, exec("return 1.0 + (short)1;"));
+        // double/int
+        assertEquals(1.0 + 1, exec("return 1.0 + 1;"));
+        // double/long
+        assertEquals(1.0 + 1L, exec("return 1.0 + 1L;"));
+        // double/float
+        assertEquals(1.0 + 1F, exec("return 1.0 + 1F;"));
+        // double/double
+        assertEquals(1.0 + 1.0, exec("return 1.0 + 1.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java
new file mode 100644
index 0000000..3af440a
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java
@@ -0,0 +1,319 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/**
+ * Tests compound assignments (+=, etc) across all data types
+ */
+public class CompoundAssignmentTests extends ScriptTestCase {
+    public void testAddition() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x += 10; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x += -10; return x;"));
+
+        // short
+        assertEquals((short) 15, exec("short x = 5; x += 10; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x += -10; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x += 10; return x;"));
+        assertEquals((char) 5, exec("char x = 10; x += -5; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x += 10; return x;"));
+        assertEquals(-5, exec("int x = 5; x += -10; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x += 10; return x;"));
+        assertEquals(-5L, exec("long x = 5; x += -10; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x += 10; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x += -10; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x += 10; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x += -10; return x;"));
+    }
+    
+    public void testSubtraction() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x -= -10; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x -= 10; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 5; x -= -10; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x -= 10; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x -= -10; return x;"));
+        assertEquals((char) 5, exec("char x = 10; x -= 5; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x -= -10; return x;"));
+        assertEquals(-5, exec("int x = 5; x -= 10; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x -= -10; return x;"));
+        assertEquals(-5L, exec("long x = 5; x -= 10; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x -= -10; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x -= 10; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x -= -10; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x -= 10; return x;"));
+    }
+    
+    public void testMultiplication() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x *= 3; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x *= -1; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 5; x *= 3; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x *= -1; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x *= 3; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x *= 3; return x;"));
+        assertEquals(-5, exec("int x = 5; x *= -1; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x *= 3; return x;"));
+        assertEquals(-5L, exec("long x = 5; x *= -1; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x *= 3; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x *= -1; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x *= 3; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x *= -1; return x;"));
+    }
+    
+    public void testDivision() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 45; x /= 3; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x /= -1; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 45; x /= 3; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x /= -1; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 45; x /= 3; return x;"));
+        // int
+        assertEquals(15, exec("int x = 45; x /= 3; return x;"));
+        assertEquals(-5, exec("int x = 5; x /= -1; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 45; x /= 3; return x;"));
+        assertEquals(-5L, exec("long x = 5; x /= -1; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 45f; x /= 3; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x /= -1; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 45.0; x /= 3; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x /= -1; return x;"));
+    }
+    
+    public void testDivisionByZero() {
+        // byte
+        try {
+            exec("byte x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        // short
+        try {
+            exec("short x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainder() {
+        // byte
+        assertEquals((byte) 3, exec("byte x = 15; x %= 4; return x;"));
+        assertEquals((byte) -3, exec("byte x = (byte) -15; x %= 4; return x;"));
+        // short
+        assertEquals((short) 3, exec("short x = 15; x %= 4; return x;"));
+        assertEquals((short) -3, exec("short x = (short) -15; x %= 4; return x;"));
+        // char
+        assertEquals((char) 3, exec("char x = (char) 15; x %= 4; return x;"));
+        // int
+        assertEquals(3, exec("int x = 15; x %= 4; return x;"));
+        assertEquals(-3, exec("int x = -15; x %= 4; return x;"));
+        // long
+        assertEquals(3L, exec("long x = 15L; x %= 4; return x;"));
+        assertEquals(-3L, exec("long x = -15L; x %= 4; return x;"));
+        // float
+        assertEquals(3F, exec("float x = 15F; x %= 4; return x;"));
+        assertEquals(-3F, exec("float x = -15F; x %= 4; return x;"));
+        // double
+        assertEquals(3D, exec("double x = 15.0; x %= 4; return x;"));
+        assertEquals(-3D, exec("double x = -15.0; x %= 4; return x;"));
+    }
+
+    public void testLeftShift() {
+        // byte
+        assertEquals((byte) 60, exec("byte x = 15; x <<= 2; return x;"));
+        assertEquals((byte) -60, exec("byte x = (byte) -15; x <<= 2; return x;"));
+        // short
+        assertEquals((short) 60, exec("short x = 15; x <<= 2; return x;"));
+        assertEquals((short) -60, exec("short x = (short) -15; x <<= 2; return x;"));
+        // char
+        assertEquals((char) 60, exec("char x = (char) 15; x <<= 2; return x;"));
+        // int
+        assertEquals(60, exec("int x = 15; x <<= 2; return x;"));
+        assertEquals(-60, exec("int x = -15; x <<= 2; return x;"));
+        // long
+        assertEquals(60L, exec("long x = 15L; x <<= 2; return x;"));
+        assertEquals(-60L, exec("long x = -15L; x <<= 2; return x;"));
+    }
+    
+    public void testRightShift() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 60; x >>= 2; return x;"));
+        assertEquals((byte) -15, exec("byte x = (byte) -60; x >>= 2; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 60; x >>= 2; return x;"));
+        assertEquals((short) -15, exec("short x = (short) -60; x >>= 2; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = (char) 60; x >>= 2; return x;"));
+        // int
+        assertEquals(15, exec("int x = 60; x >>= 2; return x;"));
+        assertEquals(-15, exec("int x = -60; x >>= 2; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 60L; x >>= 2; return x;"));
+        assertEquals(-15L, exec("long x = -60L; x >>= 2; return x;"));
+    }
+    
+    public void testUnsignedRightShift() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 60; x >>>= 2; return x;"));
+        assertEquals((byte) -15, exec("byte x = (byte) -60; x >>>= 2; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 60; x >>>= 2; return x;"));
+        assertEquals((short) -15, exec("short x = (short) -60; x >>>= 2; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = (char) 60; x >>>= 2; return x;"));
+        // int
+        assertEquals(15, exec("int x = 60; x >>>= 2; return x;"));
+        assertEquals(-60 >>> 2, exec("int x = -60; x >>>= 2; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 60L; x >>>= 2; return x;"));
+        assertEquals(-60L >>> 2, exec("long x = -60L; x >>>= 2; return x;"));
+    }
+
+    public void testAnd() {
+        // boolean
+        assertEquals(true, exec("boolean x = true; x &= true; return x;"));
+        assertEquals(false, exec("boolean x = true; x &= false; return x;"));
+        assertEquals(false, exec("boolean x = false; x &= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x &= false; return x;"));
+        assertEquals(true, exec("Boolean x = true; x &= true; return x;"));
+        assertEquals(false, exec("Boolean x = true; x &= false; return x;"));
+        assertEquals(false, exec("Boolean x = false; x &= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x &= false; return x;"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] &= false; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] &= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] &= false; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] &= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 & 14), exec("byte x = 13; x &= 14; return x;"));
+        // short
+        assertEquals((short) (13 & 14), exec("short x = 13; x &= 14; return x;"));
+        // char
+        assertEquals((char) (13 & 14), exec("char x = 13; x &= 14; return x;"));
+        // int
+        assertEquals(13 & 14, exec("int x = 13; x &= 14; return x;"));
+        // long
+        assertEquals((long) (13 & 14), exec("long x = 13L; x &= 14; return x;"));
+    }
+    
+    public void testOr() {
+        // boolean
+        assertEquals(true, exec("boolean x = true; x |= true; return x;"));
+        assertEquals(true, exec("boolean x = true; x |= false; return x;"));
+        assertEquals(true, exec("boolean x = false; x |= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x |= false; return x;"));
+        assertEquals(true, exec("Boolean x = true; x |= true; return x;"));
+        assertEquals(true, exec("Boolean x = true; x |= false; return x;"));
+        assertEquals(true, exec("Boolean x = false; x |= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x |= false; return x;"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] |= true; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] |= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] |= true; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] |= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] |= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 | 14), exec("byte x = 13; x |= 14; return x;"));
+        // short
+        assertEquals((short) (13 | 14), exec("short x = 13; x |= 14; return x;"));
+        // char
+        assertEquals((char) (13 | 14), exec("char x = 13; x |= 14; return x;"));
+        // int
+        assertEquals(13 | 14, exec("int x = 13; x |= 14; return x;"));
+        // long
+        assertEquals((long) (13 | 14), exec("long x = 13L; x |= 14; return x;"));
+    }
+    
+    public void testXor() {
+        // boolean
+        assertEquals(false, exec("boolean x = true; x ^= true; return x;"));
+        assertEquals(true, exec("boolean x = true; x ^= false; return x;"));
+        assertEquals(true, exec("boolean x = false; x ^= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x ^= false; return x;"));
+        assertEquals(false, exec("Boolean x = true; x ^= true; return x;"));
+        assertEquals(true, exec("Boolean x = true; x ^= false; return x;"));
+        assertEquals(true, exec("Boolean x = false; x ^= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x ^= false; return x;"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] ^= true; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] ^= false; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] ^= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] ^= false; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] ^= true; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] ^= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] ^= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] ^= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 ^ 14), exec("byte x = 13; x ^= 14; return x;"));
+        // short
+        assertEquals((short) (13 ^ 14), exec("short x = 13; x ^= 14; return x;"));
+        // char
+        assertEquals((char) (13 ^ 14), exec("char x = 13; x ^= 14; return x;"));
+        // int
+        assertEquals(13 ^ 14, exec("int x = 13; x ^= 14; return x;"));
+        // long
+        assertEquals((long) (13 ^ 14), exec("long x = 13L; x ^= 14; return x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java
new file mode 100644
index 0000000..bc46642
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+
+public class ConditionalTests extends ScriptTestCase {
+    public void testBasic() {
+        assertEquals(2, exec("boolean x = true; return x ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false; return x ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false, y = true; return x && y ? 2 : 3;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x && y ? 2 : 3;"));
+        assertEquals(2, exec("boolean x = true, y = false; return x || y ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false, y = false; return x || y ? 2 : 3;"));
+    }
+
+    public void testPrecedence() {
+        assertEquals(4, exec("boolean x = false, y = true; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(3, exec("boolean x = true, y = false; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = true; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(3, exec("boolean x = true, y = false; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(3, exec("boolean x = false, y = true; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(2, exec("boolean x = true, y = false; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = false; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = false; return (x ? true : y) ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = true, y = false; return (x ? false : y) ? 3 : 4;"));
+        assertEquals(3, exec("boolean x = false, y = true; return (x ? false : y) ? 3 : 4;"));
+        assertEquals(2, exec("boolean x = true, y = false; return (x ? false : y) ? (x ? 3 : 4) : x ? 2 : 1;"));
+        assertEquals(2, exec("boolean x = true, y = false; return (x ? false : y) ? x ? 3 : 4 : x ? 2 : 1;"));
+        assertEquals(4, exec("boolean x = false, y = true; return x ? false : y ? x ? 3 : 4 : x ? 2 : 1;"));
+    }
+
+    public void testAssignment() {
+        assertEquals(4D, exec("boolean x = false; double z = x ? 2 : 4.0F; return z;"));
+        assertEquals((byte)7, exec("boolean x = false; int y = 2; byte z = x ? (byte)y : 7; return z;"));
+        assertEquals((byte)7, exec("boolean x = false; int y = 2; byte z = (byte)(x ? y : 7); return z;"));
+        assertEquals(ArrayList.class, exec("boolean x = false; Object z = x ? new HashMap() : new ArrayList(); return z;").getClass());
+    }
+
+    public void testNullArguments() {
+        assertEquals(null, exec("boolean b = false, c = true; Object x; Map y; return b && c ? x : y;"));
+        assertEquals(HashMap.class, exec("boolean b = false, c = true; Object x; Map y = new HashMap(); return b && c ? x : y;").getClass());
+    }
+
+    public void testPromotion() {
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? 2 : 4.0F);"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? new Long(2) : new Float(4.0F));"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? new HashMap() : new ArrayList()) == (y ? new Long(2) : new Float(4.0F));"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? new HashMap() : new ArrayList());"));
+    }
+
+    public void testIncompatibleAssignment() {
+        try {
+            exec("boolean x = false; byte z = x ? 2 : 4.0F; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; Map z = x ? 4 : (byte)7; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; Map z = x ? new HashMap() : new ArrayList(); return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; int y = 2; byte z = x ? y : 7; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java
new file mode 100644
index 0000000..6ff5113
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java
@@ -0,0 +1,914 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class DefTests extends ScriptTestCase {
+    public void testNot() {
+        assertEquals(~1, exec("def x = (byte)1 return ~x"));
+        assertEquals(~1, exec("def x = (short)1 return ~x"));
+        assertEquals(~1, exec("def x = (char)1 return ~x"));
+        assertEquals(~1, exec("def x = 1 return ~x"));
+        assertEquals(~1L, exec("def x = 1L return ~x"));
+    }
+
+    public void testNeg() {
+        assertEquals(-1, exec("def x = (byte)1 return -x"));
+        assertEquals(-1, exec("def x = (short)1 return -x"));
+        assertEquals(-1, exec("def x = (char)1 return -x"));
+        assertEquals(-1, exec("def x = 1 return -x"));
+        assertEquals(-1L, exec("def x = 1L return -x"));
+        assertEquals(-1.0F, exec("def x = 1F return -x"));
+        assertEquals(-1.0, exec("def x = 1.0 return -x"));
+    }
+
+    public void testMul() {
+        assertEquals(4, exec("def x = (byte)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (byte)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (byte)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (byte)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (byte)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (short)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (short)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (short)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (short)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (char)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (char)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (char)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (char)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (int)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (int)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (int)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (int)2 return x * y"));
+
+        assertEquals(4L, exec("def x = (byte)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (short)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (char)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (int)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (long)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (long)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (long)2 return x * y"));
+
+        assertEquals(4F, exec("def x = (byte)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (short)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (char)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (int)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (long)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (float)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (float)2 return x * y"));
+
+        assertEquals(4D, exec("def x = (byte)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (short)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (char)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (int)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (long)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (float)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (double)2 return x * y"));
+
+        assertEquals(4, exec("def x = (Byte)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (Short)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (Character)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (Integer)2 def y = (int)2 return x * y"));
+        assertEquals(4L, exec("def x = (Long)2 def y = (long)2 return x * y"));
+        assertEquals(4F, exec("def x = (Float)2 def y = (float)2 return x * y"));
+        assertEquals(4D, exec("def x = (Double)2 def y = (double)2 return x * y"));
+    }
+
+    public void testDiv() {
+        assertEquals(1, exec("def x = (byte)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (byte)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (byte)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (byte)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (byte)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (short)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (short)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (short)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (short)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (char)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (char)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (char)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (char)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (int)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (int)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (int)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (int)2 return x / y"));
+
+        assertEquals(1L, exec("def x = (byte)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (short)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (char)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (int)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (long)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (long)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (long)2 return x / y"));
+
+        assertEquals(1F, exec("def x = (byte)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (short)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (char)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (int)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (long)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (float)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (float)2 return x / y"));
+
+        assertEquals(1D, exec("def x = (byte)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (short)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (char)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (int)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (long)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (float)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (double)2 return x / y"));
+
+        assertEquals(1, exec("def x = (Byte)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (Short)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (Character)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (Integer)2 def y = (int)2 return x / y"));
+        assertEquals(1L, exec("def x = (Long)2 def y = (long)2 return x / y"));
+        assertEquals(1F, exec("def x = (Float)2 def y = (float)2 return x / y"));
+        assertEquals(1D, exec("def x = (Double)2 def y = (double)2 return x / y"));
+    }
+
+    public void testRem() {
+        assertEquals(0, exec("def x = (byte)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (byte)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (byte)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (byte)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (byte)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (short)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (short)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (short)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (short)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (char)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (char)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (char)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (char)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (int)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (int)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (int)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (int)2 return x % y"));
+
+        assertEquals(0L, exec("def x = (byte)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (short)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (char)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (int)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (long)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (long)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (long)2 return x % y"));
+
+        assertEquals(0F, exec("def x = (byte)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (short)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (char)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (int)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (long)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (float)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (float)2 return x % y"));
+
+        assertEquals(0D, exec("def x = (byte)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (short)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (char)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (int)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (long)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (float)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (double)2 return x % y"));
+
+        assertEquals(0, exec("def x = (Byte)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (Short)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (Character)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (Integer)2 def y = (int)2 return x % y"));
+        assertEquals(0L, exec("def x = (Long)2 def y = (long)2 return x % y"));
+        assertEquals(0F, exec("def x = (Float)2 def y = (float)2 return x % y"));
+        assertEquals(0D, exec("def x = (Double)2 def y = (double)2 return x % y"));
+    }
+    
+    public void testAdd() {
+        assertEquals(2, exec("def x = (byte)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (byte)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (byte)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (byte)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (byte)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (short)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (short)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (short)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (short)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (char)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (char)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (char)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (char)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (int)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (int)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (int)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (int)1 return x + y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (long)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (long)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (long)1 return x + y"));
+
+        assertEquals(2F, exec("def x = (byte)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (short)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (char)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (int)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (long)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (float)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (float)1 return x + y"));
+
+        assertEquals(2D, exec("def x = (byte)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (short)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (char)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (int)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (long)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (float)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (double)1 return x + y"));
+
+        assertEquals(2, exec("def x = (Byte)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (Short)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (Character)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (Integer)1 def y = (int)1 return x + y"));
+        assertEquals(2L, exec("def x = (Long)1 def y = (long)1 return x + y"));
+        assertEquals(2F, exec("def x = (Float)1 def y = (float)1 return x + y"));
+        assertEquals(2D, exec("def x = (Double)1 def y = (double)1 return x + y"));
+    }
+
+    public void testSub() {
+        assertEquals(0, exec("def x = (byte)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (byte)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (byte)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (byte)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (byte)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (short)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (short)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (short)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (short)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (char)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (char)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (char)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (char)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (int)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (int)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (int)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (int)1 return x - y"));
+
+        assertEquals(0L, exec("def x = (byte)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (short)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (char)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (int)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (long)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (long)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (long)1 return x - y"));
+
+        assertEquals(0F, exec("def x = (byte)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (short)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (char)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (int)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (long)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (float)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (float)1 return x - y"));
+
+        assertEquals(0D, exec("def x = (byte)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (short)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (char)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (int)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (long)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (float)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (double)1 return x - y"));
+
+        assertEquals(0, exec("def x = (Byte)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (Short)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (Character)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (Integer)1 def y = (int)1 return x - y"));
+        assertEquals(0L, exec("def x = (Long)1 def y = (long)1 return x - y"));
+        assertEquals(0F, exec("def x = (Float)1 def y = (float)1 return x - y"));
+        assertEquals(0D, exec("def x = (Double)1 def y = (double)1 return x - y"));
+    }
+
+    public void testLsh() {
+        assertEquals(2, exec("def x = (byte)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (byte)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (short)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (char)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (int)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (long)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (float)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (double)1 return x << y"));
+
+        assertEquals(2, exec("def x = (Byte)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (Short)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (Character)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (Integer)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (Long)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (Float)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (Double)1 def y = (double)1 return x << y"));
+    }
+
+    public void testRsh() {
+        assertEquals(2, exec("def x = (byte)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (byte)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (short)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (char)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (int)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (long)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (float)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (double)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (Byte)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (Short)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (Character)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (Integer)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Long)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Float)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Double)4 def y = (double)1 return x >> y"));
+    }
+
+    public void testUsh() {
+        assertEquals(2, exec("def x = (byte)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (byte)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (short)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (char)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (int)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (long)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (float)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (double)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (Byte)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Short)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Character)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Integer)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Long)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Float)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Double)4 def y = (double)1 return x >>> y"));
+    }
+
+    public void testAnd() {
+        assertEquals(0, exec("def x = (byte)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (byte)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (short)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (char)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (int)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (long)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (float)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (double)1 return x & y"));
+
+        assertEquals(0, exec("def x = (Byte)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (Short)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (Character)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (Integer)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (Long)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (Float)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (Double)4 def y = (double)1 return x & y"));
+    }
+
+    public void testXor() {
+        assertEquals(5, exec("def x = (byte)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (byte)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (short)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (char)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (int)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (long)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (float)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (double)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (Byte)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Short)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Character)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Integer)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Long)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Float)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Double)4 def y = (double)1 return x ^ y"));
+    }
+
+    public void testOr() {
+        assertEquals(5, exec("def x = (byte)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (byte)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (short)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (char)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (int)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (long)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (float)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (double)1 return x | y"));
+
+        assertEquals(5, exec("def x = (Byte)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (Short)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (Character)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (Integer)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (Long)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (Float)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (Double)4 def y = (double)1 return x | y"));
+    }
+
+    public void testEq() {
+        assertEquals(true, exec("def x = (byte)7 def y = (int)7 return x == y"));
+        assertEquals(true, exec("def x = (short)6 def y = (int)6 return x == y"));
+        assertEquals(true, exec("def x = (char)5 def y = (int)5 return x == y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x == y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x == y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x == y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x == y"));
+
+        assertEquals(true, exec("def x = (byte)7 def y = (double)7 return x == y"));
+        assertEquals(true, exec("def x = (short)6 def y = (double)6 return x == y"));
+        assertEquals(true, exec("def x = (char)5 def y = (double)5 return x == y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x == y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x == y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x == y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x == y"));
+
+        assertEquals(true, exec("def x = new HashMap() def y = new HashMap() return x == y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x == y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x == y"));
+        assertEquals(true, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x == y"));
+    }
+
+    public void testEqr() {
+        assertEquals(false, exec("def x = (byte)7 def y = (int)7 return x === y"));
+        assertEquals(false, exec("def x = (short)6 def y = (int)6 return x === y"));
+        assertEquals(false, exec("def x = (char)5 def y = (int)5 return x === y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x === y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x === y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x === y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x === y"));
+
+        assertEquals(false, exec("def x = new HashMap() def y = new HashMap() return x === y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x === y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x === y"));
+        assertEquals(true, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x === y"));
+    }
+
+    public void testNe() {
+        assertEquals(false, exec("def x = (byte)7 def y = (int)7 return x != y"));
+        assertEquals(false, exec("def x = (short)6 def y = (int)6 return x != y"));
+        assertEquals(false, exec("def x = (char)5 def y = (int)5 return x != y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x != y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x != y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x != y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x != y"));
+
+        assertEquals(false, exec("def x = (byte)7 def y = (double)7 return x != y"));
+        assertEquals(false, exec("def x = (short)6 def y = (double)6 return x != y"));
+        assertEquals(false, exec("def x = (char)5 def y = (double)5 return x != y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x != y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x != y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x != y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x != y"));
+
+        assertEquals(false, exec("def x = new HashMap() def y = new HashMap() return x != y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x != y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x != y"));
+        assertEquals(false, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x != y"));
+    }
+
+    public void testNer() {
+        assertEquals(true, exec("def x = (byte)7 def y = (int)7 return x !== y"));
+        assertEquals(true, exec("def x = (short)6 def y = (int)6 return x !== y"));
+        assertEquals(true, exec("def x = (char)5 def y = (int)5 return x !== y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x !== y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x !== y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x !== y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x !== y"));
+
+        assertEquals(true, exec("def x = new HashMap() def y = new HashMap() return x !== y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x !== y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x !== y"));
+        assertEquals(false, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x !== y"));
+    }
+
+    public void testLt() {
+        assertEquals(true, exec("def x = (byte)1 def y = (int)7 return x < y"));
+        assertEquals(true, exec("def x = (short)2 def y = (int)6 return x < y"));
+        assertEquals(true, exec("def x = (char)3 def y = (int)5 return x < y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x < y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x < y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x < y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x < y"));
+
+        assertEquals(true, exec("def x = (byte)1 def y = (double)7 return x < y"));
+        assertEquals(true, exec("def x = (short)2 def y = (double)6 return x < y"));
+        assertEquals(true, exec("def x = (char)3 def y = (double)5 return x < y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x < y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x < y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x < y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x < y"));
+    }
+
+    public void testLte() {
+        assertEquals(true, exec("def x = (byte)1 def y = (int)7 return x <= y"));
+        assertEquals(true, exec("def x = (short)2 def y = (int)6 return x <= y"));
+        assertEquals(true, exec("def x = (char)3 def y = (int)5 return x <= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x <= y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x <= y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x <= y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x <= y"));
+
+        assertEquals(true, exec("def x = (byte)1 def y = (double)7 return x <= y"));
+        assertEquals(true, exec("def x = (short)2 def y = (double)6 return x <= y"));
+        assertEquals(true, exec("def x = (char)3 def y = (double)5 return x <= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x <= y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x <= y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x <= y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x <= y"));
+    }
+
+    public void testGt() {
+        assertEquals(false, exec("def x = (byte)1 def y = (int)7 return x > y"));
+        assertEquals(false, exec("def x = (short)2 def y = (int)6 return x > y"));
+        assertEquals(false, exec("def x = (char)3 def y = (int)5 return x > y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x > y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x > y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x > y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x > y"));
+
+        assertEquals(false, exec("def x = (byte)1 def y = (double)7 return x > y"));
+        assertEquals(false, exec("def x = (short)2 def y = (double)6 return x > y"));
+        assertEquals(false, exec("def x = (char)3 def y = (double)5 return x > y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x > y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x > y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x > y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x > y"));
+    }
+
+    public void testGte() {
+        assertEquals(false, exec("def x = (byte)1 def y = (int)7 return x >= y"));
+        assertEquals(false, exec("def x = (short)2 def y = (int)6 return x >= y"));
+        assertEquals(false, exec("def x = (char)3 def y = (int)5 return x >= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x >= y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x >= y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x >= y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x >= y"));
+
+        assertEquals(false, exec("def x = (byte)1 def y = (double)7 return x >= y"));
+        assertEquals(false, exec("def x = (short)2 def y = (double)6 return x >= y"));
+        assertEquals(false, exec("def x = (char)3 def y = (double)5 return x >= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x >= y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x >= y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x >= y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x >= y"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java
new file mode 100644
index 0000000..24849fa
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for division operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class DivisionTests extends ScriptTestCase {
+    
+    // TODO: byte,short,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1/1, exec("int x = 1; int y = 1; return x/y;"));
+        assertEquals(2/3, exec("int x = 2; int y = 3; return x/y;"));
+        assertEquals(5/10, exec("int x = 5; int y = 10; return x/y;"));
+        assertEquals(10/1/2, exec("int x = 10; int y = 1; int z = 2; return x/y/z;"));
+        assertEquals((10/1)/2, exec("int x = 10; int y = 1; int z = 2; return (x/y)/z;"));
+        assertEquals(10/(4/2), exec("int x = 10; int y = 4; int z = 2; return x/(y/z);"));
+        assertEquals(10/1, exec("int x = 10; int y = 1; return x/y;"));
+        assertEquals(0/1, exec("int x = 0; int y = 1; return x/y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1/1, exec("return 1/1;"));
+        assertEquals(2/3, exec("return 2/3;"));
+        assertEquals(5/10, exec("return 5/10;"));
+        assertEquals(10/1/2, exec("return 10/1/2;"));
+        assertEquals((10/1)/2, exec("return (10/1)/2;"));
+        assertEquals(10/(4/2), exec("return 10/(4/2);"));
+        assertEquals(10/1, exec("return 10/1;"));
+        assertEquals(0/1, exec("return 0/1;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L/1L, exec("long x = 1; long y = 1; return x/y;"));
+        assertEquals(2L/3L, exec("long x = 2; long y = 3; return x/y;"));
+        assertEquals(5L/10L, exec("long x = 5; long y = 10; return x/y;"));
+        assertEquals(10L/1L/2L, exec("long x = 10; long y = 1; long z = 2; return x/y/z;"));
+        assertEquals((10L/1L)/2L, exec("long x = 10; long y = 1; long z = 2; return (x/y)/z;"));
+        assertEquals(10L/(4L/2L), exec("long x = 10; long y = 4; long z = 2; return x/(y/z);"));
+        assertEquals(10L/1L, exec("long x = 10; long y = 1; return x/y;"));
+        assertEquals(0L/1L, exec("long x = 0; long y = 1; return x/y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L/1L, exec("return 1L/1L;"));
+        assertEquals(2L/3L, exec("return 2L/3L;"));
+        assertEquals(5L/10L, exec("return 5L/10L;"));
+        assertEquals(10L/1L/2L, exec("return 10L/1L/2L;"));
+        assertEquals((10L/1L)/2L, exec("return (10L/1L)/2L;"));
+        assertEquals(10L/(4L/2L), exec("return 10L/(4L/2L);"));
+        assertEquals(10L/1L, exec("return 10L/1L;"));
+        assertEquals(0L/1L, exec("return 0L/1L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F/1F, exec("float x = 1; float y = 1; return x/y;"));
+        assertEquals(2F/3F, exec("float x = 2; float y = 3; return x/y;"));
+        assertEquals(5F/10F, exec("float x = 5; float y = 10; return x/y;"));
+        assertEquals(10F/1F/2F, exec("float x = 10; float y = 1; float z = 2; return x/y/z;"));
+        assertEquals((10F/1F)/2F, exec("float x = 10; float y = 1; float z = 2; return (x/y)/z;"));
+        assertEquals(10F/(4F/2F), exec("float x = 10; float y = 4; float z = 2; return x/(y/z);"));
+        assertEquals(10F/1F, exec("float x = 10; float y = 1; return x/y;"));
+        assertEquals(0F/1F, exec("float x = 0; float y = 1; return x/y;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F/1F, exec("return 1F/1F;"));
+        assertEquals(2F/3F, exec("return 2F/3F;"));
+        assertEquals(5F/10F, exec("return 5F/10F;"));
+        assertEquals(10F/1F/2F, exec("return 10F/1F/2F;"));
+        assertEquals((10F/1F)/2F, exec("return (10F/1F)/2F;"));
+        assertEquals(10F/(4F/2F), exec("return 10F/(4F/2F);"));
+        assertEquals(10F/1F, exec("return 10F/1F;"));
+        assertEquals(0F/1F, exec("return 0F/1F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1.0/1.0, exec("double x = 1; double y = 1; return x/y;"));
+        assertEquals(2.0/3.0, exec("double x = 2; double y = 3; return x/y;"));
+        assertEquals(5.0/10.0, exec("double x = 5; double y = 10; return x/y;"));
+        assertEquals(10.0/1.0/2.0, exec("double x = 10; double y = 1; double z = 2; return x/y/z;"));
+        assertEquals((10.0/1.0)/2.0, exec("double x = 10; double y = 1; double z = 2; return (x/y)/z;"));
+        assertEquals(10.0/(4.0/2.0), exec("double x = 10; double y = 4; double z = 2; return x/(y/z);"));
+        assertEquals(10.0/1.0, exec("double x = 10; double y = 1; return x/y;"));
+        assertEquals(0.0/1.0, exec("double x = 0; double y = 1; return x/y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0/1.0, exec("return 1.0/1.0;"));
+        assertEquals(2.0/3.0, exec("return 2.0/3.0;"));
+        assertEquals(5.0/10.0, exec("return 5.0/10.0;"));
+        assertEquals(10.0/1.0/2.0, exec("return 10.0/1.0/2.0;"));
+        assertEquals((10.0/1.0)/2.0, exec("return (10.0/1.0)/2.0;"));
+        assertEquals(10.0/(4.0/2.0), exec("return 10.0/(4.0/2.0);"));
+        assertEquals(10.0/1.0, exec("return 10.0/1.0;"));
+        assertEquals(0.0/1.0, exec("return 0.0/1.0;"));
+    }
+
+    public void testDivideByZero() throws Exception {
+        try {
+            exec("int x = 1; int y = 0; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("long x = 1L; long y = 0L; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+    
+    public void testDivideByZeroConst() throws Exception {
+        try {
+            exec("return 1/0;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("return 1L/0L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java
new file mode 100644
index 0000000..db83755
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java
@@ -0,0 +1,184 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+// TODO: Figure out a way to test autobox caching properly from methods such as Integer.valueOf(int);
+public class EqualsTests extends ScriptTestCase {
+    public void testTypesEquals() {
+        assertEquals(true, exec("return false === false;"));
+        assertEquals(true, exec("boolean x = false; boolean y = false; return x === y;"));
+        assertEquals(false, exec("return (byte)3 === (byte)4;"));
+        assertEquals(true, exec("byte x = 3; byte y = 3; return x === y;"));
+        assertEquals(false, exec("return (char)3 === (char)4;"));
+        assertEquals(true, exec("char x = 3; char y = 3; return x === y;"));
+        assertEquals(false, exec("return (short)3 === (short)4;"));
+        assertEquals(true, exec("short x = 3; short y = 3; return x === y;"));
+        assertEquals(false, exec("return (int)3 === (int)4;"));
+        assertEquals(true, exec("int x = 3; int y = 3; return x === y;"));
+        assertEquals(false, exec("return (long)3 === (long)4;"));
+        assertEquals(true, exec("long x = 3; long y = 3; return x === y;"));
+        assertEquals(false, exec("return (float)3 === (float)4;"));
+        assertEquals(true, exec("float x = 3; float y = 3; return x === y;"));
+        assertEquals(false, exec("return (double)3 === (double)4;"));
+        assertEquals(true, exec("double x = 3; double y = 3; return x === y;"));
+
+        assertEquals(true, exec("return false == false;"));
+        assertEquals(true, exec("boolean x = false; boolean y = false; return x == y;"));
+        assertEquals(false, exec("return (byte)3 == (byte)4;"));
+        assertEquals(true, exec("byte x = 3; byte y = 3; return x == y;"));
+        assertEquals(false, exec("return (char)3 == (char)4;"));
+        assertEquals(true, exec("char x = 3; char y = 3; return x == y;"));
+        assertEquals(false, exec("return (short)3 == (short)4;"));
+        assertEquals(true, exec("short x = 3; short y = 3; return x == y;"));
+        assertEquals(false, exec("return (int)3 == (int)4;"));
+        assertEquals(true, exec("int x = 3; int y = 3; return x == y;"));
+        assertEquals(false, exec("return (long)3 == (long)4;"));
+        assertEquals(true, exec("long x = 3; long y = 3; return x == y;"));
+        assertEquals(false, exec("return (float)3 == (float)4;"));
+        assertEquals(true, exec("float x = 3; float y = 3; return x == y;"));
+        assertEquals(false, exec("return (double)3 == (double)4;"));
+        assertEquals(true, exec("double x = 3; double y = 3; return x == y;"));
+    }
+
+    public void testTypesNotEquals() {
+        assertEquals(false, exec("return true !== true;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x !== y;"));
+        assertEquals(true, exec("return (byte)3 !== (byte)4;"));
+        assertEquals(false, exec("byte x = 3; byte y = 3; return x !== y;"));
+        assertEquals(true, exec("return (char)3 !== (char)4;"));
+        assertEquals(false, exec("char x = 3; char y = 3; return x !== y;"));
+        assertEquals(true, exec("return (short)3 !== (short)4;"));
+        assertEquals(false, exec("short x = 3; short y = 3; return x !== y;"));
+        assertEquals(true, exec("return (int)3 !== (int)4;"));
+        assertEquals(false, exec("int x = 3; int y = 3; return x !== y;"));
+        assertEquals(true, exec("return (long)3 !== (long)4;"));
+        assertEquals(false, exec("long x = 3; long y = 3; return x !== y;"));
+        assertEquals(true, exec("return (float)3 !== (float)4;"));
+        assertEquals(false, exec("float x = 3; float y = 3; return x !== y;"));
+        assertEquals(true, exec("return (double)3 !== (double)4;"));
+        assertEquals(false, exec("double x = 3; double y = 3; return x !== y;"));
+
+        assertEquals(false, exec("return true != true;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x != y;"));
+        assertEquals(true, exec("return (byte)3 != (byte)4;"));
+        assertEquals(false, exec("byte x = 3; byte y = 3; return x != y;"));
+        assertEquals(true, exec("return (char)3 != (char)4;"));
+        assertEquals(false, exec("char x = 3; char y = 3; return x != y;"));
+        assertEquals(true, exec("return (short)3 != (short)4;"));
+        assertEquals(false, exec("short x = 3; short y = 3; return x != y;"));
+        assertEquals(true, exec("return (int)3 != (int)4;"));
+        assertEquals(false, exec("int x = 3; int y = 3; return x != y;"));
+        assertEquals(true, exec("return (long)3 != (long)4;"));
+        assertEquals(false, exec("long x = 3; long y = 3; return x != y;"));
+        assertEquals(true, exec("return (float)3 != (float)4;"));
+        assertEquals(false, exec("float x = 3; float y = 3; return x != y;"));
+        assertEquals(true, exec("return (double)3 != (double)4;"));
+        assertEquals(false, exec("double x = 3; double y = 3; return x != y;"));
+    }
+
+    public void testEquals() {
+        assertEquals(true, exec("return new Long(3) == new Long(3);"));
+        assertEquals(false, exec("return new Long(3) === new Long(3);"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = x; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = x; return x === y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = new Integer(3); return x == y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = new Integer(3); return x === y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); int y = 3; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); short y = 3; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Short y = (short)3; return x == y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); int y = 3; return x === y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); double y = 3; return x === y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = x; return x == y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = x; return x === y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = new int[1]; return x == y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = new int[1]; return x === y;"));
+        assertEquals(false, exec("Map x = new HashMap(); List y = new ArrayList(); return x == y;"));
+        assertEquals(false, exec("Map x = new HashMap(); List y = new ArrayList(); return x === y;"));
+    }
+
+    public void testNotEquals() {
+        assertEquals(false, exec("return new Long(3) != new Long(3);"));
+        assertEquals(true, exec("return new Long(3) !== new Long(3);"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = x; return x != y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = x; return x !== y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = new Integer(3); return x != y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = new Integer(3); return x !== y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); int y = 3; return x !== y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); double y = 3; return x !== y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = x; return x != y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = x; return x !== y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = new int[1]; return x != y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = new int[1]; return x !== y;"));
+        assertEquals(true, exec("Map x = new HashMap(); List y = new ArrayList(); return x != y;"));
+        assertEquals(true, exec("Map x = new HashMap(); List y = new ArrayList(); return x !== y;"));
+    }
+
+    public void testBranchEquals() {
+        assertEquals(0, exec("Character a = 'a'; Character b = 'b'; if (a == b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Character b = 'a'; if (a == b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = new Integer(1); Integer b = 1; if (a === b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Character b = new Character('a'); if (a === b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Object b = a; if (a === b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = 1; Number b = a; Number c = a; if (c === b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = 1; Character b = 'a'; if (a === (Object)b) return 1; else return 0;"));
+    }
+
+    public void testBranchNotEquals() {
+        assertEquals(1, exec("Character a = 'a'; Character b = 'b'; if (a != b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Character b = 'a'; if (a != b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = new Integer(1); Integer b = 1; if (a !== b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Character b = new Character('a'); if (a !== b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Object b = a; if (a !== b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = 1; Number b = a; Number c = a; if (c !== b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = 1; Character b = 'a'; if (a !== (Object)b) return 1; else return 0;"));
+    }
+
+    public void testRightHandNull() {
+        assertEquals(false, exec("Character a = 'a'; return a == null;"));
+        assertEquals(false, exec("Character a = 'a'; return a === null;"));
+        assertEquals(true, exec("Character a = 'a'; return a != null;"));
+        assertEquals(true, exec("Character a = 'a'; return a !== null;"));
+        assertEquals(true, exec("Character a = null; return a == null;"));
+        assertEquals(false, exec("Character a = null; return a != null;"));
+        assertEquals(false, exec("Character a = 'a'; Character b = null; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return a === b;"));
+        assertEquals(true, exec("Character a = 'a'; Character b = null; return a != b;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return a !== b;"));
+        assertEquals(false, exec("Integer x = null; double y = 2.0; return x == y;"));
+        assertEquals(true, exec("Integer x = null; Short y = null; return x == y;"));
+    }
+
+    public void testLeftHandNull() {
+        assertEquals(false, exec("Character a = 'a'; return null == a;"));
+        assertEquals(false, exec("Character a = 'a'; return null === a;"));
+        assertEquals(true, exec("Character a = 'a'; return null != a;"));
+        assertEquals(true, exec("Character a = 'a'; return null !== a;"));
+        assertEquals(true, exec("Character a = null; return null == a;"));
+        assertEquals(false, exec("Character a = null; return null != a;"));
+        assertEquals(false, exec("Character a = null; Character b = 'a'; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return b === a;"));
+        assertEquals(true, exec("Character a = null; Character b = 'a'; return a != b;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return b != a;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return b !== a;"));
+        assertEquals(false, exec("Integer x = null; double y = 2.0; return y == x;"));
+        assertEquals(true, exec("Integer x = null; Short y = null; return y == x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java
new file mode 100644
index 0000000..7504ed9
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.junit.Before;
+
+public class FieldTests extends ScriptTestCase {
+    public static class FieldClass {
+        public boolean z = false;
+        public byte b = 0;
+        public short s = 1;
+        public char c = 'c';
+        public int i = 2;
+        public int si = -1;
+        public long j = 3l;
+        public float f = 4.0f;
+        public double d = 5.0;
+        public String t = "s";
+        public Object l = new Object();
+
+        public float test(float a, float b) {
+            return Math.min(a, b);
+        }
+
+        public int getSi() {
+            return si;
+        }
+
+        public void setSi(final int si) {
+            this.si = si;
+        }
+    }
+
+    public static class FieldDefinition extends Definition {
+        FieldDefinition() {
+            super();
+
+            addStruct("FieldClass", FieldClass.class);
+            addConstructor("FieldClass", "new", new Type[] {}, null);
+            addField("FieldClass", "z", null, false, booleanType, null);
+            addField("FieldClass", "b", null, false, byteType, null);
+            addField("FieldClass", "s", null, false, shortType, null);
+            addField("FieldClass", "c", null, false, charType, null);
+            addField("FieldClass", "i", null, false, intType, null);
+            addField("FieldClass", "j", null, false, longType, null);
+            addField("FieldClass", "f", null, false, floatType, null);
+            addField("FieldClass", "d", null, false, doubleType, null);
+            addField("FieldClass", "t", null, false, stringType, null);
+            addField("FieldClass", "l", null, false, objectType, null);
+            addClass("FieldClass");
+            addMethod("FieldClass", "getSi", null, false, intType, new Type[] {}, null, null);
+            addMethod("FieldClass", "setSi", null, false, voidType, new Type[] {intType}, null, null);
+            addMethod("FieldClass", "test", null, false, floatType, new Type[] {floatType, floatType}, null, null);
+        }
+    }
+
+    @Before
+    public void setDefinition() {
+        scriptEngine.setDefinition(new FieldDefinition());
+    }
+
+    public void testIntField() {
+        assertEquals("s5t42", exec("def fc = new FieldClass() return fc.t += 2 + fc.j + \"t\" + 4 + (3 - 1)"));
+        assertEquals(2.0f, exec("def fc = new FieldClass(); def l = new Double(3) Byte b = new Byte((byte)2) return fc.test(l, b)"));
+        assertEquals(4, exec("def fc = new FieldClass() fc.i = 4 return fc.i"));
+        assertEquals(5, exec("FieldClass fc0 = new FieldClass() FieldClass fc1 = new FieldClass() fc0.i = 7 - fc0.i fc1.i = fc0.i return fc1.i"));
+        assertEquals(8, exec("def fc0 = new FieldClass() def fc1 = new FieldClass() fc0.i += fc1.i fc0.i += fc0.i return fc0.i"));
+    }
+
+    public void testExplicitShortcut() {
+        assertEquals(5, exec("FieldClass fc = new FieldClass() fc.setSi(5) return fc.si"));
+        assertEquals(-1, exec("FieldClass fc = new FieldClass() def x = fc.getSi() x"));
+        assertEquals(5, exec("FieldClass fc = new FieldClass() fc.si = 5 return fc.si"));
+        assertEquals(0, exec("FieldClass fc = new FieldClass() fc.si++ return fc.si"));
+        assertEquals(-1, exec("FieldClass fc = new FieldClass() def x = fc.si++ return x"));
+        assertEquals(0, exec("FieldClass fc = new FieldClass() def x = ++fc.si return x"));
+        assertEquals(-2, exec("FieldClass fc = new FieldClass() fc.si *= 2 fc.si"));
+        assertEquals("-1test", exec("FieldClass fc = new FieldClass() fc.si + \"test\""));
+    }
+
+    public void testImplicitShortcut() {
+        assertEquals(5, exec("def fc = new FieldClass() fc.setSi(5) return fc.si"));
+        assertEquals(-1, exec("def fc = new FieldClass() def x = fc.getSi() x"));
+        assertEquals(5, exec("def fc = new FieldClass() fc.si = 5 return fc.si"));
+        assertEquals(0, exec("def fc = new FieldClass() fc.si++ return fc.si"));
+        assertEquals(-1, exec("def fc = new FieldClass() def x = fc.si++ return x"));
+        assertEquals(0, exec("def fc = new FieldClass() def x = ++fc.si return x"));
+        assertEquals(-2, exec("def fc = new FieldClass() fc.si *= 2 fc.si"));
+        assertEquals("-1test", exec("def fc = new FieldClass() fc.si + \"test\""));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java
new file mode 100644
index 0000000..94beac0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java
@@ -0,0 +1,294 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests floating point overflow with numeric overflow disabled */
+public class FloatOverflowDisabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, false);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {        
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x += 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = -3.4028234663852886E38f; x += -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x += 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; x += -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentSubtractionOverflow() {    
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x -= -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = -3.4028234663852886E38f; x -= 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x -= -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; x -= 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x *= 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 3.4028234663852886E38f; x *= -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x *= 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; x *= -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x /= 1.401298464324817E-45f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 3.4028234663852886E38f; x /= -1.401298464324817E-45f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1.0f; x /= 0.0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x /= 4.9E-324; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; x /= -4.9E-324; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0f; x /= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testAddition() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x + y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x + y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAdditionConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f + 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 + 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtraction() throws Exception {
+        try {
+            exec("float x = -3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x - y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; double y = 1.7976931348623157E308; return x - y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        try {
+            exec("return -3.4028234663852886E38f - 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return -1.7976931348623157E308 - 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplication() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x * y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x * y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f * 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 * 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testDivision() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 1.401298464324817E-45f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1.0f; float y = 0.0f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 4.9E-324; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0; double y = 0.0; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f / 1.401298464324817E-45f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0f / 0.0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 / 4.9E-324;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0 / 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionNaN() throws Exception {
+        // float division, constant division, and assignment
+        try {
+            exec("float x = 0f; float y = 0f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 0f / 0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 0f; x /= 0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double division, constant division, and assignment
+        try {
+            exec("double x = 0.0; double y = 0.0; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 0.0 / 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 0.0; x /= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderNaN() throws Exception {
+        // float division, constant division, and assignment
+        try {
+            exec("float x = 1f; float y = 0f; return x % y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1f % 0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1f; x %= 0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double division, constant division, and assignment
+        try {
+            exec("double x = 1.0; double y = 0.0; return x % y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0 % 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0; x %= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java
new file mode 100644
index 0000000..ff1c315
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests floating point overflow with numeric overflow enabled */
+public class FloatOverflowEnabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, true);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {        
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x += 3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; x += -3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x += 1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; x += -1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentSubtractionOverflow() {    
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x -= -3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; x -= 3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x -= -1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; x -= 1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x *= 3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x *= -3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x *= 1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = 1.7976931348623157E308; x *= -1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x /= 1.401298464324817E-45f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x /= -1.401298464324817E-45f; return x;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 1.0f; x /= 0.0f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x /= 4.9E-324; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = 1.7976931348623157E308; x /= -4.9E-324; return x;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.0f; x /= 0.0; return x;"));
+    }
+
+    public void testAddition() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x + y;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x + y;"));
+    }
+    
+    public void testAdditionConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f + 3.4028234663852886E38f;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 + 1.7976931348623157E308;"));
+    }
+    
+    public void testSubtraction() throws Exception {
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x - y;"));        
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; double y = 1.7976931348623157E308; return x - y;"));
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        assertEquals(Float.NEGATIVE_INFINITY, exec("return -3.4028234663852886E38f - 3.4028234663852886E38f;"));        
+        assertEquals(Double.NEGATIVE_INFINITY, exec("return -1.7976931348623157E308 - 1.7976931348623157E308;"));
+    }
+    
+    public void testMultiplication() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x * y;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x * y;"));
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f * 3.4028234663852886E38f;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 * 1.7976931348623157E308;"));
+    }
+
+    public void testDivision() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 1.401298464324817E-45f; return x / y;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 1.0f; float y = 0.0f; return x / y;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 4.9E-324; return x / y;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.0; double y = 0.0; return x / y;"));
+    }
+    
+    public void testDivisionConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f / 1.401298464324817E-45f;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 1.0f / 0.0f;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 / 4.9E-324;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.0 / 0.0;"));
+    }
+    
+    public void testDivisionNaN() throws Exception {
+        // float division, constant division, and assignment
+        assertTrue(Float.isNaN((Float) exec("float x = 0f; float y = 0f; return x / y;")));
+        assertTrue(Float.isNaN((Float) exec("return 0f / 0f;")));
+        assertTrue(Float.isNaN((Float) exec("float x = 0f; x /= 0f; return x;")));
+        
+        // double division, constant division, and assignment
+        assertTrue(Double.isNaN((Double) exec("double x = 0.0; double y = 0.0; return x / y;")));
+        assertTrue(Double.isNaN((Double) exec("return 0.0 / 0.0;")));
+        assertTrue(Double.isNaN((Double) exec("double x = 0.0; x /= 0.0; return x;")));
+    }
+    
+    public void testRemainderNaN() throws Exception {
+        // float division, constant division, and assignment
+        assertTrue(Float.isNaN((Float) exec("float x = 1f; float y = 0f; return x % y;")));
+        assertTrue(Float.isNaN((Float) exec("return 1f % 0f;")));
+        assertTrue(Float.isNaN((Float) exec("float x = 1f; x %= 0f; return x;")));
+        
+        // double division, constant division, and assignment
+        assertTrue(Double.isNaN((Double) exec("double x = 1.0; double y = 0.0; return x % y;")));
+        assertTrue(Double.isNaN((Double) exec("return 1.0 % 0.0;")));
+        assertTrue(Double.isNaN((Double) exec("double x = 1.0; x %= 0.0; return x;")));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java
new file mode 100644
index 0000000..ec4ffd0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for increment/decrement operators across all data types */
+public class IncrementTests extends ScriptTestCase {
+
+    /** incrementing byte values */
+    public void testIncrementByte() {
+        assertEquals((byte)0, exec("byte x = (byte)0; return x++;"));
+        assertEquals((byte)0, exec("byte x = (byte)0; return x--;"));
+        assertEquals((byte)1, exec("byte x = (byte)0; return ++x;"));
+        assertEquals((byte)-1, exec("byte x = (byte)0; return --x;"));
+    }
+    
+    /** incrementing char values */
+    public void testIncrementChar() {
+        assertEquals((char)0, exec("char x = (char)0; return x++;"));
+        assertEquals((char)1, exec("char x = (char)1; return x--;"));
+        assertEquals((char)1, exec("char x = (char)0; return ++x;"));
+    }
+    
+    /** incrementing short values */
+    public void testIncrementShort() {
+        assertEquals((short)0, exec("short x = (short)0; return x++;"));
+        assertEquals((short)0, exec("short x = (short)0; return x--;"));
+        assertEquals((short)1, exec("short x = (short)0; return ++x;"));
+        assertEquals((short)-1, exec("short x = (short)0; return --x;"));
+    }
+
+    /** incrementing integer values */
+    public void testIncrementInt() {
+        assertEquals(0, exec("int x = 0; return x++;"));
+        assertEquals(0, exec("int x = 0; return x--;"));
+        assertEquals(1, exec("int x = 0; return ++x;"));
+        assertEquals(-1, exec("int x = 0; return --x;"));
+    }
+    
+    /** incrementing long values */
+    public void testIncrementLong() {
+        assertEquals(0L, exec("long x = 0; return x++;"));
+        assertEquals(0L, exec("long x = 0; return x--;"));
+        assertEquals(1L, exec("long x = 0; return ++x;"));
+        assertEquals(-1L, exec("long x = 0; return --x;"));
+    }
+    
+    /** incrementing float values */
+    public void testIncrementFloat() {
+        assertEquals(0F, exec("float x = 0F; return x++;"));
+        assertEquals(0F, exec("float x = 0F; return x--;"));
+        assertEquals(1F, exec("float x = 0F; return ++x;"));
+        assertEquals(-1F, exec("float x = 0F; return --x;"));
+    }
+    
+    /** incrementing double values */
+    public void testIncrementDouble() {
+        assertEquals(0D, exec("double x = 0.0; return x++;"));
+        assertEquals(0D, exec("double x = 0.0; return x--;"));
+        assertEquals(1D, exec("double x = 0.0; return ++x;"));
+        assertEquals(-1D, exec("double x = 0.0; return --x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java
new file mode 100644
index 0000000..279ea06
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java
@@ -0,0 +1,445 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests integer overflow with numeric overflow disabled */
+public class IntegerOverflowDisabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, false);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {
+        // byte
+        try {
+            exec("byte x = 0; x += 128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x += -129; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 0; x += 32768; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x += -32769; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 0; x += 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 0; x += -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x += 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = -2; x += -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x += 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -2; x += -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentSubtractionOverflow() {
+        // byte
+        try {
+            exec("byte x = 0; x -= -128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x -= 129; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 0; x -= -32768; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x -= 32769; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 0; x -= -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 0; x -= 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x -= -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = -2; x -= 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x -= -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -2; x -= 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // byte
+        try {
+            exec("byte x = 2; x *= 128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 2; x *= -128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 2; x *= 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 2; x *= -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 2; x *= 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = 2; x *= -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 2; x *= 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = 2; x *= -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // byte
+        try {
+            exec("byte x = (byte) -128; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        // short
+        try {
+            exec("short x = (short) -32768; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // cannot happen for char: unsigned
+        
+        // int
+        try {
+            exec("int x = -2147483647 - 1; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = -9223372036854775807L - 1L; x /=-1L; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testIncrementOverFlow() throws Exception {
+        // byte
+        try {
+            exec("byte x = 127; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = 127; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = (byte) -128; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = (byte) -128; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 32767; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = 32767; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = (short) -32768; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = (short) -32768; x--; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 65535; ++x; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = 65535; x++; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = (char) 0; --x; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = (char) 0; x--; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 2147483647; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = 2147483647; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = (int) -2147483648L; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = (int) -2147483648L; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 9223372036854775807L; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -9223372036854775807L - 1L; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -9223372036854775807L - 1L; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddition() throws Exception {
+        try {
+            exec("int x = 2147483647; int y = 2147483647; return x + y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x + y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAdditionConst() throws Exception {
+        try {
+            exec("return 2147483647 + 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("return 9223372036854775807L + 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    
+    public void testSubtraction() throws Exception {
+        try {
+            exec("int x = -10; int y = 2147483647; return x - y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -10L; long y = 9223372036854775807L; return x - y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        try {
+            exec("return -10 - 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("return -10L - 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplication() throws Exception {
+        try {
+            exec("int x = 2147483647; int y = 2147483647; return x * y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x * y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        try {
+            exec("return 2147483647 * 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("return 9223372036854775807L * 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testDivision() throws Exception {
+        try {
+            exec("int x = -2147483647 - 1; int y = -1; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -9223372036854775808L; long y = -1L; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionConst() throws Exception {
+        try {
+            exec("return (-2147483648) / -1;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("return (-9223372036854775808L) / -1L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testNegationOverflow() throws Exception {
+        try {
+            exec("int x = -2147483648; x = -x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -9223372036854775808L; x = -x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testNegationOverflowConst() throws Exception {
+        try {
+            exec("int x = -(-2147483648); return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -(-9223372036854775808L); return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java
new file mode 100644
index 0000000..8abd269
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java
@@ -0,0 +1,194 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests integer overflow with numeric overflow enabled */
+public class IntegerOverflowEnabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, true);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {
+        // byte
+        assertEquals((byte)(0 + 128), exec("byte x = 0; x += 128; return x;"));
+        assertEquals((byte)(0 + -129), exec("byte x = 0; x += -129; return x;"));
+        
+        // short
+        assertEquals((short)(0 + 32768), exec("short x = 0; x += 32768; return x;"));
+        assertEquals((short)(0 + -32769), exec("short x = 0; x += -32769; return x;"));
+        
+        // char
+        assertEquals((char)(0 + 65536), exec("char x = 0; x += 65536; return x;"));
+        assertEquals((char)(0 + -65536), exec("char x = 0; x += -65536; return x;"));
+        
+        // int
+        assertEquals(1 + 2147483647, exec("int x = 1; x += 2147483647; return x;"));
+        assertEquals(-2 + -2147483647, exec("int x = -2; x += -2147483647; return x;"));
+        
+        // long
+        assertEquals(1L + 9223372036854775807L, exec("long x = 1; x += 9223372036854775807L; return x;"));
+        assertEquals(-2L + -9223372036854775807L, exec("long x = -2; x += -9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentSubtractionOverflow() {
+        // byte
+        assertEquals((byte)(0 - -128), exec("byte x = 0; x -= -128; return x;"));
+        assertEquals((byte)(0 - 129), exec("byte x = 0; x -= 129; return x;"));
+        
+        // short
+        assertEquals((short)(0 - -32768), exec("short x = 0; x -= -32768; return x;"));
+        assertEquals((short)(0 - 32769), exec("short x = 0; x -= 32769; return x;"));
+        
+        // char
+        assertEquals((char)(0 - -65536), exec("char x = 0; x -= -65536; return x;"));
+        assertEquals((char)(0 - 65536), exec("char x = 0; x -= 65536; return x;"));
+        
+        // int
+        assertEquals(1 - -2147483647, exec("int x = 1; x -= -2147483647; return x;"));
+        assertEquals(-2 - 2147483647, exec("int x = -2; x -= 2147483647; return x;"));
+        
+        // long
+        assertEquals(1L - -9223372036854775807L, exec("long x = 1; x -= -9223372036854775807L; return x;"));
+        assertEquals(-2L - 9223372036854775807L, exec("long x = -2; x -= 9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // byte
+        assertEquals((byte) (2 * 128), exec("byte x = 2; x *= 128; return x;"));
+        assertEquals((byte) (2 * -128), exec("byte x = 2; x *= -128; return x;"));
+        
+        // char
+        assertEquals((char) (2 * 65536), exec("char x = 2; x *= 65536; return x;"));
+        assertEquals((char) (2 * -65536), exec("char x = 2; x *= -65536; return x;"));
+        
+        // int
+        assertEquals(2 * 2147483647, exec("int x = 2; x *= 2147483647; return x;"));
+        assertEquals(2 * -2147483647, exec("int x = 2; x *= -2147483647; return x;"));
+        
+        // long
+        assertEquals(2L * 9223372036854775807L, exec("long x = 2; x *= 9223372036854775807L; return x;"));
+        assertEquals(2L * -9223372036854775807L, exec("long x = 2; x *= -9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // byte
+        assertEquals((byte) (-128 / -1), exec("byte x = (byte) -128; x /= -1; return x;"));
+
+        // short
+        assertEquals((short) (-32768 / -1), exec("short x = (short) -32768; x /= -1; return x;"));
+        
+        // cannot happen for char: unsigned
+        
+        // int
+        assertEquals((-2147483647 - 1) / -1, exec("int x = -2147483647 - 1; x /= -1; return x;"));
+        
+        // long
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("long x = -9223372036854775807L - 1L; x /=-1L; return x;"));
+    }
+    
+    public void testIncrementOverFlow() throws Exception {
+        // byte
+        assertEquals((byte) 128, exec("byte x = 127; ++x; return x;"));
+        assertEquals((byte) 128, exec("byte x = 127; x++; return x;"));
+        assertEquals((byte) -129, exec("byte x = (byte) -128; --x; return x;"));
+        assertEquals((byte) -129, exec("byte x = (byte) -128; x--; return x;"));
+        
+        // short
+        assertEquals((short) 32768, exec("short x = 32767; ++x; return x;"));
+        assertEquals((short) 32768, exec("short x = 32767; x++; return x;"));
+        assertEquals((short) -32769, exec("short x = (short) -32768; --x; return x;"));
+        assertEquals((short) -32769, exec("short x = (short) -32768; x--; return x;"));
+        
+        // char
+        assertEquals((char) 65536, exec("char x = 65535; ++x; return x;"));
+        assertEquals((char) 65536, exec("char x = 65535; x++; return x;"));
+        assertEquals((char) -1, exec("char x = (char) 0; --x; return x;"));
+        assertEquals((char) -1, exec("char x = (char) 0; x--; return x;"));
+        
+        // int
+        assertEquals(2147483647 + 1, exec("int x = 2147483647; ++x; return x;"));        
+        assertEquals(2147483647 + 1, exec("int x = 2147483647; x++; return x;"));
+        assertEquals(-2147483648 - 1, exec("int x = (int) -2147483648L; --x; return x;"));        
+        assertEquals(-2147483648 - 1, exec("int x = (int) -2147483648L; x--; return x;"));
+        
+        // long
+        assertEquals(9223372036854775807L + 1L, exec("long x = 9223372036854775807L; ++x; return x;"));        
+        assertEquals(9223372036854775807L + 1L, exec("long x = 9223372036854775807L; x++; return x;"));
+        assertEquals(-9223372036854775807L - 1L - 1L, exec("long x = -9223372036854775807L - 1L; --x; return x;"));
+        assertEquals(-9223372036854775807L - 1L - 1L, exec("long x = -9223372036854775807L - 1L; x--; return x;"));
+    }
+    
+    public void testAddition() throws Exception {
+        assertEquals(2147483647 + 2147483647, exec("int x = 2147483647; int y = 2147483647; return x + y;"));        
+        assertEquals(9223372036854775807L + 9223372036854775807L, exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x + y;"));
+    }
+    
+    public void testAdditionConst() throws Exception {
+        assertEquals(2147483647 + 2147483647, exec("return 2147483647 + 2147483647;"));        
+        assertEquals(9223372036854775807L + 9223372036854775807L, exec("return 9223372036854775807L + 9223372036854775807L;"));
+    }
+    
+    public void testSubtraction() throws Exception {
+        assertEquals(-10 - 2147483647, exec("int x = -10; int y = 2147483647; return x - y;"));        
+        assertEquals(-10L - 9223372036854775807L, exec("long x = -10L; long y = 9223372036854775807L; return x - y;"));
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        assertEquals(-10 - 2147483647, exec("return -10 - 2147483647;"));        
+        assertEquals(-10L - 9223372036854775807L, exec("return -10L - 9223372036854775807L;"));
+    }
+    
+    public void testMultiplication() throws Exception {
+        assertEquals(2147483647 * 2147483647, exec("int x = 2147483647; int y = 2147483647; return x * y;"));        
+        assertEquals(9223372036854775807L * 9223372036854775807L, exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x * y;"));
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        assertEquals(2147483647 * 2147483647, exec("return 2147483647 * 2147483647;"));
+        assertEquals(9223372036854775807L * 9223372036854775807L, exec("return 9223372036854775807L * 9223372036854775807L;"));
+    }
+
+    public void testDivision() throws Exception {
+        assertEquals((-2147483647 - 1) / -1, exec("int x = -2147483648; int y = -1; return x / y;"));
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("long x = -9223372036854775808L; long y = -1L; return x / y;"));
+    }
+    
+    public void testDivisionConst() throws Exception {
+        assertEquals((-2147483647 - 1) / -1, exec("return (-2147483648) / -1;"));
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("return (-9223372036854775808L) / -1L;"));
+    }
+    
+    public void testNegationOverflow() throws Exception {
+        assertEquals(-(-2147483647 - 1), exec("int x = -2147483648; x = -x; return x;"));
+        assertEquals(-(-9223372036854775807L - 1L), exec("long x = -9223372036854775808L; x = -x; return x;"));
+    }
+    
+    public void testNegationOverflowConst() throws Exception {
+        assertEquals(-(-2147483647 - 1), exec("int x = -(-2147483648); return x;"));
+        assertEquals(-(-9223372036854775807L - 1L), exec("long x = -(-9223372036854775808L); return x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java
new file mode 100644
index 0000000..c5fde3b
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for multiplication operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class MultiplicationTests extends ScriptTestCase {
+    
+    // TODO: short,byte,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1*1, exec("int x = 1; int y = 1; return x*y;"));
+        assertEquals(2*3, exec("int x = 2; int y = 3; return x*y;"));
+        assertEquals(5*10, exec("int x = 5; int y = 10; return x*y;"));
+        assertEquals(1*1*2, exec("int x = 1; int y = 1; int z = 2; return x*y*z;"));
+        assertEquals((1*1)*2, exec("int x = 1; int y = 1; int z = 2; return (x*y)*z;"));
+        assertEquals(1*(1*2), exec("int x = 1; int y = 1; int z = 2; return x*(y*z);"));
+        assertEquals(10*0, exec("int x = 10; int y = 0; return x*y;"));
+        assertEquals(0*0, exec("int x = 0; int y = 0; return x*x;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1*1, exec("return 1*1;"));
+        assertEquals(2*3, exec("return 2*3;"));
+        assertEquals(5*10, exec("return 5*10;"));
+        assertEquals(1*1*2, exec("return 1*1*2;"));
+        assertEquals((1*1)*2, exec("return (1*1)*2;"));
+        assertEquals(1*(1*2), exec("return 1*(1*2);"));
+        assertEquals(10*0, exec("return 10*0;"));
+        assertEquals(0*0, exec("return 0*0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1*(byte)1, exec("byte x = 1; byte y = 1; return x*y;"));
+        assertEquals((byte)2*(byte)3, exec("byte x = 2; byte y = 3; return x*y;"));
+        assertEquals((byte)5*(byte)10, exec("byte x = 5; byte y = 10; return x*y;"));
+        assertEquals((byte)1*(byte)1*(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x*y*z;"));
+        assertEquals(((byte)1*(byte)1)*(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x*y)*z;"));
+        assertEquals((byte)1*((byte)1*(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x*(y*z);"));
+        assertEquals((byte)10*(byte)0, exec("byte x = 10; byte y = 0; return x*y;"));
+        assertEquals((byte)0*(byte)0, exec("byte x = 0; byte y = 0; return x*x;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L*1L, exec("long x = 1; long y = 1; return x*y;"));
+        assertEquals(2L*3L, exec("long x = 2; long y = 3; return x*y;"));
+        assertEquals(5L*10L, exec("long x = 5; long y = 10; return x*y;"));
+        assertEquals(1L*1L*2L, exec("long x = 1; long y = 1; int z = 2; return x*y*z;"));
+        assertEquals((1L*1L)*2L, exec("long x = 1; long y = 1; int z = 2; return (x*y)*z;"));
+        assertEquals(1L*(1L*2L), exec("long x = 1; long y = 1; int z = 2; return x*(y*z);"));
+        assertEquals(10L*0L, exec("long x = 10; long y = 0; return x*y;"));
+        assertEquals(0L*0L, exec("long x = 0; long y = 0; return x*x;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L*1L, exec("return 1L*1L;"));
+        assertEquals(2L*3L, exec("return 2L*3L;"));
+        assertEquals(5L*10L, exec("return 5L*10L;"));
+        assertEquals(1L*1L*2L, exec("return 1L*1L*2L;"));
+        assertEquals((1L*1L)*2L, exec("return (1L*1L)*2L;"));
+        assertEquals(1L*(1L*2L), exec("return 1L*(1L*2L);"));
+        assertEquals(10L*0L, exec("return 10L*0L;"));
+        assertEquals(0L*0L, exec("return 0L*0L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F*1F, exec("float x = 1; float y = 1; return x*y;"));
+        assertEquals(2F*3F, exec("float x = 2; float y = 3; return x*y;"));
+        assertEquals(5F*10F, exec("float x = 5; float y = 10; return x*y;"));
+        assertEquals(1F*1F*2F, exec("float x = 1; float y = 1; float z = 2; return x*y*z;"));
+        assertEquals((1F*1F)*2F, exec("float x = 1; float y = 1; float z = 2; return (x*y)*z;"));
+        assertEquals(1F*(1F*2F), exec("float x = 1; float y = 1; float z = 2; return x*(y*z);"));
+        assertEquals(10F*0F, exec("float x = 10; float y = 0; return x*y;"));
+        assertEquals(0F*0F, exec("float x = 0; float y = 0; return x*x;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F*1F, exec("return 1F*1F;"));
+        assertEquals(2F*3F, exec("return 2F*3F;"));
+        assertEquals(5F*10F, exec("return 5F*10F;"));
+        assertEquals(1F*1F*2F, exec("return 1F*1F*2F;"));
+        assertEquals((1F*1F)*2F, exec("return (1F*1F)*2F;"));
+        assertEquals(1F*(1F*2F), exec("return 1F*(1F*2F);"));
+        assertEquals(10F*0F, exec("return 10F*0F;"));
+        assertEquals(0F*0F, exec("return 0F*0F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1D*1D, exec("double x = 1; double y = 1; return x*y;"));
+        assertEquals(2D*3D, exec("double x = 2; double y = 3; return x*y;"));
+        assertEquals(5D*10D, exec("double x = 5; double y = 10; return x*y;"));
+        assertEquals(1D*1D*2D, exec("double x = 1; double y = 1; double z = 2; return x*y*z;"));
+        assertEquals((1D*1D)*2D, exec("double x = 1; double y = 1; double z = 2; return (x*y)*z;"));
+        assertEquals(1D*(1D*2D), exec("double x = 1; double y = 1; double z = 2; return x*(y*z);"));
+        assertEquals(10D*0D, exec("double x = 10; float y = 0; return x*y;"));
+        assertEquals(0D*0D, exec("double x = 0; float y = 0; return x*x;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0*1.0, exec("return 1.0*1.0;"));
+        assertEquals(2.0*3.0, exec("return 2.0*3.0;"));
+        assertEquals(5.0*10.0, exec("return 5.0*10.0;"));
+        assertEquals(1.0*1.0*2.0, exec("return 1.0*1.0*2.0;"));
+        assertEquals((1.0*1.0)*2.0, exec("return (1.0*1.0)*2.0;"));
+        assertEquals(1.0*(1.0*2.0), exec("return 1.0*(1.0*2.0);"));
+        assertEquals(10.0*0.0, exec("return 10.0*0.0;"));
+        assertEquals(0.0*0.0, exec("return 0.0*0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java
new file mode 100644
index 0000000..ff56ee3
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java
@@ -0,0 +1,178 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class NoSemiColonTest extends ScriptTestCase {
+
+    public void testIfStatement() {
+        assertEquals(1, exec("int x = 5 if (x == 5) return 1 return 0"));
+        assertEquals(0, exec("int x = 4 if (x == 5) return 1 else return 0"));
+        assertEquals(2, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 2 else return 0"));
+        assertEquals(1, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 1 else return 0"));
+
+        assertEquals(3, exec(
+                "int x = 5\n" +
+                        "if (x == 5) {\n" +
+                        "    int y = 2\n" +
+                        "    \n" +
+                        "    if (y == 2) {\n" +
+                        "        x = 3\n" +
+                        "    }\n" +
+                        "    \n" +
+                        "}\n" +
+                        "\n" +
+                        "return x\n"));
+    }
+
+    public void testWhileStatement() {
+
+        assertEquals("aaaaaa", exec("String c = \"a\" int x while (x < 5) { c ..= \"a\" ++x } return c"));
+
+        Object value = exec(
+                " byte[][] b = new byte[5][5]       \n" +
+                " byte x = 0, y                     \n" +
+                "                                   \n" +
+                " while (x < 5) {                   \n" +
+                "     y = 0                         \n" +
+                "                                   \n" +
+                "     while (y < 5) {               \n" +
+                "         b[x][y] = (byte)(x*y)     \n" +
+                "         ++y                       \n" +
+                "     }                             \n" +
+                "                                   \n" +
+                "     ++x                           \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return b                          \n");
+
+        byte[][] b = (byte[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDoWhileStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" int x do { c ..= \"a\" ++x } while (x < 5) return c"));
+
+        Object value = exec(
+                " long[][] l = new long[5][5]     \n" +
+                " long x = 0, y                   \n" +
+                "                                 \n" +
+                " do {                            \n" +
+                "     y = 0                       \n" +
+                "                                 \n" +
+                "     do {                        \n" +
+                "         l[(int)x][(int)y] = x*y \n" +
+                "         ++y                     \n" +
+                "     } while (y < 5)             \n" +
+                "                                 \n" +
+                "     ++x                         \n" +
+                " } while (x < 5)                 \n" +
+                "                                 \n" +
+                " return l                        \n");
+
+        long[][] l = (long[][])value;
+
+        for (long x = 0; x < 5; ++x) {
+            for (long y = 0; y < 5; ++y) {
+                assertEquals(x*y, l[(int)x][(int)y]);
+            }
+        }
+    }
+
+    public void testForStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" for (int x = 0; x < 5; ++x) c ..= \"a\" return c"));
+
+        Object value = exec(
+                " int[][] i = new int[5][5]         \n" +
+                " for (int x = 0; x < 5; ++x) {     \n" +
+                "     for (int y = 0; y < 5; ++y) { \n" +
+                "         i[x][y] = x*y             \n" +
+                "     }                             \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return i                          \n");
+
+        int[][] i = (int[][])value;
+
+        for (int x = 0; x < 5; ++x) {
+            for (int y = 0; y < 5; ++y) {
+                assertEquals(x*y, i[x][y]);
+            }
+        }
+    }
+
+    public void testDeclarationStatement() {
+        assertEquals((byte)2, exec("byte a = 2 return a"));
+        assertEquals((short)2, exec("short a = 2 return a"));
+        assertEquals((char)2, exec("char a = 2 return a"));
+        assertEquals(2, exec("int a = 2 return a"));
+        assertEquals(2L, exec("long a = 2 return a"));
+        assertEquals(2F, exec("float a = 2 return a"));
+        assertEquals(2.0, exec("double a = 2 return a"));
+        assertEquals(false, exec("boolean a = false return a"));
+        assertEquals("string", exec("String a = \"string\" return a"));
+        assertEquals(HashMap.class, exec("Map<String, Object> a = new HashMap<String, Object>() return a").getClass());
+
+        assertEquals(byte[].class, exec("byte[] a = new byte[1] return a").getClass());
+        assertEquals(short[].class, exec("short[] a = new short[1] return a").getClass());
+        assertEquals(char[].class, exec("char[] a = new char[1] return a").getClass());
+        assertEquals(int[].class, exec("int[] a = new int[1] return a").getClass());
+        assertEquals(long[].class, exec("long[] a = new long[1] return a").getClass());
+        assertEquals(float[].class, exec("float[] a = new float[1] return a").getClass());
+        assertEquals(double[].class, exec("double[] a = new double[1] return a").getClass());
+        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1] return a").getClass());
+        assertEquals(String[].class, exec("String[] a = new String[1] return a").getClass());
+        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1] return a").getClass());
+
+        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2] return a").getClass());
+        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3] return a").getClass());
+        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4] return a").getClass());
+        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5] return a").getClass());
+        assertEquals(long[][].class, exec("long[][] a = new long[1][2] return a").getClass());
+        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3] return a").getClass());
+        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4] return a").getClass());
+        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5] return a").getClass());
+        assertEquals(String[][].class, exec("String[][] a = new String[1][2] return a").getClass());
+        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3] return a").getClass());
+    }
+
+    public void testContinueStatement() {
+        assertEquals(9, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 1) continue ++y } return y"));
+    }
+
+    public void testBreakStatement() {
+        assertEquals(4, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 5) break ++y } return y"));
+    }
+
+    public void testReturnStatement() {
+        assertEquals(10, exec("return 10"));
+        assertEquals(5, exec("int x = 5 return x"));
+        assertEquals(4, exec("int[] x = new int[2] x[1] = 4 return x[1]"));
+        assertEquals(5, ((short[])exec("short[] s = new short[3] s[1] = 5 return s"))[1]);
+        assertEquals(10, ((Map)exec("Map<String,Object> s = new HashMap< String,Object>() s.put(\"x\", 10) return s")).get("x"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java
new file mode 100644
index 0000000..f3ba0c8
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for or operator across all types */
+public class OrTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 | 12, exec("int x = 5; int y = 12; return x | y;"));
+        assertEquals(5 | -12, exec("int x = 5; int y = -12; return x | y;"));
+        assertEquals(7 | 15 | 3, exec("int x = 7; int y = 15; int z = 3; return x | y | z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 | 12, exec("return 5 | 12;"));
+        assertEquals(5 | -12, exec("return 5 | -12;"));
+        assertEquals(7 | 15 | 3, exec("return 7 | 15 | 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L | 12L, exec("long x = 5; long y = 12; return x | y;"));
+        assertEquals(5L | -12L, exec("long x = 5; long y = -12; return x | y;"));
+        assertEquals(7L | 15L | 3L, exec("long x = 7; long y = 15; long z = 3; return x | y | z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L | 12L, exec("return 5L | 12L;"));
+        assertEquals(5L | -12L, exec("return 5L | -12L;"));
+        assertEquals(7L | 15L | 3L, exec("return 7L | 15L | 3L;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java
new file mode 100644
index 0000000..c2c19cc
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+/** Runs yaml rest tests */
+public class PlanARestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(PlanAPlugin.class);
+    }
+
+    public PlanARestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java
new file mode 100644
index 0000000..c7b6f7b
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for division operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class RemainderTests extends ScriptTestCase {
+    
+    // TODO: byte,short,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1%1, exec("int x = 1; int y = 1; return x%y;"));
+        assertEquals(2%3, exec("int x = 2; int y = 3; return x%y;"));
+        assertEquals(5%10, exec("int x = 5; int y = 10; return x%y;"));
+        assertEquals(10%1%2, exec("int x = 10; int y = 1; int z = 2; return x%y%z;"));
+        assertEquals((10%1)%2, exec("int x = 10; int y = 1; int z = 2; return (x%y)%z;"));
+        assertEquals(10%(4%3), exec("int x = 10; int y = 4; int z = 3; return x%(y%z);"));
+        assertEquals(10%1, exec("int x = 10; int y = 1; return x%y;"));
+        assertEquals(0%1, exec("int x = 0; int y = 1; return x%y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1%1, exec("return 1%1;"));
+        assertEquals(2%3, exec("return 2%3;"));
+        assertEquals(5%10, exec("return 5%10;"));
+        assertEquals(10%1%2, exec("return 10%1%2;"));
+        assertEquals((10%1)%2, exec("return (10%1)%2;"));
+        assertEquals(10%(4%3), exec("return 10%(4%3);"));
+        assertEquals(10%1, exec("return 10%1;"));
+        assertEquals(0%1, exec("return 0%1;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L%1L, exec("long x = 1; long y = 1; return x%y;"));
+        assertEquals(2L%3L, exec("long x = 2; long y = 3; return x%y;"));
+        assertEquals(5L%10L, exec("long x = 5; long y = 10; return x%y;"));
+        assertEquals(10L%1L%2L, exec("long x = 10; long y = 1; long z = 2; return x%y%z;"));
+        assertEquals((10L%1L)%2L, exec("long x = 10; long y = 1; long z = 2; return (x%y)%z;"));
+        assertEquals(10L%(4L%3L), exec("long x = 10; long y = 4; long z = 3; return x%(y%z);"));
+        assertEquals(10L%1L, exec("long x = 10; long y = 1; return x%y;"));
+        assertEquals(0L%1L, exec("long x = 0; long y = 1; return x%y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L%1L, exec("return 1L%1L;"));
+        assertEquals(2L%3L, exec("return 2L%3L;"));
+        assertEquals(5L%10L, exec("return 5L%10L;"));
+        assertEquals(10L%1L%2L, exec("return 10L%1L%2L;"));
+        assertEquals((10L%1L)%2L, exec("return (10L%1L)%2L;"));
+        assertEquals(10L%(4L%3L), exec("return 10L%(4L%3L);"));
+        assertEquals(10L%1L, exec("return 10L%1L;"));
+        assertEquals(0L%1L, exec("return 0L%1L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F%1F, exec("float x = 1; float y = 1; return x%y;"));
+        assertEquals(2F%3F, exec("float x = 2; float y = 3; return x%y;"));
+        assertEquals(5F%10F, exec("float x = 5; float y = 10; return x%y;"));
+        assertEquals(10F%1F%2F, exec("float x = 10; float y = 1; float z = 2; return x%y%z;"));
+        assertEquals((10F%1F)%2F, exec("float x = 10; float y = 1; float z = 2; return (x%y)%z;"));
+        assertEquals(10F%(4F%3F), exec("float x = 10; float y = 4; float z = 3; return x%(y%z);"));
+        assertEquals(10F%1F, exec("float x = 10; float y = 1; return x%y;"));
+        assertEquals(0F%1F, exec("float x = 0; float y = 1; return x%y;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F%1F, exec("return 1F%1F;"));
+        assertEquals(2F%3F, exec("return 2F%3F;"));
+        assertEquals(5F%10F, exec("return 5F%10F;"));
+        assertEquals(10F%1F%2F, exec("return 10F%1F%2F;"));
+        assertEquals((10F%1F)%2F, exec("return (10F%1F)%2F;"));
+        assertEquals(10F%(4F%3F), exec("return 10F%(4F%3F);"));
+        assertEquals(10F%1F, exec("return 10F%1F;"));
+        assertEquals(0F%1F, exec("return 0F%1F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1.0%1.0, exec("double x = 1; double y = 1; return x%y;"));
+        assertEquals(2.0%3.0, exec("double x = 2; double y = 3; return x%y;"));
+        assertEquals(5.0%10.0, exec("double x = 5; double y = 10; return x%y;"));
+        assertEquals(10.0%1.0%2.0, exec("double x = 10; double y = 1; double z = 2; return x%y%z;"));
+        assertEquals((10.0%1.0)%2.0, exec("double x = 10; double y = 1; double z = 2; return (x%y)%z;"));
+        assertEquals(10.0%(4.0%3.0), exec("double x = 10; double y = 4; double z = 3; return x%(y%z);"));
+        assertEquals(10.0%1.0, exec("double x = 10; double y = 1; return x%y;"));
+        assertEquals(0.0%1.0, exec("double x = 0; double y = 1; return x%y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0%1.0, exec("return 1.0%1.0;"));
+        assertEquals(2.0%3.0, exec("return 2.0%3.0;"));
+        assertEquals(5.0%10.0, exec("return 5.0%10.0;"));
+        assertEquals(10.0%1.0%2.0, exec("return 10.0%1.0%2.0;"));
+        assertEquals((10.0%1.0)%2.0, exec("return (10.0%1.0)%2.0;"));
+        assertEquals(10.0%(4.0%3.0), exec("return 10.0%(4.0%3.0);"));
+        assertEquals(10.0%1.0, exec("return 10.0%1.0;"));
+        assertEquals(0.0%1.0, exec("return 0.0%1.0;"));
+    }
+    
+    public void testDivideByZero() throws Exception {
+        try {
+            exec("int x = 1; int y = 0; return x % y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("long x = 1L; long y = 0L; return x % y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+    
+    public void testDivideByZeroConst() throws Exception {
+        try {
+            exec("return 1%0;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("return 1L%0L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java
new file mode 100644
index 0000000..d2bbe02
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptService;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+public class ScriptEngineTests extends ScriptTestCase {
+
+    public void testSimpleEquation() {
+        final Object value = exec("return 1 + 2;");
+        assertEquals(3, ((Number)value).intValue());
+    }
+
+    public void testMapAccess() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> obj2 = new HashMap<>();
+        obj2.put("prop2", "value2");
+        Map<String, Object> obj1 = new HashMap<>();
+        obj1.put("prop1", "value1");
+        obj1.put("obj2", obj2);
+        obj1.put("l", Arrays.asList("2", "1"));
+        vars.put("obj1", obj1);
+
+        Object value = exec("return input.get(\"obj1\");", vars);
+        obj1 = (Map<String, Object>)value;
+        assertEquals("value1", obj1.get("prop1"));
+        assertEquals("value2", ((Map<String, Object>) obj1.get("obj2")).get("prop2"));
+
+        value = exec("return ((List)((Map<String, Object>)input.get(\"obj1\")).get(\"l\")).get(0);", vars);
+        assertEquals("2", value);
+    }
+
+    public void testAccessListInScript() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> obj2 = new HashMap<>();
+        obj2.put("prop2", "value2");
+        Map<String, Object> obj1 = new HashMap<>();
+        obj1.put("prop1", "value1");
+        obj1.put("obj2", obj2);
+        vars.put("l", Arrays.asList("1", "2", "3", obj1));
+
+        assertEquals(4, exec("return ((List)input.get(\"l\")).size();", vars));
+        assertEquals("1", exec("return ((List)input.get(\"l\")).get(0);", vars));
+
+        Object value = exec("return ((List)input.get(\"l\")).get(3);", vars);
+        obj1 = (Map<String, Object>)value;
+        assertEquals("value1", obj1.get("prop1"));
+        assertEquals("value2", ((Map<String, Object>)obj1.get("obj2")).get("prop2"));
+
+        assertEquals("value1", exec("return ((Map<String, Object>)((List)input.get(\"l\")).get(3)).get(\"prop1\");", vars));
+    }
+
+    public void testChangingVarsCrossExecution1() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> ctx = new HashMap<>();
+        vars.put("ctx", ctx);
+
+        Object compiledScript = scriptEngine.compile("return ((Map<String, Object>)input.get(\"ctx\")).get(\"value\");");
+        ExecutableScript script = scriptEngine.executable(new CompiledScript(ScriptService.ScriptType.INLINE,
+                "testChangingVarsCrossExecution1", "plan-a", compiledScript), vars);
+
+        ctx.put("value", 1);
+        Object o = script.run();
+        assertEquals(1, ((Number) o).intValue());
+
+        ctx.put("value", 2);
+        o = script.run();
+        assertEquals(2, ((Number) o).intValue());
+    }
+
+    public void testChangingVarsCrossExecution2() {
+        Map<String, Object> vars = new HashMap<>();
+        Object compiledScript = scriptEngine.compile("return input.get(\"value\");");
+
+        ExecutableScript script = scriptEngine.executable(new CompiledScript(ScriptService.ScriptType.INLINE,
+                "testChangingVarsCrossExecution2", "plan-a", compiledScript), vars);
+
+        script.setNextVar("value", 1);
+        Object value = script.run();
+        assertEquals(1, ((Number)value).intValue());
+
+        script.setNextVar("value", 2);
+        value = script.run();
+        assertEquals(2, ((Number)value).intValue());
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java
new file mode 100644
index 0000000..253e371
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Map;
+
+/**
+ * Base test case for scripting unit tests.
+ * <p>
+ * Typically just asserts the output of {@code exec()}
+ */
+public abstract class ScriptTestCase extends ESTestCase {
+    protected PlanAScriptEngineService scriptEngine;
+    
+    /** Override to provide different compiler settings */
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, random().nextBoolean());
+        return builder.build();
+    }
+
+    @Before
+    public void setup() {
+        scriptEngine = new PlanAScriptEngineService(getSettings());
+    }
+
+    /** Compiles and returns the result of {@code script} */
+    public Object exec(String script) {
+        return exec(script, null);
+    }
+
+    /** Compiles and returns the result of {@code script} with access to {@code vars} */
+    public Object exec(String script, Map<String, Object> vars) {
+        Object object = scriptEngine.compile(script);
+        CompiledScript compiled = new CompiledScript(ScriptService.ScriptType.INLINE, getTestName(), "plan-a", object);
+        return scriptEngine.executable(compiled, vars).run();
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java
new file mode 100644
index 0000000..0fbcaa1
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class StringTests extends ScriptTestCase {
+    
+    public void testAppend() {
+        // boolean
+        assertEquals("cat" + true, exec("String s = \"cat\"; return s + true;"));
+        // byte
+        assertEquals("cat" + (byte)3, exec("String s = \"cat\"; return s + (byte)3;"));
+        // short
+        assertEquals("cat" + (short)3, exec("String s = \"cat\"; return s + (short)3;"));
+        // char
+        assertEquals("cat" + 't', exec("String s = \"cat\"; return s + 't';"));
+        assertEquals("cat" + (char)40, exec("String s = \"cat\"; return s + (char)40;"));
+        // int
+        assertEquals("cat" + 2, exec("String s = \"cat\"; return s + 2;"));
+        // long
+        assertEquals("cat" + 2L, exec("String s = \"cat\"; return s + 2L;"));
+        // float
+        assertEquals("cat" + 2F, exec("String s = \"cat\"; return s + 2F;"));
+        // double
+        assertEquals("cat" + 2.0, exec("String s = \"cat\"; return s + 2.0;"));
+        // String
+        assertEquals("cat" + "cat", exec("String s = \"cat\"; return s + s;"));
+    }
+
+    public void testStringAPI() {
+        assertEquals("", exec("return new String();"));
+        assertEquals('x', exec("String s = \"x\"; return s.charAt(0);"));
+        assertEquals(120, exec("String s = \"x\"; return s.codePointAt(0);"));
+        assertEquals(0, exec("String s = \"x\"; return s.compareTo(\"x\");"));
+        assertEquals("xx", exec("String s = \"x\"; return s.concat(\"x\");"));
+        assertEquals(true, exec("String s = \"xy\"; return s.endsWith(\"y\");"));
+        assertEquals(2, exec("String t = \"abcde\"; return t.indexOf(\"cd\", 1);"));
+        assertEquals(false, exec("String t = \"abcde\"; return t.isEmpty();"));
+        assertEquals(5, exec("String t = \"abcde\"; return t.length();"));
+        assertEquals("cdcde", exec("String t = \"abcde\"; return t.replace(\"ab\", \"cd\");"));
+        assertEquals(false, exec("String s = \"xy\"; return s.startsWith(\"y\");"));
+        assertEquals("e", exec("String t = \"abcde\"; return t.substring(4, 5);"));
+        assertEquals(97, ((char[])exec("String s = \"a\"; return s.toCharArray();"))[0]);
+        assertEquals("a", exec("String s = \" a \"; return s.trim();"));
+        assertEquals('x', exec("return \"x\".charAt(0);"));
+        assertEquals(120, exec("return \"x\".codePointAt(0);"));
+        assertEquals(0, exec("return \"x\".compareTo(\"x\");"));
+        assertEquals("xx", exec("return \"x\".concat(\"x\");"));
+        assertEquals(true, exec("return \"xy\".endsWith(\"y\");"));
+        assertEquals(2, exec("return \"abcde\".indexOf(\"cd\", 1);"));
+        assertEquals(false, exec("return \"abcde\".isEmpty();"));
+        assertEquals(5, exec("return \"abcde\".length();"));
+        assertEquals("cdcde", exec("return \"abcde\".replace(\"ab\", \"cd\");"));
+        assertEquals(false, exec("return \"xy\".startsWith(\"y\");"));
+        assertEquals("e", exec("return \"abcde\".substring(4, 5);"));
+        assertEquals(97, ((char[])exec("return \"a\".toCharArray();"))[0]);
+        assertEquals("a", exec("return \" a \".trim();"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java
new file mode 100644
index 0000000..1acd045
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for subtraction operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class SubtractionTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(1-1, exec("int x = 1; int y = 1; return x-y;"));
+        assertEquals(2-3, exec("int x = 2; int y = 3; return x-y;"));
+        assertEquals(5-10, exec("int x = 5; int y = 10; return x-y;"));
+        assertEquals(1-1-2, exec("int x = 1; int y = 1; int z = 2; return x-y-z;"));
+        assertEquals((1-1)-2, exec("int x = 1; int y = 1; int z = 2; return (x-y)-z;"));
+        assertEquals(1-(1-2), exec("int x = 1; int y = 1; int z = 2; return x-(y-z);"));
+        assertEquals(10-0, exec("int x = 10; int y = 0; return x-y;"));
+        assertEquals(0-0, exec("int x = 0; int y = 0; return x-x;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1-1, exec("return 1-1;"));
+        assertEquals(2-3, exec("return 2-3;"));
+        assertEquals(5-10, exec("return 5-10;"));
+        assertEquals(1-1-2, exec("return 1-1-2;"));
+        assertEquals((1-1)-2, exec("return (1-1)-2;"));
+        assertEquals(1-(1-2), exec("return 1-(1-2);"));
+        assertEquals(10-0, exec("return 10-0;"));
+        assertEquals(0-0, exec("return 0-0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1-(byte)1, exec("byte x = 1; byte y = 1; return x-y;"));
+        assertEquals((byte)2-(byte)3, exec("byte x = 2; byte y = 3; return x-y;"));
+        assertEquals((byte)5-(byte)10, exec("byte x = 5; byte y = 10; return x-y;"));
+        assertEquals((byte)1-(byte)1-(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x-y-z;"));
+        assertEquals(((byte)1-(byte)1)-(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x-y)-z;"));
+        assertEquals((byte)1-((byte)1-(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x-(y-z);"));
+        assertEquals((byte)10-(byte)1, exec("byte x = 10; byte y = 1; return x-y;"));
+        assertEquals((byte)0-(byte)0, exec("byte x = 0; byte y = 0; return x-y;"));
+    }
+    
+    public void testByteConst() throws Exception {
+        assertEquals((byte)1-(byte)1, exec("return (byte)1-(byte)1;"));
+        assertEquals((byte)2-(byte)3, exec("return (byte)2-(byte)3;"));
+        assertEquals((byte)5-(byte)10, exec("return (byte)5-(byte)10;"));
+        assertEquals((byte)1-(byte)1-(byte)2, exec("return (byte)1-(byte)1-(byte)2;"));
+        assertEquals(((byte)1-(byte)1)-(byte)2, exec("return ((byte)1-(byte)1)-(byte)2;"));
+        assertEquals((byte)1-((byte)1-(byte)2), exec("return (byte)1-((byte)1-(byte)2);"));
+        assertEquals((byte)10-(byte)1, exec("return (byte)10-(byte)1;"));
+        assertEquals((byte)0-(byte)0, exec("return (byte)0-(byte)0;"));
+    }
+    
+    public void testChar() throws Exception {
+        assertEquals((char)1-(char)1, exec("char x = 1; char y = 1; return x-y;"));
+        assertEquals((char)2-(char)3, exec("char x = 2; char y = 3; return x-y;"));
+        assertEquals((char)5-(char)10, exec("char x = 5; char y = 10; return x-y;"));
+        assertEquals((char)1-(char)1-(char)2, exec("char x = 1; char y = 1; char z = 2; return x-y-z;"));
+        assertEquals(((char)1-(char)1)-(char)2, exec("char x = 1; char y = 1; char z = 2; return (x-y)-z;"));
+        assertEquals((char)1-((char)1-(char)2), exec("char x = 1; char y = 1; char z = 2; return x-(y-z);"));
+        assertEquals((char)10-(char)1, exec("char x = 10; char y = 1; return x-y;"));
+        assertEquals((char)0-(char)0, exec("char x = 0; char y = 0; return x-y;"));
+    }
+    
+    public void testCharConst() throws Exception {
+        assertEquals((char)1-(char)1, exec("return (char)1-(char)1;"));
+        assertEquals((char)2-(char)3, exec("return (char)2-(char)3;"));
+        assertEquals((char)5-(char)10, exec("return (char)5-(char)10;"));
+        assertEquals((char)1-(char)1-(char)2, exec("return (char)1-(char)1-(char)2;"));
+        assertEquals(((char)1-(char)1)-(char)2, exec("return ((char)1-(char)1)-(char)2;"));
+        assertEquals((char)1-((char)1-(char)2), exec("return (char)1-((char)1-(char)2);"));
+        assertEquals((char)10-(char)1, exec("return (char)10-(char)1;"));
+        assertEquals((char)0-(char)0, exec("return (char)0-(char)0;"));
+    }
+    
+    public void testShort() throws Exception {
+        assertEquals((short)1-(short)1, exec("short x = 1; short y = 1; return x-y;"));
+        assertEquals((short)2-(short)3, exec("short x = 2; short y = 3; return x-y;"));
+        assertEquals((short)5-(short)10, exec("short x = 5; short y = 10; return x-y;"));
+        assertEquals((short)1-(short)1-(short)2, exec("short x = 1; short y = 1; short z = 2; return x-y-z;"));
+        assertEquals(((short)1-(short)1)-(short)2, exec("short x = 1; short y = 1; short z = 2; return (x-y)-z;"));
+        assertEquals((short)1-((short)1-(short)2), exec("short x = 1; short y = 1; short z = 2; return x-(y-z);"));
+        assertEquals((short)10-(short)1, exec("short x = 10; short y = 1; return x-y;"));
+        assertEquals((short)0-(short)0, exec("short x = 0; short y = 0; return x-y;"));
+    }
+    
+    public void testShortConst() throws Exception {
+        assertEquals((short)1-(short)1, exec("return (short)1-(short)1;"));
+        assertEquals((short)2-(short)3, exec("return (short)2-(short)3;"));
+        assertEquals((short)5-(short)10, exec("return (short)5-(short)10;"));
+        assertEquals((short)1-(short)1-(short)2, exec("return (short)1-(short)1-(short)2;"));
+        assertEquals(((short)1-(short)1)-(short)2, exec("return ((short)1-(short)1)-(short)2;"));
+        assertEquals((short)1-((short)1-(short)2), exec("return (short)1-((short)1-(short)2);"));
+        assertEquals((short)10-(short)1, exec("return (short)10-(short)1;"));
+        assertEquals((short)0-(short)0, exec("return (short)0-(short)0;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L-1L, exec("long x = 1; long y = 1; return x-y;"));
+        assertEquals(2L-3L, exec("long x = 2; long y = 3; return x-y;"));
+        assertEquals(5L-10L, exec("long x = 5; long y = 10; return x-y;"));
+        assertEquals(1L-1L-2L, exec("long x = 1; long y = 1; int z = 2; return x-y-z;"));
+        assertEquals((1L-1L)-2L, exec("long x = 1; long y = 1; int z = 2; return (x-y)-z;"));
+        assertEquals(1L-(1L-2L), exec("long x = 1; long y = 1; int z = 2; return x-(y-z);"));
+        assertEquals(10L-0L, exec("long x = 10; long y = 0; return x-y;"));
+        assertEquals(0L-0L, exec("long x = 0; long y = 0; return x-x;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L-1L, exec("return 1L-1L;"));
+        assertEquals(2L-3L, exec("return 2L-3L;"));
+        assertEquals(5L-10L, exec("return 5L-10L;"));
+        assertEquals(1L-1L-2L, exec("return 1L-1L-2L;"));
+        assertEquals((1L-1L)-2L, exec("return (1L-1L)-2L;"));
+        assertEquals(1L-(1L-2L), exec("return 1L-(1L-2L);"));
+        assertEquals(10L-0L, exec("return 10L-0L;"));
+        assertEquals(0L-0L, exec("return 0L-0L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F-1F, exec("float x = 1; float y = 1; return x-y;"));
+        assertEquals(2F-3F, exec("float x = 2; float y = 3; return x-y;"));
+        assertEquals(5F-10F, exec("float x = 5; float y = 10; return x-y;"));
+        assertEquals(1F-1F-2F, exec("float x = 1; float y = 1; float z = 2; return x-y-z;"));
+        assertEquals((1F-1F)-2F, exec("float x = 1; float y = 1; float z = 2; return (x-y)-z;"));
+        assertEquals(1F-(1F-2F), exec("float x = 1; float y = 1; float z = 2; return x-(y-z);"));
+        assertEquals(10F-0F, exec("float x = 10; float y = 0; return x-y;"));
+        assertEquals(0F-0F, exec("float x = 0; float y = 0; return x-x;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F-1F, exec("return 1F-1F;"));
+        assertEquals(2F-3F, exec("return 2F-3F;"));
+        assertEquals(5F-10F, exec("return 5F-10F;"));
+        assertEquals(1F-1F-2F, exec("return 1F-1F-2F;"));
+        assertEquals((1F-1F)-2F, exec("return (1F-1F)-2F;"));
+        assertEquals(1F-(1F-2F), exec("return 1F-(1F-2F);"));
+        assertEquals(10F-0F, exec("return 10F-0F;"));
+        assertEquals(0F-0F, exec("return 0F-0F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1D-1D, exec("double x = 1; double y = 1; return x-y;"));
+        assertEquals(2D-3D, exec("double x = 2; double y = 3; return x-y;"));
+        assertEquals(5D-10D, exec("double x = 5; double y = 10; return x-y;"));
+        assertEquals(1D-1D-2D, exec("double x = 1; double y = 1; double z = 2; return x-y-z;"));
+        assertEquals((1D-1D)-2D, exec("double x = 1; double y = 1; double z = 2; return (x-y)-z;"));
+        assertEquals(1D-(1D-2D), exec("double x = 1; double y = 1; double z = 2; return x-(y-z);"));
+        assertEquals(10D-0D, exec("double x = 10; float y = 0; return x-y;"));
+        assertEquals(0D-0D, exec("double x = 0; float y = 0; return x-x;"));
+    }
+    
+    public void testyDoubleConst() throws Exception {
+        assertEquals(1.0-1.0, exec("return 1.0-1.0;"));
+        assertEquals(2.0-3.0, exec("return 2.0-3.0;"));
+        assertEquals(5.0-10.0, exec("return 5.0-10.0;"));
+        assertEquals(1.0-1.0-2.0, exec("return 1.0-1.0-2.0;"));
+        assertEquals((1.0-1.0)-2.0, exec("return (1.0-1.0)-2.0;"));
+        assertEquals(1.0-(1.0-2.0), exec("return 1.0-(1.0-2.0);"));
+        assertEquals(10.0-0.0, exec("return 10.0-0.0;"));
+        assertEquals(0.0-0.0, exec("return 0.0-0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java
new file mode 100644
index 0000000..c0199ff
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for unary operators across different types */
+public class UnaryTests extends ScriptTestCase {
+
+    /** basic tests */
+    public void testBasics() {
+        assertEquals(false, exec("return !true;"));
+        assertEquals(true, exec("boolean x = false; return !x;"));
+        assertEquals(-2, exec("return ~1;"));
+        assertEquals(-2, exec("byte x = 1; return ~x;"));
+        assertEquals(1, exec("return +1;"));
+        assertEquals(1.0, exec("double x = 1; return +x;"));
+        assertEquals(-1, exec("return -1;"));
+        assertEquals(-2, exec("short x = 2; return -x;"));
+    }
+
+    public void testNegationInt() throws Exception {
+        assertEquals(-1, exec("return -1;"));
+        assertEquals(1, exec("return -(-1);"));
+        assertEquals(0, exec("return -0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java
new file mode 100644
index 0000000..5c9fe20
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java
@@ -0,0 +1,250 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.test.ESTestCase;
+
+/**
+ * Tests utility methods (typically built-ins)
+ */
+public class UtilityTests extends ESTestCase {
+    
+    public void testDivideWithoutOverflowInt() {
+        assertEquals(5 / 2, Utility.divideWithoutOverflow(5, 2));
+
+        try {
+            Utility.divideWithoutOverflow(Integer.MIN_VALUE, -1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5, 0);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowLong() {
+        assertEquals(5L / 2L, Utility.divideWithoutOverflow(5L, 2L));
+        
+        try {
+            Utility.divideWithoutOverflow(Long.MIN_VALUE, -1L);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5L, 0L);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToByteExact() {
+        for (int b = Byte.MIN_VALUE; b < Byte.MAX_VALUE; b++) {
+            assertEquals((byte)b, Utility.toByteExact(b));
+        }
+        
+        try {
+            Utility.toByteExact(Byte.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toByteExact(Byte.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToShortExact() {
+        for (int s = Short.MIN_VALUE; s < Short.MAX_VALUE; s++) {
+            assertEquals((short)s, Utility.toShortExact(s));
+        }
+        
+        try {
+            Utility.toShortExact(Short.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toShortExact(Short.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToCharExact() {
+        for (int c = Character.MIN_VALUE; c < Character.MAX_VALUE; c++) {
+            assertEquals((char)c, Utility.toCharExact(c));
+        }
+        
+        try {
+            Utility.toCharExact(Character.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toCharExact(Character.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddWithoutOverflowFloat() {
+        assertEquals(10F, Utility.addWithoutOverflow(5F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.addWithoutOverflow(5F, Float.NaN)));
+        assertTrue(Float.isNaN(Utility.addWithoutOverflow(Float.POSITIVE_INFINITY, Float.NEGATIVE_INFINITY)));
+
+        try {
+            Utility.addWithoutOverflow(Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.addWithoutOverflow(-Float.MAX_VALUE, -Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddWithoutOverflowDouble() {
+        assertEquals(10D, Utility.addWithoutOverflow(5D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.addWithoutOverflow(5D, Double.NaN)));
+        assertTrue(Double.isNaN(Utility.addWithoutOverflow(Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY)));
+        
+        try {
+            Utility.addWithoutOverflow(Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.addWithoutOverflow(-Double.MAX_VALUE, -Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractWithoutOverflowFloat() {
+        assertEquals(5F, Utility.subtractWithoutOverflow(10F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.subtractWithoutOverflow(5F, Float.NaN)));
+        assertTrue(Float.isNaN(Utility.subtractWithoutOverflow(Float.POSITIVE_INFINITY, Float.POSITIVE_INFINITY)));
+
+        try {
+            Utility.subtractWithoutOverflow(Float.MAX_VALUE, -Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.subtractWithoutOverflow(-Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractWithoutOverflowDouble() {
+        assertEquals(5D, Utility.subtractWithoutOverflow(10D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.subtractWithoutOverflow(5D, Double.NaN)));
+        assertTrue(Double.isNaN(Utility.subtractWithoutOverflow(Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY)));
+        
+        try {
+            Utility.subtractWithoutOverflow(Double.MAX_VALUE, -Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.subtractWithoutOverflow(-Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplyWithoutOverflowFloat() {
+        assertEquals(25F, Utility.multiplyWithoutOverflow(5F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.multiplyWithoutOverflow(5F, Float.NaN)));
+        assertEquals(Float.POSITIVE_INFINITY, Utility.multiplyWithoutOverflow(5F, Float.POSITIVE_INFINITY), 0F);
+
+        try {
+            Utility.multiplyWithoutOverflow(Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplyWithoutOverflowDouble() {
+        assertEquals(25D, Utility.multiplyWithoutOverflow(5D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.multiplyWithoutOverflow(5D, Double.NaN)));
+        assertEquals(Double.POSITIVE_INFINITY, Utility.multiplyWithoutOverflow(5D, Double.POSITIVE_INFINITY), 0D);
+        
+        try {
+            Utility.multiplyWithoutOverflow(Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowFloat() {
+        assertEquals(5F, Utility.divideWithoutOverflow(25F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.divideWithoutOverflow(5F, Float.NaN)));
+        assertEquals(Float.POSITIVE_INFINITY, Utility.divideWithoutOverflow(Float.POSITIVE_INFINITY, 5F), 0F);
+
+        try {
+            Utility.divideWithoutOverflow(Float.MAX_VALUE, Float.MIN_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(0F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowDouble() {
+        assertEquals(5D, Utility.divideWithoutOverflow(25D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.divideWithoutOverflow(5D, Double.NaN)));
+        assertEquals(Double.POSITIVE_INFINITY, Utility.divideWithoutOverflow(Double.POSITIVE_INFINITY, 5D), 0D);
+        
+        try {
+            Utility.divideWithoutOverflow(Double.MAX_VALUE, Double.MIN_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(0D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderWithoutOverflowFloat() {
+        assertEquals(1F, Utility.remainderWithoutOverflow(25F, 4F), 0F);
+        
+        try {
+            Utility.remainderWithoutOverflow(5F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderWithoutOverflowDouble() {
+        assertEquals(1D, Utility.remainderWithoutOverflow(25D, 4D), 0D);
+        
+        try {
+            Utility.remainderWithoutOverflow(5D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java
new file mode 100644
index 0000000..de2c1c9
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class WhenThingsGoWrongTests extends ScriptTestCase {
+    public void testNullPointer() {
+        try {
+            exec("int x = (int) ((Map) input).get(\"missing\"); return x;");
+            fail("should have hit npe");
+        } catch (NullPointerException expected) {}
+    }
+
+    public void testInvalidShift() {
+        try {
+            exec("float x = 15F; x <<= 2; return x;");
+            fail("should have hit cce");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("double x = 15F; x <<= 2; return x;");
+            fail("should have hit cce");
+        } catch (ClassCastException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java
new file mode 100644
index 0000000..f10477d
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for xor operator across all types */
+public class XorTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 ^ 12, exec("int x = 5; int y = 12; return x ^ y;"));
+        assertEquals(5 ^ -12, exec("int x = 5; int y = -12; return x ^ y;"));
+        assertEquals(7 ^ 15 ^ 3, exec("int x = 7; int y = 15; int z = 3; return x ^ y ^ z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 ^ 12, exec("return 5 ^ 12;"));
+        assertEquals(5 ^ -12, exec("return 5 ^ -12;"));
+        assertEquals(7 ^ 15 ^ 3, exec("return 7 ^ 15 ^ 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L ^ 12L, exec("long x = 5; long y = 12; return x ^ y;"));
+        assertEquals(5L ^ -12L, exec("long x = 5; long y = -12; return x ^ y;"));
+        assertEquals(7L ^ 15L ^ 3L, exec("long x = 7; long y = 15; long z = 3; return x ^ y ^ z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L ^ 12L, exec("return 5L ^ 12L;"));
+        assertEquals(5L ^ -12L, exec("return 5L ^ -12L;"));
+        assertEquals(7L ^ 15L ^ 3L, exec("return 7L ^ 15L ^ 3L;"));
+    }
+
+    public void testBool() throws Exception {
+        assertEquals(false, exec("boolean x = true; boolean y = true; return x ^ y;"));
+        assertEquals(true, exec("boolean x = true; boolean y = false; return x ^ y;"));
+        assertEquals(true, exec("boolean x = false; boolean y = true; return x ^ y;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x ^ y;"));
+    }
+
+    public void testBoolConst() throws Exception {
+        assertEquals(false, exec("return true ^ true;"));
+        assertEquals(true, exec("return true ^ false;"));
+        assertEquals(true, exec("return false ^ true;"));
+        assertEquals(false, exec("return false ^ false;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
new file mode 100644
index 0000000..6259780
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Plan A Plugin
+#
+"Plan A plugin loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: lang-plan-a }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml
new file mode 100644
index 0000000..0a5a3a4
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml
@@ -0,0 +1,27 @@
+# Integration tests for using a scripted field
+#
+setup:
+    - do:
+        index:
+            index: test
+            type: test
+            id: 1
+            body: { "foo": "aaa" }
+    - do:
+        indices.refresh: {}
+
+---
+
+"Scripted Field":
+    - do:
+        search:
+            body:
+                script_fields:
+                    bar:
+                        script: 
+                            inline: "input.doc.foo.0 + input.x;"
+                            lang: plan-a
+                            params:
+                                x: "bbb"
+
+    - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml
new file mode 100644
index 0000000..a8d96a0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml
@@ -0,0 +1,97 @@
+# Integration tests for Plan-A search scripting
+#
+"Plan-A Query":
+    - do:
+        index:
+            index: test
+            type: test
+            id: 1
+            body: { "test": "value beck", "num1": 1.0 }
+    - do:
+        index:
+            index: test
+            type: test
+            id: 2
+            body: { "test": "value beck", "num1": 2.0 }
+    - do:
+        index:
+            index: test
+            type: test
+            id: 3
+            body: { "test": "value beck", "num1": 3.0 }
+    - do:
+        indices.refresh: {}
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > 1;"
+                            lang: plan-a
+                script_fields:
+                    sNum1:
+                        script: 
+                            inline: "input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 2 }
+    - match: { hits.hits.0.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 3.0 }
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > input.param1;"
+                            lang: plan-a
+                            params:
+                                param1: 1
+
+                script_fields:
+                    sNum1:
+                        script:
+                            inline: "return input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 2 }
+    - match: { hits.hits.0.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 3.0 }
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > input.param1;"
+                            lang: plan-a
+                            params:
+                                param1: -1
+
+                script_fields:
+                    sNum1:
+                        script: 
+                            inline: "input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 3 }
+    - match: { hits.hits.0.fields.sNum1.0: 1.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.2.fields.sNum1.0: 3.0 }
+
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
index e5db2ed..711b8db 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
@@ -26,6 +26,56 @@ import org.elasticsearch.common.component.LifecycleComponent;
  *
  */
 public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
+
+    final class CLOUD_AWS {
+        public static final String KEY = "cloud.aws.access_key";
+        public static final String SECRET = "cloud.aws.secret_key";
+        public static final String PROTOCOL = "cloud.aws.protocol";
+        public static final String PROXY_HOST = "cloud.aws.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
+        public static final String SIGNER = "cloud.aws.signer";
+        public static final String REGION = "cloud.aws.region";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
+    }
+
+    final class CLOUD_S3 {
+        public static final String KEY = "cloud.aws.s3.access_key";
+        public static final String SECRET = "cloud.aws.s3.secret_key";
+        public static final String PROTOCOL = "cloud.aws.s3.protocol";
+        public static final String PROXY_HOST = "cloud.aws.s3.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.s3.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.s3.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.s3.proxy.password";
+        public static final String SIGNER = "cloud.aws.s3.signer";
+        public static final String ENDPOINT = "cloud.aws.s3.endpoint";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.s3.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.s3.proxy_port";
+    }
+
+    final class REPOSITORY_S3 {
+        public static final String BUCKET = "repositories.s3.bucket";
+        public static final String ENDPOINT = "repositories.s3.endpoint";
+        public static final String PROTOCOL = "repositories.s3.protocol";
+        public static final String REGION = "repositories.s3.region";
+        public static final String SERVER_SIDE_ENCRYPTION = "repositories.s3.server_side_encryption";
+        public static final String BUFFER_SIZE = "repositories.s3.buffer_size";
+        public static final String MAX_RETRIES = "repositories.s3.max_retries";
+        public static final String CHUNK_SIZE = "repositories.s3.chunk_size";
+        public static final String COMPRESS = "repositories.s3.compress";
+        public static final String STORAGE_CLASS = "repositories.s3.storage_class";
+        public static final String CANNED_ACL = "repositories.s3.canned_acl";
+        public static final String BASE_PATH = "repositories.s3.base_path";
+    }
+
+
+
     AmazonS3 client();
 
     AmazonS3 client(String endpoint, String protocol, String region, String account, String key);
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index 4752a3f..7d0b72c 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -50,8 +50,12 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
     @Inject
     public InternalAwsS3Service(Settings settings, SettingsFilter settingsFilter) {
         super(settings);
-        settingsFilter.addFilter("cloud.aws.access_key");
-        settingsFilter.addFilter("cloud.aws.secret_key");
+        settingsFilter.addFilter(CLOUD_AWS.KEY);
+        settingsFilter.addFilter(CLOUD_AWS.SECRET);
+        settingsFilter.addFilter(CLOUD_AWS.PROXY_PASSWORD);
+        settingsFilter.addFilter(CLOUD_S3.KEY);
+        settingsFilter.addFilter(CLOUD_S3.SECRET);
+        settingsFilter.addFilter(CLOUD_S3.PROXY_PASSWORD);
         settingsFilter.addFilter("access_key");
         settingsFilter.addFilter("secret_key");
     }
@@ -59,9 +63,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
     @Override
     public synchronized AmazonS3 client() {
         String endpoint = getDefaultEndpoint();
-        String account = settings.get("cloud.aws.access_key");
-        String key = settings.get("cloud.aws.secret_key");
-
+        String account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
+        String key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
         return getClient(endpoint, null, account, key, null);
     }
 
@@ -79,8 +82,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             endpoint = getDefaultEndpoint();
         }
         if (account == null || key == null) {
-            account = settings.get("cloud.aws.access_key");
-            key = settings.get("cloud.aws.secret_key");
+            account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
+            key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
         }
 
         return getClient(endpoint, protocol, account, key, maxRetries);
@@ -99,8 +102,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
         // but can force objects from every response to the old generation.
         clientConfiguration.setResponseMetadataCacheSize(0);
         if (protocol == null) {
-            protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
-            protocol = settings.get("cloud.aws.s3.protocol", protocol).toLowerCase(Locale.ROOT);
+            protocol = settings.get(CLOUD_AWS.PROTOCOL, "https").toLowerCase(Locale.ROOT);
+            protocol = settings.get(CLOUD_S3.PROTOCOL, protocol).toLowerCase(Locale.ROOT);
         }
 
         if ("http".equals(protocol)) {
@@ -111,18 +114,25 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
         }
 
-        String proxyHost = settings.get("cloud.aws.proxy_host");
-        proxyHost = settings.get("cloud.aws.s3.proxy_host", proxyHost);
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
+        proxyHost = settings.get(CLOUD_S3.PROXY_HOST, settings.get(CLOUD_S3.DEPRECATED_PROXY_HOST, proxyHost));
         if (proxyHost != null) {
-            String portString = settings.get("cloud.aws.proxy_port", "80");
-            portString = settings.get("cloud.aws.s3.proxy_port", portString);
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
+            portString = settings.get(CLOUD_S3.PROXY_PORT, settings.get(CLOUD_S3.DEPRECATED_PROXY_PORT, portString));
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
             } catch (NumberFormatException ex) {
                 throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
             }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+            String proxyUsername = settings.get(CLOUD_S3.PROXY_USERNAME, settings.get(CLOUD_AWS.PROXY_USERNAME));
+            String proxyPassword = settings.get(CLOUD_S3.PROXY_PASSWORD, settings.get(CLOUD_AWS.PROXY_PASSWORD));
+
+            clientConfiguration
+                .withProxyHost(proxyHost)
+                .withProxyPort(proxyPort)
+                .withProxyUsername(proxyUsername)
+                .withProxyPassword(proxyPassword);
         }
 
         if (maxRetries != null) {
@@ -131,7 +141,7 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
         }
 
         // #155: we might have 3rd party users using older S3 API version
-        String awsSigner = settings.get("cloud.aws.s3.signer", settings.get("cloud.aws.signer"));
+        String awsSigner = settings.get(CLOUD_S3.SIGNER, settings.get(CLOUD_AWS.SIGNER));
         if (awsSigner != null) {
             logger.debug("using AWS API signer [{}]", awsSigner);
             AwsSigner.configureSigner(awsSigner, clientConfiguration, endpoint);
@@ -161,11 +171,11 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
 
     private String getDefaultEndpoint() {
         String endpoint = null;
-        if (settings.get("cloud.aws.s3.endpoint") != null) {
-            endpoint = settings.get("cloud.aws.s3.endpoint");
+        if (settings.get(CLOUD_S3.ENDPOINT) != null) {
+            endpoint = settings.get(CLOUD_S3.ENDPOINT);
             logger.debug("using explicit s3 endpoint [{}]", endpoint);
-        } else if (settings.get("cloud.aws.region") != null) {
-            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
+        } else if (settings.get(CLOUD_AWS.REGION) != null) {
+            String region = settings.get(CLOUD_AWS.REGION).toLowerCase(Locale.ROOT);
             endpoint = getEndpoint(region);
             logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
         }
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
index 23b3fb1..760968b 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.repositories.s3;
 
 import org.elasticsearch.cloud.aws.AwsS3Service;
+import org.elasticsearch.cloud.aws.AwsS3Service.CLOUD_AWS;
+import org.elasticsearch.cloud.aws.AwsS3Service.REPOSITORY_S3;
 import org.elasticsearch.cloud.aws.blobstore.S3BlobStore;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.blobstore.BlobPath;
@@ -73,18 +75,18 @@ public class S3Repository extends BlobStoreRepository {
     public S3Repository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, AwsS3Service s3Service) throws IOException {
         super(name.getName(), repositorySettings, indexShardRepository);
 
-        String bucket = repositorySettings.settings().get("bucket", settings.get("repositories.s3.bucket"));
+        String bucket = repositorySettings.settings().get("bucket", settings.get(REPOSITORY_S3.BUCKET));
         if (bucket == null) {
             throw new RepositoryException(name.name(), "No bucket defined for s3 gateway");
         }
 
-        String endpoint = repositorySettings.settings().get("endpoint", settings.get("repositories.s3.endpoint"));
-        String protocol = repositorySettings.settings().get("protocol", settings.get("repositories.s3.protocol"));
+        String endpoint = repositorySettings.settings().get("endpoint", settings.get(REPOSITORY_S3.ENDPOINT));
+        String protocol = repositorySettings.settings().get("protocol", settings.get(REPOSITORY_S3.PROTOCOL));
 
-        String region = repositorySettings.settings().get("region", settings.get("repositories.s3.region"));
+        String region = repositorySettings.settings().get("region", settings.get(REPOSITORY_S3.REGION));
         if (region == null) {
             // Bucket setting is not set - use global region setting
-            String regionSetting = repositorySettings.settings().get("cloud.aws.region", settings.get("cloud.aws.region"));
+            String regionSetting = settings.get(CLOUD_AWS.REGION);
             if (regionSetting != null) {
                 regionSetting = regionSetting.toLowerCase(Locale.ENGLISH);
                 if ("us-east".equals(regionSetting) || "us-east-1".equals(regionSetting)) {
@@ -112,15 +114,15 @@ public class S3Repository extends BlobStoreRepository {
             }
         }
 
-        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean("repositories.s3.server_side_encryption", false));
-        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize("repositories.s3.buffer_size", null));
-        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt("repositories.s3.max_retries", 3));
-        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize("repositories.s3.chunk_size", new ByteSizeValue(100, ByteSizeUnit.MB)));
-        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean("repositories.s3.compress", false));
+        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean(REPOSITORY_S3.SERVER_SIDE_ENCRYPTION, false));
+        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize(REPOSITORY_S3.BUFFER_SIZE, null));
+        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt(REPOSITORY_S3.MAX_RETRIES, 3));
+        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize(REPOSITORY_S3.CHUNK_SIZE, new ByteSizeValue(100, ByteSizeUnit.MB)));
+        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean(REPOSITORY_S3.COMPRESS, false));
 
         // Parse and validate the user's S3 Storage Class setting
-        String storageClass = repositorySettings.settings().get("storage_class", settings.get("repositories.s3.storage_class", null));
-        String cannedACL = repositorySettings.settings().get("canned_acl", settings.get("repositories.s3.canned_acl", null));
+        String storageClass = repositorySettings.settings().get("storage_class", settings.get(REPOSITORY_S3.STORAGE_CLASS, null));
+        String cannedACL = repositorySettings.settings().get("canned_acl", settings.get(REPOSITORY_S3.CANNED_ACL, null));
 
         logger.debug("using bucket [{}], region [{}], endpoint [{}], protocol [{}], chunk_size [{}], server_side_encryption [{}], buffer_size [{}], max_retries [{}], cannedACL [{}], storageClass [{}]",
                 bucket, region, endpoint, protocol, chunkSize, serverSideEncryption, bufferSize, maxRetries, cannedACL, storageClass);
@@ -128,7 +130,7 @@ public class S3Repository extends BlobStoreRepository {
         blobStore = new S3BlobStore(settings, s3Service.client(endpoint, protocol, region, repositorySettings.settings().get("access_key"), repositorySettings.settings().get("secret_key"), maxRetries),
                 bucket, region, serverSideEncryption, bufferSize, maxRetries, cannedACL, storageClass);
 
-        String basePath = repositorySettings.settings().get("base_path", settings.get("repositories.s3.base_path"));
+        String basePath = repositorySettings.settings().get("base_path", settings.get(REPOSITORY_S3.BASE_PATH));
         if (Strings.hasLength(basePath)) {
             BlobPath path = new BlobPath();
             for(String elem : Strings.splitStringToArray(basePath, '/')) {
diff --git a/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy b/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
index 62b29a2..e5f26c3 100644
--- a/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
@@ -19,6 +19,7 @@
 
 grant {
   // needed because of problems in ClientConfiguration
-  // TODO: get this fixed in aws sdk
+  // TODO: get these fixed in aws sdk
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.RuntimePermission "getClassLoader";
 };
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
index 34a57d7..b2b2c0c 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
@@ -631,6 +631,7 @@ public class PluginManagerTests extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-stempel");
         PluginManager.checkForOfficialPlugins("delete-by-query");
         PluginManager.checkForOfficialPlugins("lang-javascript");
+        PluginManager.checkForOfficialPlugins("lang-plan-a");
         PluginManager.checkForOfficialPlugins("lang-python");
         PluginManager.checkForOfficialPlugins("mapper-attachments");
         PluginManager.checkForOfficialPlugins("mapper-murmur3");
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
index f25bd87..66c3d79 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
@@ -25,7 +25,6 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
@@ -97,7 +96,7 @@ public class TribeUnitTests extends ESTestCase {
                 .put("tribe.t1.node.mode", NODE_MODE).put("tribe.t2.node.mode", NODE_MODE)
                 .put("path.home", createTempDir()).put(extraSettings).build();
 
-        try (Node node = NodeBuilder.nodeBuilder().settings(settings).node()) {
+        try (Node node = new Node(settings).start()) {
             try (Client client = node.client()) {
                 assertBusy(new Runnable() {
                     @Override
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index 48c34fa..54978b3 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -223,6 +223,18 @@ fi
     install_and_check_plugin discovery multicast
 }
 
+@test "[$GROUP] install lang-expression plugin" {
+    install_and_check_plugin lang expression
+}
+
+@test "[$GROUP] install lang-groovy plugin" {
+    install_and_check_plugin lang groovy
+}
+
+@test "[$GROUP] install lang-plan-a plugin" {
+    install_and_check_plugin lang plan-a 
+}
+
 @test "[$GROUP] install javascript plugin" {
     install_and_check_plugin lang javascript rhino-*.jar
 }
@@ -323,6 +335,18 @@ fi
     remove_plugin discovery-multicast
 }
 
+@test "[$GROUP] remove lang-expression plugin" {
+    remove_plugin lang-expression
+}
+
+@test "[$GROUP] remove lang-groovy plugin" {
+    remove_plugin lang-groovy
+}
+
+@test "[$GROUP] remove lang-plan-a plugin" {
+    remove_plugin lang-plan-a
+}
+
 @test "[$GROUP] remove javascript plugin" {
     remove_plugin lang-javascript
 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
index 00b0ec1..9fe9bfe 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
@@ -44,6 +44,14 @@
           "type" : "string",
           "description" : "The name of the tokenizer to use for the analysis"
         },
+        "detail": {
+          "type" : "boolean",
+          "description" : "With `true`, outputs more advanced details. (default: false)"
+        },
+        "attributes": {
+          "type" : "list",
+          "description" : "A comma-separated list of token attributes to output, this parameter works only with `detail=true`"
+        },
         "format": {
           "type": "enum",
           "options" : ["detailed","text"],
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
index 4942067..0b1a090 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
@@ -71,3 +71,31 @@ setup:
     - length: {tokens: 2 }
     - match:     { tokens.0.token: foo bar }
     - match:     { tokens.1.token: baz }
+---
+"Detail response with Analyzer":
+    - do:
+        indices.analyze:
+          body: {"text": "This is troubled", "analyzer": standard, "explain": true}
+    - length: { detail.analyzer.tokens: 3 }
+    - match:     { detail.analyzer.name: standard }
+    - match:     { detail.analyzer.tokens.0.token: this }
+    - match:     { detail.analyzer.tokens.1.token: is }
+    - match:     { detail.analyzer.tokens.2.token: troubled }
+---
+"Detail output spcified attribute":
+    - do:
+        indices.analyze:
+          body: {"text": "<text>This is troubled</text>", "char_filters": ["html_strip"], "filters": ["snowball"], "tokenizer": standard, "explain": true, "attributes": ["keyword"]}
+    - length: { detail.charfilters: 1 }
+    - length: { detail.tokenizer.tokens: 3 }
+    - length: { detail.tokenfilters.0.tokens: 3 }
+    - match:     { detail.tokenizer.name: standard }
+    - match:     { detail.tokenizer.tokens.0.token: This }
+    - match:     { detail.tokenizer.tokens.1.token: is }
+    - match:     { detail.tokenizer.tokens.2.token: troubled }
+    - match:     { detail.tokenfilters.0.name: snowball }
+    - match:     { detail.tokenfilters.0.tokens.0.token: This }
+    - match:     { detail.tokenfilters.0.tokens.1.token: is }
+    - match:     { detail.tokenfilters.0.tokens.2.token: troubl }
+    - match:     { detail.tokenfilters.0.tokens.2.keyword: false }
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
index 5b45f74..041f6bb 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
@@ -32,3 +32,50 @@
         name: test_alias
 
   - match: {test_index.aliases.test_alias: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
+
+---
+"Basic test for multiple aliases":
+
+  - do:
+      indices.create:
+        index: test_index
+
+  - do:
+      indices.exists_alias:
+        name: test_alias1
+
+  - is_false: ''
+
+  - do:
+      indices.exists_alias:
+        name: test_alias2
+
+  - is_false: ''
+
+  - do:
+      indices.update_aliases:
+        body:
+          actions:
+            - add:
+                indices: [test_index]
+                aliases: [test_alias1, test_alias2]
+                routing: routing_value
+
+  - do:
+      indices.exists_alias:
+        name: test_alias1
+
+  - is_true: ''
+
+  - do:
+      indices.exists_alias:
+        name: test_alias2
+
+  - is_true: ''
+
+  - do:
+      indices.get_alias:
+        index: test_index
+
+  - match: {test_index.aliases.test_alias1: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
+  - match: {test_index.aliases.test_alias2: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml
deleted file mode 100644
index 49e34fb..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
----
-"Basic multi-search":
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     1
-          body:   { foo: bar }
-
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     2
-          body:   { foo: baz }
-
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     3
-          body:   { foo: foo }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      msearch:
-        body:
-          - index: test_1
-          - query:
-              match_all: {}
-          - index: test_2
-          - query:
-              match_all: {}
-          - search_type: query_then_fetch
-            index: test_1
-          - query:
-              match: {foo: bar}
-
-  - match:  { responses.0.hits.total:     3  }
-  - match:  { responses.1.error.root_cause.0.type: index_not_found_exception }
-  - match:  { responses.1.error.root_cause.0.reason: "/no.such.index/" }
-  - match:  { responses.1.error.root_cause.0.index: test_2 }
-  - match:  { responses.2.hits.total:     1  }
-
-  - do:
-      msearch:
-        body: 
-          - index: test_1
-          - query:
-              { "template": { "query": { "term": { "foo": { "value": "{{template}}" } } }, "params": { "template": "bar" } } }
-  - match: { responses.0.hits.total: 1 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml
deleted file mode 100644
index d2474b7..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml
+++ /dev/null
@@ -1,48 +0,0 @@
----
-"Template query":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2 value3" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": { "term": { "text": { "value": "{{template}}" } } }, "params": { "template": "value1" } } } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search: 
-        body: { "query": { "template": { "query": {"match_{{template}}": {}}, "params" : { "template" : "all" } } } }
-
-  - match: { hits.total: 2 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{ \"term\": { \"text\": { \"value\": \"{{template}}\" } } }", "params": { "template": "value1" } } } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{\"match_{{template}}\": {}}", "params" : { "template" : "all" } } } }
-
-  - match: { hits.total: 2 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{\"query_string\": { \"query\" : \"{{query}}\" }}", "params" : { "query" : "text:\"value2 value3\"" } } } }
-
-
-  - match: { hits.total: 1 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml
deleted file mode 100644
index a0a6695..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml
+++ /dev/null
@@ -1,29 +0,0 @@
----
-"Template search request":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      search_template:
-        body: { "template" : { "query": { "term": { "text": { "value": "{{template}}" } } } }, "params": { "template": "value1" } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search_template:
-        body: { "template" : { "query": { "match_{{template}}": {} } }, "params" : { "template" : "all" } }
-
-  - match: { hits.total: 2 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml
deleted file mode 100644
index bd1fd43..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml
+++ /dev/null
@@ -1,56 +0,0 @@
----
-"Indexed template":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match_all": {}}, "size": "{{my_size}}" } }
-  - match: { _id: "1" }
-
-  - do:
-      get_template:
-        id: 1
-  - match: { found: true }
-  - match: { lang: mustache }
-  - match: { _id: "1" }
-  - match: { _version: 1 }
-  - match: { template: /.*query\S\S\S\Smatch_all.*/ }
-
-  - do:
-      catch: missing
-      get_template:
-        id: 2
-  - match: { found: false }
-  - match: { lang: mustache }
-  - match: { _id: "2" }
-  - is_false: _version
-  - is_false: template
-
-  - do:
-      delete_template:
-        id: "1"
-  - match: { found: true }
-  - match: { _index: ".scripts" }
-  - match: { _id: "1" }
-  - match: { _version: 2}
-
-  - do:
-      catch: missing
-      delete_template:
-        id: "non_existing"
-  - match: { found: false }
-  - match: { _index: ".scripts" }
-  - match: { _id: "non_existing" }
-  - match: { _version: 1 }
-
-  - do:
-      catch: request
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
-
-  - do:
-      catch: /Unable\sto\sparse.*/
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
deleted file mode 100644
index 4da748a..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
+++ /dev/null
@@ -1,38 +0,0 @@
----
-"Indexed Template query tests":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1_foo" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2_foo value3_foo" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match" : { "text": "{{my_value}}" } }, "size": "{{my_size}}" } }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-
-  - do:
-      search_template:
-        body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
-  - match: { hits.total: 1 }
-
-  - do:
-      catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
-      search_template:
-        body: { "file" : "simple1"}
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml
deleted file mode 100644
index 5d5c3d5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
----
-"Indexed Template validate tests":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } } }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "foo", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "bar", "my_field": "my_other_field" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
-
-  - do:
-      render_search_template:
-        id: "1"
-        body: { "params": { "my_value": "bar", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
----
-"Inline Template validate tests":
-
-  - do:
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "foo", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
-  - do:
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "my_other_field" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
-
-  - do:
-      catch: /Improperly.closed.variable.in.query-template/
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "field1" } }
----
-"Escaped Indexed Template validate tests":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }" }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "foo", "my_size": 20 } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.size: 20 }
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
-  - do:
-      render_search_template:
-        id: "1"
-        body: { "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
----
-"Escaped Inline Template validate tests":
-
-  - do:
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "foo", "my_size": 20 } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.size: 20 }
-
-  - do:
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
-  - do:
-      catch: /Improperly.closed.variable.in.query-template/
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
diff --git a/settings.gradle b/settings.gradle
index adeabd7..e928e53 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -11,6 +11,7 @@ List projects = [
   'test-framework',
   'modules:lang-expression',
   'modules:lang-groovy',
+  'modules:lang-mustache',
   'plugins:analysis-icu',
   'plugins:analysis-kuromoji',
   'plugins:analysis-phonetic',
@@ -21,8 +22,8 @@ List projects = [
   'plugins:discovery-ec2',
   'plugins:discovery-gce',
   'plugins:discovery-multicast',
-  'plugins:ingest',
   'plugins:lang-javascript',
+  'plugins:lang-plan-a',
   'plugins:lang-python',
   'plugins:mapper-attachments',
   'plugins:mapper-murmur3',
diff --git a/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java b/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java
new file mode 100644
index 0000000..bfd4090
--- /dev/null
+++ b/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A dummy script engine used for testing. Scripts must be a number. Running the script
+ */
+public class MockScriptEngine implements ScriptEngineService {
+    public static final String NAME = "mockscript";
+
+    public static class TestPlugin extends Plugin {
+
+        public TestPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for integration tests";
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(MockScriptEngine.class);
+        }
+
+    }
+
+    @Override
+    public String[] types() {
+        return new String[]{ NAME };
+    }
+
+    @Override
+    public String[] extensions() {
+        return types();
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public Object compile(String script) {
+        return script;
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, @Nullable Map<String, Object> vars) {
+        return new AbstractExecutableScript() {
+            @Override
+            public Object run() {
+                return new BytesArray((String)compiledScript.compiled());
+            }
+        };
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
+        return new SearchScript() {
+            @Override
+            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
+                AbstractSearchScript leafSearchScript = new AbstractSearchScript() {
+
+                    @Override
+                    public Object run() {
+                        return compiledScript.compiled();
+                    }
+
+                };
+                leafSearchScript.setLookup(lookup.getLeafSearchLookup(context));
+                return leafSearchScript;
+            }
+
+            @Override
+            public boolean needsScores() {
+                return false;
+            }
+        };
+    }
+
+    @Override
+    public void scriptRemoved(@Nullable CompiledScript script) {
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+}
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
index cc5348b..05dab1c 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -450,7 +450,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
                     .setOrder(0)
                     .setSettings(randomSettingsBuilder);
             if (mappings != null) {
-                logger.info("test using _default_ mappings: [{}]", mappings.bytesStream().bytes().toUtf8());
+                logger.info("test using _default_ mappings: [{}]", mappings.bytes().toUtf8());
                 putTemplate.addMapping("_default_", mappings);
             }
             assertAcked(putTemplate.execute().actionGet());
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
index 887d5f9..287bd12 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
@@ -19,12 +19,12 @@
 package org.elasticsearch.test;
 
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -38,7 +38,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.internal.SearchContext;
@@ -48,7 +47,9 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
 
 /**
  * A test that keep a singleton node started for all tests that can be used to get
@@ -119,7 +120,7 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
     }
 
     private static Node newNode() {
-        Node build = NodeBuilder.nodeBuilder().local(true).data(true).settings(Settings.builder()
+        Node build = new Node(Settings.builder()
                 .put(ClusterName.SETTING, InternalTestCluster.clusterName("single-node-cluster", randomLong()))
                 .put("path.home", createTempDir())
                 // TODO: use a consistent data path for custom paths
@@ -132,8 +133,11 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
                 .put("script.indexed", "on")
                 .put(EsExecutors.PROCESSORS, 1) // limit the number of threads created
                 .put("http.enabled", false)
+                .put("node.local", true)
+                .put("node.data", true)
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
-        ).build();
+                .build()
+        );
         build.start();
         assertThat(DiscoveryNode.localNode(build.settings()), is(true));
         return build;
diff --git a/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java b/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
index 30a89e4..93eef96 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
@@ -35,7 +35,7 @@ public class VersionUtils {
 
     private static final List<Version> SORTED_VERSIONS;
     static {
-        Field[] declaredFields = Version.class.getDeclaredFields();
+        Field[] declaredFields = Version.class.getFields();
         Set<Integer> ids = new HashSet<>();
         for (Field field : declaredFields) {
             final int mod = field.getModifiers();
diff --git a/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 7176916..9d8ad7f 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -705,7 +705,7 @@ public class ElasticsearchAssertions {
             IllegalAccessException, InvocationTargetException {
         try {
             Class<? extends Streamable> clazz = streamable.getClass();
-            Constructor<? extends Streamable> constructor = clazz.getDeclaredConstructor();
+            Constructor<? extends Streamable> constructor = clazz.getConstructor();
             assertThat(constructor, Matchers.notNullValue());
             Streamable newInstance = constructor.newInstance();
             return newInstance;
diff --git a/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java b/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
index 3c8913f..bb07223 100644
--- a/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
+++ b/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
@@ -47,7 +47,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(xyzLogger.getLevel(), nullValue());
         assertThat(abcLogger.getLevel(), nullValue());
 
-        Method method = TestClass.class.getDeclaredMethod("annotatedTestMethod");
+        Method method = TestClass.class.getMethod("annotatedTestMethod");
         TestLogging annotation = method.getAnnotation(TestLogging.class);
         Description testDescription = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod", annotation);
         loggingListener.testStarted(testDescription);
@@ -105,7 +105,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(abcLogger.getLevel(), equalTo("ERROR"));
         assertThat(xyzLogger.getLevel(), nullValue());
 
-        Method method = TestClass.class.getDeclaredMethod("annotatedTestMethod");
+        Method method = TestClass.class.getMethod("annotatedTestMethod");
         TestLogging annotation = method.getAnnotation(TestLogging.class);
         Description testDescription = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod", annotation);
         loggingListener.testStarted(testDescription);
@@ -116,7 +116,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(abcLogger.getLevel(), equalTo("ERROR"));
         assertThat(xyzLogger.getLevel(), nullValue());
 
-        Method method2 = TestClass.class.getDeclaredMethod("annotatedTestMethod2");
+        Method method2 = TestClass.class.getMethod("annotatedTestMethod2");
         TestLogging annotation2 = method2.getAnnotation(TestLogging.class);
         Description testDescription2 = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod2", annotation2);
         loggingListener.testStarted(testDescription2);
