diff --git a/TESTING.asciidoc b/TESTING.asciidoc
index 108c6ba..3b45c4e 100644
--- a/TESTING.asciidoc
+++ b/TESTING.asciidoc
@@ -397,10 +397,10 @@ fedora-22 with:
 mvn -Dtests.vagrant -pl qa/vagrant verify -DboxesToTest=fedora-22
 --------------------------------------------
 
-or run wheezy and trusty:
+or run jessie and trusty:
 
 ------------------------------------------------------------------
-mvn -Dtests.vagrant -pl qa/vagrant verify -DboxesToTest='wheezy, trusty'
+mvn -Dtests.vagrant -pl qa/vagrant verify -DboxesToTest='jessie, trusty'
 ------------------------------------------------------------------
 
 or run all the boxes:
@@ -440,7 +440,6 @@ These are the linux flavors the Vagrantfile currently supports:
 * precise aka Ubuntu 12.04
 * trusty aka Ubuntu 14.04
 * vivid aka Ubuntun 15.04
-* wheezy aka Debian 7, the current debian oldstable distribution
 * jessie aka Debian 8, the current debina stable distribution
 * centos-6
 * centos-7
diff --git a/core/LICENSE.txt b/core/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/core/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/core/NOTICE.txt b/core/NOTICE.txt
deleted file mode 100644
index f0bbfac..0000000
--- a/core/NOTICE.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-jts-*.jar is under the LGPL license. It is an optional dependency. The original
-source code can be found at http://www.vividsolutions.com/jts/JTSHome.htm
-
-jna-*.jar is dual licensed under the Apache License v2 and the LGPLv2.
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/core/src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java b/core/src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java
index 1889c6e..e09c555 100644
--- a/core/src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java
+++ b/core/src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java
@@ -76,10 +76,6 @@ public class ExtendedCommonTermsQuery extends CommonTermsQuery {
         return lowFreqMinNumShouldMatchSpec;
     }
 
-    public float getMaxTermFrequency() {
-        return this.maxTermFrequency;
-    }
-
     @Override
     protected Query newTermQuery(Term term, TermContext context) {
         if (fieldType == null) {
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
index cb4bee3..6cac629 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.ExistsQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.ExistsQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,7 +32,7 @@ public class ExistsFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_exists_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        return new ConstantScoreQuery(ExistsQueryBuilder.newFilter(context, queryText));
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(ExistsQueryParser.newFilter(parseContext, queryText, null));
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
index 299a37a..003ff18 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
@@ -20,12 +20,12 @@
 package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
  */
 public interface FieldQueryExtension {
 
-    Query query(QueryShardContext context, String queryText);
+    Query query(QueryParseContext parseContext, String queryText);
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index 34fdcfc..fe0f640 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -19,21 +19,31 @@
 
 package org.apache.lucene.queryparser.classic;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MatchNoDocsQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
+import com.google.common.collect.ImmutableMap;
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
@@ -45,8 +55,8 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
 /**
  * A query parser that uses the {@link MapperService} in order to build smarter
  * queries based on the mapping information.
- * <p/>
- * <p>Also breaks fields with [type].[name] into a boolean query that must include the type
+ * <p>
+ * Also breaks fields with [type].[name] into a boolean query that must include the type
  * as well as the query on the name.
  */
 public class MapperQueryParser extends QueryParser {
@@ -60,27 +70,53 @@ public class MapperQueryParser extends QueryParser {
                 .build();
     }
 
-    private final QueryShardContext context;
+    private final QueryParseContext parseContext;
 
     private QueryParserSettings settings;
 
+    private Analyzer quoteAnalyzer;
+
+    private boolean forcedAnalyzer;
+    private boolean forcedQuoteAnalyzer;
+
     private MappedFieldType currentFieldType;
 
-    public MapperQueryParser(QueryShardContext context) {
+    private boolean analyzeWildcard;
+
+    private String quoteFieldSuffix;
+
+    public MapperQueryParser(QueryParseContext parseContext) {
         super(null, null);
-        this.context = context;
+        this.parseContext = parseContext;
     }
 
     public void reset(QueryParserSettings settings) {
         this.settings = settings;
-        if (settings.fieldsAndWeights().isEmpty()) {
-            this.field = settings.defaultField();
-        } else if (settings.fieldsAndWeights().size() == 1) {
-            this.field = settings.fieldsAndWeights().keySet().iterator().next();
+        this.field = settings.defaultField();
+
+        if (settings.fields() != null) {
+            if (settings.fields.size() == 1) {
+                // just mark it as the default field
+                this.field = settings.fields().get(0);
+            } else {
+                // otherwise, we need to have the default field being null...
+                this.field = null;
+            }
+        }
+
+        this.forcedAnalyzer = settings.forcedAnalyzer() != null;
+        this.setAnalyzer(forcedAnalyzer ? settings.forcedAnalyzer() : settings.defaultAnalyzer());
+        if (settings.forcedQuoteAnalyzer() != null) {
+            this.forcedQuoteAnalyzer = true;
+            this.quoteAnalyzer = settings.forcedQuoteAnalyzer();
+        } else if (forcedAnalyzer) {
+            this.forcedQuoteAnalyzer = true;
+            this.quoteAnalyzer = settings.forcedAnalyzer();
         } else {
-            this.field = null;
+            this.forcedAnalyzer = false;
+            this.quoteAnalyzer = settings.defaultQuoteAnalyzer();
         }
-        setAnalyzer(settings.analyzer());
+        this.quoteFieldSuffix = settings.quoteFieldSuffix();
         setMultiTermRewriteMethod(settings.rewriteMethod());
         setEnablePositionIncrements(settings.enablePositionIncrements());
         setAutoGeneratePhraseQueries(settings.autoGeneratePhraseQueries());
@@ -89,9 +125,10 @@ public class MapperQueryParser extends QueryParser {
         setLowercaseExpandedTerms(settings.lowercaseExpandedTerms());
         setPhraseSlop(settings.phraseSlop());
         setDefaultOperator(settings.defaultOperator());
-        setFuzzyMinSim(settings.fuzziness().asFloat());
+        setFuzzyMinSim(settings.getFuzziness().asFloat());
         setFuzzyPrefixLength(settings.fuzzyPrefixLength());
         setLocale(settings.locale());
+        this.analyzeWildcard = settings.analyzeWildcard();
     }
 
     /**
@@ -125,7 +162,7 @@ public class MapperQueryParser extends QueryParser {
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
         FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
         if (fieldQueryExtension != null) {
-            return fieldQueryExtension.query(context, queryText);
+            return fieldQueryExtension.query(parseContext, queryText);
         }
         Collection<String> fields = extractMultiFields(field);
         if (fields != null) {
@@ -187,29 +224,29 @@ public class MapperQueryParser extends QueryParser {
         Analyzer oldAnalyzer = getAnalyzer();
         try {
             if (quoted) {
-                setAnalyzer(settings.quoteAnalyzer());
-                if (settings.quoteFieldSuffix() != null) {
-                    currentFieldType = context.fieldMapper(field + settings.quoteFieldSuffix());
+                setAnalyzer(quoteAnalyzer);
+                if (quoteFieldSuffix != null) {
+                    currentFieldType = parseContext.fieldMapper(field + quoteFieldSuffix);
                 }
             }
             if (currentFieldType == null) {
-                currentFieldType = context.fieldMapper(field);
+                currentFieldType = parseContext.fieldMapper(field);
             }
             if (currentFieldType != null) {
                 if (quoted) {
-                    if (!settings.forceQuoteAnalyzer()) {
-                        setAnalyzer(context.getSearchQuoteAnalyzer(currentFieldType));
+                    if (!forcedQuoteAnalyzer) {
+                        setAnalyzer(parseContext.getSearchQuoteAnalyzer(currentFieldType));
                     }
                 } else {
-                    if (!settings.forceAnalyzer()) {
-                        setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    if (!forcedAnalyzer) {
+                        setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                     }
                 }
                 if (currentFieldType != null) {
                     Query query = null;
                     if (currentFieldType.useTermQueryWithQueryString()) {
                         try {
-                            query = currentFieldType.termQuery(queryText, context);
+                            query = currentFieldType.termQuery(queryText, parseContext);
                         } catch (RuntimeException e) {
                             if (settings.lenient()) {
                                 return null;
@@ -320,7 +357,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getRangeQuerySingle(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             if (lowercaseExpandedTerms && !currentFieldType.isNumeric()) {
                 part1 = part1 == null ? null : part1.toLowerCase(locale);
@@ -385,7 +422,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getFuzzyQuerySingle(String field, String termStr, String minSimilarity) throws ParseException {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             try {
                 return currentFieldType.fuzzyQuery(termStr, Fuzziness.build(minSimilarity), fuzzyPrefixLength, settings.fuzzyMaxExpansions(), FuzzyQuery.defaultTranspositions);
@@ -455,14 +492,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
-                if (!settings.forceAnalyzer()) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                if (!forcedAnalyzer) {
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, context);
+                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = getPossiblyAnalyzedPrefixQuery(currentFieldType.names().indexName(), termStr);
@@ -481,7 +518,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getPossiblyAnalyzedPrefixQuery(String field, String termStr) throws ParseException {
-        if (!settings.analyzeWildcard()) {
+        if (!analyzeWildcard) {
             return super.getPrefixQuery(field, termStr);
         }
         // get Analyzer from superclass and tokenize the term
@@ -519,7 +556,16 @@ public class MapperQueryParser extends QueryParser {
                 clauses.add(new BooleanClause(super.getPrefixQuery(field, token), BooleanClause.Occur.SHOULD));
             }
             return getBooleanQuery(clauses, true);
+
+            //return super.getPrefixQuery(field, termStr);
+
+            /* this means that the analyzer used either added or consumed
+* (common for a stemmer) tokens, and we can't build a PrefixQuery */
+//            throw new ParseException("Cannot build PrefixQuery with analyzer "
+//                    + getAnalyzer().getClass()
+//                    + (tlist.size() > 1 ? " - token(s) added" : " - token consumed"));
         }
+
     }
 
     @Override
@@ -538,7 +584,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(parseContext, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
@@ -587,10 +633,10 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
-                if (!settings.forceAnalyzer()) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                if (!forcedAnalyzer) {
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 indexedNameField = currentFieldType.names().indexName();
                 return getPossiblyAnalyzedWildcardQuery(indexedNameField, termStr);
@@ -607,7 +653,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getPossiblyAnalyzedWildcardQuery(String field, String termStr) throws ParseException {
-        if (!settings.analyzeWildcard()) {
+        if (!analyzeWildcard) {
             return super.getWildcardQuery(field, termStr);
         }
         boolean isWithinToken = (!termStr.startsWith("?") && !termStr.startsWith("*"));
@@ -719,14 +765,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
-                if (!settings.forceAnalyzer()) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                if (!forcedAnalyzer) {
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, context);
+                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = super.getRegexpQuery(field, termStr);
@@ -754,9 +800,9 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private void applyBoost(String field, Query q) {
-        Float fieldBoost = settings.fieldsAndWeights().get(field);
-        if (fieldBoost != null) {
-            q.setBoost(fieldBoost);
+        if (settings.boosts() != null) {
+            float boost = settings.boosts().getOrDefault(field, 1f);
+            q.setBoost(boost);
         }
     }
 
@@ -782,11 +828,11 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Collection<String> extractMultiFields(String field) {
-        Collection<String> fields;
+        Collection<String> fields = null;
         if (field != null) {
-            fields = context.simpleMatchToIndexNames(field);
+            fields = parseContext.simpleMatchToIndexNames(field);
         } else {
-            fields = settings.fieldsAndWeights().keySet();
+            fields = settings.fields();
         }
         return fields;
     }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
index f9fc8c9..ed1b704 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.MissingQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.MissingQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,11 +32,8 @@ public class MissingFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_missing_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        Query query = MissingQueryBuilder.newFilter(context, queryText, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE, MissingQueryBuilder.DEFAULT_NULL_VALUE);
-        if (query != null) {
-            return new ConstantScoreQuery(query);
-        }
-        return null;
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(MissingQueryParser.newFilter(parseContext, queryText,
+                MissingQueryParser.DEFAULT_EXISTENCE_VALUE, MissingQueryParser.DEFAULT_NULL_VALUE, null));
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java b/core/src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java
index c1fc2ae..76e8b4f 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java
@@ -19,74 +19,66 @@
 
 package org.apache.lucene.queryparser.classic;
 
+import com.carrotsearch.hppc.ObjectFloatHashMap;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.FuzzyQuery;
 import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.util.automaton.Operations;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.joda.time.DateTimeZone;
 
+import java.util.List;
 import java.util.Locale;
-import java.util.Map;
 
 /**
- * Encapsulates settings that affect query_string parsing via {@link MapperQueryParser}
+ *
  */
 public class QueryParserSettings {
 
-    private final String queryString;
+    public static final boolean DEFAULT_ALLOW_LEADING_WILDCARD = true;
+    public static final boolean DEFAULT_ANALYZE_WILDCARD = false;
+    public static final float DEFAULT_BOOST = 1.f;
 
+    private String queryString;
     private String defaultField;
-
-    private Map<String, Float> fieldsAndWeights;
-
-    private QueryParser.Operator defaultOperator;
-
-    private Analyzer analyzer;
-    private boolean forceAnalyzer;
-    private Analyzer quoteAnalyzer;
-    private boolean forceQuoteAnalyzer;
-
-    private String quoteFieldSuffix;
-
-    private boolean autoGeneratePhraseQueries;
-
-    private boolean allowLeadingWildcard;
-
-    private boolean analyzeWildcard;
-
-    private boolean lowercaseExpandedTerms;
-
-    private boolean enablePositionIncrements;
-
-    private Locale locale;
-
-    private Fuzziness fuzziness;
-    private int fuzzyPrefixLength;
-    private int fuzzyMaxExpansions;
-    private MultiTermQuery.RewriteMethod fuzzyRewriteMethod;
-
-    private int phraseSlop;
-
-    private boolean useDisMax;
-
-    private float tieBreaker;
-
-    private MultiTermQuery.RewriteMethod rewriteMethod;
-
+    private float boost = DEFAULT_BOOST;
+    private MapperQueryParser.Operator defaultOperator = QueryParser.Operator.OR;
+    private boolean autoGeneratePhraseQueries = false;
+    private boolean allowLeadingWildcard = DEFAULT_ALLOW_LEADING_WILDCARD;
+    private boolean lowercaseExpandedTerms = true;
+    private boolean enablePositionIncrements = true;
+    private int phraseSlop = 0;
+    private Fuzziness fuzziness = Fuzziness.AUTO;
+    private int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+    private int fuzzyMaxExpansions = FuzzyQuery.defaultMaxExpansions;
+    private int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
+    private MultiTermQuery.RewriteMethod fuzzyRewriteMethod = null;
+    private boolean analyzeWildcard = DEFAULT_ANALYZE_WILDCARD;
+    private boolean escape = false;
+    private Analyzer defaultAnalyzer = null;
+    private Analyzer defaultQuoteAnalyzer = null;
+    private Analyzer forcedAnalyzer = null;
+    private Analyzer forcedQuoteAnalyzer = null;
+    private String quoteFieldSuffix = null;
+    private MultiTermQuery.RewriteMethod rewriteMethod = MultiTermQuery.CONSTANT_SCORE_FILTER_REWRITE;
+    private String minimumShouldMatch;
     private boolean lenient;
-
+    private Locale locale;
     private DateTimeZone timeZone;
 
-    /** To limit effort spent determinizing regexp queries. */
-    private int maxDeterminizedStates;
-
-    public QueryParserSettings(String queryString) {
-        this.queryString = queryString;
-    }
+    List<String> fields = null;
+    ObjectFloatHashMap<String> boosts = null;
+    float tieBreaker = 0.0f;
+    boolean useDisMax = true;
 
     public String queryString() {
         return queryString;
     }
 
+    public void queryString(String queryString) {
+        this.queryString = queryString;
+    }
+
     public String defaultField() {
         return defaultField;
     }
@@ -95,12 +87,12 @@ public class QueryParserSettings {
         this.defaultField = defaultField;
     }
 
-    public Map<String, Float> fieldsAndWeights() {
-        return fieldsAndWeights;
+    public float boost() {
+        return boost;
     }
 
-    public void fieldsAndWeights(Map<String, Float> fieldsAndWeights) {
-        this.fieldsAndWeights = fieldsAndWeights;
+    public void boost(float boost) {
+        this.boost = boost;
     }
 
     public QueryParser.Operator defaultOperator() {
@@ -183,40 +175,44 @@ public class QueryParserSettings {
         this.fuzzyRewriteMethod = fuzzyRewriteMethod;
     }
 
-    public void defaultAnalyzer(Analyzer analyzer) {
-        this.analyzer = analyzer;
-        this.forceAnalyzer = false;
+    public boolean escape() {
+        return escape;
     }
 
-    public void forceAnalyzer(Analyzer analyzer) {
-        this.analyzer = analyzer;
-        this.forceAnalyzer = true;
+    public void escape(boolean escape) {
+        this.escape = escape;
     }
 
-    public Analyzer analyzer() {
-        return analyzer;
+    public Analyzer defaultAnalyzer() {
+        return defaultAnalyzer;
     }
 
-    public boolean forceAnalyzer() {
-        return forceAnalyzer;
+    public void defaultAnalyzer(Analyzer defaultAnalyzer) {
+        this.defaultAnalyzer = defaultAnalyzer;
     }
 
-    public void defaultQuoteAnalyzer(Analyzer quoteAnalyzer) {
-        this.quoteAnalyzer = quoteAnalyzer;
-        this.forceQuoteAnalyzer = false;
+    public Analyzer defaultQuoteAnalyzer() {
+        return defaultQuoteAnalyzer;
     }
 
-    public void forceQuoteAnalyzer(Analyzer quoteAnalyzer) {
-        this.quoteAnalyzer = quoteAnalyzer;
-        this.forceQuoteAnalyzer = true;
+    public void defaultQuoteAnalyzer(Analyzer defaultAnalyzer) {
+        this.defaultQuoteAnalyzer = defaultAnalyzer;
     }
 
-    public Analyzer quoteAnalyzer() {
-        return quoteAnalyzer;
+    public Analyzer forcedAnalyzer() {
+        return forcedAnalyzer;
     }
 
-    public boolean forceQuoteAnalyzer() {
-        return forceQuoteAnalyzer;
+    public void forcedAnalyzer(Analyzer forcedAnalyzer) {
+        this.forcedAnalyzer = forcedAnalyzer;
+    }
+
+    public Analyzer forcedQuoteAnalyzer() {
+        return forcedQuoteAnalyzer;
+    }
+
+    public void forcedQuoteAnalyzer(Analyzer forcedAnalyzer) {
+        this.forcedQuoteAnalyzer = forcedAnalyzer;
     }
 
     public boolean analyzeWildcard() {
@@ -235,6 +231,14 @@ public class QueryParserSettings {
         this.rewriteMethod = rewriteMethod;
     }
 
+    public String minimumShouldMatch() {
+        return this.minimumShouldMatch;
+    }
+
+    public void minimumShouldMatch(String minimumShouldMatch) {
+        this.minimumShouldMatch = minimumShouldMatch;
+    }
+
     public void quoteFieldSuffix(String quoteFieldSuffix) {
         this.quoteFieldSuffix = quoteFieldSuffix;
     }
@@ -251,6 +255,22 @@ public class QueryParserSettings {
         return this.lenient;
     }
 
+    public List<String> fields() {
+        return fields;
+    }
+
+    public void fields(List<String> fields) {
+        this.fields = fields;
+    }
+
+    public ObjectFloatHashMap<String> boosts() {
+        return boosts;
+    }
+
+    public void boosts(ObjectFloatHashMap<String> boosts) {
+        this.boosts = boosts;
+    }
+
     public float tieBreaker() {
         return tieBreaker;
     }
@@ -283,11 +303,97 @@ public class QueryParserSettings {
         return this.timeZone;
     }
 
-    public void fuzziness(Fuzziness fuzziness) {
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+
+        QueryParserSettings that = (QueryParserSettings) o;
+
+        if (autoGeneratePhraseQueries != that.autoGeneratePhraseQueries()) return false;
+        if (maxDeterminizedStates != that.maxDeterminizedStates()) return false;
+        if (allowLeadingWildcard != that.allowLeadingWildcard) return false;
+        if (Float.compare(that.boost, boost) != 0) return false;
+        if (enablePositionIncrements != that.enablePositionIncrements) return false;
+        if (escape != that.escape) return false;
+        if (analyzeWildcard != that.analyzeWildcard) return false;
+        if (fuzziness != null ? fuzziness.equals(that.fuzziness) == false : fuzziness != null) return false;
+        if (fuzzyPrefixLength != that.fuzzyPrefixLength) return false;
+        if (fuzzyMaxExpansions != that.fuzzyMaxExpansions) return false;
+        if (fuzzyRewriteMethod != null ? !fuzzyRewriteMethod.equals(that.fuzzyRewriteMethod) : that.fuzzyRewriteMethod != null)
+            return false;
+        if (lowercaseExpandedTerms != that.lowercaseExpandedTerms) return false;
+        if (phraseSlop != that.phraseSlop) return false;
+        if (defaultAnalyzer != null ? !defaultAnalyzer.equals(that.defaultAnalyzer) : that.defaultAnalyzer != null)
+            return false;
+        if (defaultQuoteAnalyzer != null ? !defaultQuoteAnalyzer.equals(that.defaultQuoteAnalyzer) : that.defaultQuoteAnalyzer != null)
+            return false;
+        if (forcedAnalyzer != null ? !forcedAnalyzer.equals(that.forcedAnalyzer) : that.forcedAnalyzer != null)
+            return false;
+        if (forcedQuoteAnalyzer != null ? !forcedQuoteAnalyzer.equals(that.forcedQuoteAnalyzer) : that.forcedQuoteAnalyzer != null)
+            return false;
+        if (defaultField != null ? !defaultField.equals(that.defaultField) : that.defaultField != null) return false;
+        if (defaultOperator != that.defaultOperator) return false;
+        if (queryString != null ? !queryString.equals(that.queryString) : that.queryString != null) return false;
+        if (rewriteMethod != null ? !rewriteMethod.equals(that.rewriteMethod) : that.rewriteMethod != null)
+            return false;
+        if (minimumShouldMatch != null ? !minimumShouldMatch.equals(that.minimumShouldMatch) : that.minimumShouldMatch != null)
+            return false;
+        if (quoteFieldSuffix != null ? !quoteFieldSuffix.equals(that.quoteFieldSuffix) : that.quoteFieldSuffix != null)
+            return false;
+        if (lenient != that.lenient) {
+            return false;
+        }
+        if (locale != null ? !locale.equals(that.locale) : that.locale != null) {
+            return false;
+        }
+        if (timeZone != null ? !timeZone.equals(that.timeZone) : that.timeZone != null) {
+            return false;
+        }
+
+        if (Float.compare(that.tieBreaker, tieBreaker) != 0) return false;
+        if (useDisMax != that.useDisMax) return false;
+        if (boosts != null ? !boosts.equals(that.boosts) : that.boosts != null) return false;
+        if (fields != null ? !fields.equals(that.fields) : that.fields != null) return false;
+
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        int result = queryString != null ? queryString.hashCode() : 0;
+        result = 31 * result + (defaultField != null ? defaultField.hashCode() : 0);
+        result = 31 * result + (boost != +0.0f ? Float.floatToIntBits(boost) : 0);
+        result = 31 * result + (defaultOperator != null ? defaultOperator.hashCode() : 0);
+        result = 31 * result + (autoGeneratePhraseQueries ? 1 : 0);
+        result = 31 * result + maxDeterminizedStates;
+        result = 31 * result + (allowLeadingWildcard ? 1 : 0);
+        result = 31 * result + (lowercaseExpandedTerms ? 1 : 0);
+        result = 31 * result + (enablePositionIncrements ? 1 : 0);
+        result = 31 * result + phraseSlop;
+        result = 31 * result + (fuzziness.hashCode());
+        result = 31 * result + fuzzyPrefixLength;
+        result = 31 * result + (escape ? 1 : 0);
+        result = 31 * result + (defaultAnalyzer != null ? defaultAnalyzer.hashCode() : 0);
+        result = 31 * result + (defaultQuoteAnalyzer != null ? defaultQuoteAnalyzer.hashCode() : 0);
+        result = 31 * result + (forcedAnalyzer != null ? forcedAnalyzer.hashCode() : 0);
+        result = 31 * result + (forcedQuoteAnalyzer != null ? forcedQuoteAnalyzer.hashCode() : 0);
+        result = 31 * result + (analyzeWildcard ? 1 : 0);
+
+        result = 31 * result + (fields != null ? fields.hashCode() : 0);
+        result = 31 * result + (boosts != null ? boosts.hashCode() : 0);
+        result = 31 * result + (tieBreaker != +0.0f ? Float.floatToIntBits(tieBreaker) : 0);
+        result = 31 * result + (useDisMax ? 1 : 0);
+        result = 31 * result + (locale != null ? locale.hashCode() : 0);
+        result = 31 * result + (timeZone != null ? timeZone.hashCode() : 0);
+        return result;
+    }
+
+    public void setFuzziness(Fuzziness fuzziness) {
         this.fuzziness = fuzziness;
     }
 
-    public Fuzziness fuzziness() {
+    public Fuzziness getFuzziness() {
         return fuzziness;
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/search/XFilteredDocIdSetIterator.java b/core/src/main/java/org/apache/lucene/search/XFilteredDocIdSetIterator.java
index 0b3600e..92f2f44 100644
--- a/core/src/main/java/org/apache/lucene/search/XFilteredDocIdSetIterator.java
+++ b/core/src/main/java/org/apache/lucene/search/XFilteredDocIdSetIterator.java
@@ -54,7 +54,7 @@ public abstract class XFilteredDocIdSetIterator extends DocIdSetIterator {
    * Validation method to determine whether a docid should be in the result set.
    * @param doc docid to be tested
    * @return true if input docid should be in the result set, false otherwise.
-   * @see #FilteredDocIdSetIterator(DocIdSetIterator)
+   * @see #XFilteredDocIdSetIterator(DocIdSetIterator)
    * @throws CollectionTerminatedException if the underlying iterator is exhausted.
    */
   protected abstract boolean match(int doc);
diff --git a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
index bac323d..5db4f93 100644
--- a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
+++ b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java
@@ -100,7 +100,7 @@ import java.util.*;
 public class XAnalyzingSuggester extends Lookup {
 
   /**
-   * FST<Weight,Surface>: 
+   * FST&lt;Weight,Surface&gt;: 
    *  input is the analyzed form, with a null byte between terms
    *  weights are encoded as costs: (Integer.MAX_VALUE-weight)
    *  surface is the original, unanalyzed form.
@@ -129,14 +129,14 @@ public class XAnalyzingSuggester extends Lookup {
    */
   private final boolean preserveSep;
 
-  /** Include this flag in the options parameter to {@link
+  /** Include this flag in the options parameter to {@code
    *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)} to always
    *  return the exact match first, regardless of score.  This
    *  has no performance impact but could result in
    *  low-quality suggestions. */
   public static final int EXACT_FIRST = 1;
 
-  /** Include this flag in the options parameter to {@link
+  /** Include this flag in the options parameter to {@code
    *  #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)} to preserve
    *  token separators when matching. */
   public static final int PRESERVE_SEP = 2;
@@ -183,7 +183,7 @@ public class XAnalyzingSuggester extends Lookup {
   private long count = 0;
 
     /**
-   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)
+   * Calls {@code #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)
    * AnalyzingSuggester(analyzer, analyzer, EXACT_FIRST |
    * PRESERVE_SEP, 256, -1)}
    */
@@ -192,7 +192,7 @@ public class XAnalyzingSuggester extends Lookup {
   }
 
   /**
-   * Calls {@link #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)
+   * Calls {@code #XAnalyzingSuggester(Analyzer,Analyzer,int,int,int,boolean,FST,boolean,int,int,int,int,int)
    * AnalyzingSuggester(indexAnalyzer, queryAnalyzer, EXACT_FIRST |
    * PRESERVE_SEP, 256, -1)}
    */
@@ -986,12 +986,12 @@ public long ramBytesUsed() {
     throw new UnsupportedOperationException();
   }
   
-  /** cost -> weight */
+  /** cost -&gt; weight */
   public static int decodeWeight(long encoded) {
     return (int)(Integer.MAX_VALUE - encoded);
   }
   
-  /** weight -> cost */
+  /** weight -&gt; cost */
   public static int encodeWeight(long value) {
     if (value < 0 || value > Integer.MAX_VALUE) {
       throw new UnsupportedOperationException("cannot encode value: " + value);
diff --git a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java
index 20f95c6..a4338f8 100644
--- a/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java
+++ b/core/src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java
@@ -115,7 +115,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {
     }
 
     /**
-     * Creates a {@link FuzzySuggester} instance with an index & a query analyzer initialized with default values.
+     * Creates a {@link FuzzySuggester} instance with an index &amp; a query analyzer initialized with default values.
      *
      * @param indexAnalyzer
      *           Analyzer that will be used for analyzing suggestions while building the index.
@@ -143,7 +143,7 @@ public final class XFuzzySuggester extends XAnalyzingSuggester {
      * @param maxGraphExpansions Maximum number of graph paths
      *        to expand from the analyzed form.  Set this to -1 for
      *        no limit.
-     * @param maxEdits must be >= 0 and <= {@link org.apache.lucene.util.automaton.LevenshteinAutomata#MAXIMUM_SUPPORTED_DISTANCE} .
+     * @param maxEdits must be &gt;= 0 and &lt;= {@link org.apache.lucene.util.automaton.LevenshteinAutomata#MAXIMUM_SUPPORTED_DISTANCE} .
      * @param transpositions <code>true</code> if transpositions should be treated as a primitive
      *        edit operation. If this is false, comparisons will implement the classic
      *        Levenshtein algorithm.
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 13f5d9a0..c95b907 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -55,7 +55,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
     /**
      * Construct a <code>ElasticsearchException</code> with the specified detail message.
      *
-     * The message can be parameterized using {@code {}} as placeholders for the given
+     * The message can be parameterized using <code>{}</code> as placeholders for the given
      * arguments
      *
      * @param msg the detail message
@@ -69,7 +69,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
      * Construct a <code>ElasticsearchException</code> with the specified detail message
      * and nested exception.
      *
-     * The message can be parameterized using {@code {}} as placeholders for the given
+     * The message can be parameterized using <code>{}</code> as placeholders for the given
      * arguments
      *
      * @param msg   the detail message
@@ -606,9 +606,8 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         exceptions.put(org.elasticsearch.indices.TypeMissingException.class, 139);
         // added in 3.x
         exceptions.put(org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class, 140);
-        exceptions.put(org.elasticsearch.index.query.QueryShardException.class, 141);
 
-        final int maxOrd = 141;
+        final int maxOrd = 140;
         assert exceptions.size() == maxOrd + 1;
         Constructor<? extends ElasticsearchException>[] idToSupplier = new Constructor[maxOrd + 1];
         for (Map.Entry<Class<? extends ElasticsearchException>, Integer> e : exceptions.entrySet()) {
diff --git a/core/src/main/java/org/elasticsearch/action/ActionFuture.java b/core/src/main/java/org/elasticsearch/action/ActionFuture.java
index f2b1d87..26a9260 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionFuture.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionFuture.java
@@ -37,8 +37,8 @@ public interface ActionFuture<T> extends Future<T> {
      * Similar to {@link #get()}, just catching the {@link InterruptedException} and throwing
      * an {@link IllegalStateException} instead. Also catches
      * {@link java.util.concurrent.ExecutionException} and throws the actual cause instead.
-     * <p/>
-     * <p>Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
+     * <p>
+     * Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
      * from {@link org.elasticsearch.transport.RemoteTransportException}. The root failure is
      * still accessible using {@link #getRootFailure()}.
      */
@@ -48,8 +48,8 @@ public interface ActionFuture<T> extends Future<T> {
      * Similar to {@link #get(long, java.util.concurrent.TimeUnit)}, just catching the {@link InterruptedException} and throwing
      * an {@link IllegalStateException} instead. Also catches
      * {@link java.util.concurrent.ExecutionException} and throws the actual cause instead.
-     * <p/>
-     * <p>Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
+     * <p>
+     * Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
      * from {@link org.elasticsearch.transport.RemoteTransportException}. The root failure is
      * still accessible using {@link #getRootFailure()}.
      */
@@ -59,8 +59,8 @@ public interface ActionFuture<T> extends Future<T> {
      * Similar to {@link #get(long, java.util.concurrent.TimeUnit)}, just catching the {@link InterruptedException} and throwing
      * an {@link IllegalStateException} instead. Also catches
      * {@link java.util.concurrent.ExecutionException} and throws the actual cause instead.
-     * <p/>
-     * <p>Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
+     * <p>
+     * Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
      * from {@link org.elasticsearch.transport.RemoteTransportException}. The root failure is
      * still accessible using {@link #getRootFailure()}.
      *
@@ -72,8 +72,8 @@ public interface ActionFuture<T> extends Future<T> {
      * Similar to {@link #get(long, java.util.concurrent.TimeUnit)}, just catching the {@link InterruptedException} and throwing
      * an {@link IllegalStateException} instead. Also catches
      * {@link java.util.concurrent.ExecutionException} and throws the actual cause instead.
-     * <p/>
-     * <p>Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
+     * <p>
+     * Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
      * from {@link org.elasticsearch.transport.RemoteTransportException}. The root failure is
      * still accessible using {@link #getRootFailure()}.
      */
@@ -83,8 +83,8 @@ public interface ActionFuture<T> extends Future<T> {
      * Similar to {@link #get(long, java.util.concurrent.TimeUnit)}, just catching the {@link InterruptedException} and throwing
      * an {@link IllegalStateException} instead. Also catches
      * {@link java.util.concurrent.ExecutionException} and throws the actual cause instead.
-     * <p/>
-     * <p>Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
+     * <p>
+     * Note, the actual cause is unwrapped to the actual failure (for example, unwrapped
      * from {@link org.elasticsearch.transport.RemoteTransportException}. The root failure is
      * still accessible using {@link #getRootFailure()}.
      */
diff --git a/core/src/main/java/org/elasticsearch/action/DocumentRequest.java b/core/src/main/java/org/elasticsearch/action/DocumentRequest.java
index b804d7f..fcfea39 100644
--- a/core/src/main/java/org/elasticsearch/action/DocumentRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/DocumentRequest.java
@@ -53,7 +53,6 @@ public interface DocumentRequest<T> extends IndicesRequest {
 
     /**
      * Set the routing for this request
-     * @param routing
      * @return the Request
      */
     T routing(String routing);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
index 32114f6..ba1e733 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java
@@ -126,7 +126,7 @@ public class ClusterHealthRequest extends MasterNodeReadRequest<ClusterHealthReq
     }
 
     /**
-     * Waits for N number of nodes. Use "12" for exact mapping, ">12" and "<12" for range.
+     * Waits for N number of nodes. Use "12" for exact mapping, "&gt;12" and "&lt;12" for range.
      */
     public ClusterHealthRequest waitForNodes(String waitForNodes) {
         this.waitForNodes = waitForNodes;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java
index 71153ae..f12ab12 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java
@@ -74,7 +74,7 @@ public class ClusterHealthRequestBuilder extends MasterNodeReadOperationRequestB
     }
 
     /**
-     * Waits for N number of nodes. Use "12" for exact mapping, ">12" and "<12" for range.
+     * Waits for N number of nodes. Use "12" for exact mapping, "&gt;12" and "&lt;12" for range.
      */
     public ClusterHealthRequestBuilder setWaitForNodes(String waitForNodes) {
         request.waitForNodes(waitForNodes);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
index bb3b17b..84ca8dc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java
@@ -30,7 +30,7 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 
 /**
  * Unregister repository request.
- * <p/>
+ * <p>
  * The unregister repository command just unregisters the repository. No data is getting deleted from the repository.
  */
 public class DeleteRepositoryRequest extends AcknowledgedRequest<DeleteRepositoryRequest> {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
index b43dbf3..a0e6de9 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java
@@ -41,7 +41,7 @@ public class GetRepositoriesRequest extends MasterNodeReadRequest<GetRepositorie
 
     /**
      * Constructs a new get repositories request with a list of repositories.
-     * <p/>
+     * <p>
      * If the list of repositories is empty or it contains a single element "_all", all registered repositories
      * are returned.
      *
@@ -71,7 +71,7 @@ public class GetRepositoriesRequest extends MasterNodeReadRequest<GetRepositorie
 
     /**
      * Sets the list or repositories.
-     * <p/>
+     * <p>
      * If the list of repositories is empty or it contains a single element "_all", all registered repositories
      * are returned.
      *
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
index 3d0977f..efd364a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java
@@ -41,7 +41,7 @@ import static org.elasticsearch.common.settings.Settings.writeSettingsToStream;
 
 /**
  * Register repository request.
- * <p/>
+ * <p>
  * Registers a repository with given name, type and settings. If the repository with the same name already
  * exists in the cluster, the new repository will replace the existing repository.
  */
@@ -98,7 +98,6 @@ public class PutRepositoryRequest extends AcknowledgedRequest<PutRepositoryReque
 
     /**
      * The type of the repository
-     * <p/>
      * <ul>
      * <li>"fs" - shared filesystem repository</li>
      * </ul>
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
index 3330577..7166eed 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/VerifyRepositoryRequest.java
@@ -30,7 +30,7 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 
 /**
  * Unregister repository request.
- * <p/>
+ * <p>
  * The unregister repository command just unregisters the repository. No data is getting deleted from the repository.
  */
 public class VerifyRepositoryRequest extends AcknowledgedRequest<VerifyRepositoryRequest> {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
index 4dbef01..41d3f9c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java
@@ -49,7 +49,7 @@ import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBo
 
 /**
  * Create snapshot request
- * <p/>
+ * <p>
  * The only mandatory parameter is repository name. The repository name has to satisfy the following requirements
  * <ul>
  * <li>be a non-empty string</li>
@@ -162,12 +162,11 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets a list of indices that should be included into the snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are supported. An empty list or {"_all"} will snapshot all open
      * indices in the cluster.
      *
-     * @param indices
      * @return this request
      */
     @Override
@@ -178,12 +177,11 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets a list of indices that should be included into the snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are supported. An empty list or {"_all"} will snapshot all open
      * indices in the cluster.
      *
-     * @param indices
      * @return this request
      */
     public CreateSnapshotRequest indices(List<String> indices) {
@@ -268,7 +266,7 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -281,7 +279,7 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -294,7 +292,7 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets repository-specific snapshot settings in JSON, YAML or properties format
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -307,7 +305,7 @@ public class CreateSnapshotRequest extends MasterNodeRequest<CreateSnapshotReque
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java
index 507c77a..ebdd206 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java
@@ -69,12 +69,11 @@ public class CreateSnapshotRequestBuilder extends MasterNodeOperationRequestBuil
 
     /**
      * Sets a list of indices that should be included into the snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are supported. An empty list or {"_all"} will snapshot all open
      * indices in the cluster.
      *
-     * @param indices
      * @return this builder
      */
     public CreateSnapshotRequestBuilder setIndices(String... indices) {
@@ -117,7 +116,7 @@ public class CreateSnapshotRequestBuilder extends MasterNodeOperationRequestBuil
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -130,7 +129,7 @@ public class CreateSnapshotRequestBuilder extends MasterNodeOperationRequestBuil
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -143,7 +142,7 @@ public class CreateSnapshotRequestBuilder extends MasterNodeOperationRequestBuil
 
     /**
      * Sets repository-specific snapshot settings in YAML, JSON or properties format
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -156,7 +155,7 @@ public class CreateSnapshotRequestBuilder extends MasterNodeOperationRequestBuil
 
     /**
      * Sets repository-specific snapshot settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java
index c718abc..c1c5e9f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java
@@ -69,14 +69,11 @@ public class CreateSnapshotResponse extends ActionResponse implements ToXContent
 
     /**
      * Returns HTTP status
-     * <p/>
      * <ul>
-     * <li>{@link RestStatus#ACCEPTED}</li> if snapshot is still in progress
-     * <li>{@link RestStatus#OK}</li> if snapshot was successful or partially successful
-     * <li>{@link RestStatus#INTERNAL_SERVER_ERROR}</li> if snapshot failed completely
+     * <li>{@link RestStatus#ACCEPTED} if snapshot is still in progress</li>
+     * <li>{@link RestStatus#OK} if snapshot was successful or partially successful</li>
+     * <li>{@link RestStatus#INTERNAL_SERVER_ERROR} if snapshot failed completely</li>
      * </ul>
-     *
-     * @return
      */
     public RestStatus status() {
         if (snapshotInfo == null) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java
index d997786..246d366 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java
@@ -30,7 +30,7 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 
 /**
  * Delete snapshot request
- * <p/>
+ * <p>
  * Delete snapshot request removes the snapshot record from the repository and cleans up all
  * files that are associated with this particular snapshot. All files that are shared with
  * at least one other existing snapshot are left intact.
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
index cccd697..a0d2797 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java
@@ -105,7 +105,6 @@ public class GetSnapshotsRequest extends MasterNodeRequest<GetSnapshotsRequest>
     /**
      * Sets the list of snapshots to be returned
      *
-     * @param snapshots
      * @return this request
      */
     public GetSnapshotsRequest snapshots(String[] snapshots) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
index 7b349e4..0f79ceb 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java
@@ -147,7 +147,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets the list of indices that should be restored from snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are not supported. An empty list or {"_all"} will restore all open
      * indices in the snapshot.
@@ -162,7 +162,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets the list of indices that should be restored from snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are not supported. An empty list or {"_all"} will restore all open
      * indices in the snapshot.
@@ -177,8 +177,6 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Returns list of indices that should be restored from snapshot
-     *
-     * @return
      */
     public String[] indices() {
         return indices;
@@ -208,7 +206,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets rename pattern that should be applied to restored indices.
-     * <p/>
+     * <p>
      * Indices that match the rename pattern will be renamed according to {@link #renameReplacement(String)}. The
      * rename pattern is applied according to the {@link java.util.regex.Matcher#appendReplacement(StringBuffer, String)}
      * The request will fail if two or more indices will be renamed into the same name.
@@ -232,11 +230,10 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets rename replacement
-     * <p/>
+     * <p>
      * See {@link #renamePattern(String)} for more information.
      *
      * @param renameReplacement rename replacement
-     * @return
      */
     public RestoreSnapshotRequest renameReplacement(String renameReplacement) {
         this.renameReplacement = renameReplacement;
@@ -294,7 +291,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets repository-specific restore settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -307,7 +304,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets repository-specific restore settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -320,7 +317,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets repository-specific restore settings in JSON, YAML or properties format
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -333,7 +330,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Sets repository-specific restore settings
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -384,7 +381,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * If set to true the restore procedure will restore global cluster state.
-     * <p/>
+     * <p>
      * The global cluster state includes persistent settings and index template definitions.
      *
      * @param includeGlobalState true if global state should be restored from the snapshot
@@ -548,7 +545,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Parses restore definition
-     * <p/>
+     * <p>
      * JSON, YAML and properties formats are supported
      *
      * @param source restore definition
@@ -567,7 +564,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Parses restore definition
-     * <p/>
+     * <p>
      * JSON, YAML and properties formats are supported
      *
      * @param source restore definition
@@ -579,7 +576,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Parses restore definition
-     * <p/>
+     * <p>
      * JSON, YAML and properties formats are supported
      *
      * @param source restore definition
@@ -600,7 +597,7 @@ public class RestoreSnapshotRequest extends MasterNodeRequest<RestoreSnapshotReq
 
     /**
      * Parses restore definition
-     * <p/>
+     * <p>
      * JSON, YAML and properties formats are supported
      *
      * @param source restore definition
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java
index 1fda33e..661a1a1 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java
@@ -71,7 +71,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets the list of indices that should be restored from snapshot
-     * <p/>
+     * <p>
      * The list of indices supports multi-index syntax. For example: "+test*" ,"-test42" will index all indices with
      * prefix "test" except index "test42". Aliases are not supported. An empty list or {"_all"} will restore all open
      * indices in the snapshot.
@@ -99,7 +99,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets rename pattern that should be applied to restored indices.
-     * <p/>
+     * <p>
      * Indices that match the rename pattern will be renamed according to {@link #setRenameReplacement(String)}. The
      * rename pattern is applied according to the {@link java.util.regex.Matcher#appendReplacement(StringBuffer, String)}
      * The request will fail if two or more indices will be renamed into the same name.
@@ -114,7 +114,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets rename replacement
-     * <p/>
+     * <p>
      * See {@link #setRenamePattern(String)} for more information.
      *
      * @param renameReplacement rename replacement
@@ -128,7 +128,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets repository-specific restore settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -141,7 +141,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets repository-specific restore settings.
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param settings repository-specific snapshot settings
@@ -154,7 +154,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets repository-specific restore settings in JSON, YAML or properties format
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -167,7 +167,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * Sets repository-specific restore settings
-     * <p/>
+     * <p>
      * See repository documentation for more information.
      *
      * @param source repository-specific snapshot settings
@@ -191,7 +191,7 @@ public class RestoreSnapshotRequestBuilder extends MasterNodeOperationRequestBui
 
     /**
      * If set to true the restore procedure will restore global cluster state.
-     * <p/>
+     * <p>
      * The global cluster state includes persistent settings and index template definitions.
      *
      * @param restoreGlobalState true if global state should be restored from the snapshot
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
index 7e1a47c..91b890b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
@@ -149,7 +149,6 @@ public class SnapshotStatus implements ToXContent, Streamable {
      *
      * @param in stream input
      * @return deserialized snapshot status
-     * @throws IOException
      */
     public static SnapshotStatus readSnapshotStatus(StreamInput in) throws IOException {
         SnapshotStatus snapshotInfo = new SnapshotStatus();
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java
index b7b2b63..015134a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java
@@ -105,7 +105,6 @@ public class SnapshotsStatusRequest extends MasterNodeRequest<SnapshotsStatusReq
     /**
      * Sets the list of snapshots to be returned
      *
-     * @param snapshots
      * @return this request
      */
     public SnapshotsStatusRequest snapshots(String[] snapshots) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java
index c219d85..be7a3f0 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java
@@ -297,7 +297,7 @@ public class ClusterStatsIndices implements ToXContent, Streamable {
         }
 
         /**
-         * maximum replication factor across the indices. See {@link #getReplication
+         * maximum replication factor across the indices. See {@link #getReplication}
          */
         public double getMaxIndexReplication() {
             return this.maxIndexReplication;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
index d5d291d..97323b8 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java
@@ -55,8 +55,8 @@ import static org.elasticsearch.common.settings.Settings.writeSettingsToStream;
 
 /**
  * A request to create an index. Best created with {@link org.elasticsearch.client.Requests#createIndexRequest(String)}.
- * <p/>
- * <p>The index created can optionally be created with {@link #settings(org.elasticsearch.common.settings.Settings)}.
+ * <p>
+ * The index created can optionally be created with {@link #settings(org.elasticsearch.common.settings.Settings)}.
  *
  * @see org.elasticsearch.client.IndicesAdminClient#create(CreateIndexRequest)
  * @see org.elasticsearch.client.Requests#createIndexRequest(String)
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java
index e8916e1..9e5dc88 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java
@@ -53,7 +53,7 @@ public class DeleteIndexRequestBuilder extends MasterNodeOperationRequestBuilder
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public DeleteIndexRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java
index 7fe5b1e..5f01c26 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java
@@ -39,7 +39,7 @@ public class IndicesExistsRequestBuilder extends MasterNodeReadOperationRequestB
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public IndicesExistsRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
index ad8a719..0152254 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java
@@ -30,8 +30,8 @@ import java.io.IOException;
  * A flush request to flush one or more indices. The flush process of an index basically frees memory from the index
  * by flushing data to the index storage and clearing the internal transaction log. By default, Elasticsearch uses
  * memory heuristics in order to automatically trigger flush operations as required in order to clear memory.
- * <p/>
- * <p>Best created with {@link org.elasticsearch.client.Requests#flushRequest(String...)}.
+ * <p>
+ * Best created with {@link org.elasticsearch.client.Requests#flushRequest(String...)}.
  *
  * @see org.elasticsearch.client.Requests#flushRequest(String...)
  * @see org.elasticsearch.client.IndicesAdminClient#flush(FlushRequest)
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java
index 3ebf327..967ea31 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java
@@ -49,7 +49,7 @@ public class GetFieldMappingsRequest extends ActionRequest<GetFieldMappingsReque
 
     /**
      * Indicate whether the receiving node should operate based on local index information or forward requests,
-     * where needed, to other nodes. If running locally, request will not raise errors if running locally & missing indices.
+     * where needed, to other nodes. If running locally, request will not raise errors if running locally &amp; missing indices.
      */
     public GetFieldMappingsRequest local(boolean local) {
         this.local = local;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
index a708939..7cfcbd7 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java
@@ -40,8 +40,8 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * Puts mapping definition registered under a specific type into one or more indices. Best created with
  * {@link org.elasticsearch.client.Requests#putMappingRequest(String...)}.
- * <p/>
- * <p>If the mappings already exists, the new mappings will be merged with the new one. If there are elements
+ * <p>
+ * If the mappings already exists, the new mappings will be merged with the new one. If there are elements
  * that can't be merged are detected, the request will be rejected.
  *
  * @see org.elasticsearch.client.Requests#putMappingRequest(String...)
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java
index 6b5d5d0..28f289b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java
@@ -42,7 +42,7 @@ public class PutMappingRequestBuilder extends AcknowledgedRequestBuilder<PutMapp
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public PutMappingRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java
index 08f322a..cf3006b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java
@@ -28,8 +28,8 @@ import java.io.IOException;
 /**
  * A request to optimize one or more indices. In order to optimize on all the indices, pass an empty array or
  * <tt>null</tt> for the indices.
- * <p/>
- * <p>{@link #maxNumSegments(int)} allows to control the number of segments to optimize down to. By default, will
+ * <p>
+ * {@link #maxNumSegments(int)} allows to control the number of segments to optimize down to. By default, will
  * cause the optimize process to optimize down to half the configured number of segments.
  *
  * @see org.elasticsearch.client.Requests#optimizeRequest(String...)
@@ -62,7 +62,7 @@ public class OptimizeRequest extends BroadcastRequest<OptimizeRequest> {
     }
 
     /**
-     * Will optimize the index down to <= maxNumSegments. By default, will cause the optimize
+     * Will optimize the index down to &lt;= maxNumSegments. By default, will cause the optimize
      * process to optimize down to half the configured number of segments.
      */
     public int maxNumSegments() {
@@ -70,7 +70,7 @@ public class OptimizeRequest extends BroadcastRequest<OptimizeRequest> {
     }
 
     /**
-     * Will optimize the index down to <= maxNumSegments. By default, will cause the optimize
+     * Will optimize the index down to &lt;= maxNumSegments. By default, will cause the optimize
      * process to optimize down to half the configured number of segments.
      */
     public OptimizeRequest maxNumSegments(int maxNumSegments) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java
index 0493598..d318492 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.client.ElasticsearchClient;
 /**
  * A request to optimize one or more indices. In order to optimize on all the indices, pass an empty array or
  * <tt>null</tt> for the indices.
- * <p/>
  * <p>{@link #setMaxNumSegments(int)} allows to control the number of segments to optimize down to. By default, will
  * cause the optimize process to optimize down to half the configured number of segments.
  */
@@ -36,7 +35,7 @@ public class OptimizeRequestBuilder extends BroadcastOperationRequestBuilder<Opt
     }
 
     /**
-     * Will optimize the index down to <= maxNumSegments. By default, will cause the optimize
+     * Will optimize the index down to &lt;= maxNumSegments. By default, will cause the optimize
      * process to optimize down to half the configured number of segments.
      */
     public OptimizeRequestBuilder setMaxNumSegments(int maxNumSegments) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java
index 4047d0a..5a2ca7a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java
@@ -44,7 +44,7 @@ public class GetSettingsRequestBuilder extends MasterNodeReadOperationRequestBui
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public GetSettingsRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java
index f5a3189..770c54b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java
@@ -45,7 +45,7 @@ public class UpdateSettingsRequestBuilder extends AcknowledgedRequestBuilder<Upd
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public UpdateSettingsRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java
index fc893db..792dde1 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java
@@ -28,10 +28,10 @@ import java.io.IOException;
 
 /**
  * A request to get indices level stats. Allow to enable different stats to be returned.
- * <p/>
- * <p>By default, all statistics are enabled.
- * <p/>
- * <p>All the stats to be returned can be cleared using {@link #clear()}, at which point, specific
+ * <p>
+ * By default, all statistics are enabled.
+ * <p>
+ * All the stats to be returned can be cleared using {@link #clear()}, at which point, specific
  * stats can be enabled.
  */
 public class IndicesStatsRequest extends BroadcastRequest<IndicesStatsRequest> {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java
index 8e78c54..21540d3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java
@@ -24,11 +24,11 @@ import org.elasticsearch.client.ElasticsearchClient;
 
 /**
  * A request to get indices level stats. Allow to enable different stats to be returned.
- * <p/>
- * <p>By default, the {@link #setDocs(boolean)}, {@link #setStore(boolean)}, {@link #setIndexing(boolean)}
+ * <p>
+ * By default, the {@link #setDocs(boolean)}, {@link #setStore(boolean)}, {@link #setIndexing(boolean)}
  * are enabled. Other stats can be enabled as well.
- * <p/>
- * <p>All the stats to be returned can be cleared using {@link #clear()}, at which point, specific
+ * <p>
+ * All the stats to be returned can be cleared using {@link #clear()}, at which point, specific
  * stats can be enabled.
  */
 public class IndicesStatsRequestBuilder extends BroadcastOperationRequestBuilder<IndicesStatsRequest, IndicesStatsResponse, IndicesStatsRequestBuilder> {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java
index af328ce..5472660 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/post/UpgradeRequest.java
@@ -28,7 +28,6 @@ import java.io.IOException;
 /**
  * A request to upgrade one or more indices. In order to optimize on all the indices, pass an empty array or
  * <tt>null</tt> for the indices.
- * <p/>
  * @see org.elasticsearch.client.Requests#upgradeRequest(String...)
  * @see org.elasticsearch.client.IndicesAdminClient#upgrade(UpgradeRequest)
  * @see UpgradeResponse
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index c64a594..502f455 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
@@ -189,8 +188,8 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
             }
             if (request.rewrite()) {
                 explanation = getRewrittenQuery(searcher.searcher(), searchContext.query());
-            }
-        } catch (QueryShardException|ParsingException e) {
+            }   
+        } catch (ParsingException e) {
             valid = false;
             error = e.getDetailedMessage();
         } catch (AssertionError|IOException e) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
index 20fb541..7c7869b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java
@@ -40,8 +40,8 @@ import java.util.Map;
 
 /**
  * A request to validate a specific query.
- * <p/>
- * <p>The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
+ * <p>
+ * The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
  * or {@link #source(byte[])}.
  */
 public class ValidateQueryRequest extends BroadcastRequest<ValidateQueryRequest> {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
index 515ecd1..4acdfdc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.validate.query;
 
+import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
index 61b033f..39312e5 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java
@@ -48,7 +48,7 @@ public class DeleteWarmerRequest extends AcknowledgedRequest<DeleteWarmerRequest
     /**
      * Constructs a new delete warmer request for the specified name.
      *
-     * @param names: the name (or wildcard expression) of the warmer to match, null to delete all.
+     * @param names the name (or wildcard expression) of the warmer to match, null to delete all.
      */
     public DeleteWarmerRequest(String... names) {
         names(names);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java
index 152c5ac..fdba95b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java
@@ -50,7 +50,7 @@ public class DeleteWarmerRequestBuilder extends AcknowledgedRequestBuilder<Delet
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public DeleteWarmerRequestBuilder setIndicesOptions(IndicesOptions options) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java
index e1a2af4..053cc75 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/package-info.java
@@ -25,6 +25,6 @@
  *     search. This includes things such as the query cache, filesystem cache, and loading field data for fields.
  * </p>
  *
- * @see the reference guide for more detailed information about the Indices / Search Warmer
+ * See the reference guide for more detailed information about the Indices / Search Warmer
  */
 package org.elasticsearch.action.admin.indices.warmer;
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
index ab4366d..42a9344 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
@@ -40,7 +40,7 @@ import java.util.concurrent.atomic.AtomicLong;
  * A bulk processor is a thread safe bulk processing class, allowing to easily set when to "flush" a new bulk request
  * (either based on number of actions, based on the size, or time), and to easily control the number of concurrent bulk
  * requests allowed to be executed in parallel.
- * <p/>
+ * <p>
  * In order to create a new bulk processor, use the {@link Builder}.
  */
 public class BulkProcessor implements Closeable {
@@ -127,7 +127,7 @@ public class BulkProcessor implements Closeable {
 
         /**
          * Sets a flush interval flushing *any* bulk actions pending if the interval passes. Defaults to not set.
-         * <p/>
+         * <p>
          * Note, both {@link #setBulkActions(int)} and {@link #setBulkSize(org.elasticsearch.common.unit.ByteSizeValue)}
          * can be set to <tt>-1</tt> with the flush interval set allowing for complete async processing of bulk actions.
          */
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
index 9a8419f..fa6b643 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
@@ -195,7 +195,7 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
     /**
      * The list of optional payloads associated with requests in the same order as the requests. Note, elements within
      * it might be null if no payload has been provided.
-     * <p/>
+     * <p>
      * Note, if no payloads have been provided, this method will return null (as to conserve memory overhead).
      */
     @Nullable
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
index 1d35af4..05e193a 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
@@ -44,8 +44,8 @@ import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_
 /**
  * A request to count the number of documents matching a specific query. Best created with
  * {@link org.elasticsearch.client.Requests#countRequest(String...)}.
- * <p/>
- * <p>The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
+ * <p>
+ * The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
  * or {@link #source(byte[])}.
  *
  * @see CountResponse
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
index 3716bf2..54c60e5 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
@@ -71,7 +71,7 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
     /**
      * Sets the preference to execute the search. Defaults to randomize across shards. Can be set to
      * <tt>_local</tt> to prefer local shards, <tt>_primary</tt> to execute only on primary shards,
-     * _shards:x,y to operate on shards x & y, or a custom value, which guarantees that the same order
+     * _shards:x,y to operate on shards x &amp; y, or a custom value, which guarantees that the same order
      * will be used across different requests.
      */
     public CountRequestBuilder setPreference(String preference) {
diff --git a/core/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java b/core/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java
index c5d5e78..037f743 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java
@@ -36,8 +36,8 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * A request to delete a document from an index based on its type and id. Best created using
  * {@link org.elasticsearch.client.Requests#deleteRequest(String)}.
- * <p/>
- * <p>The operation requires the {@link #index()}, {@link #type(String)} and {@link #id(String)} to
+ * <p>
+ * The operation requires the {@link #index()}, {@link #type(String)} and {@link #id(String)} to
  * be set.
  *
  * @see DeleteResponse
diff --git a/core/src/main/java/org/elasticsearch/action/exists/ExistsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/exists/ExistsRequestBuilder.java
index 3838e09..c7ef5a1 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/ExistsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/ExistsRequestBuilder.java
@@ -51,7 +51,7 @@ public class ExistsRequestBuilder extends BroadcastOperationRequestBuilder<Exist
     /**
      * Sets the preference to execute the search. Defaults to randomize across shards. Can be set to
      * <tt>_local</tt> to prefer local shards, <tt>_primary</tt> to execute only on primary shards,
-     * _shards:x,y to operate on shards x & y, or a custom value, which guarantees that the same order
+     * _shards:x,y to operate on shards x &amp; y, or a custom value, which guarantees that the same order
      * will be used across different requests.
      */
     public ExistsRequestBuilder setPreference(String preference) {
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index b35fe78..7dc5dc8 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -41,7 +41,7 @@ import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
@@ -166,10 +166,10 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             BytesReference source = request.querySource();
             if (source != null && source.length() > 0) {
                 try {
-                    QueryShardContext.setTypes(request.types());
+                    QueryParseContext.setTypes(request.types());
                     context.parsedQuery(indexService.queryParserService().parseQuery(source));
                 } finally {
-                    QueryShardContext.removeTypes();
+                    QueryParseContext.removeTypes();
                 }
             }
             context.preProcess();
diff --git a/core/src/main/java/org/elasticsearch/action/get/GetRequest.java b/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
index 8403282..935170e 100644
--- a/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/get/GetRequest.java
@@ -36,8 +36,8 @@ import java.io.IOException;
 /**
  * A request to get a document (its source) from an index based on its type (optional) and id. Best created using
  * {@link org.elasticsearch.client.Requests#getRequest(String)}.
- * <p/>
- * <p>The operation requires the {@link #index()}, {@link #type(String)} and {@link #id(String)}
+ * <p>
+ * The operation requires the {@link #index()}, {@link #type(String)} and {@link #id(String)}
  * to be set.
  *
  * @see org.elasticsearch.action.get.GetResponse
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index 7edf865..b057966 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -49,15 +49,15 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * Index request to index a typed JSON document into a specific index and make it searchable. Best
  * created using {@link org.elasticsearch.client.Requests#indexRequest(String)}.
- * <p/>
- * <p>The index requires the {@link #index()}, {@link #type(String)}, {@link #id(String)} and
+ * <p>
+ * The index requires the {@link #index()}, {@link #type(String)}, {@link #id(String)} and
  * {@link #source(byte[])} to be set.
- * <p/>
- * <p>The source (content to index) can be set in its bytes form using ({@link #source(byte[])}),
+ * <p>
+ * The source (content to index) can be set in its bytes form using ({@link #source(byte[])}),
  * its string form ({@link #source(String)}) or using a {@link org.elasticsearch.common.xcontent.XContentBuilder}
  * ({@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}).
- * <p/>
- * <p>If the {@link #id(String)} is not set, it will be automatically generated.
+ * <p>
+ * If the {@link #id(String)} is not set, it will be automatically generated.
  *
  * @see IndexResponse
  * @see org.elasticsearch.client.Requests#indexRequest(String)
@@ -316,7 +316,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
     }
 
     /**
-     * Sets the relative ttl value. It musts be > 0 as it makes little sense otherwise. Setting it
+     * Sets the relative ttl value. It musts be &gt; 0 as it makes little sense otherwise. Setting it
      * to <tt>null</tt> will reset to have no ttl.
      */
     public IndexRequest ttl(Long ttl) throws ElasticsearchGenerationException {
@@ -372,8 +372,8 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     /**
      * Sets the document source to index.
-     * <p/>
-     * <p>Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
+     * <p>
+     * Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #source(byte[])}.
      */
     public IndexRequest source(String source) {
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
index 5b6674e..2df8fec 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
@@ -107,8 +107,8 @@ public class IndexRequestBuilder extends ReplicationRequestBuilder<IndexRequest,
 
     /**
      * Sets the document source to index.
-     * <p/>
-     * <p>Note, its preferable to either set it using {@link #setSource(org.elasticsearch.common.xcontent.XContentBuilder)}
+     * <p>
+     * Note, its preferable to either set it using {@link #setSource(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #setSource(byte[])}.
      */
     public IndexRequestBuilder setSource(String source) {
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index ad875c4..60dfa9e 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -55,8 +55,8 @@ import org.elasticsearch.transport.TransportService;
 
 /**
  * Performs the index operation.
- * <p/>
- * <p>Allows for the following settings:
+ * <p>
+ * Allows for the following settings:
  * <ul>
  * <li><b>autoCreateIndex</b>: When set to <tt>true</tt>, will automatically create an index if one does not exists.
  * Defaults to <tt>true</tt>.
diff --git a/core/src/main/java/org/elasticsearch/action/indexedscripts/delete/DeleteIndexedScriptRequest.java b/core/src/main/java/org/elasticsearch/action/indexedscripts/delete/DeleteIndexedScriptRequest.java
index b757e64..4e8019a 100644
--- a/core/src/main/java/org/elasticsearch/action/indexedscripts/delete/DeleteIndexedScriptRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/indexedscripts/delete/DeleteIndexedScriptRequest.java
@@ -35,8 +35,8 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 
 /**
  * A request to delete a script from the script index based on its scriptLang and id. Best created using
- * <p/>
- * <p>The operation requires the , {@link #scriptLang(String)} and {@link #id(String)} to
+ * <p>
+ * The operation requires the , {@link #scriptLang(String)} and {@link #id(String)} to
  * be set.
  *
  * @see DeleteIndexedScriptResponse
diff --git a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
index d3654d5..76b2dcc 100644
--- a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequest.java
@@ -46,15 +46,15 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 
 /**
  * Index request to index a script to the script index and make it available at search time.
- * <p/>
- * <p>The request requires the  {@link #scriptLang(String)}, {@link #id(String)} and
+ * <p>
+ * The request requires the  {@link #scriptLang(String)}, {@link #id(String)} and
  * {@link #source(byte[])} to be set.
- * <p/>
- * <p>The source (content to index) can be set in its bytes form using ({@link #source()} (byte[])}),
+ * <p>
+ * The source (content to index) can be set in its bytes form using ({@link #source()} (byte[])}),
  * its string form ({@link #source(String)}) or using a {@link org.elasticsearch.common.xcontent.XContentBuilder}
  * ({@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}).
- * <p/>
- * <p>If the {@link #id(String)} is not set, it will be automatically generated.
+ * <p>
+ * If the {@link #id(String)} is not set, it will be automatically generated.
  *
  * @see PutIndexedScriptResponse
  */
@@ -200,8 +200,8 @@ public class PutIndexedScriptRequest extends ActionRequest<PutIndexedScriptReque
 
     /**
      * Sets the document source to index.
-     * <p/>
-     * <p>Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
+     * <p>
+     * Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #source(byte[])}.
      */
     public PutIndexedScriptRequest source(String source) {
diff --git a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequestBuilder.java
index 50c4c96..eb07941 100644
--- a/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/indexedscripts/put/PutIndexedScriptRequestBuilder.java
@@ -85,8 +85,8 @@ public class PutIndexedScriptRequestBuilder extends ActionRequestBuilder<PutInde
 
     /**
      * Sets the document source to index.
-     * <p/>
-     * <p>Note, its preferable to either set it using {@link #setSource(org.elasticsearch.common.xcontent.XContentBuilder)}
+     * <p>
+     * Note, its preferable to either set it using {@link #setSource(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #setSource(byte[])}.
      */
     public PutIndexedScriptRequestBuilder setSource(String source) {
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java
index 54d88cb..df36f9f 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java
@@ -49,7 +49,7 @@ public class MultiPercolateRequestBuilder extends ActionRequestBuilder<MultiPerc
 
     /**
      * Specifies how to globally ignore indices that are not available and how to deal with wildcard indices expressions.
-     * <p/>
+     * <p>
      * Invoke this method before invoking {@link #add(PercolateRequestBuilder)}.
      */
     public MultiPercolateRequestBuilder setIndicesOptions(IndicesOptions indicesOptions) {
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java
index 58697fd..a0d1e4f 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java
@@ -35,7 +35,7 @@ public class MultiSearchRequestBuilder extends ActionRequestBuilder<MultiSearchR
     /**
      * Add a search request to execute. Note, the order is important, the search response will be returned in the
      * same order as the search requests.
-     * <p/>
+     * <p>
      * If ignoreIndices has been set on the search request, then the indicesOptions of the multi search request
      * will not be used (if set).
      */
@@ -64,7 +64,7 @@ public class MultiSearchRequestBuilder extends ActionRequestBuilder<MultiSearchR
     /**
      * Specifies what type of requested indices to ignore and how to deal with wildcard indices expressions.
      * For example indices that don't exist.
-     * <p/>
+     * <p>
      * Invoke this method before invoking {@link #add(SearchRequestBuilder)}.
      */
     public MultiSearchRequestBuilder setIndicesOptions(IndicesOptions indicesOptions) {
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
index abc1ba5..9348185 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
@@ -50,11 +50,11 @@ import static org.elasticsearch.search.Scroll.readScroll;
 /**
  * A request to execute search against one or more indices (or all). Best created using
  * {@link org.elasticsearch.client.Requests#searchRequest(String...)}.
- * <p/>
- * <p>Note, the search {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}
+ * <p>
+ * Note, the search {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}
  * is required. The search source is the different search options, including aggregations and such.
- * <p/>
- * <p>There is an option to specify an addition search source using the {@link #extraSource(org.elasticsearch.search.builder.SearchSourceBuilder)}.
+ * <p>
+ * There is an option to specify an addition search source using the {@link #extraSource(org.elasticsearch.search.builder.SearchSourceBuilder)}.
  *
  * @see org.elasticsearch.client.Requests#searchRequest(String...)
  * @see org.elasticsearch.client.Client#search(SearchRequest)
@@ -319,7 +319,7 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * The name of the stored template
      * 
-     * @deprecated use {@link #template(Template))} instead.
+     * @deprecated use {@link #template(Template)} instead.
      */
     @Deprecated
     public void templateName(String templateName) {
@@ -329,7 +329,7 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * The type of the stored template
      * 
-     * @deprecated use {@link #template(Template))} instead.
+     * @deprecated use {@link #template(Template)} instead.
      */
     @Deprecated
     public void templateType(ScriptService.ScriptType templateType) {
@@ -339,7 +339,7 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * Template parameters used for rendering
      * 
-     * @deprecated use {@link #template(Template))} instead.
+     * @deprecated use {@link #template(Template)} instead.
      */
     @Deprecated
     public void templateParams(Map<String, Object> params) {
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 4ebcfec..d600715 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -167,7 +167,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public SearchRequestBuilder setIndicesOptions(IndicesOptions indicesOptions) {
@@ -715,13 +715,8 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
-    public SearchRequestBuilder addParentChildInnerHits(String name, String type,  InnerHitsBuilder.InnerHit innerHit) {
-        innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
-        return this;
-    }
-
-    public SearchRequestBuilder addNestedInnerHits(String name, String path,  InnerHitsBuilder.InnerHit innerHit) {
-        innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+    public SearchRequestBuilder addInnerHit(String name, InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addInnerHit(name, innerHit);
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
index 764975e..0d1c493 100644
--- a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequest.java
@@ -37,8 +37,8 @@ import java.util.Arrays;
 /**
  * A request to get suggestions for corrections of phrases. Best created with
  * {@link org.elasticsearch.client.Requests#suggestRequest(String...)}.
- * <p/>
- * <p>The request requires the suggest query source to be set either using
+ * <p>
+ * The request requires the suggest query source to be set either using
  * {@link #suggest(org.elasticsearch.common.bytes.BytesReference)} / {@link #suggest(org.elasticsearch.common.bytes.BytesReference)}
  * or by using {@link #suggest(org.elasticsearch.search.suggest.SuggestBuilder)}
  * (Best created using the {link @org.elasticsearch.search.suggest.SuggestBuilders)}).
diff --git a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java
index 954b86c..06a2b00 100644
--- a/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java
@@ -66,7 +66,7 @@ public class SuggestRequestBuilder extends BroadcastOperationRequestBuilder<Sugg
     /**
      * Sets the preference to execute the search. Defaults to randomize across shards. Can be set to
      * <tt>_local</tt> to prefer local shards, <tt>_primary</tt> to execute only on primary shards,
-     * _shards:x,y to operate on shards x & y, or a custom value, which guarantees that the same order
+     * _shards:x,y to operate on shards x &amp; y, or a custom value, which guarantees that the same order
      * will be used across different requests.
      */
     public SuggestRequestBuilder setPreference(String preference) {
diff --git a/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java b/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
index 77f9665..7154c74 100644
--- a/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java
@@ -127,7 +127,6 @@ public abstract class TransportBroadcastByNodeAction<Request extends BroadcastRe
      *
      * @param in input stream
      * @return a deserialized shard-level result
-     * @throws IOException
      */
     protected abstract ShardOperationResult readShardResult(StreamInput in) throws IOException;
 
@@ -150,7 +149,6 @@ public abstract class TransportBroadcastByNodeAction<Request extends BroadcastRe
      *
      * @param in input stream
      * @return a de-serialized request
-     * @throws IOException
      */
     protected abstract Request readRequestFrom(StreamInput in) throws IOException;
 
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index 18890dc..6e2ea50 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -341,7 +341,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
     /**
      * Responsible for performing all operations up to the point we start starting sending requests to replica shards.
      * Including forwarding the request to another node if the primary is not assigned locally.
-     * <p/>
+     * <p>
      * Note that as soon as we start sending request to replicas, state responsibility is transferred to {@link ReplicationPhase}
      */
     final class PrimaryPhase extends AbstractRunnable {
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
index 127756e..7be1061 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java
@@ -42,10 +42,10 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * offsets and payloads even if positions are not present. You must call
  * nextPosition() anyway to move the counter although this method only returns
  * <tt>-1,</tt>, if no positions were returned by the {@link TermVectorsRequest}.
- * <p/>
+ * <p>
  * The data is stored in two byte arrays ({@code headerRef} and
  * {@code termVectors}, both {@link BytesRef}) that have the following format:
- * <p/>
+ * <p>
  * {@code headerRef}: Stores offsets per field in the {@code termVectors} array
  * and some header information as {@link BytesRef}. Format is
  * <ul>
@@ -54,6 +54,7 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * <li>boolean: hasTermStatistics (are the term statistics stored?)</li>
  * <li>boolean: hasFieldStatitsics (are the field statistics stored?)</li>
  * <li>vint: number of fields</li>
+ * <li>
  * <ul>
  * <li>String: field name 1</li>
  * <li>vint: offset in {@code termVectors} for field 1</li>
@@ -61,10 +62,11 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * <li>String: field name last field</li>
  * <li>vint: offset in {@code termVectors} for last field</li>
  * </ul>
+ * </li>
  * </ul>
- * <p/>
+ * <p>
  * termVectors: Stores the actual term vectors as a {@link BytesRef}.
- * <p/>
+ * <p>
  * Term vectors for each fields are stored in blocks, one for each field. The
  * offsets in {@code headerRef} are used to find where the block for a field
  * starts. Each block begins with a
@@ -82,14 +84,13 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * <li>vint: number of documents in the shard that has an entry for this field
  * (docCount)</li>
  * </ul>
- * <p/>
+ * <p>
  * After that, for each term it stores
  * <ul>
- * <ul>
  * <li>vint: term lengths</li>
  * <li>BytesRef: term name</li>
  * </ul>
- * <p/>
+ * <p>
  * If term statistics are requested ({@code hasTermStatistics} is true, see
  * {@code headerRef}):
  * <ul>
@@ -99,6 +100,7 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * After that
  * <ul>
  * <li>vint: frequency (always returned)</li>
+ * <li>
  * <ul>
  * <li>vint: position_1 (if positions == true)</li>
  * <li>vint: startOffset_1 (if offset == true)</li>
@@ -107,8 +109,8 @@ import static org.apache.lucene.util.ArrayUtil.grow;
  * <li>...</li>
  * <li>vint: endOffset_freqency (if offset == true)</li>
  * <li>BytesRef: payload_freqency (if payloads == true)</li>
- * <ul>
- * </ul> </ul>
+ * </ul></li>
+ * </ul>
  */
 
 public final class TermVectorsFields extends Fields {
@@ -494,4 +496,4 @@ public final class TermVectorsFields extends Fields {
     long readPotentiallyNegativeVLong(StreamInput stream) throws IOException {
         return stream.readVLong() - 1;
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java
index fdc9a41..ef99892 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java
@@ -52,7 +52,7 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 /**
  * Request returning the term vector (doc frequency, positions, offsets) for a
  * document.
- * <p/>
+ * <p>
  * Note, the {@link #index()}, {@link #type(String)} and {@link #id(String)} are
  * required.
  */
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequestBuilder.java
index 8502704..9bf9957 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequestBuilder.java
@@ -30,7 +30,7 @@ import java.util.Map;
 /**
  * The builder class for a term vector request.
  * Returns the term vector (doc frequency, positions, offsets) for a document.
- * <p/>
+ * <p>
  * Note, the {@code index}, {@code type} and {@code id} are
  * required.
  */
diff --git a/core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java
index 918d2d7..5b0a5bb 100644
--- a/core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java
@@ -76,7 +76,7 @@ public class UpdateRequestBuilder extends InstanceShardOperationRequestBuilder<U
     /**
      * The script to execute. Note, make sure not to send different script each times and instead
      * use script params if possible with the same (automatically compiled) script.
-     * <p/>
+     * <p>
      * The script works with the variable <code>ctx</code>, which is bound to the entry,
      * e.g. <code>ctx._source.mycounter += 1</code>.
      *
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java b/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
index 2a129f0..8924812 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/JNAKernel32Library.java
@@ -68,7 +68,6 @@ final class JNAKernel32Library {
     /**
      * Adds a Console Ctrl Handler.
      *
-     * @param handler
      * @return true if the handler is correctly set
      * @throws java.lang.UnsatisfiedLinkError if the Kernel32 library is not loaded or if the native function is not found
      * @throws java.lang.NoClassDefFoundError if the library for native calls is missing
@@ -92,8 +91,6 @@ final class JNAKernel32Library {
     /**
      * Native call to the Kernel32 API to set a new Console Ctrl Handler.
      *
-     * @param handler
-     * @param add
      * @return true if the handler is correctly set
      * @throws java.lang.UnsatisfiedLinkError if the Kernel32 library is not loaded or if the native function is not found
      * @throws java.lang.NoClassDefFoundError if the library for native calls is missing
@@ -102,7 +99,7 @@ final class JNAKernel32Library {
 
     /**
      * Handles consoles event with WIN API
-     * <p/>
+     * <p>
      * See http://msdn.microsoft.com/en-us/library/windows/desktop/ms683242%28v=vs.85%29.aspx
      */
     class NativeHandlerCallback implements StdCallLibrary.StdCallCallback {
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index df2f648..c2dffc5 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -40,7 +40,7 @@ import java.util.regex.Pattern;
 
 /** 
  * Initializes SecurityManager with necessary permissions.
- * <p>
+ * <br>
  * <h1>Initialization</h1>
  * The JVM is not initially started with security manager enabled,
  * instead we turn it on early in the startup process. This is a tradeoff
@@ -51,7 +51,7 @@ import java.util.regex.Pattern;
  *   <li>Allows for some contained usage of native code that would not
  *       otherwise be permitted.</li>
  * </ul>
- * <p>
+ * <br>
  * <h1>Permissions</h1>
  * Permissions use a policy file packaged as a resource, this file is
  * also used in tests. File permissions are generated dynamically and
@@ -71,13 +71,13 @@ import java.util.regex.Pattern;
  * behalf (no package protections are yet in place, this would need some
  * cleanups to the scripting apis). But still it can provide some defense for users
  * that enable dynamic scripting without being fully aware of the consequences.
- * <p>
+ * <br>
  * <h1>Disabling Security</h1>
  * SecurityManager can be disabled completely with this setting:
  * <pre>
  * es.security.manager.enabled = false
  * </pre>
- * <p>
+ * <br>
  * <h1>Debugging Security</h1>
  * A good place to start when there is a problem is to turn on security debugging:
  * <pre>
@@ -163,9 +163,11 @@ final class Security {
     static final Map<String,String> SPECIAL_PLUGINS;
     static {
         Map<String,String> m = new HashMap<>();
-        m.put("repository-s3", "org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin");
-        m.put("discovery-ec2", "org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin");
-        m.put("cloud-gce",     "org.elasticsearch.plugin.cloud.gce.CloudGcePlugin");
+        m.put("repository-s3",       "org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin");
+        m.put("discovery-ec2",       "org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin");
+        m.put("cloud-gce",           "org.elasticsearch.plugin.cloud.gce.CloudGcePlugin");
+        m.put("lang-javascript",     "org.elasticsearch.plugin.javascript.JavaScriptPlugin");
+        m.put("lang-python",         "org.elasticsearch.plugin.python.PythonPlugin");
         SPECIAL_PLUGINS = Collections.unmodifiableMap(m);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/client/Client.java b/core/src/main/java/org/elasticsearch/client/Client.java
index 3f60f26..eafac2b 100644
--- a/core/src/main/java/org/elasticsearch/client/Client.java
+++ b/core/src/main/java/org/elasticsearch/client/Client.java
@@ -71,12 +71,12 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * A client provides a one stop interface for performing actions/operations against the cluster.
- * <p/>
- * <p>All operations performed are asynchronous by nature. Each action/operation has two flavors, the first
+ * <p>
+ * All operations performed are asynchronous by nature. Each action/operation has two flavors, the first
  * simply returns an {@link org.elasticsearch.action.ActionFuture}, while the second accepts an
  * {@link org.elasticsearch.action.ActionListener}.
- * <p/>
- * <p>A client can either be retrieved from a {@link org.elasticsearch.node.Node} started, or connected remotely
+ * <p>
+ * A client can either be retrieved from a {@link org.elasticsearch.node.Node} started, or connected remotely
  * to one or more nodes using {@link org.elasticsearch.client.transport.TransportClient}.
  *
  * @see org.elasticsearch.node.Node#client()
@@ -94,8 +94,8 @@ public interface Client extends ElasticsearchClient, Releasable {
 
     /**
      * Index a JSON source associated with a given index and type.
-     * <p/>
-     * <p>The id is optional, if it is not provided, one will be generated automatically.
+     * <p>
+     * The id is optional, if it is not provided, one will be generated automatically.
      *
      * @param request The index request
      * @return The result future
@@ -105,8 +105,8 @@ public interface Client extends ElasticsearchClient, Releasable {
 
     /**
      * Index a document associated with a given index and type.
-     * <p/>
-     * <p>The id is optional, if it is not provided, one will be generated automatically.
+     * <p>
+     * The id is optional, if it is not provided, one will be generated automatically.
      *
      * @param request  The index request
      * @param listener A listener to be notified with a result
@@ -116,8 +116,8 @@ public interface Client extends ElasticsearchClient, Releasable {
 
     /**
      * Index a document associated with a given index and type.
-     * <p/>
-     * <p>The id is optional, if it is not provided, one will be generated automatically.
+     * <p>
+     * The id is optional, if it is not provided, one will be generated automatically.
      */
     IndexRequestBuilder prepareIndex();
 
@@ -149,8 +149,8 @@ public interface Client extends ElasticsearchClient, Releasable {
 
     /**
      * Index a document associated with a given index and type.
-     * <p/>
-     * <p>The id is optional, if it is not provided, one will be generated automatically.
+     * <p>
+     * The id is optional, if it is not provided, one will be generated automatically.
      *
      * @param index The index to index the document to
      * @param type  The type to index the document to
@@ -159,8 +159,8 @@ public interface Client extends ElasticsearchClient, Releasable {
 
     /**
      * Index a document associated with a given index and type.
-     * <p/>
-     * <p>The id is optional, if it is not provided, one will be generated automatically.
+     * <p>
+     * The id is optional, if it is not provided, one will be generated automatically.
      *
      * @param index The index to index the document to
      * @param type  The type to index the document to
@@ -482,7 +482,6 @@ public interface Client extends ElasticsearchClient, Releasable {
      * An action that returns the term vectors for a specific document.
      *
      * @param request The term vector request
-     * @return The response future
      */
     void termVectors(TermVectorsRequest request, ActionListener<TermVectorsResponse> listener);
 
@@ -513,7 +512,6 @@ public interface Client extends ElasticsearchClient, Releasable {
      * An action that returns the term vectors for a specific document.
      *
      * @param request The term vector request
-     * @return The response future
      */
     @Deprecated
     void termVector(TermVectorsRequest request, ActionListener<TermVectorsResponse> listener);
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index c56124f..1755101 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -65,8 +65,8 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 /**
  * The transport client allows to create a client that is not part of the cluster, but simply connects to one
  * or more nodes directly by adding their respective addresses using {@link #addTransportAddress(org.elasticsearch.common.transport.TransportAddress)}.
- * <p/>
- * <p>The transport client important modules used is the {@link org.elasticsearch.transport.TransportModule} which is
+ * <p>
+ * The transport client important modules used is the {@link org.elasticsearch.transport.TransportModule} which is
  * started in client mode (only connects, no bind).
  */
 public class TransportClient extends AbstractClient {
@@ -196,8 +196,8 @@ public class TransportClient extends AbstractClient {
 
     /**
      * Returns the current connected transport nodes that this client will use.
-     * <p/>
-     * <p>The nodes include all the nodes that are currently alive based on the transport
+     * <p>
+     * The nodes include all the nodes that are currently alive based on the transport
      * addresses provided.
      */
     public List<DiscoveryNode> connectedNodes() {
@@ -221,11 +221,11 @@ public class TransportClient extends AbstractClient {
 
     /**
      * Adds a transport address that will be used to connect to.
-     * <p/>
-     * <p>The Node this transport address represents will be used if its possible to connect to it.
+     * <p>
+     * The Node this transport address represents will be used if its possible to connect to it.
      * If it is unavailable, it will be automatically connected to once it is up.
-     * <p/>
-     * <p>In order to get the list of all the current connected nodes, please see {@link #connectedNodes()}.
+     * <p>
+     * In order to get the list of all the current connected nodes, please see {@link #connectedNodes()}.
      */
     public TransportClient addTransportAddress(TransportAddress transportAddress) {
         nodesService.addTransportAddresses(transportAddress);
@@ -234,11 +234,11 @@ public class TransportClient extends AbstractClient {
 
     /**
      * Adds a list of transport addresses that will be used to connect to.
-     * <p/>
-     * <p>The Node this transport address represents will be used if its possible to connect to it.
+     * <p>
+     * The Node this transport address represents will be used if its possible to connect to it.
      * If it is unavailable, it will be automatically connected to once it is up.
-     * <p/>
-     * <p>In order to get the list of all the current connected nodes, please see {@link #connectedNodes()}.
+     * <p>
+     * In order to get the list of all the current connected nodes, please see {@link #connectedNodes()}.
      */
     public TransportClient addTransportAddresses(TransportAddress... transportAddress) {
         nodesService.addTransportAddresses(transportAddress);
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
index 21962fb..3a71530 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
@@ -295,7 +295,7 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
 
     /**
      * a cluster state supersedes another state iff they are from the same master and the version this state is higher thant the other state.
-     * <p/>
+     * <p>
      * In essence that means that all the changes from the other cluster state are also reflected by the current one
      */
     public boolean supersedes(ClusterState other) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java b/core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java
index ca46416..a035cf7 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java
@@ -60,7 +60,6 @@ public class ClusterStateObserver {
     }
 
     /**
-     * @param clusterService
      * @param timeout        a global timeout for this observer. After it has expired the observer
      *                       will fail any existing or new #waitForNextChange calls. Set to null
      *                       to wait indefinitely
@@ -157,8 +156,6 @@ public class ClusterStateObserver {
 
     /**
      * reset this observer to the give cluster state. Any pending waits will be canceled.
-     *
-     * @param toState
      */
     public void reset(ClusterState toState) {
         if (observingContext.getAndSet(null) != null) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/Diff.java b/core/src/main/java/org/elasticsearch/cluster/Diff.java
index 2e571f4..1a9fff2 100644
--- a/core/src/main/java/org/elasticsearch/cluster/Diff.java
+++ b/core/src/main/java/org/elasticsearch/cluster/Diff.java
@@ -35,8 +35,6 @@ public interface Diff<T> {
 
     /**
      * Writes the differences into the output stream
-     * @param out
-     * @throws IOException
      */
     void writeTo(StreamOutput out) throws IOException;
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/IncompatibleClusterStateVersionException.java b/core/src/main/java/org/elasticsearch/cluster/IncompatibleClusterStateVersionException.java
index 9bead85..f191c20 100644
--- a/core/src/main/java/org/elasticsearch/cluster/IncompatibleClusterStateVersionException.java
+++ b/core/src/main/java/org/elasticsearch/cluster/IncompatibleClusterStateVersionException.java
@@ -20,12 +20,13 @@
 package org.elasticsearch.cluster;
 
 import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
 
 /**
- * Thrown by {@link Diffable#readDiffAndApply(org.elasticsearch.common.io.stream.StreamInput)} method
+ * Thrown by {@link Diff#apply} method
  */
 public class IncompatibleClusterStateVersionException extends ElasticsearchException {
     public IncompatibleClusterStateVersionException(String msg) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
index 9d64369..019e245 100644
--- a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
@@ -301,7 +301,6 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
 
     /**
      * Refreshes the ClusterInfo in a blocking fashion
-     * @return
      */
     public final ClusterInfo refresh() {
         if (logger.isTraceEnabled()) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
index eabe615..5776fe6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
@@ -270,7 +270,6 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          *
          * @param in stream input
          * @return restore status
-         * @throws IOException
          */
         public static ShardRestoreStatus readShardRestoreStatus(StreamInput in) throws IOException {
             ShardRestoreStatus shardSnapshotStatus = new ShardRestoreStatus();
@@ -282,7 +281,6 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          * Reads restore status from stream input
          *
          * @param in stream input
-         * @throws IOException
          */
         public void readFrom(StreamInput in) throws IOException {
             nodeId = in.readOptionalString();
@@ -294,7 +292,6 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          * Writes restore status to stream output
          *
          * @param out stream input
-         * @throws IOException
          */
         public void writeTo(StreamOutput out) throws IOException {
             out.writeOptionalString(nodeId);
@@ -471,7 +468,6 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
      * @param entry   restore operation metadata
      * @param builder XContent builder
      * @param params  serialization parameters
-     * @throws IOException
      */
     public void toXContent(Entry entry, XContentBuilder builder, ToXContent.Params params) throws IOException {
         builder.startObject();
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
index 2010f67..b13c799 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
@@ -105,7 +105,7 @@ public class MappingUpdatedAction extends AbstractComponent {
     }
 
     /**
-     * Same as {@link #updateMappingOnMasterSynchronously(String, String, String, Mapping, TimeValue)}
+     * Same as {@link #updateMappingOnMasterSynchronously(String, String, Mapping, TimeValue)}
      * using the default timeout.
      */
     public void updateMappingOnMasterSynchronously(String index, String type, Mapping mappingUpdate) throws Throwable {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
index d5b398c..f12824d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.indices.InvalidAliasNameException;
 
 import java.io.IOException;
@@ -142,10 +142,10 @@ public class AliasValidator extends AbstractComponent {
     }
 
     private void validateAliasFilter(XContentParser parser, IndexQueryParserService indexQueryParserService) throws IOException {
-        QueryShardContext context = indexQueryParserService.getShardContext();
+        QueryParseContext context = indexQueryParserService.getParseContext();
         try {
             context.reset(parser);
-            context.parseContext().parseInnerFilter();
+            context.parseInnerFilter();
         } finally {
             context.reset(null);
             parser.close();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index 2e2458c..6ea1d0e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -430,7 +430,7 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     /**
      * Sometimes, the default mapping exists and an actual mapping is not created yet (introduced),
      * in this case, we want to return the default mapping in case it has some default mapping definitions.
-     * <p/>
+     * <p>
      * Note, once the mapping type is introduced, the default mapping is applied on the actual typed MappingMetaData,
      * setting its routing, timestamp, and so on if needed.
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
index 5c7bb8e..05c580f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java
@@ -222,9 +222,8 @@ public class IndexNameExpressionResolver extends AbstractComponent {
     /**
      * Iterates through the list of indices and selects the effective list of filtering aliases for the
      * given index.
-     * <p/>
      * <p>Only aliases with filters are returned. If the indices list contains a non-filtering reference to
-     * the index itself - null is returned. Returns <tt>null</tt> if no filtering is required.</p>
+     * the index itself - null is returned. Returns <tt>null</tt> if no filtering is required.
      */
     public String[] filteringAliases(ClusterState state, String index, String... expressions) {
         // expand the aliases wildcard
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index bc033fc..365783b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -43,7 +43,7 @@ import java.util.Set;
 
 /**
  * This service is responsible for upgrading legacy index metadata to the current version
- * <p/>
+ * <p>
  * Every time an existing index is introduced into cluster this service should be used
  * to upgrade the existing index metadata to the latest version of the cluster. It typically
  * occurs during cluster upgrade, when dangling indices are imported into the cluster or indices
@@ -97,7 +97,7 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
     /**
      * Checks that the index can be upgraded to the current version of the master node.
      *
-     * <p/>
+     * <p>
      * If the index does not need upgrade it returns the index metadata unchanged, otherwise it returns a modified index metadata. If index
      * cannot be updated the method throws an exception.
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
index 23a4c32..b5646e1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java
@@ -195,7 +195,6 @@ public class RepositoriesMetaData extends AbstractDiffable<Custom> implements Me
      * @param repository repository metadata
      * @param builder    XContent builder
      * @param params     serialization parameters
-     * @throws IOException
      */
     public static void toXContent(RepositoryMetaData repository, XContentBuilder builder, ToXContent.Params params) throws IOException {
         builder.startObject(repository.name(), XContentBuilder.FieldCaseConversion.NONE);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java
index 182c06b..3c13a10 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/RepositoryMetaData.java
@@ -78,7 +78,6 @@ public class RepositoryMetaData {
      *
      * @param in stream input
      * @return repository metadata
-     * @throws IOException
      */
     public static RepositoryMetaData readFrom(StreamInput in) throws IOException {
         String name = in.readString();
@@ -91,7 +90,6 @@ public class RepositoryMetaData {
      * Writes repository metadata to stream output
      *
      * @param out stream output
-     * @throws IOException
      */
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(name);
@@ -119,4 +117,4 @@ public class RepositoryMetaData {
         result = 31 * result + settings.hashCode();
         return result;
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/SnapshotId.java b/core/src/main/java/org/elasticsearch/cluster/metadata/SnapshotId.java
index 132145b..88c60f1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/SnapshotId.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/SnapshotId.java
@@ -99,7 +99,6 @@ public class SnapshotId implements Streamable {
      *
      * @param in stream input
      * @return snapshot id
-     * @throws IOException
      */
     public static SnapshotId readSnapshotId(StreamInput in) throws IOException {
         SnapshotId snapshot = new SnapshotId();
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java b/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
index 695b8e6..efc987a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/AllocationId.java
@@ -30,7 +30,7 @@ import java.io.IOException;
 /**
  * Uniquely identifies an allocation. An allocation is a shard moving from unassigned to initializing,
  * or relocation.
- * <p/>
+ * <p>
  * Relocation is a special case, where the origin shard is relocating with a relocationId and same id, and
  * the target shard (only materialized in RoutingNodes) is initializing with the id set to the origin shard
  * relocationId. Once relocation is done, the new allocation id is set to the relocationId. This is similar
@@ -83,7 +83,7 @@ public class AllocationId implements ToXContent {
 
     /**
      * Creates a new allocation id representing a cancelled relocation.
-     * <p/>
+     * <p>
      * Note that this is expected to be called on the allocation id
      * of the *source* shard
      */
@@ -94,7 +94,7 @@ public class AllocationId implements ToXContent {
 
     /**
      * Creates a new allocation id finalizing a relocation.
-     * <p/>
+     * <p>
      * Note that this is expected to be called on the allocation id
      * of the *target* shard and thus it only needs to clear the relocating id.
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
index 83ddb7c..99977ee 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
@@ -34,7 +34,7 @@ public interface HashFunction {
     /**
      * Calculate a hash value for routing and its type
      * @param type types name
-     * @param routing String to calculate the hash value from 
+     * @param id String to calculate the hash value from 
      * @return hash value of the given type and routing string
      */
     @Deprecated
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
index 062c7b4..42b1293 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java
@@ -286,8 +286,8 @@ public class IndexRoutingTable extends AbstractDiffable<IndexRoutingTable> imple
     /**
      * A groups shards iterator where each groups is a single {@link ShardRouting} and a group
      * is created for each shard routing.
-     * <p/>
-     * <p>This basically means that components that use the {@link GroupShardsIterator} will iterate
+     * <p>
+     * This basically means that components that use the {@link GroupShardsIterator} will iterate
      * over *all* the shards (all the replicas) within the index.</p>
      */
     public GroupShardsIterator groupByAllIt() {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
index 55301b4..e740a4f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
@@ -379,9 +379,6 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
     /**
      * Returns shards based on nodeAttributes given  such as node name , node attribute, node IP
      * Supports node specifications in cluster API
-     *
-     * @param nodeAttribute
-     * @param discoveryNodes
      */
     public ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeAttribute, DiscoveryNodes discoveryNodes) {
         ArrayList<ShardRouting> ordered = new ArrayList<>(activeShards.size() + allInitializingShards.size());
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index 9c9cfaa..227d59f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -424,7 +424,6 @@ public class RoutingNodes implements Iterable<RoutingNode> {
     /**
      * Cancels the give shard from the Routing nodes internal statistics and cancels
      * the relocation if the shard is relocating.
-     * @param shard
      */
     private void remove(ShardRouting shard) {
         ensureMutable();
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
index 60764ab..8ee8205 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/ShardRouting.java
@@ -528,7 +528,7 @@ public final class ShardRouting implements Streamable, ToXContent {
 
     /**
      * returns true if this routing has the same allocation ID as another.
-     * <p/>
+     * <p>
      * Note: if both shard routing has a null as their {@link #allocationId()}, this method returns false as the routing describe
      * no allocation at all..
      **/
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java b/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
index e6dc718..e329541 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/UnassignedInfo.java
@@ -47,7 +47,7 @@ public class UnassignedInfo implements ToXContent, Writeable<UnassignedInfo> {
 
     /**
      * Reason why the shard is in unassigned state.
-     * <p/>
+     * <p>
      * Note, ordering of the enum is important, make sure to add new values
      * at the end and handle version serialization properly.
      */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationExplanation.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationExplanation.java
index 9949f1b..6811013 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationExplanation.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationExplanation.java
@@ -52,7 +52,7 @@ public class AllocationExplanation implements Streamable {
         /**
          * Creates a new {@link NodeExplanation}
          *  
-         * @param node node referenced by {@link This} {@link NodeExplanation}
+         * @param node node referenced by this {@link NodeExplanation}
          * @param description a message associated with the given node 
          */
         public NodeExplanation(DiscoveryNode node, String description) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
index a907ef5..a3050f60 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
@@ -65,8 +65,8 @@ public class AllocationService extends AbstractComponent {
 
     /**
      * Applies the started shards. Note, shards can be called several times within this method.
-     * <p/>
-     * <p>If the same instance of the routing table is returned, then no change has been made.</p>
+     * <p>
+     * If the same instance of the routing table is returned, then no change has been made.</p>
      */
     public RoutingAllocation.Result applyStartedShards(ClusterState clusterState, List<? extends ShardRouting> startedShards) {
         return applyStartedShards(clusterState, startedShards, true);
@@ -94,8 +94,8 @@ public class AllocationService extends AbstractComponent {
 
     /**
      * Applies the failed shards. Note, shards can be called several times within this method.
-     * <p/>
-     * <p>If the same instance of the routing table is returned, then no change has been made.</p>
+     * <p>
+     * If the same instance of the routing table is returned, then no change has been made.</p>
      */
     public RoutingAllocation.Result applyFailedShards(ClusterState clusterState, List<FailedRerouteAllocation.FailedShard> failedShards) {
         RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
@@ -139,8 +139,8 @@ public class AllocationService extends AbstractComponent {
 
     /**
      * Reroutes the routing table based on the live nodes.
-     * <p/>
-     * <p>If the same instance of the routing table is returned, then no change has been made.
+     * <p>
+     * If the same instance of the routing table is returned, then no change has been made.
      */
     public RoutingAllocation.Result reroute(ClusterState clusterState) {
         return reroute(clusterState, false);
@@ -148,8 +148,8 @@ public class AllocationService extends AbstractComponent {
 
     /**
      * Reroutes the routing table based on the live nodes.
-     * <p/>
-     * <p>If the same instance of the routing table is returned, then no change has been made.
+     * <p>
+     * If the same instance of the routing table is returned, then no change has been made.
      */
     public RoutingAllocation.Result reroute(ClusterState clusterState, boolean debug) {
         RoutingNodes routingNodes = getMutableRoutingNodes(clusterState);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index 0cb2105..b0ac162 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -49,7 +49,6 @@ import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
  * The {@link BalancedShardsAllocator} re-balances the nodes allocations
  * within an cluster based on a {@link WeightFunction}. The clusters balance is defined by four parameters which can be set
  * in the cluster update API that allows changes in real-time:
- * <p/>
  * <ul><li><code>cluster.routing.allocation.balance.shard</code> - The <b>shard balance</b> defines the weight factor
  * for shards allocated on a {@link RoutingNode}</li>
  * <li><code>cluster.routing.allocation.balance.index</code> - The <b>index balance</b> defines a factor to the number
@@ -57,7 +56,7 @@ import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
  * <li><code>cluster.routing.allocation.balance.threshold</code> - A <b>threshold</b> to set the minimal optimization
  * value of operations that should be performed</li>
  * </ul>
- * <p/>
+ * <p>
  * These parameters are combined in a {@link WeightFunction} that allows calculation of node weights which
  * are used to re-balance shards based on global as well as per-index factors.
  */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java
index f4204fd..d9ad8c4 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java
@@ -126,7 +126,6 @@ public class CancelAllocationCommand implements AllocationCommand {
      * 
      * @param shardId id of the shard which allocation should be canceled
      * @param node id of the node that manages the shard which allocation should be canceled
-     * @param allowPrimary 
      */
     public CancelAllocationCommand(ShardId shardId, String node, boolean allowPrimary) {
         this.shardId = shardId;
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
index 614397a..f54fce4 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java
@@ -37,7 +37,7 @@ import org.elasticsearch.index.shard.ShardId;
 import java.io.IOException;
 
 /**
- * A command that moves a shard from a specific node to another node.<br />
+ * A command that moves a shard from a specific node to another node.<br>
  * <b>Note:</b> The shard needs to be in the state
  * {@link ShardRoutingState#STARTED} in order to be moved.
  */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
index 376b0d1..54b6d40 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
@@ -39,40 +39,35 @@ import java.util.Map;
  * attributes like node or physical rack locations. Awareness attributes accept
  * arbitrary configuration keys like a rack data-center identifier. For example
  * the setting:
- * <p/>
  * <pre>
  * cluster.routing.allocation.awareness.attributes: rack_id
  * </pre>
- * <p/>
+ * <p>
  * will cause allocations to be distributed over different racks such that
  * ideally at least one replicas of the all shard is available on the same rack.
  * To enable allocation awareness in this example nodes should contain a value
  * for the <tt>rack_id</tt> key like:
- * <p/>
  * <pre>
  * node.rack_id:1
  * </pre>
- * <p/>
+ * <p>
  * Awareness can also be used to prevent over-allocation in the case of node or
  * even "zone" failure. For example in cloud-computing infrastructures like
  * Amazone AWS a cluster might span over multiple "zones". Awareness can be used
  * to distribute replicas to individual zones by setting:
- * <p/>
  * <pre>
  * cluster.routing.allocation.awareness.attributes: zone
  * </pre>
- * <p/>
+ * <p>
  * and forcing allocation to be aware of the following zone the data resides in:
- * <p/>
  * <pre>
  * cluster.routing.allocation.awareness.force.zone.values: zone1,zone2
  * </pre>
- * <p/>
+ * <p>
  * In contrast to regular awareness this setting will prevent over-allocation on
  * <tt>zone1</tt> even if <tt>zone2</tt> fails partially or becomes entirely
  * unavailable. Nodes that belong to a certain zone / group should be started
  * with the zone id configured on the node-level settings like:
- * <p/>
  * <pre>
  * node.zone: zone1
  * </pre>
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
index d41beed..f83aa56 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
@@ -31,7 +31,7 @@ import org.elasticsearch.node.settings.NodeSettingsService;
  * re-balance (relocation) operations and restricts node allocations if the
  * configured threashold is reached. The default number of concurrent rebalance
  * operations is set to <tt>2</tt>
- * <p/>
+ * <p>
  * Re-balance operations can be controlled in real-time via the cluster update API using
  * <tt>cluster.routing.allocation.cluster_concurrent_rebalance</tt>. Iff this
  * setting is set to <tt>-1</tt> the number of concurrent re-balance operations
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
index de4bb4c..8fc6b4f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
@@ -41,7 +41,6 @@ import java.util.Locale;
  *     <li> <code>PRIMARIES</code> - only primary shards are allowed to be allocated
  *     <li> <code>ALL</code> - all shards are allowed to be allocated
  * </ul>
- * </p>
  *
  * <p>
  * Rebalancing settings can have the following values (non-casesensitive):
@@ -51,7 +50,6 @@ import java.util.Locale;
  *     <li> <code>PRIMARIES</code> - only primary shards are allowed to be balanced
  *     <li> <code>ALL</code> - all shards are allowed to be balanced
  * </ul>
- * </p>
  *
  * @see Rebalance
  * @see Allocation
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
index 138a08b..e0e2caa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
@@ -51,10 +51,8 @@ import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
  * <ol>
  * <li><tt>required</tt> - filters required allocations.
  * If any <tt>required</tt> filters are set the allocation is denied if the index is <b>not</b> in the set of <tt>required</tt> to allocate on the filtered node</li>
- * <p/>
  * <li><tt>include</tt> - filters "allowed" allocations.
  * If any <tt>include</tt> filters are set the allocation is denied if the index is <b>not</b> in the set of <tt>include</tt> filters for the filtered node</li>
- * <p/>
  * <li><tt>exclude</tt> - filters "prohibited" allocations.
  * If any <tt>exclude</tt> filters are set the allocation is denied if the index is in the set of <tt>exclude</tt> filters for the filtered node</li>
  * </ol>
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
index 59d6cd1..ed6814d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
@@ -31,17 +31,15 @@ import org.elasticsearch.node.settings.NodeSettingsService;
  * {@link ThrottlingAllocationDecider} controls the recovery process per node in
  * the cluster. It exposes two settings via the cluster update API that allow
  * changes in real-time:
- * <p/>
  * <ul>
  * <li><tt>cluster.routing.allocation.node_initial_primaries_recoveries</tt> -
  * restricts the number of initial primary shard recovery operations on a single
  * node. The default is <tt>4</tt></li>
- * <p/>
  * <li><tt>cluster.routing.allocation.node_concurrent_recoveries</tt> -
  * restricts the number of total concurrent shards initializing on a single node. The
  * default is <tt>2</tt></li>
  * </ul>
- * <p/>
+ * <p>
  * If one of the above thresholds is exceeded per node this allocation decider
  * will return {@link Decision#THROTTLE} as a hit to upstream logic to throttle
  * the allocation process to prevent overloading nodes due to too many concurrent recovery
diff --git a/core/src/main/java/org/elasticsearch/common/Base64.java b/core/src/main/java/org/elasticsearch/common/Base64.java
index f148858..390b370 100644
--- a/core/src/main/java/org/elasticsearch/common/Base64.java
+++ b/core/src/main/java/org/elasticsearch/common/Base64.java
@@ -24,37 +24,33 @@ import java.util.Locale;
 /**
  * <p>Encodes and decodes to and from Base64 notation.</p>
  * <p>Homepage: <a href="http://iharder.net/base64">http://iharder.net/base64</a>.</p>
- * <p/>
- * <p>Example:</p>
- * <p/>
+ * <p>
+ * Example:
+ * <p>
  * <code>String encoded = Base64.encode( myByteArray );</code>
- * <br />
+ * <br>
  * <code>byte[] myByteArray = Base64.decode( encoded );</code>
- * <p/>
- * <p>The <tt>options</tt> parameter, which appears in a few places, is used to pass
+ * <p>
+ * The <tt>options</tt> parameter, which appears in a few places, is used to pass
  * several pieces of information to the encoder. In the "higher level" methods such as
  * encodeBytes( bytes, options ) the options parameter can be used to indicate such
  * things as first gzipping the bytes before encoding them, not inserting linefeeds,
- * and encoding using the URL-safe and Ordered dialects.</p>
- * <p/>
- * <p>Note, according to <a href="http://www.faqs.org/rfcs/rfc3548.html">RFC3548</a>,
+ * and encoding using the URL-safe and Ordered dialects.
+ * <p>
+ * Note, according to <a href="http://www.faqs.org/rfcs/rfc3548.html">RFC3548</a>,
  * Section 2.1, implementations should not add line feeds unless explicitly told
  * to do so. I've got Base64 set to this behavior now, although earlier versions
- * broke lines by default.</p>
- * <p/>
- * <p>The constants defined in Base64 can be OR-ed together to combine options, so you
- * might make a call like this:</p>
- * <p/>
+ * broke lines by default.
+ * <p>
+ * The constants defined in Base64 can be OR-ed together to combine options, so you
+ * might make a call like this:
+ * <p>
  * <code>String encoded = Base64.encodeBytes( mybytes, Base64.GZIP | Base64.DO_BREAK_LINES );</code>
- * <p>to compress the data before encoding it and then making the output have newline characters.</p>
- * <p>Also...</p>
+ * <p>to compress the data before encoding it and then making the output have newline characters.
+ * <p>Also...
  * <code>String encoded = Base64.encodeBytes( crazyString.getBytes() );</code>
- * <p/>
- * <p/>
- * <p/>
  * <p>
  * Change Log:
- * </p>
  * <ul>
  * <li>v2.3.7 - Fixed subtle bug when base 64 input stream contained the
  * value 01111111, which is an invalid base 64 character but should not
@@ -65,14 +61,14 @@ import java.util.Locale;
  * <li>v2.3.6 - Fixed bug when breaking lines and the final byte of the encoded
  * string ended in the last column; the buffer was not properly shrunk and
  * contained an extra (null) byte that made it into the string.</li>
- * <li>v2.3.5 - Fixed bug in {@link #encodeFromFile} where estimated buffer size
+ * <li>v2.3.5 - Fixed bug in {@code #encodeFromFile} where estimated buffer size
  * was wrong for files of size 31, 34, and 37 bytes.</li>
  * <li>v2.3.4 - Fixed bug when working with gzipped streams whereby flushing
  * the Base64.OutputStream closed the Base64 encoding (by padding with equals
  * signs) too soon. Also added an option to suppress the automatic decoding
  * of gzipped streams. Also added experimental support for specifying a
  * class loader when using the
- * {@link #decodeToObject(java.lang.String, int, java.lang.ClassLoader)}
+ * {@code #decodeToObject(java.lang.String, int, java.lang.ClassLoader)}
  * method.</li>
  * <li>v2.3.3 - Changed default char encoding to US-ASCII which reduces the internal Java
  * footprint with its CharEncoders and so forth. Fixed some javadocs that were
@@ -127,7 +123,6 @@ import java.util.Locale;
  * Special thanks to Jim Kellerman at <a href="http://www.powerset.com/">http://www.powerset.com/</a>
  * for contributing the new Base64 dialects.
  * </li>
- * <p/>
  * <li>v2.1 - Cleaned up javadoc comments and unused variables and methods. Added
  * some convenience methods for reading and writing to and from files.</li>
  * <li>v2.0.2 - Now specifies UTF-8 encoding in places where the code fails on systems
@@ -155,14 +150,12 @@ import java.util.Locale;
  * <li>v1.3.4 - Fixed when "improperly padded stream" error was thrown at the wrong time.</li>
  * <li>v1.3.3 - Fixed I/O streams which were totally messed up.</li>
  * </ul>
- * <p/>
  * <p>
  * I am placing this code in the Public Domain. Do with it as you will.
  * This software comes with no guarantees or warranties but with
  * plenty of well-wishing instead!
  * Please visit <a href="http://iharder.net/base64">http://iharder.net/base64</a>
  * periodically to check for updates or to contribute improvements.
- * </p>
  *
  * @author Robert Harder
  * @author rob@iharder.net
@@ -669,8 +662,6 @@ public class Base64 {
      * Example: <code>encodeBytes( myData, Base64.GZIP )</code> or
      * <p>
      * Example: <code>encodeBytes( myData, Base64.GZIP | Base64.DO_BREAK_LINES )</code>
-     * <p/>
-     * <p/>
      * <p>As of v 2.3, if there is an error with the GZIP stream,
      * the method will throw an java.io.IOException. <b>This is new to v2.3!</b>
      * In earlier versions, it just returned a null value, but
@@ -692,8 +683,8 @@ public class Base64 {
     /**
      * Encodes a byte array into Base64 notation.
      * Does not GZip-compress data.
-     * <p/>
-     * <p>As of v 2.3, if there is an error,
+     * <p>
+     * As of v 2.3, if there is an error,
      * the method will throw an java.io.IOException. <b>This is new to v2.3!</b>
      * In earlier versions, it just returned a null value, but
      * in retrospect that's a pretty poor way to handle it.</p>
@@ -733,12 +724,11 @@ public class Base64 {
      * Example: <code>encodeBytes( myData, Base64.GZIP )</code> or
      * <p>
      * Example: <code>encodeBytes( myData, Base64.GZIP | Base64.DO_BREAK_LINES )</code>
-     * <p/>
-     * <p/>
-     * <p>As of v 2.3, if there is an error with the GZIP stream,
+     * <p>
+     * As of v 2.3, if there is an error with the GZIP stream,
      * the method will throw an java.io.IOException. <b>This is new to v2.3!</b>
      * In earlier versions, it just returned a null value, but
-     * in retrospect that's a pretty poor way to handle it.</p>
+     * in retrospect that's a pretty poor way to handle it.
      *
      * @param source  The data to convert
      * @param off     Offset in array where conversion should begin
@@ -1263,13 +1253,13 @@ public class Base64 {
         /**
          * Constructs a {@link Base64.InputStream} in
          * either ENCODE or DECODE mode.
-         * <p/>
+         * <p>
          * Valid options:<pre>
          *   ENCODE or DECODE: Encode or Decode as data is read.
          *   DO_BREAK_LINES: break lines at 76 characters
-         *     (only meaningful when encoding)</i>
+         *     (only meaningful when encoding)
          * </pre>
-         * <p/>
+         * <p>
          * Example: <code>new Base64.InputStream( in, Base64.DECODE )</code>
          *
          * @param in      the <tt>java.io.InputStream</tt> from which to read data.
@@ -1470,13 +1460,13 @@ public class Base64 {
         /**
          * Constructs a {@link Base64.OutputStream} in
          * either ENCODE or DECODE mode.
-         * <p/>
+         * <p>
          * Valid options:<pre>
          *   ENCODE or DECODE: Encode or Decode as data is read.
          *   DO_BREAK_LINES: don't break lines at 76 characters
-         *     (only meaningful when encoding)</i>
+         *     (only meaningful when encoding)
          * </pre>
-         * <p/>
+         * <p>
          * Example: <code>new Base64.OutputStream( out, Base64.ENCODE )</code>
          *
          * @param out     the <tt>java.io.OutputStream</tt> to which data will be written.
diff --git a/core/src/main/java/org/elasticsearch/common/Booleans.java b/core/src/main/java/org/elasticsearch/common/Booleans.java
index 94adaee..6b1b9b0 100644
--- a/core/src/main/java/org/elasticsearch/common/Booleans.java
+++ b/core/src/main/java/org/elasticsearch/common/Booleans.java
@@ -80,7 +80,6 @@ public class Booleans {
 
     /***
      *
-     * @param value
      * @return true/false
      * throws exception if string cannot be parsed to boolean
      */
diff --git a/core/src/main/java/org/elasticsearch/common/Numbers.java b/core/src/main/java/org/elasticsearch/common/Numbers.java
index 52d0337..df57c55 100644
--- a/core/src/main/java/org/elasticsearch/common/Numbers.java
+++ b/core/src/main/java/org/elasticsearch/common/Numbers.java
@@ -171,11 +171,4 @@ public final class Numbers {
         return longToBytes(Double.doubleToRawLongBits(val));
     }
 
-    /** Returns true if value is neither NaN nor infinite. */
-    public static boolean isValidDouble(double value) {
-        if (Double.isNaN(value) || Double.isInfinite(value)) {
-            return false;
-        }
-        return true;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/ParseField.java b/core/src/main/java/org/elasticsearch/common/ParseField.java
index 0d2d0d8..0aad723 100644
--- a/core/src/main/java/org/elasticsearch/common/ParseField.java
+++ b/core/src/main/java/org/elasticsearch/common/ParseField.java
@@ -104,4 +104,16 @@ public class ParseField {
     public String toString() {
         return getPreferredName();
     }
+
+    public String getAllReplacedWith() {
+        return allReplacedWith;
+    }
+
+    public String getCamelCaseName() {
+        return camelCaseName;
+    }
+
+    public String[] getDeprecatedNames() {
+        return deprecatedNames;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/ParsingException.java b/core/src/main/java/org/elasticsearch/common/ParsingException.java
index f66ed92..8ce2dd1 100644
--- a/core/src/main/java/org/elasticsearch/common/ParsingException.java
+++ b/core/src/main/java/org/elasticsearch/common/ParsingException.java
@@ -32,8 +32,7 @@ import org.elasticsearch.rest.RestStatus;
 import java.io.IOException;
 
 /**
- * Exception that can be used when parsing queries with a given {@link QueryParseContext}.
- * Can contain information about location of the error.
+ *
  */
 public class ParsingException extends ElasticsearchException {
 
@@ -76,15 +75,9 @@ public class ParsingException extends ElasticsearchException {
         this.columnNumber = col;
     }
 
-    public ParsingException(StreamInput in) throws IOException{
-        super(in);
-        lineNumber = in.readInt();
-        columnNumber = in.readInt();
-    }
-
     /**
      * Line number of the location of the error
-     *
+     * 
      * @return the line number or -1 if unknown
      */
     public int getLineNumber() {
@@ -93,7 +86,7 @@ public class ParsingException extends ElasticsearchException {
 
     /**
      * Column number of the location of the error
-     *
+     * 
      * @return the column number or -1 if unknown
      */
     public int getColumnNumber() {
@@ -120,4 +113,11 @@ public class ParsingException extends ElasticsearchException {
         out.writeInt(lineNumber);
         out.writeInt(columnNumber);
     }
+
+    public ParsingException(StreamInput in) throws IOException{
+        super(in);
+        lineNumber = in.readInt();
+        columnNumber = in.readInt();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/common/StopWatch.java b/core/src/main/java/org/elasticsearch/common/StopWatch.java
index cc18e93..f842dde 100644
--- a/core/src/main/java/org/elasticsearch/common/StopWatch.java
+++ b/core/src/main/java/org/elasticsearch/common/StopWatch.java
@@ -30,14 +30,14 @@ import java.util.concurrent.TimeUnit;
 /**
  * Simple stop watch, allowing for timing of a number of tasks,
  * exposing total running time and running time for each named task.
- * <p/>
- * <p>Conceals use of <code>System.nanoTime()</code>, improving the
+ * <p>
+ * Conceals use of <code>System.nanoTime()</code>, improving the
  * readability of application code and reducing the likelihood of calculation errors.
- * <p/>
- * <p>Note that this object is not designed to be thread-safe and does not
+ * <p>
+ * Note that this object is not designed to be thread-safe and does not
  * use synchronization.
- * <p/>
- * <p>This class is normally used to verify performance during proof-of-concepts
+ * <p>
+ * This class is normally used to verify performance during proof-of-concepts
  * and in development, rather than as part of production applications.
  *
  *
diff --git a/core/src/main/java/org/elasticsearch/common/Strings.java b/core/src/main/java/org/elasticsearch/common/Strings.java
index 40d9602..0877769 100644
--- a/core/src/main/java/org/elasticsearch/common/Strings.java
+++ b/core/src/main/java/org/elasticsearch/common/Strings.java
@@ -64,10 +64,10 @@ public class Strings {
 
     /**
      * Splits a backslash escaped string on the separator.
-     * <p/>
+     * <p>
      * Current backslash escaping supported:
      * <br> \n \t \r \b \f are escaped the same as a Java String
-     * <br> Other characters following a backslash are produced verbatim (\c => c)
+     * <br> Other characters following a backslash are produced verbatim (\c =&gt; c)
      *
      * @param s         the string to split
      * @param separator the separator to split on
@@ -131,7 +131,7 @@ public class Strings {
     /**
      * Check that the given CharSequence is neither <code>null</code> nor of length 0.
      * Note: Will return <code>true</code> for a CharSequence that purely consists of whitespace.
-     * <p><pre>
+     * <pre>
      * StringUtils.hasLength(null) = false
      * StringUtils.hasLength("") = false
      * StringUtils.hasLength(" ") = true
@@ -174,7 +174,7 @@ public class Strings {
     /**
      * Check that the given CharSequence is either <code>null</code> or of length 0.
      * Note: Will return <code>false</code> for a CharSequence that purely consists of whitespace.
-     * <p><pre>
+     * <pre>
      * StringUtils.isEmpty(null) = true
      * StringUtils.isEmpty("") = true
      * StringUtils.isEmpty(" ") = false
@@ -193,7 +193,7 @@ public class Strings {
      * Check whether the given CharSequence has actual text.
      * More specifically, returns <code>true</code> if the string not <code>null</code>,
      * its length is greater than 0, and it contains at least one non-whitespace character.
-     * <p><pre>
+     * <pre>
      * StringUtils.hasText(null) = false
      * StringUtils.hasText("") = false
      * StringUtils.hasText(" ") = false
@@ -400,7 +400,7 @@ public class Strings {
      *
      * @param str the input String (e.g. "myString")
      * @return the quoted String (e.g. "'myString'"),
-     *         or <code>null<code> if the input was <code>null</code>
+     *         or <code>null</code> if the input was <code>null</code>
      */
     public static String quote(String str) {
         return (str != null ? "'" + str + "'" : null);
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java
index 9df7f26..b95a7f2 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/support/AbstractLegacyBlobContainer.java
@@ -41,7 +41,7 @@ public abstract class AbstractLegacyBlobContainer extends AbstractBlobContainer
 
     /**
      * Creates a new {@link InputStream} for the given blob name
-     * <p/>
+     * <p>
      * This method is deprecated and is used only for compatibility with older blob containers
      * The new blob containers should use readBlob/writeBlob methods instead
      */
@@ -50,7 +50,7 @@ public abstract class AbstractLegacyBlobContainer extends AbstractBlobContainer
 
     /**
      * Creates a new OutputStream for the given blob name
-     * <p/>
+     * <p>
      * This method is deprecated and is used only for compatibility with older blob containers
      * The new blob containers should override readBlob/writeBlob methods instead
      */
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java
index f330ce8..813a125 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobStore.java
@@ -42,7 +42,7 @@ public class URLBlobStore extends AbstractComponent implements BlobStore {
 
     /**
      * Constructs new read-only URL-based blob store
-     * <p/>
+     * <p>
      * The following settings are supported
      * <dl>
      * <dt>buffer_size</dt>
@@ -98,8 +98,6 @@ public class URLBlobStore extends AbstractComponent implements BlobStore {
 
     /**
      * This operation is not supported by URL Blob Store
-     *
-     * @param path
      */
     @Override
     public void delete(BlobPath path) {
@@ -119,7 +117,6 @@ public class URLBlobStore extends AbstractComponent implements BlobStore {
      *
      * @param path relative path
      * @return Base URL + path
-     * @throws MalformedURLException
      */
     private URL buildPath(BlobPath path) throws MalformedURLException {
         String[] paths = path.toArray();
diff --git a/core/src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java b/core/src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java
index d87c316..9284b7b 100644
--- a/core/src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java
+++ b/core/src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java
@@ -102,10 +102,9 @@ public class ChildMemoryCircuitBreaker implements CircuitBreaker {
      * Add a number of bytes, tripping the circuit breaker if the aggregated
      * estimates are above the limit. Automatically trips the breaker if the
      * memory limit is set to 0. Will never trip the breaker if the limit is
-     * set < 0, but can still be used to aggregate estimations.
+     * set &lt; 0, but can still be used to aggregate estimations.
      * @param bytes number of bytes to add to the breaker
      * @return number of "used" bytes so far
-     * @throws CircuitBreakingException
      */
     @Override
     public double addEstimateBytesAndMaybeBreak(long bytes, String label) throws CircuitBreakingException {
diff --git a/core/src/main/java/org/elasticsearch/common/breaker/CircuitBreaker.java b/core/src/main/java/org/elasticsearch/common/breaker/CircuitBreaker.java
index afd8efd..7811ff3 100644
--- a/core/src/main/java/org/elasticsearch/common/breaker/CircuitBreaker.java
+++ b/core/src/main/java/org/elasticsearch/common/breaker/CircuitBreaker.java
@@ -66,7 +66,6 @@ public interface CircuitBreaker {
      * @param bytes number of bytes to add
      * @param label string label describing the bytes being added
      * @return the number of "used" bytes for the circuit breaker
-     * @throws CircuitBreakingException
      */
     public double addEstimateBytesAndMaybeBreak(long bytes, String label) throws CircuitBreakingException;
 
diff --git a/core/src/main/java/org/elasticsearch/common/breaker/MemoryCircuitBreaker.java b/core/src/main/java/org/elasticsearch/common/breaker/MemoryCircuitBreaker.java
index d08d618..b069456 100644
--- a/core/src/main/java/org/elasticsearch/common/breaker/MemoryCircuitBreaker.java
+++ b/core/src/main/java/org/elasticsearch/common/breaker/MemoryCircuitBreaker.java
@@ -75,7 +75,6 @@ public class MemoryCircuitBreaker implements CircuitBreaker {
 
     /**
      * Method used to trip the breaker
-     * @throws CircuitBreakingException
      */
     @Override
     public void circuitBreak(String fieldName, long bytesNeeded) throws CircuitBreakingException {
@@ -90,10 +89,9 @@ public class MemoryCircuitBreaker implements CircuitBreaker {
      * Add a number of bytes, tripping the circuit breaker if the aggregated
      * estimates are above the limit. Automatically trips the breaker if the
      * memory limit is set to 0. Will never trip the breaker if the limit is
-     * set < 0, but can still be used to aggregate estimations.
+     * set &lt; 0, but can still be used to aggregate estimations.
      * @param bytes number of bytes to add to the breaker
      * @return number of "used" bytes so far
-     * @throws CircuitBreakingException
      */
     @Override
     public double addEstimateBytesAndMaybeBreak(long bytes, String label) throws CircuitBreakingException {
diff --git a/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java b/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java
index ccadbcf..b807d48 100644
--- a/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java
+++ b/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenIntMap.java
@@ -32,7 +32,7 @@ import java.util.Map;
 
 /**
  * An immutable map implementation based on open hash map.
- * <p/>
+ * <p>
  * Can be constructed using a {@link #builder()}, or using {@link #builder(org.elasticsearch.common.collect.ImmutableOpenIntMap)} (which is an optimized
  * option to copy over existing content and modify it).
  */
@@ -47,7 +47,7 @@ public final class ImmutableOpenIntMap<VType> implements Iterable<IntObjectCurso
     /**
      * @return Returns the value associated with the given key or the default value
      * for the key type, if the key is not associated with any value.
-     * <p/>
+     * <p>
      * <b>Important note:</b> For primitive type values, the value returned for a non-existing
      * key may not be the default value of the primitive type (it may be any value previously
      * assigned to that slot).
@@ -91,8 +91,8 @@ public final class ImmutableOpenIntMap<VType> implements Iterable<IntObjectCurso
      *       + &quot; value=&quot; + c.value);
      * }
      * </pre>
-     * <p/>
-     * <p>The <code>index</code> field inside the cursor gives the internal index inside
+     * <p>
+     * The <code>index</code> field inside the cursor gives the internal index inside
      * the container's implementation. The interpretation of this index depends on
      * to the container.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java b/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java
index 814df93..47c1bdf 100644
--- a/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java
+++ b/core/src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java
@@ -31,7 +31,7 @@ import java.util.Map;
 
 /**
  * An immutable map implementation based on open hash map.
- * <p/>
+ * <p>
  * Can be constructed using a {@link #builder()}, or using {@link #builder(ImmutableOpenMap)} (which is an optimized
  * option to copy over existing content and modify it).
  */
@@ -46,7 +46,7 @@ public final class ImmutableOpenMap<KType, VType> implements Iterable<ObjectObje
     /**
      * @return Returns the value associated with the given key or the default value
      * for the key type, if the key is not associated with any value.
-     * <p/>
+     * <p>
      * <b>Important note:</b> For primitive type values, the value returned for a non-existing
      * key may not be the default value of the primitive type (it may be any value previously
      * assigned to that slot).
@@ -98,8 +98,8 @@ public final class ImmutableOpenMap<KType, VType> implements Iterable<ObjectObje
      *       + &quot; value=&quot; + c.value);
      * }
      * </pre>
-     * <p/>
-     * <p>The <code>index</code> field inside the cursor gives the internal index inside
+     * <p>
+     * The <code>index</code> field inside the cursor gives the internal index inside
      * the container's implementation. The interpretation of this index depends on
      * to the container.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java b/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
index e6cbf26..7afccb3 100644
--- a/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
+++ b/core/src/main/java/org/elasticsearch/common/component/Lifecycle.java
@@ -23,15 +23,14 @@ package org.elasticsearch.common.component;
 /**
  * Lifecycle state. Allows the following transitions:
  * <ul>
- * <li>INITIALIZED -> STARTED, STOPPED, CLOSED</li>
- * <li>STARTED     -> STOPPED</li>
- * <li>STOPPED     -> STARTED, CLOSED</li>
- * <li>CLOSED      -> </li>
+ * <li>INITIALIZED -&gt; STARTED, STOPPED, CLOSED</li>
+ * <li>STARTED     -&gt; STOPPED</li>
+ * <li>STOPPED     -&gt; STARTED, CLOSED</li>
+ * <li>CLOSED      -&gt; </li>
  * </ul>
- * <p/>
- * <p>Also allows to stay in the same state. For example, when calling stop on a component, the
+ * <p>
+ * Also allows to stay in the same state. For example, when calling stop on a component, the
  * following logic can be applied:
- * <p/>
  * <pre>
  * public void stop() {
  *  if (!lifeccycleState.moveToStopped()) {
@@ -40,10 +39,9 @@ package org.elasticsearch.common.component;
  * // continue with stop logic
  * }
  * </pre>
- * <p/>
- * <p>Note, closed is only allowed to be called when stopped, so make sure to stop the component first.
+ * <p>
+ * Note, closed is only allowed to be called when stopped, so make sure to stop the component first.
  * Here is how the logic can be applied:
- * <p/>
  * <pre>
  * public void close() {
  *  if (lifecycleState.started()) {
diff --git a/core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java b/core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java
index 72c57a9..4af2e96 100644
--- a/core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/compress/CompressorFactory.java
@@ -112,7 +112,7 @@ public class CompressorFactory {
     }
 
     /**
-     * Uncompress the provided data, data can be detected as compressed using {@link #isCompressed(byte[], int, int)}.
+     * Uncompress the provided data, data can be detected as compressed using {@link #isCompressed(BytesReference)}.
      */
     public static BytesReference uncompressIfNeeded(BytesReference bytes) throws IOException {
         Compressor compressor = compressor(bytes);
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java b/core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java
index 2a31596..fca8097 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/GeoDistance.java
@@ -21,9 +21,6 @@ package org.elasticsearch.common.geo;
 
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.SloppyMath;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.fielddata.FieldData;
 import org.elasticsearch.index.fielddata.GeoPointValues;
@@ -31,17 +28,17 @@ import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDoubleValues;
-import java.io.IOException;
+
 import java.util.Locale;
 
 /**
  * Geo distance calculation.
  */
-public enum GeoDistance implements Writeable<GeoDistance> {
+public enum GeoDistance {
     /**
      * Calculates distance as points on a plane. Faster, but less accurate than {@link #ARC}.
      */
-    PLANE {
+    PLANE() {
         @Override
         public double calculate(double sourceLatitude, double sourceLongitude, double targetLatitude, double targetLongitude, DistanceUnit unit) {
             double px = targetLongitude - sourceLongitude;
@@ -63,7 +60,7 @@ public enum GeoDistance implements Writeable<GeoDistance> {
     /**
      * Calculates distance factor.
      */
-    FACTOR {
+    FACTOR() {
         @Override
         public double calculate(double sourceLatitude, double sourceLongitude, double targetLatitude, double targetLongitude, DistanceUnit unit) {
             double longitudeDifference = targetLongitude - sourceLongitude;
@@ -85,7 +82,7 @@ public enum GeoDistance implements Writeable<GeoDistance> {
     /**
      * Calculates distance as points on a globe.
      */
-    ARC {
+    ARC() {
         @Override
         public double calculate(double sourceLatitude, double sourceLongitude, double targetLatitude, double targetLongitude, DistanceUnit unit) {
             double x1 = sourceLatitude * Math.PI / 180D;
@@ -112,7 +109,7 @@ public enum GeoDistance implements Writeable<GeoDistance> {
      * Calculates distance as points on a globe in a sloppy way. Close to the pole areas the accuracy
      * of this function decreases.
      */
-    SLOPPY_ARC {
+    SLOPPY_ARC() {
 
         @Override
         public double normalize(double distance, DistanceUnit unit) {
@@ -130,31 +127,12 @@ public enum GeoDistance implements Writeable<GeoDistance> {
         }
     };
 
-    /** Returns a GeoDistance object as read from the StreamInput. */
-    @Override
-    public GeoDistance readFrom(StreamInput in) throws IOException {
-        int ord = in.readVInt();
-        if (ord < 0 || ord >= values().length) {
-            throw new IOException("Unknown GeoDistance ordinal [" + ord + "]");
-        }
-        return GeoDistance.values()[ord];
-    }
-
-    public static GeoDistance readGeoDistanceFrom(StreamInput in) throws IOException {
-        return DEFAULT.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal());
-    }
-
     /**
      * Default {@link GeoDistance} function. This method should be used, If no specific function has been selected.
      * This is an alias for <code>SLOPPY_ARC</code>
      */
-    public static final GeoDistance DEFAULT = SLOPPY_ARC;
-
+    public static final GeoDistance DEFAULT = SLOPPY_ARC; 
+    
     public abstract double normalize(double distance, DistanceUnit unit);
 
     public abstract double calculate(double sourceLatitude, double sourceLongitude, double targetLatitude, double targetLongitude, DistanceUnit unit);
@@ -202,14 +180,14 @@ public enum GeoDistance implements Writeable<GeoDistance> {
 
     /**
      * Get a {@link GeoDistance} according to a given name. Valid values are
-     *
+     * 
      * <ul>
      *     <li><b>plane</b> for <code>GeoDistance.PLANE</code></li>
      *     <li><b>sloppy_arc</b> for <code>GeoDistance.SLOPPY_ARC</code></li>
      *     <li><b>factor</b> for <code>GeoDistance.FACTOR</code></li>
      *     <li><b>arc</b> for <code>GeoDistance.ARC</code></li>
      * </ul>
-     *
+     * 
      * @param name name of the {@link GeoDistance}
      * @return a {@link GeoDistance}
      */
@@ -358,7 +336,7 @@ public enum GeoDistance implements Writeable<GeoDistance> {
 
     /**
      * Basic implementation of {@link FixedSourceDistance}. This class keeps the basic parameters for a distance
-     * functions based on a fixed source. Namely latitude, longitude and unit.
+     * functions based on a fixed source. Namely latitude, longitude and unit. 
      */
     public static abstract class FixedSourceDistanceBase implements FixedSourceDistance {
         protected final double sourceLatitude;
@@ -371,7 +349,7 @@ public enum GeoDistance implements Writeable<GeoDistance> {
             this.unit = unit;
         }
     }
-
+    
     public static class ArcFixedSourceDistance extends FixedSourceDistanceBase {
 
         public ArcFixedSourceDistance(double sourceLatitude, double sourceLongitude, DistanceUnit unit) {
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
index c50b85a..6b4ffd2 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java
@@ -19,11 +19,6 @@
 
 package org.elasticsearch.common.geo;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 
 import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.XGeoHashUtils;
@@ -32,14 +27,11 @@ import org.apache.lucene.util.XGeoUtils;
 /**
  *
  */
-public final class GeoPoint implements Writeable<GeoPoint> {
+public final class GeoPoint {
 
     private double lat;
     private double lon;
     private final static double TOLERANCE = XGeoUtils.TOLERANCE;
-    
-    // for serialization purposes
-    private static final GeoPoint PROTOTYPE = new GeoPoint(Double.NaN, Double.NaN);
 
     public GeoPoint() {
     }
@@ -59,10 +51,6 @@ public final class GeoPoint implements Writeable<GeoPoint> {
         this.lon = lon;
     }
 
-    public GeoPoint(GeoPoint template) {
-        this(template.getLat(), template.getLon());
-    }
-
     public GeoPoint reset(double lat, double lon) {
         this.lat = lat;
         this.lon = lon;
@@ -164,7 +152,8 @@ public final class GeoPoint implements Writeable<GeoPoint> {
     }
 
     public static GeoPoint parseFromLatLon(String latLon) {
-        GeoPoint point = new GeoPoint(latLon);
+        GeoPoint point = new GeoPoint();
+        point.resetFromString(latLon);
         return point;
     }
 
@@ -179,21 +168,4 @@ public final class GeoPoint implements Writeable<GeoPoint> {
     public static GeoPoint fromIndexLong(long indexLong) {
         return new GeoPoint().resetFromIndexHash(indexLong);
     }
-
-    @Override
-    public GeoPoint readFrom(StreamInput in) throws IOException {
-        double lat = in.readDouble();
-        double lon = in.readDouble();
-        return new GeoPoint(lat, lon);
-    }
-
-    public static GeoPoint readGeoPointFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeDouble(lat);
-        out.writeDouble(lon);
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java b/core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java
index 8028805..be06d4b 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/GeoUtils.java
@@ -23,7 +23,6 @@ import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
 import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
 import org.apache.lucene.util.SloppyMath;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
@@ -35,19 +34,10 @@ import java.io.IOException;
  */
 public class GeoUtils {
 
-    /** Maximum valid latitude in degrees. */
-    public static final double MAX_LAT = 90.0;
-    /** Minimum valid latitude in degrees. */
-    public static final double MIN_LAT = -90.0;
-    /** Maximum valid longitude in degrees. */
-    public static final double MAX_LON = 180.0;
-    /** Minimum valid longitude in degrees. */
-    public static final double MIN_LON = -180.0;
-
     public static final String LATITUDE = GeoPointFieldMapper.Names.LAT;
     public static final String LONGITUDE = GeoPointFieldMapper.Names.LON;
     public static final String GEOHASH = GeoPointFieldMapper.Names.GEOHASH;
-
+    
     /** Earth ellipsoid major axis defined by WGS 84 in meters */
     public static final double EARTH_SEMI_MAJOR_AXIS = 6378137.0;      // meters (WGS 84)
 
@@ -66,22 +56,6 @@ public class GeoUtils {
     /** Earth ellipsoid polar distance in meters */
     public static final double EARTH_POLAR_DISTANCE = Math.PI * EARTH_SEMI_MINOR_AXIS;
 
-    /** Returns true if latitude is actually a valid latitude value.*/
-    public static boolean isValidLatitude(double latitude) {
-        if (Double.isNaN(latitude) || Double.isInfinite(latitude) || latitude < GeoUtils.MIN_LAT || latitude > GeoUtils.MAX_LAT) {
-            return false;
-        }
-        return true;
-    }
-
-    /** Returns true if longitude is actually a valid longitude value. */
-    public static boolean isValidLongitude(double longitude) {
-        if (Double.isNaN(longitude) || Double.isNaN(longitude) || longitude < GeoUtils.MIN_LON || longitude > GeoUtils.MAX_LON) {
-            return false;
-        }
-        return true;
-    }
-
     /**
      * Return an approximate value of the diameter of the earth (in meters) at the given latitude (in radians).
      */
@@ -233,7 +207,7 @@ public class GeoUtils {
 
     /**
      * Normalize latitude to lie within the -90 to 90 (both inclusive) range.
-     * <p/>
+     * <p>
      * Note: You should not normalize longitude and latitude separately,
      * because when normalizing latitude it may be necessary to
      * add a shift of 180&deg; in the longitude.
@@ -257,7 +231,7 @@ public class GeoUtils {
     /**
      * Normalize the geo {@code Point} for its coordinates to lie within their
      * respective normalized ranges.
-     * <p/>
+     * <p>
      * Note: A shift of 180&deg; is applied in the longitude if necessary,
      * in order to normalize properly the latitude.
      *
@@ -270,9 +244,9 @@ public class GeoUtils {
     /**
      * Normalize the geo {@code Point} for the given coordinates to lie within
      * their respective normalized ranges.
-     * <p/>
+     * <p>
      * You can control which coordinate gets normalized with the two flags.
-     * <p/>
+     * <p>
      * Note: A shift of 180&deg; is applied in the longitude if necessary,
      * in order to normalize properly the latitude.
      * If normalizing latitude but not longitude, it is assumed that
@@ -334,9 +308,6 @@ public class GeoUtils {
      * 
      * @param parser {@link XContentParser} to parse the value from
      * @return new {@link GeoPoint} parsed from the parse
-     * 
-     * @throws IOException
-     * @throws org.elasticsearch.ElasticsearchParseException
      */
     public static GeoPoint parseGeoPoint(XContentParser parser) throws IOException, ElasticsearchParseException {
         return parseGeoPoint(parser, new GeoPoint());
@@ -355,9 +326,6 @@ public class GeoUtils {
      * @param parser {@link XContentParser} to parse the value from
      * @param point A {@link GeoPoint} that will be reset by the values parsed
      * @return new {@link GeoPoint} parsed from the parse
-     * 
-     * @throws IOException
-     * @throws org.elasticsearch.ElasticsearchParseException
      */
     public static GeoPoint parseGeoPoint(XContentParser parser, GeoPoint point) throws IOException, ElasticsearchParseException {
         double lat = Double.NaN;
diff --git a/core/src/main/java/org/elasticsearch/common/geo/ShapeRelation.java b/core/src/main/java/org/elasticsearch/common/geo/ShapeRelation.java
index 67287b6..6ee1693 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/ShapeRelation.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/ShapeRelation.java
@@ -19,18 +19,13 @@
 
 package org.elasticsearch.common.geo;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 import java.util.Locale;
 
 /**
  * Enum representing the relationship between a Query / Filter Shape and indexed Shapes
  * that will be used to determine if a Document should be matched or not
  */
-public enum ShapeRelation implements Writeable<ShapeRelation>{
+public enum ShapeRelation {
 
     INTERSECTS("intersects"),
     DISJOINT("disjoint"),
@@ -42,20 +37,6 @@ public enum ShapeRelation implements Writeable<ShapeRelation>{
         this.relationName = relationName;
     }
 
-    @Override
-    public ShapeRelation readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown ShapeRelation ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-
     public static ShapeRelation getRelationByName(String name) {
         name = name.toLowerCase(Locale.ENGLISH);
         for (ShapeRelation relation : ShapeRelation.values()) {
diff --git a/core/src/main/java/org/elasticsearch/common/geo/SpatialStrategy.java b/core/src/main/java/org/elasticsearch/common/geo/SpatialStrategy.java
index 23c1dbb..a83f291 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/SpatialStrategy.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/SpatialStrategy.java
@@ -18,16 +18,11 @@
  */
 package org.elasticsearch.common.geo;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
 
 /**
  *
  */
-public enum SpatialStrategy implements Writeable<SpatialStrategy> {
+public enum SpatialStrategy {
 
     TERM("term"),
     RECURSIVE("recursive");
@@ -41,27 +36,4 @@ public enum SpatialStrategy implements Writeable<SpatialStrategy> {
     public String getStrategyName() {
         return strategyName;
     }
-
-    @Override
-    public SpatialStrategy readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown SpatialStrategy ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-
-    public static SpatialStrategy fromString(String strategyName) {
-        for (SpatialStrategy strategy : values()) {
-            if (strategy.strategyName.equals(strategyName)) {
-                return strategy;
-            }
-        }
-        return null;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/geo/XShapeCollection.java b/core/src/main/java/org/elasticsearch/common/geo/XShapeCollection.java
index 2cc4a09..695db01 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/XShapeCollection.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/XShapeCollection.java
@@ -61,7 +61,7 @@ public class XShapeCollection<S extends Shape> extends ShapeCollection<S> {
 
   /**
    * Spatial4J shapes have no knowledge of directed edges. For this reason, a bounding box
-   * that wraps the dateline can have a min longitude that is mathematically > than the
+   * that wraps the dateline can have a min longitude that is mathematically &gt; than the
    * Rectangles' minX value.  This is an issue for geometric collections (e.g., MultiPolygon
    * and ShapeCollection) Until geometry logic can be cleaned up in Spatial4J, ES provides
    * the following expansion algorithm for GeometryCollections
diff --git a/core/src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java b/core/src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java
index 27d7318..f1054e1 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java
@@ -68,7 +68,7 @@ public class CircleBuilder extends ShapeBuilder {
 
     /**
      * Set the radius of the circle
-     * @param radius radius of the circle (see {@link DistanceUnit.Distance})
+     * @param radius radius of the circle (see {@link org.elasticsearch.common.unit.DistanceUnit.Distance})
      * @return this
      */
     public CircleBuilder radius(Distance radius) {
diff --git a/core/src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java b/core/src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java
index 8174efa..de1db18 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java
@@ -110,7 +110,6 @@ public abstract class PointCollection<E extends PointCollection<E>> extends Shap
      * @param builder builder to use 
      * @param closed repeat the first point at the end of the array if it's not already defines as last element of the array  
      * @return the builder
-     * @throws IOException
      */
     protected XContentBuilder coordinatesToXcontent(XContentBuilder builder, boolean closed) throws IOException {
         builder.startArray();
diff --git a/core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java b/core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java
index e33d63a..0c1a12d 100644
--- a/core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java
@@ -270,7 +270,7 @@ public abstract class ShapeBuilder implements ToXContent {
      * Create a new {@link ShapeBuilder} from {@link XContent}
      * @param parser parser to read the GeoShape from
      * @return {@link ShapeBuilder} read from the parser or null
-     *          if the parsers current token has been <code><null</code>
+     *          if the parsers current token has been <code>null</code>
      * @throws IOException if the input could not be read
      */
     public static ShapeBuilder parse(XContentParser parser) throws IOException {
@@ -284,7 +284,7 @@ public abstract class ShapeBuilder implements ToXContent {
      *                     to the shape construction process (e.g., orientation)
      *                     todo: refactor to place build specific parameters in the SpatialContext
      * @return {@link ShapeBuilder} read from the parser or null
-     *          if the parsers current token has been <code><null</code>
+     *          if the parsers current token has been <code>null</code>
      * @throws IOException if the input could not be read
      */
     public static ShapeBuilder parse(XContentParser parser, GeoShapeFieldMapper geoDocMapper) throws IOException {
@@ -385,7 +385,7 @@ public abstract class ShapeBuilder implements ToXContent {
 
     /**
      * Node used to represent a tree of coordinates.
-     * <p/>
+     * <p>
      * Can either be a leaf node consisting of a Coordinate, or a parent with
      * children
      */
@@ -606,7 +606,6 @@ public abstract class ShapeBuilder implements ToXContent {
 
         /**
          * Transforms coordinates in the eastern hemisphere (-180:0) to a (180:360) range 
-         * @param points
          */
         protected static void translate(Coordinate[] points) {
             for (Coordinate c : points) {
@@ -714,7 +713,6 @@ public abstract class ShapeBuilder implements ToXContent {
          * @param parser - parse utility object including source document
          * @param shapeMapper - field mapper needed for index specific parameters
          * @return ShapeBuilder - a builder instance used to create the geometry
-         * @throws IOException
          */
         public static ShapeBuilder parse(XContentParser parser, GeoShapeFieldMapper shapeMapper) throws IOException {
             if (parser.currentToken() == XContentParser.Token.VALUE_NULL) {
diff --git a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
index b21935e..ed2de6e 100644
--- a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
+++ b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
@@ -452,8 +452,8 @@ public class HttpDownloadHelper {
 
         /**
          * Has the download completed successfully?
-         * <p/>
-         * <p>Re-throws any exception caught during executaion.</p>
+         * <p>
+         * Re-throws any exception caught during executaion.</p>
          */
         boolean wasSuccessful() throws IOException {
             if (ioexception != null) {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java b/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
index 86ef158..7c0a5ce 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -32,7 +32,6 @@ import java.util.Objects;
  * a more readable configuration. Simply extend this class, implement {@link
  * #configure()}, and call the inherited methods which mirror those found in
  * {@link Binder}. For example:
- * <p/>
  * <pre>
  * public class MyModule extends AbstractModule {
  *   protected void configure() {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/AbstractProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/AbstractProcessor.java
index 1fdc377..154ce88 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/AbstractProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/AbstractProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -24,8 +24,8 @@ import java.util.List;
 
 /**
  * Abstract base class for creating an injector from module elements.
- * <p/>
- * <p>Extending classes must return {@code true} from any overridden
+ * <p>
+ * Extending classes must return {@code true} from any overridden
  * {@code visit*()} methods, in order for the element processor to remove the
  * handled element.
  *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Binder.java b/core/src/main/java/org/elasticsearch/common/inject/Binder.java
index 9315513..437591b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Binder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Binder.java
@@ -31,9 +31,8 @@ import java.lang.annotation.Annotation;
  * used to create an {@link Injector}. Guice provides this object to your
  * application's {@link Module} implementors so they may each contribute
  * their own bindings and other registrations.
- * <p/>
  * <h3>The Guice Binding EDSL</h3>
- * <p/>
+ * <p>
  * Guice uses an <i>embedded domain-specific language</i>, or EDSL, to help you
  * create bindings simply and readably.  This approach is great for overall
  * usability, but it does come with a small cost: <b>it is difficult to
@@ -42,7 +41,6 @@ import java.lang.annotation.Annotation;
  * examples below.  To save space, these examples omit the opening
  * {@code binder}, just as you will if your module extends
  * {@link AbstractModule}.
- * <p/>
  * <pre>
  *     bind(ServiceImpl.class);</pre>
  *
@@ -112,7 +110,7 @@ import java.lang.annotation.Annotation;
  * contribute their own custom scopes for use here as well.
  *
  * <pre>
- *     bind(new TypeLiteral&lt;PaymentService&lt;CreditCard>>() {})
+ *     bind(new TypeLiteral&lt;PaymentService&lt;CreditCard&gt;&gt;() {})
  *         .to(CreditCardPaymentService.class);</pre>
  *
  * This admittedly odd construct is the way to bind a parameterized type. It
@@ -141,7 +139,7 @@ import java.lang.annotation.Annotation;
  *
  * Sets up a constant binding. Constant injections must always be annotated.
  * When a constant binding's value is a string, it is eligile for conversion to
- * all primitive types, to {@link Enum#valueOf(Class, String) all enums}, and to
+ * all primitive types, to {@link Enum#valueOf all enums}, and to
  * {@link Class#forName class literals}. Conversions for other types can be
  * configured using {@link #convertToTypes(Matcher, TypeConverter)
  * convertToTypes()}.
@@ -178,7 +176,7 @@ import java.lang.annotation.Annotation;
  * the problems at runtime, as soon as you try to create your Injector.
  *
  * <p>The other methods of Binder such as {@link #bindScope},
- * {@link #bindInterceptor}, {@link #install}, {@link #requestStaticInjection},
+ * {@link #install}, {@link #requestStaticInjection},
  * {@link #addError} and {@link #currentStage} are not part of the Binding EDSL;
  * you can learn how to use these in the usual way, from the method
  * documentation.
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Binding.java b/core/src/main/java/org/elasticsearch/common/inject/Binding.java
index b854d1d..e0d0420 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Binding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Binding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -24,8 +24,8 @@ import org.elasticsearch.common.inject.spi.Element;
  * A mapping from a key (type and optional annotation) to the strategy for getting instances of the
  * type. This interface is part of the introspection API and is intended primarily for use by
  * tools.
- * <p/>
- * <p>Bindings are created in several ways:
+ * <p>
+ * Bindings are created in several ways:
  * <ul>
  * <li>Explicitly in a module, via {@code bind()} and {@code bindConstant()}
  * statements:
diff --git a/core/src/main/java/org/elasticsearch/common/inject/BindingAnnotation.java b/core/src/main/java/org/elasticsearch/common/inject/BindingAnnotation.java
index 816c306..3d18b57 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/BindingAnnotation.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/BindingAnnotation.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,7 +26,6 @@ import static java.lang.annotation.RetentionPolicy.RUNTIME;
  * Annotates annotations which are used for binding. Only one such annotation
  * may apply to a single injection point. You must also annotate binder
  * annotations with {@code @Retention(RUNTIME)}. For example:
- * <p/>
  * <pre>
  *   {@code @}Retention(RUNTIME)
  *   {@code @}Target({ FIELD, PARAMETER, METHOD })
diff --git a/core/src/main/java/org/elasticsearch/common/inject/BindingProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/BindingProcessor.java
index 58ea0be..0ef8c26 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/BindingProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/BindingProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/BoundProviderFactory.java b/core/src/main/java/org/elasticsearch/common/inject/BoundProviderFactory.java
index b1a0fce..57676f4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/BoundProviderFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/BoundProviderFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConfigurationException.java b/core/src/main/java/org/elasticsearch/common/inject/ConfigurationException.java
index 4afc653..3aaed0d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConfigurationException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConfigurationException.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConstantFactory.java b/core/src/main/java/org/elasticsearch/common/inject/ConstantFactory.java
index e451d04..d2fb6ae 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConstantFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConstantFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxy.java b/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxy.java
index d88e84d..391d9c8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxy.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxy.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxyFactory.java b/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxyFactory.java
index 081e43d..9a0cd36 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxyFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConstructionProxyFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjector.java b/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjector.java
index f732a0d..c8792af 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjectorStore.java b/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjectorStore.java
index cca1f7e..ce63da6 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjectorStore.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ConstructorInjectorStore.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ContextualCallable.java b/core/src/main/java/org/elasticsearch/common/inject/ContextualCallable.java
index 19ddf4f..b6d09e7 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ContextualCallable.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ContextualCallable.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/CreationException.java b/core/src/main/java/org/elasticsearch/common/inject/CreationException.java
index 5c44111..4900efe 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/CreationException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/CreationException.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java b/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
index 437ad03..b57f92e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/DefaultConstructionProxyFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/DeferredLookups.java b/core/src/main/java/org/elasticsearch/common/inject/DeferredLookups.java
index c6ca9ba..40d589f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/DeferredLookups.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/DeferredLookups.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java b/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
index fc2bd48..8b8b7b7 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/EncounterImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Exposed.java b/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
index 992da18..8e0598a 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Exposed.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ExposedKeyFactory.java b/core/src/main/java/org/elasticsearch/common/inject/ExposedKeyFactory.java
index 34f7547..0e00108 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ExposedKeyFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ExposedKeyFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/FactoryProxy.java b/core/src/main/java/org/elasticsearch/common/inject/FactoryProxy.java
index a7f88f6..b275ea6 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/FactoryProxy.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/FactoryProxy.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Guice.java b/core/src/main/java/org/elasticsearch/common/inject/Guice.java
index 7b303e5..128664c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Guice.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Guice.java
@@ -21,8 +21,8 @@ import java.util.Arrays;
 /**
  * The entry point to the Guice framework. Creates {@link Injector}s from
  * {@link Module}s.
- * <p/>
- * <p>Guice supports a model of development that draws clear boundaries between
+ * <p>
+ * Guice supports a model of development that draws clear boundaries between
  * APIs, Implementations of these APIs, Modules which configure these
  * implementations, and finally Applications which consist of a collection of
  * Modules. It is the Application, which typically defines your {@code main()}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ImplementedBy.java b/core/src/main/java/org/elasticsearch/common/inject/ImplementedBy.java
index 52e3509..652be0f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ImplementedBy.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ImplementedBy.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InheritingState.java b/core/src/main/java/org/elasticsearch/common/inject/InheritingState.java
index d70f47a..12c317b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InheritingState.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InheritingState.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Initializable.java b/core/src/main/java/org/elasticsearch/common/inject/Initializable.java
index 01a48e7..26c3da5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Initializable.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Initializable.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Initializables.java b/core/src/main/java/org/elasticsearch/common/inject/Initializables.java
index e1148bb..5704892 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Initializables.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Initializables.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Initializer.java b/core/src/main/java/org/elasticsearch/common/inject/Initializer.java
index f1288c5..1d68f16 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Initializer.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Initializer.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Inject.java b/core/src/main/java/org/elasticsearch/common/inject/Inject.java
index c118c4d..ff67b64 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Inject.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Inject.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -27,23 +27,20 @@ import static java.lang.annotation.RetentionPolicy.RUNTIME;
  * Annotates members of your implementation class (constructors, methods
  * and fields) into which the {@link Injector} should inject values.
  * The Injector fulfills injection requests for:
- * <p/>
  * <ul>
  * <li>Every instance it constructs. The class being constructed must have
  * exactly one of its constructors marked with {@code @Inject} or must have a
  * constructor taking no parameters. The Injector then proceeds to perform
  * method and field injections.
- * <p/>
  * <li>Pre-constructed instances passed to {@link Injector#injectMembers},
  * {@link org.elasticsearch.common.inject.binder.LinkedBindingBuilder#toInstance(Object)} and
  * {@link org.elasticsearch.common.inject.binder.LinkedBindingBuilder#toProvider(Provider)}.
  * In this case all constructors are, of course, ignored.
- * <p/>
  * <li>Static fields and methods of classes which any {@link Module} has
  * specifically requested static injection for, using
  * {@link Binder#requestStaticInjection}.
  * </ul>
- * <p/>
+ * <p>
  * In all cases, a member can be injected regardless of its Java access
  * specifier (private, default, protected, public).
  *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
index da87efa..e8b38b5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectionRequestProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Injector.java b/core/src/main/java/org/elasticsearch/common/inject/Injector.java
index d7ee0e8..bfc1fb4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Injector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Injector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -24,26 +24,25 @@ import java.util.Map;
  * for each type and uses bindings to inject them. This is the core of Guice, although you rarely
  * interact with it directly. This "behind-the-scenes" operation is what distinguishes dependency
  * injection from its cousin, the service locator pattern.
- * <p/>
- * <p>Contains several default bindings:
- * <p/>
+ * <p>
+ * Contains several default bindings:
  * <ul>
  * <li>This {@link Injector} instance itself
  * <li>A {@code Provider<T>} for each binding of type {@code T}
  * <li>The {@link java.util.logging.Logger} for the class being injected
  * <li>The {@link Stage} in which the Injector was created
  * </ul>
- * <p/>
+ * <p>
  * Injectors are created using the facade class {@link Guice}.
- * <p/>
- * <p>An injector can also {@link #injectMembers(Object) inject the dependencies} of
+ * <p>
+ * An injector can also {@link #injectMembers(Object) inject the dependencies} of
  * already-constructed instances. This can be used to interoperate with objects created by other
  * frameworks or services.
- * <p/>
- * <p>Injectors can be {@link #createChildInjector(Iterable) hierarchical}. Child injectors inherit
+ * <p>
+ * Injectors can be {@link #createChildInjector(Iterable) hierarchical}. Child injectors inherit
  * the configuration of their parent injectors, but the converse does not hold.
- * <p/>
- * <p>The injector's {@link #getBindings() internal bindings} are available for introspection. This
+ * <p>
+ * The injector's {@link #getBindings() internal bindings} are available for introspection. This
  * enables tools and extensions to operate on an injector reflectively.
  *
  * @author crazybob@google.com (Bob Lee)
@@ -54,8 +53,8 @@ public interface Injector {
     /**
      * Injects dependencies into the fields and methods of {@code instance}. Ignores the presence or
      * absence of an injectable constructor.
-     * <p/>
-     * <p>Whenever Guice creates an instance, it performs this injection automatically (after first
+     * <p>
+     * Whenever Guice creates an instance, it performs this injection automatically (after first
      * performing constructor injection), so if you're able to let Guice create all your objects for
      * you, you'll never need to use this method.
      *
@@ -90,13 +89,13 @@ public interface Injector {
 
     /**
      * Returns all explicit bindings.
-     * <p/>
-     * <p>The returned map does not include bindings inherited from a {@link #getParent() parent
+     * <p>
+     * The returned map does not include bindings inherited from a {@link #getParent() parent
      * injector}, should one exist. The returned map is guaranteed to iterate (for example, with
      * its {@link java.util.Map#entrySet()} iterator) in the order of insertion. In other words,
      * the order in which bindings appear in user Modules.
-     * <p/>
-     * <p>This method is part of the Guice SPI and is intended for use by tools and extensions.
+     * <p>
+     * This method is part of the Guice SPI and is intended for use by tools and extensions.
      */
     Map<Key<?>, Binding<?>> getBindings();
 
@@ -104,8 +103,8 @@ public interface Injector {
      * Returns the binding for the given injection key. This will be an explicit bindings if the key
      * was bound explicitly by a module, or an implicit binding otherwise. The implicit binding will
      * be created if necessary.
-     * <p/>
-     * <p>This method is part of the Guice SPI and is intended for use by tools and extensions.
+     * <p>
+     * This method is part of the Guice SPI and is intended for use by tools and extensions.
      *
      * @throws ConfigurationException if this injector cannot find or create the binding.
      */
@@ -115,8 +114,8 @@ public interface Injector {
      * Returns the binding for the given type. This will be an explicit bindings if the injection key
      * was bound explicitly by a module, or an implicit binding otherwise. The implicit binding will
      * be created if necessary.
-     * <p/>
-     * <p>This method is part of the Guice SPI and is intended for use by tools and extensions.
+     * <p>
+     * This method is part of the Guice SPI and is intended for use by tools and extensions.
      *
      * @throws ConfigurationException if this injector cannot find or create the binding.
      * @since 2.0
@@ -125,8 +124,8 @@ public interface Injector {
 
     /**
      * Returns all explicit bindings for {@code type}.
-     * <p/>
-     * <p>This method is part of the Guice SPI and is intended for use by tools and extensions.
+     * <p>
+     * This method is part of the Guice SPI and is intended for use by tools and extensions.
      */
     <T> List<Binding<T>> findBindingsByType(TypeLiteral<T> type);
 
@@ -179,12 +178,12 @@ public interface Injector {
      * Returns a new injector that inherits all state from this injector. All bindings, scopes,
      * interceptors and type converters are inherited -- they are visible to the child injector.
      * Elements of the child injector are not visible to its parent.
-     * <p/>
-     * <p>Just-in-time bindings created for child injectors will be created in an ancestor injector
+     * <p>
+     * Just-in-time bindings created for child injectors will be created in an ancestor injector
      * whenever possible. This allows for scoped instances to be shared between injectors. Use
      * explicit bindings to prevent bindings from being shared with the parent injector.
-     * <p/>
-     * <p>No key may be bound by both an injector and one of its ancestors. This includes just-in-time
+     * <p>
+     * No key may be bound by both an injector and one of its ancestors. This includes just-in-time
      * bindings. The lone exception is the key for {@code Injector.class}, which is bound by each
      * injector to itself.
      *
@@ -196,12 +195,12 @@ public interface Injector {
      * Returns a new injector that inherits all state from this injector. All bindings, scopes,
      * interceptors and type converters are inherited -- they are visible to the child injector.
      * Elements of the child injector are not visible to its parent.
-     * <p/>
-     * <p>Just-in-time bindings created for child injectors will be created in an ancestor injector
+     * <p>
+     * Just-in-time bindings created for child injectors will be created in an ancestor injector
      * whenever possible. This allows for scoped instances to be shared between injectors. Use
      * explicit bindings to prevent bindings from being shared with the parent injector.
-     * <p/>
-     * <p>No key may be bound by both an injector and one of its ancestors. This includes just-in-time
+     * <p>
+     * No key may be bound by both an injector and one of its ancestors. This includes just-in-time
      * bindings. The lone exception is the key for {@code Injector.class}, which is bound by each
      * injector to itself.
      *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectorBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/InjectorBuilder.java
index 5f7175d..4a9ba4a 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectorBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectorBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -30,8 +30,8 @@ import java.util.Set;
  * Builds a tree of injectors. This is a primary injector, plus child injectors needed for each
  * {@link Binder#newPrivateBinder() private environment}. The primary injector is not necessarily a
  * top-level injector.
- * <p/>
- * <p>Injector construction happens in two phases.
+ * <p>
+ * Injector construction happens in two phases.
  * <ol>
  * <li>Static building. In this phase, we interpret commands, create bindings, and inspect
  * dependencies. During this phase, we hold a lock to ensure consistency with parent injectors.
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
index 7aef2e7..15fb181 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectorImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InjectorShell.java b/core/src/main/java/org/elasticsearch/common/inject/InjectorShell.java
index 8acc7d7..aeb3f2e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InjectorShell.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InjectorShell.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Injectors.java b/core/src/main/java/org/elasticsearch/common/inject/Injectors.java
index 40a0ae1..900d258 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Injectors.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Injectors.java
@@ -50,10 +50,10 @@ public class Injectors {
     /**
      * Returns an instance of the given type with the {@link org.elasticsearch.common.inject.name.Named}
      * annotation value.
-     * <p/>
+     * <p>
      * This method allows you to switch this code
      * <code>injector.getInstance(Key.get(type, Names.named(name)));</code>
-     * <p/>
+     * <p>
      * to the more concise
      * <code>Injectors.getInstance(injector, type, name);</code>
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/InternalFactoryToProviderAdapter.java b/core/src/main/java/org/elasticsearch/common/inject/InternalFactoryToProviderAdapter.java
index d748cec..8cffd7e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/InternalFactoryToProviderAdapter.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/InternalFactoryToProviderAdapter.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Key.java b/core/src/main/java/org/elasticsearch/common/inject/Key.java
index 92925c6..3af3b4e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Key.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Key.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -27,21 +27,20 @@ import java.util.Objects;
 /**
  * Binding key consisting of an injection type and an optional annotation.
  * Matches the type and annotation at a point of injection.
- * <p/>
- * <p>For example, {@code Key.get(Service.class, Transactional.class)} will
+ * <p>
+ * For example, {@code Key.get(Service.class, Transactional.class)} will
  * match:
- * <p/>
  * <pre>
  *   {@literal @}Inject
  *   public void setService({@literal @}Transactional Service service) {
  *     ...
  *   }
  * </pre>
- * <p/>
- * <p>{@code Key} supports generic types via subclassing just like {@link
+ * <p>
+ * {@code Key} supports generic types via subclassing just like {@link
  * TypeLiteral}.
- * <p/>
- * <p>Keys do not differentiate between primitive types (int, char, etc.) and
+ * <p>
+ * Keys do not differentiate between primitive types (int, char, etc.) and
  * their correpsonding wrapper types (Integer, Character, etc.). Primitive
  * types will be replaced with their wrapper types when keys are created.
  *
@@ -56,15 +55,15 @@ public class Key<T> {
 
     /**
      * Constructs a new key. Derives the type from this class's type parameter.
-     * <p/>
-     * <p>Clients create an empty anonymous subclass. Doing so embeds the type
+     * <p>
+     * Clients create an empty anonymous subclass. Doing so embeds the type
      * parameter in the anonymous class's type hierarchy so we can reconstitute it
      * at runtime despite erasure.
-     * <p/>
-     * <p>Example usage for a binding of type {@code Foo} annotated with
+     * <p>
+     * Example usage for a binding of type {@code Foo} annotated with
      * {@code @Bar}:
-     * <p/>
-     * <p>{@code new Key<Foo>(Bar.class) {}}.
+     * <p>
+     * {@code new Key<Foo>(Bar.class) {}}.
      */
     @SuppressWarnings("unchecked")
     protected Key(Class<? extends Annotation> annotationType) {
@@ -75,15 +74,15 @@ public class Key<T> {
 
     /**
      * Constructs a new key. Derives the type from this class's type parameter.
-     * <p/>
-     * <p>Clients create an empty anonymous subclass. Doing so embeds the type
+     * <p>
+     * Clients create an empty anonymous subclass. Doing so embeds the type
      * parameter in the anonymous class's type hierarchy so we can reconstitute it
      * at runtime despite erasure.
-     * <p/>
-     * <p>Example usage for a binding of type {@code Foo} annotated with
+     * <p>
+     * Example usage for a binding of type {@code Foo} annotated with
      * {@code @Bar}:
-     * <p/>
-     * <p>{@code new Key<Foo>(new Bar()) {}}.
+     * <p>
+     * {@code new Key<Foo>(new Bar()) {}}.
      */
     @SuppressWarnings("unchecked")
     protected Key(Annotation annotation) {
@@ -95,14 +94,14 @@ public class Key<T> {
 
     /**
      * Constructs a new key. Derives the type from this class's type parameter.
-     * <p/>
-     * <p>Clients create an empty anonymous subclass. Doing so embeds the type
+     * <p>
+     * Clients create an empty anonymous subclass. Doing so embeds the type
      * parameter in the anonymous class's type hierarchy so we can reconstitute it
      * at runtime despite erasure.
-     * <p/>
-     * <p>Example usage for a binding of type {@code Foo}:
-     * <p/>
-     * <p>{@code new Key<Foo>() {}}.
+     * <p>
+     * Example usage for a binding of type {@code Foo}:
+     * <p>
+     * {@code new Key<Foo>() {}}.
      */
     @SuppressWarnings("unchecked")
     protected Key() {
diff --git a/core/src/main/java/org/elasticsearch/common/inject/LookupProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/LookupProcessor.java
index 0bcfaa2..9e85fb0 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/LookupProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/LookupProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Lookups.java b/core/src/main/java/org/elasticsearch/common/inject/Lookups.java
index b5eb690..9156983 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Lookups.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Lookups.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MembersInjector.java b/core/src/main/java/org/elasticsearch/common/inject/MembersInjector.java
index 3893bf1..0a4464a 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MembersInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MembersInjector.java
@@ -30,8 +30,8 @@ public interface MembersInjector<T> {
     /**
      * Injects dependencies into the fields and methods of {@code instance}. Ignores the presence or
      * absence of an injectable constructor.
-     * <p/>
-     * <p>Whenever Guice creates an instance, it performs this injection automatically (after first
+     * <p>
+     * Whenever Guice creates an instance, it performs this injection automatically (after first
      * performing constructor injection), so if you're able to let Guice create all your objects for
      * you, you'll never need to use this method.
      *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
index 399a231..4208971 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
index d8ee0d7..7b6f256 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MembersInjectorStore.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/MessageProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/MessageProcessor.java
index 2acba68..5bb1e5e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/MessageProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/MessageProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Module.java b/core/src/main/java/org/elasticsearch/common/inject/Module.java
index 9af716d..f3a43d8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Module.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Module.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -21,11 +21,11 @@ package org.elasticsearch.common.inject;
  * bindings, which will be used to create an {@link Injector}. A Guice-based
  * application is ultimately composed of little more than a set of
  * {@code Module}s and some bootstrapping code.
- * <p/>
- * <p>Your Module classes can use a more streamlined syntax by extending
+ * <p>
+ * Your Module classes can use a more streamlined syntax by extending
  * {@link AbstractModule} rather than implementing this interface directly.
- * <p/>
- * <p>In addition to the bindings configured via {@link #configure}, bindings
+ * <p>
+ * In addition to the bindings configured via {@link #configure}, bindings
  * will be created for all methods annotated with {@literal @}{@link Provides}.
  * Use scope and binding annotations on these methods to configure the
  * bindings.
@@ -34,8 +34,8 @@ public interface Module {
 
     /**
      * Contributes bindings and other configurations for this module to {@code binder}.
-     * <p/>
-     * <p><strong>Do not invoke this method directly</strong> to install submodules. Instead use
+     * <p>
+     * <strong>Do not invoke this method directly</strong> to install submodules. Instead use
      * {@link Binder#install(Module)}, which ensures that {@link Provides provider methods} are
      * discovered.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/OutOfScopeException.java b/core/src/main/java/org/elasticsearch/common/inject/OutOfScopeException.java
index 14096a3..353fe7c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/OutOfScopeException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/OutOfScopeException.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/PrivateBinder.java b/core/src/main/java/org/elasticsearch/common/inject/PrivateBinder.java
index f6a4293..cc5e356 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/PrivateBinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/PrivateBinder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/PrivateElementProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/PrivateElementProcessor.java
index 2b7dc69..8555291 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/PrivateElementProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/PrivateElementProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/PrivateModule.java b/core/src/main/java/org/elasticsearch/common/inject/PrivateModule.java
index a69ba75..4413028 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/PrivateModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/PrivateModule.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -31,17 +31,16 @@ import java.lang.annotation.Annotation;
  * A module whose configuration information is hidden from its environment by default. Only bindings
  * that are explicitly exposed will be available to other modules and to the users of the injector.
  * This module may expose the bindings it creates and the bindings of the modules it installs.
- * <p/>
- * <p>A private module can be nested within a regular module or within another private module using
+ * <p>
+ * A private module can be nested within a regular module or within another private module using
  * {@link Binder#install install()}.  Its bindings live in a new environment that inherits bindings,
  * type converters, scopes, and interceptors from the surrounding ("parent") environment.  When you
  * nest multiple private modules, the result is a tree of environments where the injector's
  * environment is the root.
- * <p/>
- * <p>Guice EDSL bindings can be exposed with {@link #expose(Class) expose()}. {@literal @}{@link
+ * <p>
+ * Guice EDSL bindings can be exposed with {@link #expose(Class) expose()}. {@literal @}{@link
  * org.elasticsearch.common.inject.Provides Provides} bindings can be exposed with the {@literal @}{@link
  * Exposed} annotation:
- * <p/>
  * <pre>
  * public class FooBarBazModule extends PrivateModule {
  *   protected void configure() {
@@ -61,20 +60,20 @@ import java.lang.annotation.Annotation;
  *   }
  * }
  * </pre>
- * <p/>
- * <p>Private modules are implemented using {@link Injector#createChildInjector(Module[]) parent
+ * <p>
+ * Private modules are implemented using {@link Injector#createChildInjector(Module[]) parent
  * injectors}. When it can satisfy their dependencies, just-in-time bindings will be created in the
  * root environment. Such bindings are shared among all environments in the tree.
- * <p/>
- * <p>The scope of a binding is constrained to its environment. A singleton bound in a private
+ * <p>
+ * The scope of a binding is constrained to its environment. A singleton bound in a private
  * module will be unique to its environment. But a binding for the same type in a different private
  * module will yield a different instance.
- * <p/>
- * <p>A shared binding that injects the {@code Injector} gets the root injector, which only has
+ * <p>
+ * A shared binding that injects the {@code Injector} gets the root injector, which only has
  * access to bindings in the root environment. An explicit binding that injects the {@code Injector}
  * gets access to all bindings in the child environment.
- * <p/>
- * <p>To promote a just-in-time binding to an explicit binding, bind it:
+ * <p>
+ * To promote a just-in-time binding to an explicit binding, bind it:
  * <pre>
  *   bind(FooImpl.class);
  * </pre>
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ProvidedBy.java b/core/src/main/java/org/elasticsearch/common/inject/ProvidedBy.java
index 6232dd7..945de83 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ProvidedBy.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ProvidedBy.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Provider.java b/core/src/main/java/org/elasticsearch/common/inject/Provider.java
index f9f4466..610062d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Provider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Provider.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -19,21 +19,17 @@ package org.elasticsearch.common.inject;
 /**
  * An object capable of providing instances of type {@code T}. Providers are used in numerous ways
  * by Guice:
- * <p/>
  * <ul>
  * <li>When the default means for obtaining instances (an injectable or parameterless constructor)
  * is insufficient for a particular binding, the module can specify a custom {@code Provider}
  * instead, to control exactly how Guice creates or obtains instances for the binding.
- * <p/>
  * <li>An implementation class may always choose to have a {@code Provider<T>} instance injected,
  * rather than having a {@code T} injected directly.  This may give you access to multiple
  * instances, instances you wish to safely mutate and discard, instances which are out of scope
  * (e.g. using a {@code @RequestScoped} object from within a {@code @SessionScoped} object), or
  * instances that will be initialized lazily.
- * <p/>
  * <li>A custom {@link Scope} is implemented as a decorator of {@code Provider<T>}, which decides
  * when to delegate to the backing provider and when to provide the instance some other way.
- * <p/>
  * <li>The {@link Injector} offers access to the {@code Provider<T>} it uses to fulfill requests
  * for a given key, via the {@link Injector#getProvider} methods.
  * </ul>
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ProviderToInternalFactoryAdapter.java b/core/src/main/java/org/elasticsearch/common/inject/ProviderToInternalFactoryAdapter.java
index ceb016b..d7b6afb 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ProviderToInternalFactoryAdapter.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ProviderToInternalFactoryAdapter.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Provides.java b/core/src/main/java/org/elasticsearch/common/inject/Provides.java
index 0920b85..587005f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Provides.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Provides.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java b/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
index 9497169..6fc7c30 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ProvisionException.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
index ba34e6a..22c542b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Scope.java b/core/src/main/java/org/elasticsearch/common/inject/Scope.java
index ccaa4c8..dacb105 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Scope.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Scope.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -24,8 +24,8 @@ package org.elasticsearch.common.inject;
  * and then immediately forgets it. Associating a scope with a particular
  * binding allows the created instance to be "remembered" and possibly used
  * again for other injections.
- * <p/>
- * <p>An example of a scope is {@link Scopes#SINGLETON}.
+ * <p>
+ * An example of a scope is {@link Scopes#SINGLETON}.
  *
  * @author crazybob@google.com (Bob Lee)
  */
@@ -35,8 +35,8 @@ public interface Scope {
      * Scopes a provider. The returned provider returns objects from this scope.
      * If an object does not exist in this scope, the provider can use the given
      * unscoped provider to retrieve one.
-     * <p/>
-     * <p>Scope implementations are strongly encouraged to override
+     * <p>
+     * Scope implementations are strongly encouraged to override
      * {@link Object#toString} in the returned provider and include the backing
      * provider's {@code toString()} output.
      *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ScopeAnnotation.java b/core/src/main/java/org/elasticsearch/common/inject/ScopeAnnotation.java
index 7c6ab29..ea1dd37 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ScopeAnnotation.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ScopeAnnotation.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,7 +26,6 @@ import static java.lang.annotation.RetentionPolicy.RUNTIME;
  * Annotates annotations which are used for scoping. Only one such annotation
  * may apply to a single implementation class. You must also annotate scope
  * annotations with {@code @Retention(RUNTIME)}. For example:
- * <p/>
  * <pre>
  *   {@code @}Retention(RUNTIME)
  *   {@code @}Target(TYPE)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/ScopeBindingProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/ScopeBindingProcessor.java
index 187db3b..9705867 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/ScopeBindingProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/ScopeBindingProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Scopes.java b/core/src/main/java/org/elasticsearch/common/inject/Scopes.java
index babc4bd..f94460d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Scopes.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Scopes.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -81,8 +81,8 @@ public class Scopes {
      * Injector obtains an instance of an object with "no scope", it injects this
      * instance then immediately forgets it.  When the next request for the same
      * binding arrives it will need to obtain the instance over again.
-     * <p/>
-     * <p>This exists only in case a class has been annotated with a scope
+     * <p>
+     * This exists only in case a class has been annotated with a scope
      * annotation such as {@link Singleton @Singleton}, and you need to override
      * this to "no scope" in your binding.
      *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
index a1375de..10ba17d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleFieldInjector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleMemberInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleMemberInjector.java
index 791a4f8..cca0075 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleMemberInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleMemberInjector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
index 65f7b06..9c40779 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleMethodInjector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/SingleParameterInjector.java b/core/src/main/java/org/elasticsearch/common/inject/SingleParameterInjector.java
index 372b483..b9570e1 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/SingleParameterInjector.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/SingleParameterInjector.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Singleton.java b/core/src/main/java/org/elasticsearch/common/inject/Singleton.java
index eeca5e0..68b0544 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Singleton.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Singleton.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Stage.java b/core/src/main/java/org/elasticsearch/common/inject/Stage.java
index f6969a2..5533cae 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Stage.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Stage.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/State.java b/core/src/main/java/org/elasticsearch/common/inject/State.java
index 53d1bdd..0388bb0 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/State.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/State.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/TypeConverterBindingProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/TypeConverterBindingProcessor.java
index ba711e4..485fab7 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/TypeConverterBindingProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/TypeConverterBindingProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/TypeListenerBindingProcessor.java b/core/src/main/java/org/elasticsearch/common/inject/TypeListenerBindingProcessor.java
index 4139fc0..533576d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/TypeListenerBindingProcessor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/TypeListenerBindingProcessor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java b/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
index ee50b12..81ee9cb 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/TypeLiteral.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -31,28 +31,27 @@ import static org.elasticsearch.common.inject.internal.MoreTypes.canonicalize;
  * represent generic types, so this class does. Forces clients to create a
  * subclass of this class which enables retrieval the type information even at
  * runtime.
- * <p/>
- * <p>For example, to create a type literal for {@code List<String>}, you can
+ * <p>
+ * For example, to create a type literal for {@code List<String>}, you can
  * create an empty anonymous inner class:
- * <p/>
- * <p/>
+ * <p>
  * {@code TypeLiteral<List<String>> list = new TypeLiteral<List<String>>() {};}
- * <p/>
- * <p>This syntax cannot be used to create type literals that have wildcard
+ * <p>
+ * This syntax cannot be used to create type literals that have wildcard
  * parameters, such as {@code Class<?>} or {@code List<? extends CharSequence>}.
  * Such type literals must be constructed programatically, either by {@link
  * Method#getGenericReturnType extracting types from members} or by using the
  * {@link Types} factory class.
- * <p/>
- * <p>Along with modeling generic types, this class can resolve type parameters.
+ * <p>
+ * Along with modeling generic types, this class can resolve type parameters.
  * For example, to figure out what type {@code keySet()} returns on a {@code
- * Map<Integer, String>}, use this code:<pre>   {@code
- * <p/>
+ * Map<Integer, String>}, use this code:{@code
+ * <p>
  *   TypeLiteral<Map<Integer, String>> mapType
  *       = new TypeLiteral<Map<Integer, String>>() {};
  *   TypeLiteral<?> keySetType
  *       = mapType.getReturnType(Map.class.getMethod("keySet"));
- *   System.out.println(keySetType); // prints "Set<Integer>"}</pre>
+ *   System.out.println(keySetType); // prints "Set<Integer>"}
  *
  * @author crazybob@google.com (Bob Lee)
  * @author jessewilson@google.com (Jesse Wilson)
@@ -66,8 +65,8 @@ public class TypeLiteral<T> {
     /**
      * Constructs a new type literal. Derives represented class from type
      * parameter.
-     * <p/>
-     * <p>Clients create an empty anonymous subclass. Doing so embeds the type
+     * <p>
+     * Clients create an empty anonymous subclass. Doing so embeds the type
      * parameter in the anonymous class's type hierarchy so we can reconstitute it
      * at runtime despite erasure.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/WeakKeySet.java b/core/src/main/java/org/elasticsearch/common/inject/WeakKeySet.java
index f13ff34..6e49e01 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/WeakKeySet.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/WeakKeySet.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -28,8 +28,8 @@ final class WeakKeySet {
 
     /**
      * We store strings rather than keys so we don't hold strong references.
-     * <p/>
-     * <p>One potential problem with this approach is that parent and child injectors cannot define
+     * <p>
+     * One potential problem with this approach is that parent and child injectors cannot define
      * keys whose class names are equal but class loaders are different. This shouldn't be an issue
      * in practice.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Assisted.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Assisted.java
index 7dba708..32b1d60 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Assisted.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Assisted.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -40,4 +40,4 @@ public @interface Assisted {
      * parameter with the same value. Names are not necessary when the parameter types are distinct.
      */
     String value() default "";
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
index e3410bf..edceb3f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedConstructor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedInject.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedInject.java
index cbba8be..ce661d2 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedInject.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/AssistedInject.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,8 +26,8 @@ import static java.lang.annotation.RetentionPolicy.RUNTIME;
  * <p>Constructors annotated with {@code @AssistedInject} indicate that they can be instantiated by
  * the {@link FactoryProvider}. Each constructor must exactly match one corresponding factory method
  * within the factory interface.
- * <p/>
- * <p>Constructor parameters must be either supplied by the factory interface and marked with
+ * <p>
+ * Constructor parameters must be either supplied by the factory interface and marked with
  * <code>@Assisted</code>, or they must be injectable.
  *
  * @author jmourits@google.com (Jerome Mourits)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
index d66037b..78aeb56 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -44,7 +44,6 @@ import java.util.Set;
 /**
  * Provides a factory that combines the caller's arguments with injector-supplied values to
  * construct objects.
- * <p/>
  * <h3>Defining a factory</h3>
  * Create an interface whose methods return the constructed type, or any of its supertypes. The
  * method's parameters are the arguments required to build the constructed type.
@@ -53,7 +52,6 @@ import java.util.Set;
  * }</pre>
  * You can name your factory methods whatever you like, such as <i>create</i>, <i>createPayment</i>
  * or <i>newPayment</i>.
- * <p/>
  * <h3>Creating a type that accepts factory parameters</h3>
  * {@code constructedType} is a concrete class with an {@literal @}{@link Inject}-annotated
  * constructor. In addition to injector-supplied parameters, the constructor should have
@@ -71,7 +69,6 @@ import java.util.Set;
  *   }
  * }</pre>
  * Any parameter that permits a null value should also be annotated {@code @Nullable}.
- * <p/>
  * <h3>Configuring factories</h3>
  * In your {@link org.elasticsearch.common.inject.Module module}, bind the factory interface to the returned
  * factory:
@@ -79,24 +76,20 @@ import java.util.Set;
  *     FactoryProvider.newFactory(PaymentFactory.class, RealPayment.class));</pre>
  * As a side-effect of this binding, Guice will inject the factory to initialize it for use. The
  * factory cannot be used until the injector has been initialized.
- * <p/>
  * <h3>Using the factory</h3>
  * Inject your factory into your application classes. When you use the factory, your arguments
  * will be combined with values from the injector to construct an instance.
  * <pre>public class PaymentAction {
  *   {@literal @}Inject private PaymentFactory paymentFactory;
- * <p/>
  *   public void doPayment(Money amount) {
  *     Payment payment = paymentFactory.create(new Date(), amount);
  *     payment.apply();
  *   }
  * }</pre>
- * <p/>
  * <h3>Making parameter types distinct</h3>
  * The types of the factory method's parameters must be distinct. To use multiple parameters of
  * the same type, use a named {@literal @}{@link Assisted} annotation to disambiguate the
  * parameters. The names must be applied to the factory method's parameters:
- * <p/>
  * <pre>public interface PaymentFactory {
  *   Payment create(
  *       <strong>{@literal @}Assisted("startDate")</strong> Date startDate,
@@ -115,21 +108,17 @@ import java.util.Set;
  *     ...
  *   }
  * }</pre>
- * <p/>
  * <h3>Values are created by Guice</h3>
  * Returned factories use child injectors to create values. The values are eligible for method
  * interception. In addition, {@literal @}{@literal Inject} members will be injected before they are
  * returned.
- * <p/>
  * <h3>Backwards compatibility using {@literal @}AssistedInject</h3>
  * Instead of the {@literal @}Inject annotation, you may annotate the constructed classes with
  * {@literal @}{@link AssistedInject}. This triggers a limited backwards-compatibility mode.
- * <p/>
  * <p>Instead of matching factory method arguments to constructor parameters using their names, the
  * <strong>parameters are matched by their order</strong>. The first factory method argument is
  * used for the first {@literal @}Assisted constructor parameter, etc.. Annotation names have no
  * effect.
- * <p/>
  * <p>Returned values are <strong>not created by Guice</strong>. These types are not eligible for
  * method interception. They do receive post-construction member injection.
  *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
index bedd796..c52b2dd 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Parameter.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Parameter.java
index 16e4deb..e067cc8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Parameter.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/Parameter.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -101,7 +101,7 @@ class Parameter {
     /**
      * Replace annotation instances with annotation types, this is only
      * appropriate for testing if a key is bound and not for injecting.
-     * <p/>
+     * <p>
      * See Guice bug 125,
      * http://code.google.com/p/google-guice/issues/detail?id=125
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
index cfaa980..fea6a62 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/ParameterListKey.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2007 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -63,4 +63,4 @@ class ParameterListKey {
     public String toString() {
         return paramList.toString();
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedBindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedBindingBuilder.java
index a093f7d..a6370a0 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedBindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedBindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedConstantBindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedConstantBindingBuilder.java
index bd92496..2d0a523 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedConstantBindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedConstantBindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedElementBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedElementBuilder.java
index d41f308..14c5292 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedElementBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/binder/AnnotatedElementBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/binder/LinkedBindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/binder/LinkedBindingBuilder.java
index 831935c..42c9228 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/binder/LinkedBindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/binder/LinkedBindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/binder/ScopedBindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/binder/ScopedBindingBuilder.java
index b25bed5..a73619f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/binder/ScopedBindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/binder/ScopedBindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/AbstractBindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/internal/AbstractBindingBuilder.java
index e6c3b1c..d037b1d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/AbstractBindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/AbstractBindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java
index 24b3b80..6cf8821 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/BindingBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/internal/BindingBuilder.java
index 45a1259..6b03be5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/BindingBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/BindingBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/BindingImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/BindingImpl.java
index 1c7552d..291ecb2 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/BindingImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/BindingImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ConstantBindingBuilderImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ConstantBindingBuilderImpl.java
index 5befe11..1cf37c1 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ConstantBindingBuilderImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ConstantBindingBuilderImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ConstructionContext.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ConstructionContext.java
index 7934fb4..34c9faf 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ConstructionContext.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ConstructionContext.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorHandler.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorHandler.java
index 86f1321..63173bd 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorHandler.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorHandler.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
index a38b2c0..6c9d155 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Errors.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -52,12 +52,12 @@ import java.util.Locale;
 /**
  * A collection of error messages. If this type is passed as a method parameter, the method is
  * considered to have executed successfully only if new errors were not added to this collection.
- * <p/>
- * <p>Errors can be chained to provide additional context. To add context, call {@link #withSource}
+ * <p>
+ * Errors can be chained to provide additional context. To add context, call {@link #withSource}
  * to create a new Errors instance that contains additional context. All messages added to the
  * returned instance will contain full context.
- * <p/>
- * <p>To avoid messages with redundant context, {@link #withSource} should be added sparingly. A
+ * <p>
+ * To avoid messages with redundant context, {@link #withSource} should be added sparingly. A
  * good rule of thumb is to assume a ethod's caller has already specified enough context to
  * identify that method. When calling a method that's defined in a different context, call that
  * method with an errors object that includes its context.
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorsException.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorsException.java
index c570853..570a787 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorsException.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ErrorsException.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ExposedBindingImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ExposedBindingImpl.java
index 64961cd..335b415 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ExposedBindingImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ExposedBindingImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ExposureBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ExposureBuilder.java
index 6b5f5f9..36b148e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ExposureBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ExposureBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/FailableCache.java b/core/src/main/java/org/elasticsearch/common/inject/internal/FailableCache.java
index 23c6b15..c0dd95f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/FailableCache.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/FailableCache.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/InstanceBindingImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/InstanceBindingImpl.java
index 64bae0b..73c447e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/InstanceBindingImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/InstanceBindingImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/InternalContext.java b/core/src/main/java/org/elasticsearch/common/inject/internal/InternalContext.java
index aed51b1..1184a97 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/InternalContext.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/InternalContext.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java b/core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java
index c1b36d6..b2203ed 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/InternalFactory.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Join.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Join.java
index db0afe9..1c8c835 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Join.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Join.java
@@ -29,8 +29,8 @@ import java.util.Objects;
  * iterators, collections, arrays, and varargs, and can append to any
  * {@link Appendable} or just return a {@link String}. For example,
  * {@code join(":", "a", "b", "c")} returns {@code "a:b:c"}.
- * <p/>
- * <p>All methods of this class throw {@link NullPointerException} when a value
+ * <p>
+ * All methods of this class throw {@link NullPointerException} when a value
  * of {@code null} is supplied for any parameter. The elements within the
  * collection, iterator, array, or varargs parameter list <i>may</i> be null --
  * these will be represented in the output by the string {@code "null"}.
@@ -45,8 +45,8 @@ public final class Join {
      * Returns a string containing the {@code tokens}, converted to strings if
      * necessary, separated by {@code delimiter}. If {@code tokens} is empty, it
      * returns an empty string.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -64,8 +64,8 @@ public final class Join {
      * Returns a string containing the {@code tokens}, converted to strings if
      * necessary, separated by {@code delimiter}. If {@code tokens} is empty, it
      * returns an empty string.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -82,8 +82,8 @@ public final class Join {
     /**
      * Returns a string containing the {@code tokens}, converted to strings if
      * necessary, separated by {@code delimiter}.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -104,8 +104,8 @@ public final class Join {
      * Returns a string containing the {@code tokens}, converted to strings if
      * necessary, separated by {@code delimiter}. If {@code tokens} is empty, it
      * returns an empty string.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -125,8 +125,8 @@ public final class Join {
      * Returns a string containing the contents of {@code map}, with entries
      * separated by {@code entryDelimiter}, and keys and values separated with
      * {@code keyValueSeparator}.
-     * <p/>
-     * <p>Each key and value will be converted to a {@link CharSequence} using
+     * <p>
+     * Each key and value will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -148,8 +148,8 @@ public final class Join {
     /**
      * Appends each of the {@code tokens} to {@code appendable}, separated by
      * {@code delimiter}.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -169,8 +169,8 @@ public final class Join {
     /**
      * Appends each of the {@code tokens} to {@code appendable}, separated by
      * {@code delimiter}.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -190,8 +190,8 @@ public final class Join {
     /**
      * Appends each of the {@code tokens} to {@code appendable}, separated by
      * {@code delimiter}.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -213,8 +213,8 @@ public final class Join {
     /**
      * Appends each of the {@code tokens} to {@code appendable}, separated by
      * {@code delimiter}.
-     * <p/>
-     * <p>Each token will be converted to a {@link CharSequence} using
+     * <p>
+     * Each token will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -251,8 +251,8 @@ public final class Join {
      * Appends the contents of {@code map} to {@code appendable}, with entries
      * separated by {@code entryDelimiter}, and keys and values separated with
      * {@code keyValueSeparator}.
-     * <p/>
-     * <p>Each key and value will be converted to a {@link CharSequence} using
+     * <p>
+     * Each key and value will be converted to a {@link CharSequence} using
      * {@link String#valueOf(Object)}, if it isn't a {@link CharSequence} already.
      * Note that this implies that null tokens will be appended as the
      * four-character string {@code "null"}.
@@ -316,4 +316,4 @@ public final class Join {
 
         private static final long serialVersionUID = 1L;
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
index 2a61116..737b8fa 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
index 621ff4d..aad0c3a 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
@@ -4,11 +4,9 @@ import java.lang.annotation.Annotation;
 
 /**
  * Whether a member supports null values injected.
- * <p/>
  * <p>Support for {@code Nullable} annotations in Guice is loose.
  * Any annotation type whose simplename is "Nullable" is sufficient to indicate
  * support for null values injected.
- * <p/>
  * <p>This allows support for JSR-305's
  * <a href="http://groups.google.com/group/jsr-305/web/proposed-annotations">
  * javax.annotation.meta.Nullable</a> annotation and IntelliJ IDEA's
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullable.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullable.java
index ec48aed..4dd499e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullable.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullable.java
@@ -22,8 +22,8 @@ import java.lang.annotation.*;
  * The presence of this annotation on a method parameter indicates that
  * {@code null} is an acceptable value for that parameter.  It should not be
  * used for parameters of primitive types.
- * <p/>
- * <p>This annotation may be used with the Google Web Toolkit (GWT).
+ * <p>
+ * This annotation may be used with the Google Web Toolkit (GWT).
  *
  * @author Kevin Bourrillion
  */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
index 6ab5454..685073b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
index 84fbae4..a5d2f09 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
index aa556ed..fdd9402 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Scoping.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Scoping.java
index b870f42..f6e2659 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Scoping.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Scoping.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/SourceProvider.java b/core/src/main/java/org/elasticsearch/common/inject/internal/SourceProvider.java
index a149c57..d498de9 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/SourceProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/SourceProvider.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/StackTraceElements.java b/core/src/main/java/org/elasticsearch/common/inject/internal/StackTraceElements.java
index 2b55ddd..5589c7b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/StackTraceElements.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/StackTraceElements.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Stopwatch.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Stopwatch.java
index 849e869..a7e3321 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Stopwatch.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Stopwatch.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Strings.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Strings.java
index 7c2e703..4021581 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Strings.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Strings.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -31,8 +31,8 @@ public class Strings {
      * The returned string will have the same value as the specified string if
      * its first character is non-alphabetic, if its first character is already
      * uppercase, or if the specified string is of length 0.
-     * <p/>
-     * <p>For example:
+     * <p>
+     * For example:
      * <pre>
      *    capitalize("foo bar").equals("Foo bar");
      *    capitalize("2b or not 2b").equals("2b or not 2b")
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ToStringBuilder.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ToStringBuilder.java
index 1b36d63..4e97528 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ToStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ToStringBuilder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/UniqueAnnotations.java b/core/src/main/java/org/elasticsearch/common/inject/internal/UniqueAnnotations.java
index 15c7736..4324eb4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/UniqueAnnotations.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/UniqueAnnotations.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/matcher/AbstractMatcher.java b/core/src/main/java/org/elasticsearch/common/inject/matcher/AbstractMatcher.java
index 5000716..a180eca 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/matcher/AbstractMatcher.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/matcher/AbstractMatcher.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/matcher/Matcher.java b/core/src/main/java/org/elasticsearch/common/inject/matcher/Matcher.java
index e2bfa1b..a6da311 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/matcher/Matcher.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/matcher/Matcher.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/matcher/Matchers.java b/core/src/main/java/org/elasticsearch/common/inject/matcher/Matchers.java
index c29a48d..96df76c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/matcher/Matchers.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/matcher/Matchers.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Element.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Element.java
index 02323dc..68ffa4e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Element.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Element.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
index 01bd59f..81f6656 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/MapBinder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -48,43 +48,43 @@ import static org.elasticsearch.common.inject.util.Types.newParameterizedTypeWit
  *     mapbinder.addBinding("skittles").to(Skittles.class);
  *   }
  * }</code></pre>
- * <p/>
- * <p>With this binding, a {@link Map}{@code <String, Snack>} can now be
+ * <p>
+ * With this binding, a {@link Map}{@code <String, Snack>} can now be
  * injected:
  * <pre><code>
  * class SnackMachine {
  *   {@literal @}Inject
  *   public SnackMachine(Map&lt;String, Snack&gt; snacks) { ... }
  * }</code></pre>
- * <p/>
- * <p>In addition to binding {@code Map<K, V>}, a mapbinder will also bind
+ * <p>
+ * In addition to binding {@code Map<K, V>}, a mapbinder will also bind
  * {@code Map<K, Provider<V>>} for lazy value provision:
  * <pre><code>
  * class SnackMachine {
  *   {@literal @}Inject
  *   public SnackMachine(Map&lt;String, Provider&lt;Snack&gt;&gt; snackProviders) { ... }
  * }</code></pre>
- * <p/>
- * <p>Creating mapbindings from different modules is supported. For example, it
+ * <p>
+ * Creating mapbindings from different modules is supported. For example, it
  * is okay to have both {@code CandyModule} and {@code ChipsModule} both
  * create their own {@code MapBinder<String, Snack>}, and to each contribute
  * bindings to the snacks map. When that map is injected, it will contain
  * entries from both modules.
- * <p/>
- * <p>Values are resolved at map injection time. If a value is bound to a
+ * <p>
+ * Values are resolved at map injection time. If a value is bound to a
  * provider, that provider's get method will be called each time the map is
  * injected (unless the binding is also scoped, or a map of providers is injected).
- * <p/>
- * <p>Annotations are used to create different maps of the same key/value
+ * <p>
+ * Annotations are used to create different maps of the same key/value
  * type. Each distinct annotation gets its own independent map.
- * <p/>
- * <p><strong>Keys must be distinct.</strong> If the same key is bound more than
+ * <p>
+ * <strong>Keys must be distinct.</strong> If the same key is bound more than
  * once, map injection will fail.
- * <p/>
- * <p><strong>Keys must be non-null.</strong> {@code addBinding(null)} will
+ * <p>
+ * <strong>Keys must be non-null.</strong> {@code addBinding(null)} will
  * throw an unchecked exception.
- * <p/>
- * <p><strong>Values must be non-null to use map injection.</strong> If any
+ * <p>
+ * <strong>Values must be non-null to use map injection.</strong> If any
  * value is null, map injection will fail (although injecting a map of providers
  * will not).
  *
@@ -195,38 +195,38 @@ public abstract class MapBinder<K, V> {
      * Returns a binding builder used to add a new entry in the map. Each
      * key must be distinct (and non-null). Bound providers will be evaluated each
      * time the map is injected.
-     * <p/>
-     * <p>It is an error to call this method without also calling one of the
+     * <p>
+     * It is an error to call this method without also calling one of the
      * {@code to} methods on the returned binding builder.
-     * <p/>
-     * <p>Scoping elements independently is supported. Use the {@code in} method
+     * <p>
+     * Scoping elements independently is supported. Use the {@code in} method
      * to specify a binding scope.
      */
     public abstract LinkedBindingBuilder<V> addBinding(K key);
 
     /**
      * The actual mapbinder plays several roles:
-     * <p/>
-     * <p>As a MapBinder, it acts as a factory for LinkedBindingBuilders for
+     * <p>
+     * As a MapBinder, it acts as a factory for LinkedBindingBuilders for
      * each of the map's values. It delegates to a {@link Multibinder} of
      * entries (keys to value providers).
-     * <p/>
-     * <p>As a Module, it installs the binding to the map itself, as well as to
+     * <p>
+     * As a Module, it installs the binding to the map itself, as well as to
      * a corresponding map whose values are providers. It uses the entry set
      * multibinder to construct the map and the provider map.
-     * <p/>
-     * <p>As a module, this implements equals() and hashcode() in order to trick
+     * <p>
+     * As a module, this implements equals() and hashcode() in order to trick
      * Guice into executing its configure() method only once. That makes it so
      * that multiple mapbinders can be created for the same target map, but
      * only one is bound. Since the list of bindings is retrieved from the
      * injector itself (and not the mapbinder), each mapbinder has access to
      * all contributions from all equivalent mapbinders.
-     * <p/>
-     * <p>Rather than binding a single Map.Entry&lt;K, V&gt;, the map binder
+     * <p>
+     * Rather than binding a single Map.Entry&lt;K, V&gt;, the map binder
      * binds keys and values independently. This allows the values to be properly
      * scoped.
-     * <p/>
-     * <p>We use a subclass to hide 'implements Module' from the public API.
+     * <p>
+     * We use a subclass to hide 'implements Module' from the public API.
      */
     public static final class RealMapBinder<K, V> extends MapBinder<K, V> implements Module {
         private final TypeLiteral<V> valueType;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
index 87ee3c3..9455dc5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/Multibinder.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -56,32 +56,32 @@ import java.util.Set;
  *     multibinder.addBinding().to(Skittles.class);
  *   }
  * }</code></pre>
- * <p/>
- * <p>With this binding, a {@link Set}{@code <Snack>} can now be injected:
+ * <p>
+ * With this binding, a {@link Set}{@code <Snack>} can now be injected:
  * <pre><code>
  * class SnackMachine {
  *   {@literal @}Inject
  *   public SnackMachine(Set&lt;Snack&gt; snacks) { ... }
  * }</code></pre>
- * <p/>
- * <p>Create multibindings from different modules is supported. For example, it
+ * <p>
+ * Create multibindings from different modules is supported. For example, it
  * is okay to have both {@code CandyModule} and {@code ChipsModule} to both
  * create their own {@code Multibinder<Snack>}, and to each contribute bindings
  * to the set of snacks. When that set is injected, it will contain elements
  * from both modules.
- * <p/>
- * <p>Elements are resolved at set injection time. If an element is bound to a
+ * <p>
+ * Elements are resolved at set injection time. If an element is bound to a
  * provider, that provider's get method will be called each time the set is
  * injected (unless the binding is also scoped).
- * <p/>
- * <p>Annotations are be used to create different sets of the same element
+ * <p>
+ * Annotations are be used to create different sets of the same element
  * type. Each distinct annotation gets its own independent collection of
  * elements.
- * <p/>
- * <p><strong>Elements must be distinct.</strong> If multiple bound elements
+ * <p>
+ * <strong>Elements must be distinct.</strong> If multiple bound elements
  * have the same value, set injection will fail.
- * <p/>
- * <p><strong>Elements must be non-null.</strong> If any set element is null,
+ * <p>
+ * <strong>Elements must be non-null.</strong> If any set element is null,
  * set injection will fail.
  *
  * @author jessewilson@google.com (Jesse Wilson)
@@ -164,33 +164,33 @@ public abstract class Multibinder<T> {
      * Returns a binding builder used to add a new element in the set. Each
      * bound element must have a distinct value. Bound providers will be
      * evaluated each time the set is injected.
-     * <p/>
-     * <p>It is an error to call this method without also calling one of the
+     * <p>
+     * It is an error to call this method without also calling one of the
      * {@code to} methods on the returned binding builder.
-     * <p/>
-     * <p>Scoping elements independently is supported. Use the {@code in} method
+     * <p>
+     * Scoping elements independently is supported. Use the {@code in} method
      * to specify a binding scope.
      */
     public abstract LinkedBindingBuilder<T> addBinding();
 
     /**
      * The actual multibinder plays several roles:
-     * <p/>
-     * <p>As a Multibinder, it acts as a factory for LinkedBindingBuilders for
+     * <p>
+     * As a Multibinder, it acts as a factory for LinkedBindingBuilders for
      * each of the set's elements. Each binding is given an annotation that
      * identifies it as a part of this set.
-     * <p/>
-     * <p>As a Module, it installs the binding to the set itself. As a module,
+     * <p>
+     * As a Module, it installs the binding to the set itself. As a module,
      * this implements equals() and hashcode() in order to trick Guice into
      * executing its configure() method only once. That makes it so that
      * multiple multibinders can be created for the same target collection, but
      * only one is bound. Since the list of bindings is retrieved from the
      * injector itself (and not the multibinder), each multibinder has access to
      * all contributions from all multibinders.
-     * <p/>
-     * <p>As a Provider, this constructs the set instances.
-     * <p/>
-     * <p>We use a subclass to hide 'implements Module, Provider' from the public
+     * <p>
+     * As a Provider, this constructs the set instances.
+     * <p>
+     * We use a subclass to hide 'implements Module, Provider' from the public
      * API.
      */
     public static final class RealMultibinder<T> extends Multibinder<T>
diff --git a/core/src/main/java/org/elasticsearch/common/inject/multibindings/RealElement.java b/core/src/main/java/org/elasticsearch/common/inject/multibindings/RealElement.java
index cc123de..acc8591 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/multibindings/RealElement.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/multibindings/RealElement.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/name/Named.java b/core/src/main/java/org/elasticsearch/common/inject/name/Named.java
index 0d914be..54ec471 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/name/Named.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/name/Named.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/name/NamedImpl.java b/core/src/main/java/org/elasticsearch/common/inject/name/NamedImpl.java
index 8cf7af1..7c43e7c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/name/NamedImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/name/NamedImpl.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/name/Names.java b/core/src/main/java/org/elasticsearch/common/inject/name/Names.java
index 7f3636f..c3da515 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/name/Names.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/name/Names.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/package-info.java b/core/src/main/java/org/elasticsearch/common/inject/package-info.java
index 44ff5b2..2fa93ef 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/package-info.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/package-info.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/BindingScopingVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/BindingScopingVisitor.java
index fb2f4ff..6025466 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/BindingScopingVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/BindingScopingVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/BindingTargetVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/BindingTargetVisitor.java
index b136115..56ca0c5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/BindingTargetVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/BindingTargetVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ConstructorBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ConstructorBinding.java
index df464fa..85951da 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ConstructorBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ConstructorBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ConvertedConstantBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ConvertedConstantBinding.java
index ebabfa3..3c4ec4d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ConvertedConstantBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ConvertedConstantBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingScopingVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingScopingVisitor.java
index 106f9e7..deced4e 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingScopingVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingScopingVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingTargetVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingTargetVisitor.java
index 508c289..75a3b61 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingTargetVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingTargetVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultElementVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultElementVisitor.java
index 7ee9e0f..d86f0ba 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultElementVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/DefaultElementVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
index 919e8d3..2bac853 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Dependency.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,8 +26,8 @@ import java.util.Set;
 
 /**
  * A variable that can be resolved by an injector.
- * <p/>
- * <p>Use {@link #get} to build a freestanding dependency, or {@link InjectionPoint} to build one
+ * <p>
+ * Use {@link #get} to build a freestanding dependency, or {@link InjectionPoint} to build one
  * that's attached to a constructor, method or field.
  *
  * @author crazybob@google.com (Bob Lee)
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Element.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Element.java
index a222c37..f9128d9 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Element.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Element.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -20,13 +20,13 @@ import org.elasticsearch.common.inject.Binder;
 
 /**
  * A core component of a module or injector.
- * <p/>
- * <p>The elements of a module can be inspected, validated and rewritten. Use {@link
+ * <p>
+ * The elements of a module can be inspected, validated and rewritten. Use {@link
  * Elements#getElements(org.elasticsearch.common.inject.Module[]) Elements.getElements()} to read the elements
  * from a module, and {@link Elements#getModule(Iterable) Elements.getModule()} to rewrite them.
  * This can be used for static analysis and generation of Guice modules.
- * <p/>
- * <p>The elements of an injector can be inspected and exercised. Use {@link
+ * <p>
+ * The elements of an injector can be inspected and exercised. Use {@link
  * org.elasticsearch.common.inject.Injector#getBindings Injector.getBindings()} to reflect on Guice injectors.
  *
  * @author jessewilson@google.com (Jesse Wilson)
@@ -38,8 +38,8 @@ public interface Element {
     /**
      * Returns an arbitrary object containing information about the "place" where this element was
      * configured. Used by Guice in the production of descriptive error messages.
-     * <p/>
-     * <p>Tools might specially handle types they know about; {@code StackTraceElement} is a good
+     * <p>
+     * Tools might specially handle types they know about; {@code StackTraceElement} is a good
      * example. Tools should simply call {@code toString()} on the source object if the type is
      * unfamiliar.
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ElementVisitor.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ElementVisitor.java
index 1cf17af..6711456 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ElementVisitor.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ElementVisitor.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
index afc8545..38456a4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Elements.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ExposedBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ExposedBinding.java
index 33dc6c9..d5490c8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ExposedBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ExposedBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -37,4 +37,4 @@ public interface ExposedBinding<T> extends Binding<T>, HasDependencies {
      */
     @Override
     void applyTo(Binder binder);
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/HasDependencies.java b/core/src/main/java/org/elasticsearch/common/inject/spi/HasDependencies.java
index 3ff1ffd..29367ac 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/HasDependencies.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/HasDependencies.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
index 72fde2c..2aa07b4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionRequest.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionRequest.java
index f52e6c3..00c6be4 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionRequest.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionRequest.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InstanceBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InstanceBinding.java
index 13712fe..bd0bf66 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InstanceBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InstanceBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/LinkedKeyBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/LinkedKeyBinding.java
index 31b1b74..82c36c7 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/LinkedKeyBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/LinkedKeyBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/MembersInjectorLookup.java b/core/src/main/java/org/elasticsearch/common/inject/spi/MembersInjectorLookup.java
index 9bde624..d12c6bd 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/MembersInjectorLookup.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/MembersInjectorLookup.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java b/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
index 37aa3b9..e5488d0 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/Message.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2006 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/PrivateElements.java b/core/src/main/java/org/elasticsearch/common/inject/spi/PrivateElements.java
index 64ead9c..6da4f29 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/PrivateElements.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/PrivateElements.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -50,8 +50,8 @@ public interface PrivateElements extends Element {
     /**
      * Returns an arbitrary object containing information about the "place" where this key was
      * exposed. Used by Guice in the production of descriptive error messages.
-     * <p/>
-     * <p>Tools might specially handle types they know about; {@code StackTraceElement} is a good
+     * <p>
+     * Tools might specially handle types they know about; {@code StackTraceElement} is a good
      * example. Tools should simply call {@code toString()} on the source object if the type is
      * unfamiliar.
      *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderBinding.java
index b6f67b7..04fb78d 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderInstanceBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderInstanceBinding.java
index 246bc8a..9277d9f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderInstanceBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderInstanceBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderKeyBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderKeyBinding.java
index 4d934f8..15aa751 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderKeyBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderKeyBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
index 464b850..9b925cd 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderLookup.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderWithDependencies.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderWithDependencies.java
index 2c21437..81cd2cd 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderWithDependencies.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ProviderWithDependencies.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/ScopeBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/ScopeBinding.java
index 9db84af..a1c52c5 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/ScopeBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/ScopeBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/StaticInjectionRequest.java b/core/src/main/java/org/elasticsearch/common/inject/spi/StaticInjectionRequest.java
index b7a0525..cc0fc09 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/StaticInjectionRequest.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/StaticInjectionRequest.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeConverterBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeConverterBinding.java
index 84215c7..bff5bc1 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeConverterBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeConverterBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeEncounter.java b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeEncounter.java
index b8487a1..49e84c8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeEncounter.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeEncounter.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListener.java b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListener.java
index f066683..134faa2 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListener.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListener.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -22,9 +22,9 @@ import org.elasticsearch.common.inject.TypeLiteral;
  * Listens for Guice to encounter injectable types. If a given type has its constructor injected in
  * one situation but only its methods and fields injected in another, Guice will notify this
  * listener once.
- * <p/>
- * <p>Useful for extra type checking, {@linkplain TypeEncounter#register(InjectionListener)
- * registering injection listeners}, and {@linkplain TypeEncounter#bindInterceptor(
+ * <p>
+ * Useful for extra type checking, {@linkplain TypeEncounter#register(InjectionListener)
+ * registering injection listeners}, and {@code TypeEncounter#bindInterceptor(
  *org.elasticsearch.common.inject.matcher.Matcher, org.aopalliance.intercept.MethodInterceptor[])
  * binding method interceptors}.
  *
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListenerBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListenerBinding.java
index 88feaa8..c52c239 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListenerBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/TypeListenerBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2009 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -23,9 +23,8 @@ import org.elasticsearch.common.inject.matcher.Matcher;
 /**
  * Binds types (picked using a Matcher) to an type listener. Registrations are created explicitly in
  * a module using {@link org.elasticsearch.common.inject.Binder#bindListener(Matcher, TypeListener)} statements:
- * <p/>
  * <pre>
- *     register(only(new TypeLiteral&lt;PaymentService&lt;CreditCard>>() {}), listener);</pre>
+ *     register(only(new TypeLiteral&lt;PaymentService&lt;CreditCard&gt;&gt;() {}), listener);</pre>
  *
  * @author jessewilson@google.com (Jesse Wilson)
  * @since 2.0
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/UntargettedBinding.java b/core/src/main/java/org/elasticsearch/common/inject/spi/UntargettedBinding.java
index ba2dc9a..8244696 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/UntargettedBinding.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/UntargettedBinding.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/inject/util/Modules.java b/core/src/main/java/org/elasticsearch/common/inject/util/Modules.java
index 6e0fdca..f29e5aa 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/util/Modules.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/util/Modules.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -64,8 +64,8 @@ public final class Modules {
      * Module functionalTestModule
      *     = Modules.override(new ProductionModule()).with(new TestModule());
      * </pre>
-     * <p/>
-     * <p>Prefer to write smaller modules that can be reused and tested without overrides.
+     * <p>
+     * Prefer to write smaller modules that can be reused and tested without overrides.
      *
      * @param modules the modules whose bindings are open to be overridden
      */
@@ -81,8 +81,8 @@ public final class Modules {
      * Module functionalTestModule
      *     = Modules.override(getProductionModules()).with(getTestModules());
      * </pre>
-     * <p/>
-     * <p>Prefer to write smaller modules that can be reused and tested without overrides.
+     * <p>
+     * Prefer to write smaller modules that can be reused and tested without overrides.
      *
      * @param modules the modules whose bindings are open to be overridden
      */
diff --git a/core/src/main/java/org/elasticsearch/common/inject/util/Types.java b/core/src/main/java/org/elasticsearch/common/inject/util/Types.java
index ffd8aa6..43bb97c 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/util/Types.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/util/Types.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright (C) 2008 Google Inc.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/core/src/main/java/org/elasticsearch/common/io/Channels.java b/core/src/main/java/org/elasticsearch/common/io/Channels.java
index 79e89fc..2fa7ca1 100644
--- a/core/src/main/java/org/elasticsearch/common/io/Channels.java
+++ b/core/src/main/java/org/elasticsearch/common/io/Channels.java
@@ -167,7 +167,6 @@ public final class Channels {
      * @param sourceIndex index in <i>source</i> to start copying from
      * @param length      how many bytes to copy
      * @param channel     target GatheringByteChannel
-     * @throws IOException
      */
     public static void writeToChannel(ChannelBuffer source, int sourceIndex, int length, GatheringByteChannel channel) throws IOException {
         while (length > 0) {
@@ -184,7 +183,6 @@ public final class Channels {
      *
      * @param source  byte array to copy from
      * @param channel target WritableByteChannel
-     * @throws IOException
      */
     public static void writeToChannel(byte[] source, WritableByteChannel channel) throws IOException {
         writeToChannel(source, 0, source.length, channel);
@@ -198,7 +196,6 @@ public final class Channels {
      * @param offset  start copying from this offset
      * @param length  how many bytes to copy
      * @param channel target WritableByteChannel
-     * @throws IOException
      */
     public static void writeToChannel(byte[] source, int offset, int length, WritableByteChannel channel) throws IOException {
         int toWrite = Math.min(length, WRITE_CHUNK_SIZE);
@@ -219,7 +216,6 @@ public final class Channels {
      *
      * @param byteBuffer source buffer
      * @param channel    channel to write to
-     * @throws IOException
      */
     public static void writeToChannel(ByteBuffer byteBuffer, WritableByteChannel channel) throws IOException {
         if (byteBuffer.isDirect() || (byteBuffer.remaining() <= WRITE_CHUNK_SIZE)) {
diff --git a/core/src/main/java/org/elasticsearch/common/io/FastCharArrayReader.java b/core/src/main/java/org/elasticsearch/common/io/FastCharArrayReader.java
index f202f38..53b932a 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FastCharArrayReader.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FastCharArrayReader.java
@@ -61,8 +61,8 @@ public class FastCharArrayReader extends Reader {
 
     /**
      * Creates a CharArrayReader from the specified array of chars.
-     * <p/>
-     * <p> The resulting reader will start reading at the given
+     * <p>
+     * The resulting reader will start reading at the given
      * <tt>offset</tt>.  The total number of <tt>char</tt> values that can be
      * read from this reader will be either <tt>length</tt> or
      * <tt>buf.length-offset</tt>, whichever is smaller.
@@ -143,8 +143,8 @@ public class FastCharArrayReader extends Reader {
 
     /**
      * Skips characters.  Returns the number of characters that were skipped.
-     * <p/>
-     * <p>The <code>n</code> parameter may be negative, even though the
+     * <p>
+     * The <code>n</code> parameter may be negative, even though the
      * <code>skip</code> method of the {@link Reader} superclass throws
      * an exception in this case. If <code>n</code> is negative, then
      * this method does nothing and returns <code>0</code>.
diff --git a/core/src/main/java/org/elasticsearch/common/io/FastCharArrayWriter.java b/core/src/main/java/org/elasticsearch/common/io/FastCharArrayWriter.java
index 8594b2c..87313eae 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FastCharArrayWriter.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FastCharArrayWriter.java
@@ -124,10 +124,9 @@ public class FastCharArrayWriter extends Writer {
 
     /**
      * Appends the specified character sequence to this writer.
-     * <p/>
-     * <p> An invocation of this method of the form <tt>out.append(csq)</tt>
+     * <p>
+     * An invocation of this method of the form <tt>out.append(csq)</tt>
      * behaves in exactly the same way as the invocation
-     * <p/>
      * <pre>
      *     out.write(csq.toString()) </pre>
      *
@@ -152,11 +151,10 @@ public class FastCharArrayWriter extends Writer {
 
     /**
      * Appends a subsequence of the specified character sequence to this writer.
-     * <p/>
-     * <p> An invocation of this method of the form <tt>out.append(csq, start,
+     * <p>
+     * An invocation of this method of the form <tt>out.append(csq, start,
      * end)</tt> when <tt>csq</tt> is not <tt>null</tt>, behaves in
      * exactly the same way as the invocation
-     * <p/>
      * <pre>
      *     out.write(csq.subSequence(start, end).toString()) </pre>
      *
@@ -182,10 +180,9 @@ public class FastCharArrayWriter extends Writer {
 
     /**
      * Appends the specified character to this writer.
-     * <p/>
-     * <p> An invocation of this method of the form <tt>out.append(c)</tt>
+     * <p>
+     * An invocation of this method of the form <tt>out.append(c)</tt>
      * behaves in exactly the same way as the invocation
-     * <p/>
      * <pre>
      *     out.write(c) </pre>
      *
diff --git a/core/src/main/java/org/elasticsearch/common/io/FastStringReader.java b/core/src/main/java/org/elasticsearch/common/io/FastStringReader.java
index 13bea14..2501914 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FastStringReader.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FastStringReader.java
@@ -24,7 +24,7 @@ import java.io.Reader;
 
 /**
  * A character stream whose source is a string that is <b>not thread safe</b>
- * <p/>
+ * <p>
  * (shay.banon
  * )
  */
@@ -110,15 +110,15 @@ public class FastStringReader extends CharSequenceReader {
     /**
      * Skips the specified number of characters in the stream. Returns
      * the number of characters that were skipped.
-     * <p/>
-     * <p>The <code>ns</code> parameter may be negative, even though the
+     * <p>
+     * The <code>ns</code> parameter may be negative, even though the
      * <code>skip</code> method of the {@link Reader} superclass throws
      * an exception in this case. Negative values of <code>ns</code> cause the
      * stream to skip backwards. Negative return values indicate a skip
      * backwards. It is not possible to skip backwards past the beginning of
      * the string.
-     * <p/>
-     * <p>If the entire string has been read or skipped, then this method has
+     * <p>
+     * If the entire string has been read or skipped, then this method has
      * no effect and always returns 0.
      *
      * @throws IOException If an I/O error occurs
@@ -164,7 +164,7 @@ public class FastStringReader extends CharSequenceReader {
      *                       the stream's input comes from a string, there
      *                       is no actual limit, so this argument must not
      *                       be negative, but is otherwise ignored.
-     * @throws IllegalArgumentException If readAheadLimit is < 0
+     * @throws IllegalArgumentException If readAheadLimit is &lt; 0
      * @throws IOException              If an I/O error occurs
      */
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
index bf7eb5e..cb5ca5f 100644
--- a/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java
@@ -100,7 +100,7 @@ public final class FileSystemUtils {
     }
 
     /**
-     * Appends the path to the given base and strips N elements off the path if strip is > 0.
+     * Appends the path to the given base and strips N elements off the path if strip is &gt; 0.
      */
     public static Path append(Path base, Path path, int strip) {
         for (Path subPath : path) {
diff --git a/core/src/main/java/org/elasticsearch/common/io/Streams.java b/core/src/main/java/org/elasticsearch/common/io/Streams.java
index caa7053..8adf091 100644
--- a/core/src/main/java/org/elasticsearch/common/io/Streams.java
+++ b/core/src/main/java/org/elasticsearch/common/io/Streams.java
@@ -38,8 +38,8 @@ import java.util.Objects;
  * Simple utility methods for file and stream copying.
  * All copy methods use a block size of 4096 bytes,
  * and close all affected streams when done.
- * <p/>
- * <p>Mainly for use within the framework,
+ * <p>
+ * Mainly for use within the framework,
  * but also useful for application code.
  */
 public abstract class Streams {
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
index 2107a99..155a8ca 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java
@@ -28,8 +28,8 @@ import org.elasticsearch.common.util.ByteArray;
 import java.io.IOException;
 
 /**
- * A @link {@link StreamOutput} that uses{@link BigArrays} to acquire pages of
- * bytes, which avoids frequent reallocation & copying of the internal data.
+ * A @link {@link StreamOutput} that uses {@link BigArrays} to acquire pages of
+ * bytes, which avoids frequent reallocation &amp; copying of the internal data.
  */
 public class BytesStreamOutput extends StreamOutput implements BytesStream {
 
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
index 5f3bd01..0dac786 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
@@ -68,4 +68,4 @@ public abstract class FilterStreamInput extends StreamInput {
     public void setVersion(Version version) {
         delegate.setVersion(version);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/ReleasableBytesStreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/ReleasableBytesStreamOutput.java
index cf451c2..4e5380e 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/ReleasableBytesStreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/ReleasableBytesStreamOutput.java
@@ -26,7 +26,7 @@ import org.elasticsearch.common.util.BigArrays;
 /**
  * An bytes stream output that allows providing a {@link BigArrays} instance
  * expecting it to require releasing its content ({@link #bytes()}) once done.
- * <p/>
+ * <p>
  * Please note, its is the responsibility of the caller to make sure the bytes
  * reference do not "escape" and are released only once.
  */
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 92d499a..1b412bb 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.StringAndBytesText;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -46,18 +45,8 @@ import static org.elasticsearch.ElasticsearchException.readStackTrace;
 
 public abstract class StreamInput extends InputStream {
 
-    private final NamedWriteableRegistry namedWriteableRegistry;
-
     private Version version = Version.CURRENT;
 
-    protected StreamInput() {
-        this.namedWriteableRegistry = new NamedWriteableRegistry();
-    }
-
-    protected StreamInput(NamedWriteableRegistry namedWriteableRegistry) {
-        this.namedWriteableRegistry = namedWriteableRegistry;
-    }
-
     public Version getVersion() {
         return this.version;
     }
@@ -350,13 +339,6 @@ public abstract class StreamInput extends InputStream {
         return ret;
     }
 
-    public String[] readOptionalStringArray() throws IOException {
-        if (readBoolean()) {
-            return readStringArray();
-        }
-        return null;
-    }
-
     @Nullable
     @SuppressWarnings("unchecked")
     public Map<String, Object> readMap() throws IOException {
@@ -579,13 +561,6 @@ public abstract class StreamInput extends InputStream {
         throw new UnsupportedOperationException();
     }
 
-    /**
-     * Reads a {@link QueryBuilder} from the current stream
-     */
-    public QueryBuilder readQuery() throws IOException {
-        return readNamedWriteable(QueryBuilder.class);
-    }
-
     public static StreamInput wrap(BytesReference reference) {
         if (reference.hasArray() == false) {
             reference = reference.toBytesArray();
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 1f1562f..9621d04 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -31,7 +31,6 @@ import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -316,18 +315,6 @@ public abstract class StreamOutput extends OutputStream {
         }
     }
 
-    /**
-     * Writes a string array, for nullable string, writes false.
-     */
-    public void writeOptionalStringArray(@Nullable String[] array) throws IOException {
-        if (array == null) {
-            writeBoolean(false);
-        } else {
-            writeBoolean(true);
-            writeStringArray(array);
-        }
-    }
-
     public void writeMap(@Nullable Map<String, Object> map) throws IOException {
         writeGenericValue(map);
     }
@@ -581,11 +568,4 @@ public abstract class StreamOutput extends OutputStream {
         writeString(namedWriteable.getWriteableName());
         namedWriteable.writeTo(this);
     }
-
-    /**
-     * Writes a {@link QueryBuilder} to the current stream
-     */
-    public void writeQuery(QueryBuilder queryBuilder) throws IOException {
-        writeNamedWriteable(queryBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/ConsoleAppender.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/ConsoleAppender.java
index 940749b..30c3aa9 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/ConsoleAppender.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/ConsoleAppender.java
@@ -32,7 +32,6 @@ import java.io.OutputStream;
  * ConsoleAppender appends log events to <code>System.out</code> or
  * <code>System.err</code> using a layout specified by the user. The
  * default target is <code>System.out</code>.
- * <p/>
  * <p>Elasticsearch: Adapter from log4j to allow to disable console logging...</p>
  *
  * @author Ceki G&uuml;lc&uuml;
@@ -99,7 +98,7 @@ public class ConsoleAppender extends WriterAppender {
     /**
      * Returns the current value of the <b>Target</b> property. The
      * default value of the option is "System.out".
-     * <p/>
+     * <p>
      * See also {@link #setTarget}.
      */
     public String getTarget() {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
index db2bb12..060482e 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
@@ -265,7 +265,7 @@ public class Lucene {
     }
 
     /**
-     * Performs an exists (count > 0) query on the <code>searcher</code> for <code>query</code>
+     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
      * with <code>filter</code> using the given <code>collector</code>
      *
      * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
@@ -279,7 +279,7 @@ public class Lucene {
 
 
     /**
-     * Performs an exists (count > 0) query on the <code>searcher</code> for <code>query</code>
+     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
      * using the given <code>collector</code>
      *
      * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
@@ -318,7 +318,7 @@ public class Lucene {
     }
 
     /**
-     * Performs an exists (count > 0) query on the searcher from the <code>searchContext</code> for <code>query</code>
+     * Performs an exists (count &gt; 0) query on the searcher from the <code>searchContext</code> for <code>query</code>
      * using the given <code>collector</code>
      *
      * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/XMoreLikeThis.java b/core/src/main/java/org/elasticsearch/common/lucene/search/XMoreLikeThis.java
index 4275647..85e1899 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/XMoreLikeThis.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/XMoreLikeThis.java
@@ -17,7 +17,7 @@
  * under the License.
  */
 
-/**
+/*
  * Copyright 2004-2005 The Apache Software Foundation.
  *
  * Licensed under the Apache License, Version 2.0 (the "License");
@@ -56,12 +56,11 @@ import java.util.*;
 /**
  * Generate "more like this" similarity queries.
  * Based on this mail:
- * <code><pre>
+ * <pre>
  * Lucene does let you access the document frequency of terms, with IndexReader.docFreq().
  * Term frequencies can be computed by re-tokenizing the text, which, for a single document,
  * is usually fast enough.  But looking up the docFreq() of every term in the document is
  * probably too slow.
- * <p/>
  * You can use some heuristics to prune the set of terms, to avoid calling docFreq() too much,
  * or at all.  Since you're trying to maximize a tf*idf score, you're probably most interested
  * in terms with a high tf. Choosing a tf threshold even as low as two or three will radically
@@ -70,44 +69,36 @@ import java.util.*;
  * number of characters, not selecting anything less than, e.g., six or seven characters.
  * With these sorts of heuristics you can usually find small set of, e.g., ten or fewer terms
  * that do a pretty good job of characterizing a document.
- * <p/>
  * It all depends on what you're trying to do.  If you're trying to eek out that last percent
  * of precision and recall regardless of computational difficulty so that you can win a TREC
  * competition, then the techniques I mention above are useless.  But if you're trying to
  * provide a "more like this" button on a search results page that does a decent job and has
  * good performance, such techniques might be useful.
- * <p/>
  * An efficient, effective "more-like-this" query generator would be a great contribution, if
  * anyone's interested.  I'd imagine that it would take a Reader or a String (the document's
  * text), analyzer Analyzer, and return a set of representative terms using heuristics like those
  * above.  The frequency and length thresholds could be parameters, etc.
- * <p/>
  * Doug
- * </pre></code>
- * <p/>
- * <p/>
- * <p/>
+ * </pre>
  * <h3>Initial Usage</h3>
- * <p/>
+ * <p>
  * This class has lots of options to try to make it efficient and flexible.
  * The simplest possible usage is as follows. The bold
  * fragment is specific to this class.
- * <p/>
  * <pre class="prettyprint">
- * <p/>
  * IndexReader ir = ...
  * IndexSearcher is = ...
- * <p/>
+
  * MoreLikeThis mlt = new MoreLikeThis(ir);
  * Reader target = ... // orig source of doc you want to find similarities to
  * Query query = mlt.like( target);
- * <p/>
+
  * Hits hits = is.search(query);
  * // now the usual iteration thru 'hits' - the only thing to watch for is to make sure
  * //you ignore the doc if it matches your 'target' document, as it should be similar to itself
- * <p/>
+
  * </pre>
- * <p/>
+ * <p>
  * Thus you:
  * <ol>
  * <li> do your normal, Lucene setup for searching,
@@ -116,13 +107,11 @@ import java.util.*;
  * <li> then call one of the like() calls to generate a similarity query
  * <li> call the searcher to find the similar docs
  * </ol>
- * <p/>
  * <h3>More Advanced Usage</h3>
- * <p/>
+ * <p>
  * You may want to use {@link #setFieldNames setFieldNames(...)} so you can examine
  * multiple fields (e.g. body and title) for similarity.
- * <p/>
- * <p/>
+ * <p>
  * Depending on the size of your index and the size and makeup of your documents you
  * may want to call the other set methods to control how the similarity queries are
  * generated:
@@ -137,7 +126,6 @@ import java.util.*;
  * <li> {@link #setMaxNumTokensParsed setMaxNumTokensParsed(...)}
  * <li> {@link #setStopWords setStopWord(...)}
  * </ul>
- * <p/>
  * <hr>
  * <pre>
  * Changes: Mark Harwood 29/02/04
@@ -702,7 +690,7 @@ public final class XMoreLikeThis {
     }
 
     /**
-     * Create a PriorityQueue from a word->tf map.
+     * Create a PriorityQueue from a word-&gt;tf map.
      *
      * @param words a map of words keyed on the word(String) with Int objects as the values.
      */
@@ -711,7 +699,7 @@ public final class XMoreLikeThis {
     }
 
     /**
-     * Create a PriorityQueue from a word->tf map.
+     * Create a PriorityQueue from a word-&gt;tf map.
      *
      * @param words a map of words keyed on the word(String) with Int objects as the values.
      * @param fieldNames an array of field names to override defaults.
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java b/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
index a95f24e..bc1962a 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java
@@ -26,7 +26,7 @@ import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 
 /**
- * Pseudo randomly generate a score for each {@link #score}.
+ * Pseudo randomly generate a score for each {@link LeafScoreFunction#score}.
  */
 public class RandomScoreFunction extends ScoreFunction {
 
diff --git a/core/src/main/java/org/elasticsearch/common/metrics/EWMA.java b/core/src/main/java/org/elasticsearch/common/metrics/EWMA.java
index a4d06af..b0cfd1b 100644
--- a/core/src/main/java/org/elasticsearch/common/metrics/EWMA.java
+++ b/core/src/main/java/org/elasticsearch/common/metrics/EWMA.java
@@ -28,7 +28,7 @@ import java.util.concurrent.TimeUnit;
  *
  * @see <a href="http://www.teamquest.com/pdfs/whitepaper/ldavg1.pdf">UNIX Load Average Part 1: How It Works</a>
  * @see <a href="http://www.teamquest.com/pdfs/whitepaper/ldavg2.pdf">UNIX Load Average Part 2: Not Your Average Average</a>
- *      <p/>
+ *      <p>
  *      Taken from codahale metric module, changed to use LongAdder
  */
 public class EWMA {
diff --git a/core/src/main/java/org/elasticsearch/common/metrics/MeterMetric.java b/core/src/main/java/org/elasticsearch/common/metrics/MeterMetric.java
index 531e0e9..4bd995c 100644
--- a/core/src/main/java/org/elasticsearch/common/metrics/MeterMetric.java
+++ b/core/src/main/java/org/elasticsearch/common/metrics/MeterMetric.java
@@ -31,7 +31,7 @@ import java.util.concurrent.TimeUnit;
  * fifteen-minute exponentially-weighted moving average throughputs.
  *
  * @see <a href="http://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average">EMA</a>
- *      <p/>
+ *      <p>
  *      taken from codahale metric module, replaced with LongAdder
  */
 public class MeterMetric implements Metric {
diff --git a/core/src/main/java/org/elasticsearch/common/netty/NettyUtils.java b/core/src/main/java/org/elasticsearch/common/netty/NettyUtils.java
index b33ed2b..164ed57 100644
--- a/core/src/main/java/org/elasticsearch/common/netty/NettyUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/netty/NettyUtils.java
@@ -32,43 +32,43 @@ public class NettyUtils {
 
     /**
      * Here we go....
-     * <p/>
+     * <p>
      * When using the socket or file channel API to write or read using heap ByteBuffer, the sun.nio
      * package will convert it to a direct buffer before doing the actual operation. The direct buffer is
      * cached on an array of buffers under the nio.ch.Util$BufferCache on a thread local.
-     * <p/>
+     * <p>
      * In netty specifically, if we send a single ChannelBuffer that is bigger than
      * SocketSendBufferPool#DEFAULT_PREALLOCATION_SIZE (64kb), it will just convert the ChannelBuffer
      * to a ByteBuffer and send it. The problem is, that then same size DirectByteBuffer will be
      * allocated (or reused) and kept around on a thread local in the sun.nio BufferCache. If very
      * large buffer is sent, imagine a 10mb one, then a 10mb direct buffer will be allocated as an
      * entry within the thread local buffers.
-     * <p/>
+     * <p>
      * In ES, we try and page the buffers allocated, all serialized data uses {@link org.elasticsearch.common.bytes.PagedBytesReference}
      * typically generated from {@link org.elasticsearch.common.io.stream.BytesStreamOutput}. When sending it over
      * to netty, it creates a {@link org.jboss.netty.buffer.CompositeChannelBuffer} that wraps the relevant pages.
-     * <p/>
+     * <p>
      * The idea with the usage of composite channel buffer is that a single large buffer will not be sent over
      * to the sun.nio layer. But, this will only happen if the composite channel buffer is created with a gathering
      * flag set to true. In such a case, the GatheringSendBuffer is used in netty, resulting in calling the sun.nio
      * layer with a ByteBuffer array.
-     * <p/>
+     * <p>
      * This, potentially would have been as disastrous if the sun.nio layer would have tried to still copy over
      * all of it to a direct buffer. But, the write(ByteBuffer[]) API (see sun.nio.ch.IOUtil), goes one buffer
      * at a time, and gets a temporary direct buffer from the BufferCache, up to a limit of IOUtil#IOV_MAX (which
      * is 1024 on most OSes). This means that there will be a max of 1024 direct buffer per thread.
-     * <p/>
+     * <p>
      * This is still less than optimal to be honest, since it means that if not all data was written successfully
      * (1024 paged buffers), then the rest of the data will need to be copied over again to the direct buffer
      * and re-transmitted, but its much better than trying to send the full large buffer over and over again.
-     * <p/>
+     * <p>
      * In ES, we use by default, in our paged data structures, a page of 16kb, so this is not so terrible.
-     * <p/>
+     * <p>
      * Note, on the read size of netty, it uses a single direct buffer that is defined in both the transport
      * and http configuration (based on the direct memory available), and the upstream handlers (SizeHeaderFrameDecoder,
      * or more specifically the FrameDecoder base class) makes sure to use a cumulation buffer and not copy it
      * over all the time.
-     * <p/>
+     * <p>
      * TODO: potentially, a more complete solution would be to write a netty channel handler that is the last
      * in the pipeline, and if the buffer is composite, verifies that its a gathering one with reasonable
      * sized pages, and if its a single one, makes sure that it gets sliced and wrapped in a composite
diff --git a/core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java b/core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java
index 11f7d8b..a210f3ae6 100644
--- a/core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java
+++ b/core/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java
@@ -30,8 +30,8 @@ import java.util.Set;
  * Utility class for working with Strings that have placeholder values in them. A placeholder takes the form
  * <tt>${name}</tt>. Using <tt>PropertyPlaceholder</tt> these placeholders can be substituted for
  * user-supplied values.
- * <p/>
- * <p> Values for substitution can be supplied using a {@link Properties} instance or using a
+ * <p>
+ * Values for substitution can be supplied using a {@link Properties} instance or using a
  * {@link PlaceholderResolver}.
  */
 public class PropertyPlaceholder {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 2c2440c..e368430 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -511,13 +511,12 @@ public final class Settings implements ToXContent {
     /**
      * The values associated with a setting prefix as an array. The settings array is in the format of:
      * <tt>settingPrefix.[index]</tt>.
-     * <p/>
-     * <p>It will also automatically load a comma separated list under the settingPrefix and merge with
+     * <p>
+     * It will also automatically load a comma separated list under the settingPrefix and merge with
      * the numbered format.
      *
      * @param settingPrefix The setting prefix to load the array by
      * @return The setting array values
-     * @throws org.elasticsearch.common.settings.SettingsException
      */
     public String[] getAsArray(String settingPrefix) throws SettingsException {
         return getAsArray(settingPrefix, Strings.EMPTY_ARRAY, true);
@@ -526,13 +525,12 @@ public final class Settings implements ToXContent {
     /**
      * The values associated with a setting prefix as an array. The settings array is in the format of:
      * <tt>settingPrefix.[index]</tt>.
-     * <p/>
-     * <p>If commaDelimited is true, it will automatically load a comma separated list under the settingPrefix and merge with
+     * <p>
+     * If commaDelimited is true, it will automatically load a comma separated list under the settingPrefix and merge with
      * the numbered format.
      *
      * @param settingPrefix The setting prefix to load the array by
      * @return The setting array values
-     * @throws org.elasticsearch.common.settings.SettingsException
      */
     public String[] getAsArray(String settingPrefix, String[] defaultArray) throws SettingsException {
         return getAsArray(settingPrefix, defaultArray, true);
@@ -541,15 +539,14 @@ public final class Settings implements ToXContent {
     /**
      * The values associated with a setting prefix as an array. The settings array is in the format of:
      * <tt>settingPrefix.[index]</tt>.
-     * <p/>
-     * <p>It will also automatically load a comma separated list under the settingPrefix and merge with
+     * <p>
+     * It will also automatically load a comma separated list under the settingPrefix and merge with
      * the numbered format.
      *
      * @param settingPrefix  The setting prefix to load the array by
      * @param defaultArray   The default array to use if no value is specified
      * @param commaDelimited Whether to try to parse a string as a comma-delimited value
      * @return The setting array values
-     * @throws org.elasticsearch.common.settings.SettingsException
      */
     public String[] getAsArray(String settingPrefix, String[] defaultArray, Boolean commaDelimited) throws SettingsException {
         List<String> result = new ArrayList<>();
@@ -1122,8 +1119,8 @@ public final class Settings implements ToXContent {
         /**
          * Runs across all the settings set on this builder and replaces <tt>${...}</tt> elements in the
          * each setting value according to the following logic:
-         * <p/>
-         * <p>First, tries to resolve it against a System property ({@link System#getProperty(String)}), next,
+         * <p>
+         * First, tries to resolve it against a System property ({@link System#getProperty(String)}), next,
          * tries and resolve it against an environment variable ({@link System#getenv(String)}), and last, tries
          * and replace it with another setting already set on this builder.
          */
diff --git a/core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java b/core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java
index d2bde25..421e008 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/SettingsFilter.java
@@ -50,8 +50,6 @@ public class SettingsFilter extends AbstractComponent {
 
     /**
      * Adds a new simple pattern to the list of filters
-     *
-     * @param pattern
      */
     public void addFilter(String pattern) {
         patterns.add(pattern);
@@ -59,8 +57,6 @@ public class SettingsFilter extends AbstractComponent {
 
     /**
      * Removes a simple pattern from the list of filters
-     *
-     * @param pattern
      */
     public void removeFilter(String pattern) {
         patterns.remove(pattern);
diff --git a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
index 9bfbcaa..d8e121b 100644
--- a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
+++ b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
@@ -33,8 +33,8 @@ import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 /**
  * A global registry of all different types of {@link org.elasticsearch.common.transport.TransportAddress} allowing
  * to perform serialization of them.
- * <p/>
- * <p>By default, adds {@link org.elasticsearch.common.transport.InetSocketTransportAddress}.
+ * <p>
+ * By default, adds {@link org.elasticsearch.common.transport.InetSocketTransportAddress}.
  *
  *
  */
diff --git a/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java b/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
index cb89ca8..9abce69 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
@@ -214,7 +214,6 @@ public enum DistanceUnit {
      * 
      * @param out {@link StreamOutput} to write to
      * @param unit {@link DistanceUnit} to write 
-     * @throws IOException
      */
     public static void writeDistanceUnit(StreamOutput out, DistanceUnit unit) throws IOException {
         out.writeByte((byte) unit.ordinal());
diff --git a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
index 18641b8..3b4a073 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
@@ -19,24 +19,19 @@
 package org.elasticsearch.common.unit;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
-import java.util.Locale;
-import java.util.Objects;
 
 /**
  * A unit class that encapsulates all in-exact search
  * parsing and conversion from similarities to edit distances
  * etc.
  */
-public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
+public final class Fuzziness implements ToXContent {
 
     public static final XContentBuilderString X_FIELD_NAME = new XContentBuilderString("fuzziness");
     public static final Fuzziness ZERO = new Fuzziness(0);
@@ -47,10 +42,6 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
 
     private final String fuzziness;
 
-    /** the prototype constant is intended for deserialization when used with
-     * {@link org.elasticsearch.common.io.stream.StreamableReader#readFrom(StreamInput)} */
-    static final Fuzziness PROTOTYPE = AUTO;
-
     private Fuzziness(int fuzziness) {
         if (fuzziness != 0 && fuzziness != 1 && fuzziness != 2) {
             throw new IllegalArgumentException("Valid edit distances are [0, 1, 2] but was [" + fuzziness + "]");
@@ -59,10 +50,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     private Fuzziness(String fuzziness) {
-        if (fuzziness == null) {
-            throw new IllegalArgumentException("fuzziness can't be null!");
-        }
-        this.fuzziness = fuzziness.toUpperCase(Locale.ROOT);
+        this.fuzziness = fuzziness;
     }
 
     /**
@@ -132,7 +120,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asDistance(String text) {
-        if (this.equals(AUTO)) { //AUTO
+        if (this == AUTO) { //AUTO
             final int len = termLen(text);
             if (len <= 2) {
                 return 0;
@@ -146,7 +134,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public TimeValue asTimeValue() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return TimeValue.timeValueMillis(1);
         } else {
             return TimeValue.parseTimeValue(fuzziness.toString(), null, "fuzziness");
@@ -154,7 +142,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public long asLong() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -165,7 +153,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asInt() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -176,7 +164,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public short asShort() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -187,7 +175,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public byte asByte() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -198,14 +186,14 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public double asDouble() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1d;
         }
         return Double.parseDouble(fuzziness.toString());
     }
 
     public float asFloat() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1f;
         }
         return Float.parseFloat(fuzziness.toString());
@@ -218,35 +206,4 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     public String asString() {
         return fuzziness.toString();
     }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        Fuzziness other = (Fuzziness) obj;
-        return Objects.equals(fuzziness, other.fuzziness);
-    }
-
-    @Override
-    public int hashCode() {
-        return fuzziness.hashCode();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(fuzziness);
-    }
-
-    @Override
-    public Fuzziness readFrom(StreamInput in) throws IOException {
-        return new Fuzziness(in.readString());
-    }
-
-    public static Fuzziness readFuzzinessFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/BigArrays.java b/core/src/main/java/org/elasticsearch/common/util/BigArrays.java
index 0f0bced..faa377b 100644
--- a/core/src/main/java/org/elasticsearch/common/util/BigArrays.java
+++ b/core/src/main/java/org/elasticsearch/common/util/BigArrays.java
@@ -494,7 +494,7 @@ public class BigArrays {
         return resize(array, newSize);
     }
 
-    /** @see Arrays.hashCode(byte[]) */
+    /** @see Arrays#hashCode(byte[]) */
     public int hashCode(ByteArray array) {
         if (array == null) {
             return 0;
@@ -508,7 +508,7 @@ public class BigArrays {
         return hash;
     }
 
-    /** @see Arrays.equals(byte[], byte[]) */
+    /** @see Arrays#equals(byte[], byte[]) */
     public boolean equals(ByteArray array, ByteArray other) {
         if (array == other) {
             return true;
diff --git a/core/src/main/java/org/elasticsearch/common/util/BloomFilter.java b/core/src/main/java/org/elasticsearch/common/util/BloomFilter.java
index 360bd39..b19d727 100644
--- a/core/src/main/java/org/elasticsearch/common/util/BloomFilter.java
+++ b/core/src/main/java/org/elasticsearch/common/util/BloomFilter.java
@@ -285,7 +285,7 @@ public class BloomFilter {
     /**
      * Computes the optimal k (number of hashes per element inserted in Bloom filter), given the
      * expected insertions and total number of bits in the Bloom filter.
-     * <p/>
+     * <p>
      * See http://en.wikipedia.org/wiki/File:Bloom_filter_fp_probability.svg for the formula.
      *
      * @param n expected insertions (must be positive)
@@ -298,11 +298,11 @@ public class BloomFilter {
     /**
      * Computes m (total bits of Bloom filter) which is expected to achieve, for the specified
      * expected insertions, the required false positive probability.
-     * <p/>
+     * <p>
      * See http://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives for the formula.
      *
      * @param n expected insertions (must be positive)
-     * @param p false positive rate (must be 0 < p < 1)
+     * @param p false positive rate (must be 0 &lt; p &lt; 1)
      */
     static long optimalNumOfBits(long n, double p) {
         if (p == 0) {
diff --git a/core/src/main/java/org/elasticsearch/common/util/BytesRefHash.java b/core/src/main/java/org/elasticsearch/common/util/BytesRefHash.java
index 3e5dac5..5a2e21e 100644
--- a/core/src/main/java/org/elasticsearch/common/util/BytesRefHash.java
+++ b/core/src/main/java/org/elasticsearch/common/util/BytesRefHash.java
@@ -61,8 +61,8 @@ public final class BytesRefHash extends AbstractHash {
     }
 
     /**
-     * Return the key at <code>0 &lte; index &lte; capacity()</code>. The result is undefined if the slot is unused.
-     * <p color="red">Beware that the content of the {@link BytesRef} may become invalid as soon as {@link #close()} is called</p>
+     * Return the key at <code>0 &lt;= index &lt;= capacity()</code>. The result is undefined if the slot is unused.
+     * <p>Beware that the content of the {@link BytesRef} may become invalid as soon as {@link #close()} is called</p>
      */
     public BytesRef get(long id, BytesRef dest) {
         final long startOffset = startOffsets.get(id);
@@ -135,7 +135,7 @@ public final class BytesRefHash extends AbstractHash {
     }
 
     /**
-     * Try to add <code>key</code>. Return its newly allocated id if it wasn't in the hash table yet, or </code>-1-id</code>
+     * Try to add <code>key</code>. Return its newly allocated id if it wasn't in the hash table yet, or <code>-1-id</code>
      * if it was already present in the hash table.
      */
     public long add(BytesRef key, int code) {
diff --git a/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java b/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
index 67b844b..781218c 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
@@ -53,7 +53,7 @@ public class CancellableThreads {
      * the default implementation always throws an {@link ExecutionCancelledException}, suppressing
      * any other exception that occurred before cancellation
      *
-     * @param reason              reason for failure supplied by the caller of {@link @cancel}
+     * @param reason              reason for failure supplied by the caller of {@link #cancel}
      * @param suppressedException any error that was encountered during the execution before the operation was cancelled.
      */
     protected void onCancel(String reason, @Nullable Throwable suppressedException) {
diff --git a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
index a5878e1..056142a 100644
--- a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
@@ -218,7 +218,7 @@ public abstract class ExtensionPoint {
         }
 
         /**
-         * Registers a mapping from {@param key} to {@param value}
+         * Registers a mapping from {@code key} to {@code value}
          *
          * @throws IllegalArgumentException iff the key is already registered
          */
diff --git a/core/src/main/java/org/elasticsearch/common/util/LongHash.java b/core/src/main/java/org/elasticsearch/common/util/LongHash.java
index ef82566..9f19e6c 100644
--- a/core/src/main/java/org/elasticsearch/common/util/LongHash.java
+++ b/core/src/main/java/org/elasticsearch/common/util/LongHash.java
@@ -45,7 +45,7 @@ public final class LongHash extends AbstractHash {
     }
 
     /**
-     * Return the key at <code>0 &lte; index &lte; capacity()</code>. The result is undefined if the slot is unused.
+     * Return the key at <code>0 &lt;= index &lt;= capacity()</code>. The result is undefined if the slot is unused.
      */
     public long get(long id) {
         return keys.get(id);
@@ -98,7 +98,7 @@ public final class LongHash extends AbstractHash {
     }
 
     /**
-     * Try to add <code>key</code>. Return its newly allocated id if it wasn't in the hash table yet, or </code>-1-id</code>
+     * Try to add <code>key</code>. Return its newly allocated id if it wasn't in the hash table yet, or <code>-1-id</code>
      * if it was already present in the hash table.
      */
     public long add(long key) {
diff --git a/core/src/main/java/org/elasticsearch/common/util/URIPattern.java b/core/src/main/java/org/elasticsearch/common/util/URIPattern.java
index 0b9bb22..13337ff 100644
--- a/core/src/main/java/org/elasticsearch/common/util/URIPattern.java
+++ b/core/src/main/java/org/elasticsearch/common/util/URIPattern.java
@@ -37,7 +37,6 @@ public class URIPattern {
 
     /**
      * Constructs uri pattern
-     * @param pattern
      */
     public URIPattern(String pattern) {
         try {
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
index ae80671..cd5789f 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
@@ -33,22 +33,22 @@ import java.util.concurrent.locks.AbstractQueuedSynchronizer;
  * {@code Runnable}. (If you want a {@code Runnable} implementation of {@code
  * ListenableFuture}, create a {@link com.google.common.util.concurrent.ListenableFutureTask}, or submit your
  * tasks to a {@link com.google.common.util.concurrent.ListeningExecutorService}.)
- * <p/>
- * <p>This class implements all methods in {@code ListenableFuture}.
+ * <p>
+ * This class implements all methods in {@code ListenableFuture}.
  * Subclasses should provide a way to set the result of the computation through
  * the protected methods {@link #set(Object)} and
  * {@link #setException(Throwable)}. Subclasses may also override {@link
  * #interruptTask()}, which will be invoked automatically if a call to {@link
  * #cancel(boolean) cancel(true)} succeeds in canceling the future.
- * <p/>
- * <p>{@code AbstractFuture} uses an {@link AbstractQueuedSynchronizer} to deal
+ * <p>
+ * {@code AbstractFuture} uses an {@link AbstractQueuedSynchronizer} to deal
  * with concurrency issues and guarantee thread safety.
- * <p/>
- * <p>The state changing methods all return a boolean indicating success or
+ * <p>
+ * The state changing methods all return a boolean indicating success or
  * failure in changing the future's state.  Valid states are running,
  * completed, failed, or cancelled.
- * <p/>
- * <p>This class uses an {@link com.google.common.util.concurrent.ExecutionList} to guarantee that all registered
+ * <p>
+ * This class uses an {@link com.google.common.util.concurrent.ExecutionList} to guarantee that all registered
  * listeners will be executed, either when the future finishes or, for listeners
  * that are added after the future completes, immediately.
  * {@code Runnable}-{@code Executor} pairs are stored in the execution list but
@@ -77,8 +77,8 @@ public abstract class BaseFuture<V> implements Future<V> {
 
     /**
      * {@inheritDoc}
-     * <p/>
-     * <p>The default {@link BaseFuture} implementation throws {@code
+     * <p>
+     * The default {@link BaseFuture} implementation throws {@code
      * InterruptedException} if the current thread is interrupted before or during
      * the call, even if the value is already available.
      *
@@ -100,8 +100,8 @@ public abstract class BaseFuture<V> implements Future<V> {
 
     /**
      * {@inheritDoc}
-     * <p/>
-     * <p>The default {@link BaseFuture} implementation throws {@code
+     * <p>
+     * The default {@link BaseFuture} implementation throws {@code
      * InterruptedException} if the current thread is interrupted before or during
      * the call, even if the value is already available.
      *
@@ -141,8 +141,8 @@ public abstract class BaseFuture<V> implements Future<V> {
      * Subclasses can override this method to implement interruption of the
      * future's computation. The method is invoked automatically by a successful
      * call to {@link #cancel(boolean) cancel(true)}.
-     * <p/>
-     * <p>The default implementation does nothing.
+     * <p>
+     * The default implementation does nothing.
      *
      * @since 10.0
      */
@@ -203,13 +203,13 @@ public abstract class BaseFuture<V> implements Future<V> {
      * in a thread-safe manner.  The current state of the future is held in the
      * Sync state, and the lock is released whenever the state changes to either
      * {@link #COMPLETED} or {@link #CANCELLED}.
-     * <p/>
-     * <p>To avoid races between threads doing release and acquire, we transition
+     * <p>
+     * To avoid races between threads doing release and acquire, we transition
      * to the final state in two steps.  One thread will successfully CAS from
      * RUNNING to COMPLETING, that thread will then set the result of the
      * computation, and only then transition to COMPLETED or CANCELLED.
-     * <p/>
-     * <p>We don't use the integer argument passed between acquire methods so we
+     * <p>
+     * We don't use the integer argument passed between acquire methods so we
      * pass around a -1 everywhere.
      */
     static final class Sync<V> extends AbstractQueuedSynchronizer {
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java
index aad6cee..d0d2906 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedEsThreadPoolExecutor.java
@@ -38,7 +38,7 @@ import java.util.concurrent.atomic.AtomicLong;
  * A prioritizing executor which uses a priority queue as a work queue. The jobs that will be submitted will be treated
  * as {@link PrioritizedRunnable} and/or {@link PrioritizedCallable}, those tasks that are not instances of these two will
  * be wrapped and assign a default {@link Priority#NORMAL} priority.
- * <p/>
+ * <p>
  * Note, if two tasks have the same priority, the first to arrive will be executed first (FIFO style).
  */
 public class PrioritizedEsThreadPoolExecutor extends EsThreadPoolExecutor {
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java
index 1892aa7..a3d10ff 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/ThreadBarrier.java
@@ -32,12 +32,12 @@ import java.util.concurrent.TimeoutException;
  * <code>ThreadBarrier</code> adds a <i>cause</i> to
  * {@link BrokenBarrierException} thrown by a {@link #reset()} operation defined
  * by {@link CyclicBarrier}.
- * <p/>
- * <p/>
+ * <p>
  * <b>Sample usage:</b> <br>
- * <li>Barrier as a synchronization and Exception handling aid </li>
- * <li>Barrier as a trigger for elapsed notification events </li>
- * <p/>
+ * <ul>
+ *  <li>Barrier as a synchronization and Exception handling aid </li>
+ *  <li>Barrier as a trigger for elapsed notification events </li>
+ * </ul>
  * <pre>
  *    class MyTestClass implements RemoteEventListener
  *    {
@@ -86,7 +86,7 @@ import java.util.concurrent.TimeoutException;
  *
  *               // too many notifications?
  *               Assert.assertFalse(&quot;Exceeded notification count&quot;,
- *                                          actualNotificationCount > EXPECTED_COUNT);
+ *                                          actualNotificationCount &gt; EXPECTED_COUNT);
  *            }
  *          catch(Throwable t) {
  *              log(&quot;Worker thread caught exception&quot;, t);
@@ -211,7 +211,7 @@ public class ThreadBarrier extends CyclicBarrier {
 
     /**
      * breaks this barrier if it has been reset or broken for any other reason.
-     * <p/>
+     * <p>
      * Note: This call is not atomic in respect to await/reset calls. A
      * breakIfBroken() may be context switched to invoke a reset() prior to
      * await(). This resets the barrier to its initial state - parties not
@@ -244,7 +244,7 @@ public class ThreadBarrier extends CyclicBarrier {
      * Measurement.
      *
      * @see ThreadBarrier#ThreadBarrier(int, Runnable)
-     *      <p/>
+     *      <p>
      *      <B>Usage example:</B><br>
      *      <pre><code>
      *                                                                                             BarrierTimer timer = new BarrierTimer();
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
new file mode 100644
index 0000000..059a706
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
@@ -0,0 +1,326 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.xcontent;
+
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.index.Index;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.function.BiConsumer;
+import java.util.function.BiFunction;
+import java.util.function.Supplier;
+
+/**
+ * A declarative Object parser to parse any kind of XContent structures into existing object with setters.
+ * The Parser is designed to be declarative and stateless. A single parser is defined for one object level, nested
+ * elements can be added via {@link #declareObject(BiConsumer, BiFunction, ParseField)} which is commonly done by
+ * declaring yet another instance of {@link ObjectParser}. Instances of {@link ObjectParser} are thread-safe and can be
+ * re-used across parsing operations. It's recommended to use the high level declare methods like {@link #declareString(BiConsumer, ParseField)}
+ * instead of {@link #declareField} which can be used to implement exceptional parsing operations not covered by the high level methods.
+ */
+public final class ObjectParser<Value, Context> implements BiFunction<XContentParser, Context, Value> {
+
+    private final String name;
+    private final Supplier<Value> valueSupplier;
+
+    /**
+     * Creates a new ObjectParser instance with a name. This name is used to reference the parser in exceptions and messages.
+     */
+    public ObjectParser(String name) {
+        this(name, null);
+    }
+
+    /**
+     * Creates a new ObjectParser instance which a name.
+     * @param name the parsers name, used to reference the parser in exceptions and messages.
+     * @param valueSupplier a supplier that creates a new Value instance used when the parser is used as an inner object parser.
+     */
+    public ObjectParser(String name, Supplier<Value> valueSupplier) {
+        this.name = name;
+        this.valueSupplier = valueSupplier;
+    }
+
+    /**
+     * Parses a Value from the given {@link XContentParser}
+     * @param parser the parser to build a value from
+     * @return a new value instance drawn from the provided value supplier on {@link #ObjectParser(String, Supplier)}
+     * @throws IOException if an IOException occurs.
+     */
+    public Value parse(XContentParser parser) throws IOException {
+        if (valueSupplier == null) {
+            throw new NullPointerException("valueSupplier is not set");
+        }
+        return parse(parser, valueSupplier.get(), null);
+    }
+
+    /**
+     * Parses a Value from the given {@link XContentParser}
+     * @param parser the parser to build a value from
+     * @param value the value to fill from the parser
+     * @return the parsed value
+     * @throws IOException if an IOException occurs.
+     */
+    public Value parse(XContentParser parser, Value value) throws IOException {
+        return parse(parser, value, null);
+    }
+
+    /**
+     * Parses a Value from the given {@link XContentParser}
+     * @param parser the parser to build a value from
+     * @param value the value to fill from the parser
+     * @param context an optional context that is passed along to all declared field parsers
+     * @return the parsed value
+     * @throws IOException if an IOException occurs.
+     */
+    public Value parse(XContentParser parser, Value value, Context context) throws IOException {
+        XContentParser.Token token;
+        if (parser.currentToken() == XContentParser.Token.START_OBJECT) {
+            token = parser.currentToken();
+        } else {
+            token = parser.nextToken();
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new IllegalStateException("[" + name  + "] Expected START_OBJECT but was: " + token);
+            }
+        }
+
+        FieldParser<Value> fieldParser = null;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+                fieldParser = getParser(currentFieldName);
+            } else {
+                if (currentFieldName == null) {
+                    throw new IllegalStateException("[" + name  + "] no field found");
+                }
+                assert fieldParser != null;
+                fieldParser.assertSupports(name, token, currentFieldName, parser.getParseFieldMatcher());
+                parseSub(parser, fieldParser, currentFieldName, value, context);
+                fieldParser = null;
+            }
+        }
+        return value;
+    }
+
+    private void parseArray(XContentParser parser, FieldParser<Value> fieldParser, String currentFieldName, Value value, Context context) throws IOException {
+        assert parser.currentToken() == XContentParser.Token.START_ARRAY : "Token was: " + parser.currentToken();
+        parseValue(parser, fieldParser, currentFieldName, value, context);
+    }
+
+    private void parseValue(XContentParser parser, FieldParser<Value> fieldParser, String currentFieldName, Value value, Context context) throws IOException {
+        try {
+            fieldParser.parser.parse(parser, value, context);
+        } catch (Exception ex) {
+            throw new ParsingException(new Index("_na_"), parser, "[" + name  + "] failed to parse field [" + currentFieldName + "]", ex);
+        }
+    }
+
+    private void parseSub(XContentParser parser, FieldParser<Value> fieldParser, String currentFieldName, Value value, Context context) throws IOException {
+        final XContentParser.Token token = parser.currentToken();
+        switch (token) {
+            case START_OBJECT:
+                parseValue(parser, fieldParser, currentFieldName, value, context);
+                break;
+            case START_ARRAY:
+                parseArray(parser, fieldParser, currentFieldName, value, context);
+                break;
+            case END_OBJECT:
+            case END_ARRAY:
+            case FIELD_NAME:
+                throw new IllegalStateException("[" + name  + "]" + token + " is unexpected");
+            case VALUE_STRING:
+            case VALUE_NUMBER:
+            case VALUE_BOOLEAN:
+            case VALUE_EMBEDDED_OBJECT:
+            case VALUE_NULL:
+                parseValue(parser, fieldParser, currentFieldName, value, context);
+        }
+    }
+
+    protected FieldParser getParser(String fieldName) {
+        FieldParser<Value> parser = fieldParserMap.get(fieldName);
+        if (parser == null) {
+            throw new IllegalArgumentException("[" + name  + "] unknown field [" + fieldName + "], parser not found");
+        }
+        return parser;
+    }
+
+    @Override
+    public Value apply(XContentParser parser, Context context) {
+        if (valueSupplier == null) {
+            throw new NullPointerException("valueSupplier is not set");
+        }
+        try {
+            return parse(parser, valueSupplier.get(), context);
+        } catch (IOException e) {
+            throw new ParsingException(new Index("_na_"), parser, "[" + name  + "] failed to parse object", e);
+        }
+    }
+
+    public interface Parser<Value, Context> {
+        void parse(XContentParser parser, Value value, Context context) throws IOException;
+    }
+
+    private interface IOSupplier<T> {
+        T get() throws IOException;
+    }
+
+    private final Map<String, FieldParser> fieldParserMap = new HashMap<>();
+
+    public void declareField(Parser<Value, Context> p, ParseField parseField, ValueType type) {
+        FieldParser fieldParser = new FieldParser(p, type.supportedTokens(), parseField, type);
+        for (String fieldValue : parseField.getAllNamesIncludedDeprecated()) {
+            fieldParserMap.putIfAbsent(fieldValue, fieldParser);
+        }
+    }
+
+    public void declareStringArray(BiConsumer<Value, List<String>> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, p::text)), field, ValueType.STRING_ARRAY);
+    }
+
+    public void declareDoubleArray(BiConsumer<Value, List<Double>> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, p::doubleValue)), field, ValueType.DOUBLE_ARRAY);
+    }
+
+    public void declareFloatArray(BiConsumer<Value, List<Float>> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, p::floatValue)), field, ValueType.FLOAT_ARRAY);
+    }
+
+    public void declareLongArray(BiConsumer<Value, List<Long>> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, p::longValue)), field, ValueType.LONG_ARRAY);
+    }
+
+    public void declareIntArray(BiConsumer<Value, List<Integer>> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, p::intValue)), field, ValueType.INT_ARRAY);
+    }
+
+    private final <T> List<T> parseArray(XContentParser parser, IOSupplier<T> supplier) throws IOException {
+        List<T> list = new ArrayList<>();
+        while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
+            list.add(supplier.get());
+        }
+        return list;
+    }
+
+    public <T> void declareObject(BiConsumer<Value, T> consumer, BiFunction<XContentParser, Context, T> objectParser, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, objectParser.apply(p, c)), field, ValueType.OBJECT);
+    }
+
+    public void declareFloat(BiConsumer<Value, Float> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.floatValue()), field, ValueType.FLOAT);
+    }
+
+    public void declareDouble(BiConsumer<Value, Double> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.doubleValue()), field, ValueType.DOUBLE);
+    }
+
+    public void declareLong(BiConsumer<Value, Long> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.longValue()), field, ValueType.LONG);
+    }
+
+    public void declareInt(BiConsumer<Value, Integer> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.intValue()), field, ValueType.INT);
+    }
+
+    public void declareString(BiConsumer<Value, String> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.text()), field, ValueType.STRING);
+    }
+
+    public void declareStringOrNull(BiConsumer<Value, String> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.currentToken() == XContentParser.Token.VALUE_NULL ? null : p.text()), field, ValueType.STRING_OR_NULL);
+    }
+
+    public void declareBoolean(BiConsumer<Value, Boolean> consumer, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, p.booleanValue()), field, ValueType.BOOLEAN);
+    }
+
+    public static class FieldParser<T> {
+        private final Parser parser;
+        private final EnumSet<XContentParser.Token> supportedTokens;
+        private final ParseField parseField;
+        private final ValueType type;
+
+        public FieldParser(Parser parser, EnumSet<XContentParser.Token> supportedTokens, ParseField parseField, ValueType type) {
+            this.parser = parser;
+            this.supportedTokens = supportedTokens;
+            this.parseField = parseField;
+            this.type = type;
+        }
+
+        public void assertSupports(String parserName, XContentParser.Token token, String currentFieldName, ParseFieldMatcher matcher) {
+            if (matcher.match(currentFieldName, parseField) == false) {
+                throw new IllegalStateException("[" + parserName  + "] parsefield doesn't accept: " + currentFieldName);
+            }
+            if (supportedTokens.contains(token) == false) {
+                throw new IllegalArgumentException("[" + parserName  + "] " + currentFieldName + " doesn't support values of type: " + token);
+            }
+        }
+
+        @Override
+        public String toString() {
+            String[] deprecatedNames = parseField.getDeprecatedNames();
+            String allReplacedWith = parseField.getAllReplacedWith();
+            return "FieldParser{" +
+                    "preferred_name=" + parseField.getPreferredName() +
+                    ", supportedTokens=" + supportedTokens +
+                    (deprecatedNames == null || deprecatedNames.length == 0 ? "" : ", deprecated_names="  + Arrays.toString(deprecatedNames )) +
+                    (allReplacedWith == null ? "" : ", replaced_with=" + allReplacedWith) +
+                    ", type=" + type.name() +
+                    '}';
+        }
+
+    }
+
+    public enum ValueType {
+        STRING(EnumSet.of(XContentParser.Token.VALUE_STRING)),
+        STRING_OR_NULL(EnumSet.of(XContentParser.Token.VALUE_STRING, XContentParser.Token.VALUE_NULL)),
+        FLOAT(EnumSet.of(XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
+        DOUBLE(EnumSet.of(XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
+        LONG(EnumSet.of(XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
+        INT(EnumSet.of(XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
+        BOOLEAN(EnumSet.of(XContentParser.Token.VALUE_BOOLEAN)), STRING_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        FLOAT_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        DOUBLE_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        LONG_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        INT_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        BOOLEAN_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY)),
+        OBJECT(EnumSet.of(XContentParser.Token.START_OBJECT));
+
+        private final EnumSet<XContentParser.Token> tokens;
+
+        ValueType(EnumSet<XContentParser.Token> tokens) {
+            this.tokens = tokens;
+        }
+
+        public EnumSet<XContentParser.Token> supportedTokens() {
+            return this.tokens;
+        }
+    }
+
+    @Override
+    public String toString() {
+        return "ObjectParser{" +
+                "name='" + name + '\'' +
+                ", fields=" + fieldParserMap.values() +
+                '}';
+    }
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
index a6b4f46..b68d3e1 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.common.xcontent;
 
 import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.lease.Releasable;
 
 import java.io.IOException;
@@ -217,28 +218,28 @@ public interface XContentParser extends Releasable {
     /**
      * Reads a plain binary value that was written via one of the following methods:
      *
-     * <li>
-     *     <ul>{@link XContentBuilder#field(String, org.apache.lucene.util.BytesRef)}</ul>
-     *     <ul>{@link XContentBuilder#field(String, org.elasticsearch.common.bytes.BytesReference)}</ul>
-     *     <ul>{@link XContentBuilder#field(String, byte[], int, int)}}</ul>
-     *     <ul>{@link XContentBuilder#field(String, byte[])}}</ul>
-     * </li>
+     * <ul>
+     *     <li>{@link XContentBuilder#field(String, org.apache.lucene.util.BytesRef)}</li>
+     *     <li>{@link XContentBuilder#field(String, org.elasticsearch.common.bytes.BytesReference)}</li>
+     *     <li>{@link XContentBuilder#field(String, byte[], int, int)}}</li>
+     *     <li>{@link XContentBuilder#field(String, byte[])}}</li>
+     * </ul>
      *
      * as well as via their <code>XContentBuilderString</code> variants of the separated value methods.
      * Note: Do not use this method to read values written with:
-     * <li>
-     *     <ul>{@link XContentBuilder#utf8Field(XContentBuilderString, org.apache.lucene.util.BytesRef)}</ul>
-     *     <ul>{@link XContentBuilder#utf8Field(String, org.apache.lucene.util.BytesRef)}</ul>
-     * </li>
+     * <ul>
+     *     <li>{@link XContentBuilder#utf8Field(XContentBuilderString, org.apache.lucene.util.BytesRef)}</li>
+     *     <li>{@link XContentBuilder#utf8Field(String, org.apache.lucene.util.BytesRef)}</li>
+     * </ul>
      *
      * these methods write UTF-8 encoded strings and must be read through:
-     * <li>
-     *     <ul>{@link XContentParser#utf8Bytes()}</ul>
-     *     <ul>{@link XContentParser#utf8BytesOrNull()}}</ul>
-     *     <ul>{@link XContentParser#text()} ()}</ul>
-     *     <ul>{@link XContentParser#textOrNull()} ()}</ul>
-     *     <ul>{@link XContentParser#textCharacters()} ()}}</ul>
-     * </li>
+     * <ul>
+     *     <li>{@link XContentParser#utf8Bytes()}</li>
+     *     <li>{@link XContentParser#utf8BytesOrNull()}}</li>
+     *     <li>{@link XContentParser#text()} ()}</li>
+     *     <li>{@link XContentParser#textOrNull()} ()}</li>
+     *     <li>{@link XContentParser#textCharacters()} ()}}</li>
+     * </ul>
      *
      */
     byte[] binaryValue() throws IOException;
@@ -252,4 +253,15 @@ public interface XContentParser extends Releasable {
     XContentLocation getTokenLocation();
 
     boolean isClosed();
+
+    /**
+     * Returns this parsers {@link ParseFieldMatcher}
+     */
+    ParseFieldMatcher getParseFieldMatcher();
+
+
+    /**
+     * Sets this parsers {@link ParseFieldMatcher}
+     */
+    void setParseFieldMatcher(ParseFieldMatcher matcher) ;
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java
index 039fd26..ccd4da7 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java
@@ -21,6 +21,7 @@ package org.elasticsearch.common.xcontent.support;
 
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Booleans;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -31,6 +32,8 @@ import java.util.*;
  */
 public abstract class AbstractXContentParser implements XContentParser {
 
+    private ParseFieldMatcher matcher = ParseFieldMatcher.STRICT;
+
     //Currently this is not a setting that can be changed and is a policy 
     // that relates to how parsing of things like "boost" are done across
     // the whole of Elasticsearch (eg if String "1.0" is a valid float).
@@ -322,4 +325,12 @@ public abstract class AbstractXContentParser implements XContentParser {
 
     @Override
     public abstract boolean isClosed();
+
+    public ParseFieldMatcher getParseFieldMatcher() {
+        return matcher;
+    }
+
+    public void setParseFieldMatcher(ParseFieldMatcher matcher) {
+        this.matcher = matcher;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilterContext.java b/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilterContext.java
index 215af37..66f20cc 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilterContext.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/support/filtering/FilterContext.java
@@ -198,9 +198,6 @@ public class FilterContext {
 
     /**
      * Ensure that the full path to the current field is write by the JsonGenerator
-     *
-     * @param generator
-     * @throws IOException
      */
     public void writePath(JsonGenerator generator) throws IOException {
         if (parent != null) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java b/core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java
index 40aca3f..3fe99fa 100644
--- a/core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java
+++ b/core/src/main/java/org/elasticsearch/discovery/BlockingClusterStatePublishResponseHandler.java
@@ -71,7 +71,6 @@ public class BlockingClusterStatePublishResponseHandler {
      * Allows to wait for all non master nodes to reply to the publish event up to a timeout
      * @param timeout the timeout
      * @return true if the timeout expired or not, false otherwise
-     * @throws InterruptedException
      */
     public boolean awaitAllNodes(TimeValue timeout) throws InterruptedException {
         boolean success = latch.await(timeout.millis(), TimeUnit.MILLISECONDS);
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
index c86033b..9a8b19e 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
@@ -127,7 +127,7 @@ public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryServic
     /**
      * Publish all the changes to the cluster from the master (can be called just by the master). The publish
      * process should not publish this state to the master as well! (the master is sending it...).
-     * <p/>
+     * <p>
      * The {@link org.elasticsearch.discovery.Discovery.AckListener} allows to acknowledge the publish
      * event based on the response gotten from all nodes
      */
diff --git a/core/src/main/java/org/elasticsearch/discovery/InitialStateDiscoveryListener.java b/core/src/main/java/org/elasticsearch/discovery/InitialStateDiscoveryListener.java
index 8488488..1ec55c8 100644
--- a/core/src/main/java/org/elasticsearch/discovery/InitialStateDiscoveryListener.java
+++ b/core/src/main/java/org/elasticsearch/discovery/InitialStateDiscoveryListener.java
@@ -22,8 +22,8 @@ package org.elasticsearch.discovery;
 /**
  * A listener that should be called by the {@link org.elasticsearch.discovery.Discovery} component
  * when the first valid initial cluster state has been submitted and processed by the cluster service.
- * <p/>
- * <p>Note, this listener should be registered with the discovery service before it has started.
+ * <p>
+ * Note, this listener should be registered with the discovery service before it has started.
  *
  *
  */
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java b/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
index 8fc623a..6aa6ac5 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java
@@ -68,9 +68,9 @@ public class NodeJoinController extends AbstractComponent {
 
     /**
      * waits for enough incoming joins from master eligible nodes to complete the master election
-     * <p/>
+     * <p>
      * You must start accumulating joins before calling this method. See {@link #startAccumulatingJoins()}
-     * <p/>
+     * <p>
      * The method will return once the local node has been elected as master or some failure/timeout has happened.
      * The exact outcome is communicated via the callback parameter, which is guaranteed to be called.
      *
@@ -175,7 +175,7 @@ public class NodeJoinController extends AbstractComponent {
 
     /**
      * processes or queues an incoming join request.
-     * <p/>
+     * <p>
      * Note: doesn't do any validation. This should have been done before.
      */
     public void handleJoinRequest(final DiscoveryNode node, final MembershipAction.JoinCallback callback) {
@@ -436,4 +436,4 @@ public class NodeJoinController extends AbstractComponent {
             }
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
index 80b3ec0..9164a85 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
@@ -98,8 +98,6 @@ public class ElectMasterService extends AbstractComponent {
     /**
      * Returns the given nodes sorted by likelyhood of being elected as master, most likely first.
      * Non-master nodes are not removed but are rather put in the end
-     * @param nodes
-     * @return
      */
     public List<DiscoveryNode> sortByMasterLikelihood(Iterable<DiscoveryNode> nodes) {
         ArrayList<DiscoveryNode> sortedNodes = CollectionUtils.iterableAsArrayList(nodes);
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java b/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
index d3e644f..436ef6b 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
@@ -30,7 +30,7 @@ import org.elasticsearch.transport.TransportService;
 import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
 
 /**
- * A base class for {@link org.elasticsearch.discovery.zen.fd.MasterFaultDetection} & {@link org.elasticsearch.discovery.zen.fd.NodesFaultDetection},
+ * A base class for {@link org.elasticsearch.discovery.zen.fd.MasterFaultDetection} &amp; {@link org.elasticsearch.discovery.zen.fd.NodesFaultDetection},
  * making sure both use the same setting.
  */
 public abstract class FaultDetection extends AbstractComponent {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java
index fc894f3..e3550e6 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PendingClusterStatesQueue.java
@@ -31,10 +31,10 @@ import java.util.Objects;
  * A queue that holds all "in-flight" incoming cluster states from the master. Once a master commits a cluster
  * state, it is made available via {@link #getNextClusterStateToProcess()}. The class also takes care of batching
  * cluster states for processing and failures.
- * <p/>
+ * <p>
  * The queue is bound by {@link #maxQueueSize}. When the queue is at capacity and a new cluster state is inserted
  * the oldest cluster state will be dropped. This is safe because:
- * 1) Under normal operations, master will publish & commit a cluster state before processing another change (i.e., the queue length is 1)
+ * 1) Under normal operations, master will publish &amp; commit a cluster state before processing another change (i.e., the queue length is 1)
  * 2) If the master fails to commit a change, it will step down, causing a master election, which will flush the queue.
  * 3) In general it's safe to process the incoming cluster state as a replacement to the cluster state that's dropped.
  * a) If the dropped cluster is from the same master as the incoming one is, it is likely to be superseded by the incoming state (or another state in the queue).
@@ -42,7 +42,7 @@ import java.util.Objects;
  * b) If the dropping cluster state is not from the same master, it means that:
  * i) we are no longer following the master of the dropped cluster state but follow the incoming one
  * ii) we are no longer following any master, in which case it doesn't matter which cluster state will be processed first.
- * <p/>
+ * <p>
  * The class is fully thread safe and can be used concurrently.
  */
 public class PendingClusterStatesQueue {
@@ -130,7 +130,7 @@ public class PendingClusterStatesQueue {
     /**
      * indicates that a cluster state was successfully processed. Any committed state that is {@link ClusterState#supersedes(ClusterState)}-ed
      * by the processed state will be marked as processed as well.
-     * <p/>
+     * <p>
      * NOTE: successfully processing a state indicates we are following the master it came from. Any committed state from another master will
      * be failed by this method
      */
@@ -204,7 +204,7 @@ public class PendingClusterStatesQueue {
 
     /**
      * Gets the next committed state to process.
-     * <p/>
+     * <p>
      * The method tries to batch operation by getting the cluster state the highest possible committed states
      * which succeeds the first committed state in queue (i.e., it comes from the same master).
      */
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
index 9d31b2d..a8c2952 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java
@@ -96,8 +96,8 @@ public class PublishClusterStateAction extends AbstractComponent {
     /**
      * publishes a cluster change event to other nodes. if at least minMasterNodes acknowledge the change it is committed and will
      * be processed by the master and the other nodes.
-     * <p/>
-     * The method is guaranteed to throw a {@link Discovery.FailedToCommitClusterStateException} if the change is not committed and should be rejected.
+     * <p>
+     * The method is guaranteed to throw a {@link org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException} if the change is not committed and should be rejected.
      * Any other exception signals the something wrong happened but the change is committed.
      */
     public void publish(final ClusterChangedEvent clusterChangedEvent, final int minMasterNodes, final Discovery.AckListener ackListener) throws Discovery.FailedToCommitClusterStateException {
@@ -606,4 +606,4 @@ public class PublishClusterStateAction extends AbstractComponent {
             publishingTimedOut.set(isTimedOut);
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java b/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
index 7fe71d8..41f209b 100644
--- a/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
+++ b/core/src/main/java/org/elasticsearch/env/NodeEnvironment.java
@@ -377,7 +377,7 @@ public class NodeEnvironment extends AbstractComponent implements Closeable {
      * @param index the index to delete
      * @param lockTimeoutMS how long to wait for acquiring the indices shard locks
      * @param indexSettings settings for the index being deleted
-     * @throws Exception if any of the shards data directories can't be locked or deleted
+     * @throws IOException if any of the shards data directories can't be locked or deleted
      */
     public void deleteIndexDirectorySafe(Index index, long lockTimeoutMS, @IndexSettings Settings indexSettings) throws IOException {
         // This is to ensure someone doesn't use Settings.EMPTY
diff --git a/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java b/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
index d937fa6..20a3dd6 100644
--- a/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
+++ b/core/src/main/java/org/elasticsearch/gateway/AsyncShardFetch.java
@@ -43,7 +43,7 @@ import java.util.*;
 /**
  * Allows to asynchronously fetch shard related data from other nodes for allocation, without blocking
  * the cluster update thread.
- * <p/>
+ * <p>
  * The async fetch logic maintains a map of which nodes are being fetched from in an async manner,
  * and once the results are back, it makes sure to schedule a reroute to make sure those results will
  * be taken into account.
@@ -93,7 +93,7 @@ public abstract class AsyncShardFetch<T extends BaseNodeResponse> implements Rel
     /**
      * Fetches the data for the relevant shard. If there any ongoing async fetches going on, or new ones have
      * been initiated by this call, the result will have no data.
-     * <p/>
+     * <p>
      * The ignoreNodes are nodes that are supposed to be ignored for this round, since fetching is async, we need
      * to keep them around and make sure we add them back when all the responses are fetched and returned.
      */
diff --git a/core/src/main/java/org/elasticsearch/index/VersionType.java b/core/src/main/java/org/elasticsearch/index/VersionType.java
index a5d8cae..7800226 100644
--- a/core/src/main/java/org/elasticsearch/index/VersionType.java
+++ b/core/src/main/java/org/elasticsearch/index/VersionType.java
@@ -18,17 +18,12 @@
  */
 package org.elasticsearch.index;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.uid.Versions;
 
-import java.io.IOException;
-
 /**
  *
  */
-public enum VersionType implements Writeable<VersionType> {
+public enum VersionType {
     INTERNAL((byte) 0) {
         @Override
         public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
@@ -224,8 +219,6 @@ public enum VersionType implements Writeable<VersionType> {
 
     private final byte value;
 
-    private static final VersionType PROTOTYPE = INTERNAL;
-
     VersionType(byte value) {
         this.value = value;
     }
@@ -311,20 +304,4 @@ public enum VersionType implements Writeable<VersionType> {
         }
         throw new IllegalArgumentException("No version type match [" + value + "]");
     }
-
-    @Override
-    public VersionType readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        assert (ordinal == 0 || ordinal == 1 || ordinal == 2 || ordinal == 3);
-        return VersionType.values()[ordinal];
-    }
-
-    public static VersionType readVersionTypeFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java b/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java
index fc93153..2b519c2 100644
--- a/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java
+++ b/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java
@@ -56,8 +56,8 @@ public class IndexAliasesService extends AbstractIndexComponent {
 
     /**
      * Returns the filter associated with listed filtering aliases.
-     * <p/>
-     * <p>The list of filtering aliases should be obtained by calling MetaData.filteringAliases.
+     * <p>
+     * The list of filtering aliases should be obtained by calling MetaData.filteringAliases.
      * Returns <tt>null</tt> if no filtering is required.</p>
      */
     public Query aliasFilter(String... aliasNames) {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisSettingsRequired.java b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisSettingsRequired.java
index 155b78a..847752c 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisSettingsRequired.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisSettingsRequired.java
@@ -22,11 +22,11 @@ import java.lang.annotation.*;
 
 /**
  * A marker annotation on {@link CharFilterFactory}, {@link AnalyzerProvider}, {@link TokenFilterFactory},
- * or {@link @TokenizerFactory} which will cause the provider/factory to only be created when explicit settings
+ * or {@link TokenizerFactory} which will cause the provider/factory to only be created when explicit settings
  * are provided.
  */
 @Target({ElementType.TYPE})
 @Retention(RetentionPolicy.RUNTIME)
 @Documented
 public @interface AnalysisSettingsRequired {
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/CJKBigramFilterFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/CJKBigramFilterFactory.java
index 4b3a1e3..7221c4b 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/CJKBigramFilterFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/CJKBigramFilterFactory.java
@@ -34,15 +34,15 @@ import java.util.Set;
 /**
  * Factory that creates a {@link CJKBigramFilter} to form bigrams of CJK terms
  * that are generated from StandardTokenizer or ICUTokenizer.
- * <p/>
+ * <p>
  * CJK types are set by these tokenizers, but you can also use flags to
  * explicitly control which of the CJK scripts are turned into bigrams.
- * <p/>
+ * <p>
  * By default, when a CJK character has no adjacent characters to form a bigram,
  * it is output in unigram form. If you want to always output both unigrams and
  * bigrams, set the <code>outputUnigrams</code> flag. This can be used for a
  * combined unigram+bigram approach.
- * <p/>
+ * <p>
  * In all cases, all non-CJK input is passed thru unmodified.
  */
 public final class CJKBigramFilterFactory extends AbstractTokenFilterFactory {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java
index 6b8e81f..9f17dad 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java
@@ -33,12 +33,11 @@ import java.util.HashSet;
 import java.util.Set;
 
 /**
- * A {@link TokenFilterFactory} for {@link TypeFilter}. This filter only
+ * A {@link TokenFilterFactory} for {@link TypeTokenFilter}. This filter only
  * keep tokens that are contained in the set configured via
  * {@value #KEEP_TYPES_KEY} setting. 
- * <p/>
+ * <p>
  * Configuration options:
- * <p/>
  * <ul>
  * <li>{@value #KEEP_TYPES_KEY} the array of words / tokens to keep.</li>
  * </ul>
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java
index a92ade2..2c08228 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java
@@ -36,20 +36,16 @@ import org.elasticsearch.index.settings.IndexSettings;
  * keep tokens that are contained in the term set configured via
  * {@value #KEEP_WORDS_KEY} setting. This filter acts like an inverse stop
  * filter.
- * <p/>
+ * <p>
  * Configuration options:
- * <p/>
  * <ul>
  * <li>{@value #KEEP_WORDS_KEY} the array of words / tokens to keep.</li>
- * <p/>
  * <li>{@value #KEEP_WORDS_PATH_KEY} an reference to a file containing the words
  * / tokens to keep. Note: this is an alternative to {@value #KEEP_WORDS_KEY} if
  * both are set an exception will be thrown.</li>
- * <p/>
  * <li>{@value #ENABLE_POS_INC_KEY} <code>true</code> iff the filter should
  * maintain position increments for dropped tokens. The default is
  * <code>true</code>.</li>
- * <p/>
  * <li>{@value #KEEP_WORDS_CASE_KEY} to use case sensitive keep words. The
  * default is <code>false</code> which corresponds to case-sensitive.</li>
  * </ul>
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzer.java b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzer.java
index 76bf8d4..4ce0bee 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzer.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzer.java
@@ -28,6 +28,7 @@ import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.std40.StandardTokenizer40;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.util.Version;
@@ -83,4 +84,4 @@ public final class SnowballAnalyzer extends Analyzer {
     result = new SnowballFilter(result, name);
     return new TokenStreamComponents(tokenizer, result);
   }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
index 1d1a9eb..39cf56b 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
@@ -39,7 +39,7 @@ import org.elasticsearch.index.settings.IndexSettings;
  * Stemmer, use them directly with the SnowballFilter and a CustomAnalyzer.
  * Configuration of language is done with the "language" attribute or the analyzer.
  * Also supports additional stopwords via "stopwords" attribute
- * <p/>
+ * <p>
  * The SnowballAnalyzer comes with a StandardFilter, LowerCaseFilter, StopFilter
  * and the SnowballFilter.
  *
@@ -73,4 +73,4 @@ public class SnowballAnalyzerProvider extends AbstractIndexAnalyzerProvider<Snow
     public SnowballAnalyzer get() {
         return this.analyzer;
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java b/core/src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java
index 7202f1f..a6cfe91 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java
@@ -34,8 +34,7 @@ import org.apache.lucene.util.Version;
 public class StandardHtmlStripAnalyzer extends StopwordAnalyzerBase {
 
     /**
-     * @deprecated use {@link StandardHtmlStripAnalyzer#StandardHtmlStripAnalyzer(org.apache.lucene.util.Version,
-     * org.apache.lucene.analysis.util.CharArraySet)} instead
+     * @deprecated use {@link StandardHtmlStripAnalyzer#StandardHtmlStripAnalyzer(CharArraySet)} instead
      */
     @Deprecated
     public StandardHtmlStripAnalyzer() {
@@ -62,4 +61,4 @@ public class StandardHtmlStripAnalyzer extends StopwordAnalyzerBase {
         return new TokenStreamComponents(src, tok);
     }
 
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
index 58768cd..30c0905 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
@@ -65,7 +65,7 @@ import java.util.concurrent.*;
 
 /**
  * This is a cache for {@link BitDocIdSet} based filters and is unbounded by size or time.
- * <p/>
+ * <p>
  * Use this cache with care, only components that require that a filter is to be materialized as a {@link BitDocIdSet}
  * and require that it should always be around should use this cache, otherwise the
  * {@link org.elasticsearch.index.cache.query.QueryCache} should be used instead.
diff --git a/core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java b/core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java
index aaf2730..947968d 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/query/QueryCacheStats.java
@@ -27,6 +27,8 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
+import org.apache.lucene.search.DocIdSet;
+
 import java.io.IOException;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/index/codec/CodecService.java b/core/src/main/java/org/elasticsearch/index/codec/CodecService.java
index aa29f79..77d8319 100644
--- a/core/src/main/java/org/elasticsearch/index/codec/CodecService.java
+++ b/core/src/main/java/org/elasticsearch/index/codec/CodecService.java
@@ -34,12 +34,10 @@ import org.elasticsearch.index.settings.IndexSettings;
 
 /**
  * Since Lucene 4.0 low level index segments are read and written through a
- * codec layer that allows to use use-case specific file formats &
+ * codec layer that allows to use use-case specific file formats &amp;
  * data-structures per field. Elasticsearch exposes the full
  * {@link Codec} capabilities through this {@link CodecService}.
  *
- * @see PostingsFormatService
- * @see DocValuesFormatService
  */
 public class CodecService extends AbstractIndexComponent {
 
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
index 9b29c9c..71a52a7 100644
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
+++ b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
@@ -39,8 +39,7 @@ import java.util.Map.Entry;
  * </p>
  * <p>
  * This is a special bloom filter version, based on {@link org.elasticsearch.common.util.BloomFilter} and inspired
- * by Lucene {@link org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat}.
- * </p>
+ * by Lucene {@code org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat}.
  * @deprecated only for reading old segments
  */
 @Deprecated
@@ -67,7 +66,7 @@ public class BloomFilterPostingsFormat extends PostingsFormat {
      *
      * @param delegatePostingsFormat The PostingsFormat that records all the non-bloom filter data i.e.
      *                               postings info.
-     * @param bloomFilterFactory     The {@link BloomFilter.Factory} responsible for sizing BloomFilters
+     * @param bloomFilterFactory     The {@link org.elasticsearch.common.util.BloomFilter.Factory} responsible for sizing BloomFilters
      *                               appropriately
      */
     public BloomFilterPostingsFormat(PostingsFormat delegatePostingsFormat,
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index 92434d3..0c66b51 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -1131,7 +1131,6 @@ public abstract class Engine implements Closeable {
 
     /**
      * Returns <code>true</code> the internal writer has any uncommitted changes. Otherwise <code>false</code>
-     * @return
      */
     public abstract boolean hasUncommittedChanges();
 
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java b/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
index ef55708..062e79e 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java
@@ -27,8 +27,8 @@ import java.io.IOException;
 
 /**
  * An engine is already closed.
- * <p/>
- * <p>Note, the relationship between shard and engine indicates that engine closed is shard closed, and
+ * <p>
+ * Note, the relationship between shard and engine indicates that engine closed is shard closed, and
  * we might get something slipping through the the shard and into the engine while the shard is closing.
  *
  *
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/FieldData.java b/core/src/main/java/org/elasticsearch/index/fielddata/FieldData.java
index 0e28621..97750cf 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/FieldData.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/FieldData.java
@@ -218,9 +218,9 @@ public enum FieldData {
 
     /**
      * Returns a single-valued view of the {@link SortedNumericDoubleValues},
-     * if it was previously wrapped with {@link #singleton(NumericDocValues, Bits)},
+     * if it was previously wrapped with {@link DocValues#singleton(NumericDocValues, Bits)},
      * or null.
-     * @see #unwrapSingletonBits(SortedNumericDocValues)
+     * @see DocValues#unwrapSingletonBits(SortedNumericDocValues)
      */
     public static NumericDoubleValues unwrapSingleton(SortedNumericDoubleValues values) {
         if (values instanceof SingletonSortedNumericDoubleValues) {
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/MultiGeoPointValues.java b/core/src/main/java/org/elasticsearch/index/fielddata/MultiGeoPointValues.java
index 1d79077..6fa9c79 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/MultiGeoPointValues.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/MultiGeoPointValues.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.geo.GeoPoint;
  *   GeoPointValues values = ..;
  *   values.setDocId(docId);
  *   final int numValues = values.count();
- *   for (int i = 0; i < numValues; i++) {
+ *   for (int i = 0; i &lt; numValues; i++) {
  *       GeoPoint value = values.valueAt(i);
  *       // process value
  *   }
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java b/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
index fa7eef6..3b66adf 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java
@@ -34,7 +34,7 @@ import java.io.IOException;
 import java.util.Arrays;
 
 /**
- * Simple class to build document ID <-> ordinal mapping. Note: Ordinals are
+ * Simple class to build document ID &lt;-&gt; ordinal mapping. Note: Ordinals are
  * <tt>1</tt> based monotonically increasing positive integers. <tt>0</tt>
  * donates the missing value in this context.
  */
@@ -75,7 +75,7 @@ public final class OrdinalsBuilder implements Closeable {
      * with document 2: it has 2 more ordinals on level 1: 3 and 4 and its next level index is 1 meaning that there are remaining
      * ordinals on the next level. On level 2 at index 1, we can read [5  0  0  0] meaning that 5 is an ordinal as well, but the
      * fact that it is followed by zeros means that there are no more ordinals. In the end, document 2 has 2, 3, 4 and 5 as ordinals.
-     * <p/>
+     * <p>
      * In addition to these structures, there is another array which stores the current position (level + slice + offset in the slice)
      * in order to be able to append data in constant time.
      */
@@ -300,7 +300,7 @@ public final class OrdinalsBuilder implements Closeable {
     }
 
     /**
-     * Return a {@link PackedInts.Reader} instance mapping every doc ID to its first ordinal + 1 if it exists and 0 otherwise.
+     * Return a {@link org.apache.lucene.util.packed.PackedInts.Reader} instance mapping every doc ID to its first ordinal + 1 if it exists and 0 otherwise.
      */
     public PackedInts.Reader getFirstOrdinals() {
         return ordinals.firstOrdinals;
@@ -419,7 +419,7 @@ public final class OrdinalsBuilder implements Closeable {
     /**
      * A {@link TermsEnum} that iterates only full precision prefix coded 64 bit values.
      *
-     * @see #buildFromTerms(TermsEnum, Bits)
+     * @see #buildFromTerms(TermsEnum)
      */
     public static TermsEnum wrapNumeric64Bit(TermsEnum termsEnum) {
         return new FilteredTermsEnum(termsEnum, false) {
@@ -434,7 +434,7 @@ public final class OrdinalsBuilder implements Closeable {
     /**
      * A {@link TermsEnum} that iterates only full precision prefix coded 32 bit values.
      *
-     * @see #buildFromTerms(TermsEnum, Bits)
+     * @see #buildFromTerms(TermsEnum)
      */
     public static TermsEnum wrapNumeric32Bit(TermsEnum termsEnum) {
         return new FilteredTermsEnum(termsEnum, false) {
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java b/core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java
index 1601edc..4cd172f 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java
@@ -99,7 +99,7 @@ public abstract class AbstractIndexFieldData<FD extends AtomicFieldData> extends
      * the memory overhead for loading the data. Each field data
      * implementation should implement its own {@code PerValueEstimator} if it
      * intends to take advantage of the MemoryCircuitBreaker.
-     * <p/>
+     * <p>
      * Note that the .beforeLoad(...) and .afterLoad(...) methods must be
      * manually called.
      */
@@ -118,7 +118,6 @@ public abstract class AbstractIndexFieldData<FD extends AtomicFieldData> extends
          *
          * @param terms terms to be estimated
          * @return A TermsEnum for the given terms
-         * @throws IOException
          */
         public TermsEnum beforeLoad(Terms terms) throws IOException;
 
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java b/core/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java
index 1b54e38..5f95d99 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java
@@ -418,7 +418,6 @@ public class PackedArrayIndexFieldData extends AbstractIndexFieldData<AtomicNume
 
         /**
          * @return A TermsEnum wrapped in a RamAccountingTermsEnum
-         * @throws IOException
          */
         @Override
         public TermsEnum beforeLoad(Terms terms) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java b/core/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java
index dce5e40..48c9543 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java
@@ -189,7 +189,6 @@ public class PagedBytesIndexFieldData extends AbstractIndexOrdinalsFieldData {
          *
          * @param terms terms to be estimated
          * @return A possibly wrapped TermsEnum for the terms
-         * @throws IOException
          */
         @Override
         public TermsEnum beforeLoad(Terms terms) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java b/core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java
index 354ad43..0c78d2e 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java
@@ -37,7 +37,7 @@ import java.util.Collections;
 
 /**
  * FieldData backed by {@link LeafReader#getSortedNumericDocValues(String)}
- * @see FieldInfo.DocValuesType#SORTED_NUMERIC
+ * @see DocValuesType#SORTED_NUMERIC
  */
 public class SortedNumericDVIndexFieldData extends DocValuesIndexFieldData implements IndexNumericFieldData {
     private final NumericType numericType;
@@ -132,9 +132,7 @@ public class SortedNumericDVIndexFieldData extends DocValuesIndexFieldData imple
      * Order of values within a document is consistent with
      * {@link Float#compareTo(Float)}, hence the following reversible
      * transformation is applied at both index and search:
-     * {code}
-     *   bits ^ (bits >> 31) & 0x7fffffff
-     * {code}
+     * {@code bits ^ (bits >> 31) & 0x7fffffff}
      * <p>
      * Although the API is multi-valued, most codecs in Lucene specialize 
      * for the case where documents have at most one value. In this case
@@ -223,9 +221,7 @@ public class SortedNumericDVIndexFieldData extends DocValuesIndexFieldData imple
      * Order of values within a document is consistent with
      * {@link Double#compareTo(Double)}, hence the following reversible
      * transformation is applied at both index and search:
-     * {code}
-     *   bits ^ (bits >> 63) & 0x7fffffffffffffffL
-     * {code}
+     * {@code bits ^ (bits >> 63) & 0x7fffffffffffffffL}
      * <p>
      * Although the API is multi-valued, most codecs in Lucene specialize 
      * for the case where documents have at most one value. In this case
diff --git a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
index 5048bae..31b0a08 100644
--- a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
+++ b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
@@ -46,8 +46,10 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.index.StoredFieldVisitor;
+
 /**
- * Base {@link StoredFieldsVisitor} that retrieves all non-redundant metadata.
+ * Base {@link StoredFieldVisitor} that retrieves all non-redundant metadata.
  */
 public class FieldsVisitor extends StoredFieldVisitor {
 
diff --git a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
index 212362c..1cf68ad3 100644
--- a/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
+++ b/core/src/main/java/org/elasticsearch/index/get/ShardGetService.java
@@ -102,10 +102,10 @@ public final class ShardGetService extends AbstractIndexShardComponent {
     }
 
     /**
-     * Returns {@link GetResult} based on the specified {@link Engine.GetResult} argument.
+     * Returns {@link GetResult} based on the specified {@link org.elasticsearch.index.engine.Engine.GetResult} argument.
      * This method basically loads specified fields for the associated document in the engineGetResult.
      * This method load the fields from the Lucene index and not from transaction log and therefore isn't realtime.
-     * <p/>
+     * <p>
      * Note: Call <b>must</b> release engine searcher associated with engineGetResult!
      */
     public GetResult get(Engine.GetResult engineGetResult, String id, String type, String[] fields, FetchSourceContext fetchSourceContext, boolean ignoreErrorsOnGeneratedFields) {
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
index bb4c109..858453f 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
@@ -35,7 +35,7 @@ public abstract class IndexingOperationListener {
     /**
      * Called after the indexing occurs, under a locking scheme to maintain
      * concurrent updates to the same doc.
-     * <p/>
+     * <p>
      * Note, long operations should not occur under this callback.
      */
     public void postCreateUnderLock(Engine.Create create) {
@@ -66,7 +66,7 @@ public abstract class IndexingOperationListener {
     /**
      * Called after the indexing occurs, under a locking scheme to maintain
      * concurrent updates to the same doc.
-     * <p/>
+     * <p>
      * Note, long operations should not occur under this callback.
      */
     public void postIndexUnderLock(Engine.Index index) {
@@ -97,7 +97,7 @@ public abstract class IndexingOperationListener {
     /**
      * Called after the delete occurs, under a locking scheme to maintain
      * concurrent updates to the same doc.
-     * <p/>
+     * <p>
      * Note, long operations should not occur under this callback.
      */
     public void postDeleteUnderLock(Engine.Delete delete) {
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java
index 5bda1de..af4add9 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java
@@ -113,7 +113,6 @@ public class IndexingStats implements Streamable, ToXContent {
 
         /**
          * Returns if the index is under merge throttling control
-         * @return
          */
         public boolean isThrottled() {
             return isThrottled;
@@ -121,7 +120,6 @@ public class IndexingStats implements Streamable, ToXContent {
 
         /**
          * Gets the amount of time in milliseconds that the index has been under merge throttling control
-         * @return
          */
         public long getThrottleTimeInMillis() {
             return throttleTimeInMillis;
@@ -129,7 +127,6 @@ public class IndexingStats implements Streamable, ToXContent {
 
         /**
          * Gets the amount of time in a TimeValue that the index has been under merge throttling control
-         * @return
          */
         public TimeValue getThrottleTime() {
             return new TimeValue(throttleTimeInMillis);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
index 2e1f9df..1d34e4e 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
@@ -33,7 +33,7 @@ import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.io.IOException;
@@ -437,7 +437,7 @@ public abstract class MappedFieldType extends FieldType {
     }
 
     /**
-     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryShardContext)}  be used when detecting this
+     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryParseContext)}  be used when detecting this
      * field in query string.
      */
     public boolean useTermQueryWithQueryString() {
@@ -449,11 +449,11 @@ public abstract class MappedFieldType extends FieldType {
         return new Term(names().indexName(), indexedValueForSearch(value));
     }
 
-    public Query termQuery(Object value, @Nullable QueryShardContext context) {
+    public Query termQuery(Object value, @Nullable QueryParseContext context) {
         return new TermQuery(createTerm(value));
     }
 
-    public Query termsQuery(List values, @Nullable QueryShardContext context) {
+    public Query termsQuery(List values, @Nullable QueryParseContext context) {
         BytesRef[] bytesRefs = new BytesRef[values.size()];
         for (int i = 0; i < bytesRefs.length; i++) {
             bytesRefs[i] = indexedValueForSearch(values.get(i));
@@ -472,7 +472,7 @@ public abstract class MappedFieldType extends FieldType {
         return new FuzzyQuery(createTerm(value), fuzziness.asDistance(BytesRefs.toString(value)), prefixLength, maxExpansions, transpositions);
     }
 
-    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         PrefixQuery query = new PrefixQuery(createTerm(value));
         if (method != null) {
             query.setRewriteMethod(method);
@@ -480,7 +480,7 @@ public abstract class MappedFieldType extends FieldType {
         return query;
     }
 
-    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         RegexpQuery query = new RegexpQuery(createTerm(value), flags, maxDeterminizedStates);
         if (method != null) {
             query.setRewriteMethod(method);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
index 863d71f..222d262 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java
@@ -69,7 +69,7 @@ import static org.elasticsearch.index.mapper.core.TypeParsers.parsePathType;
 
 /**
  * Parsing: We handle:
- * <p/>
+ * <p>
  * - "field" : "geo_hash"
  * - "field" : "lat,lon"
  * - "field" : {
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
index b99d200..770df63 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
@@ -61,13 +61,13 @@ import static org.elasticsearch.index.mapper.MapperBuilders.geoShapeField;
 
 /**
  * FieldMapper for indexing {@link com.spatial4j.core.shape.Shape}s.
- * <p/>
+ * <p>
  * Currently Shapes can only be indexed and can only be queried using
  * {@link org.elasticsearch.index.query.GeoShapeQueryParser}, consequently
  * a lot of behavior in this Mapper is disabled.
- * <p/>
+ * <p>
  * Format supported:
- * <p/>
+ * <p>
  * "field" : {
  * "type" : "polygon",
  * "coordinates" : [
@@ -125,7 +125,6 @@ public class GeoShapeFieldMapper extends FieldMapper {
             super(name, Defaults.FIELD_TYPE);
         }
 
-        @Override
         public GeoShapeFieldType fieldType() {
             return (GeoShapeFieldType)fieldType;
         }
@@ -401,10 +400,6 @@ public class GeoShapeFieldMapper extends FieldMapper {
             return this.defaultStrategy;
         }
 
-        public PrefixTreeStrategy resolveStrategy(SpatialStrategy strategy) {
-            return resolveStrategy(strategy.getStrategyName());
-        }
-
         public PrefixTreeStrategy resolveStrategy(String strategyName) {
             if (SpatialStrategy.RECURSIVE.getStrategyName().equals(strategyName)) {
                 recursiveStrategy.setPointsOnly(pointsOnly());
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..f872207 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -40,7 +40,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityLookupService;
 
 import java.io.IOException;
@@ -186,7 +186,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, QueryShardContext context) {
+        public Query termQuery(Object value, QueryParseContext context) {
             return queryStringTermQuery(createTerm(value));
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
index 70948b1..c21e07c 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
@@ -24,12 +24,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
+import org.apache.lucene.search.*;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
@@ -41,15 +36,8 @@ import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
-import org.elasticsearch.index.mapper.MetadataFieldMapper;
-import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -60,7 +48,7 @@ import java.util.Map;
 import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;
 
 /**
- *
+ * 
  */
 public class IdFieldMapper extends MetadataFieldMapper {
 
@@ -167,7 +155,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termQuery(value, context);
             }
@@ -176,7 +164,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termsQuery(values, context);
             }
@@ -184,7 +172,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.prefixQuery(value, method, context);
             }
@@ -201,7 +189,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.regexpQuery(value, flags, maxDeterminizedStates, method, context);
             }
@@ -236,7 +224,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         super(NAME, fieldType, Defaults.FIELD_TYPE, indexSettings);
         this.path = path;
     }
-
+    
     private static MappedFieldType idFieldType(Settings indexSettings, MappedFieldType existing) {
         if (existing != null) {
             return existing.clone();
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
index 1b7168a..3f395a8 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
@@ -38,7 +38,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -157,7 +157,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
          * indices
          */
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termQuery(value, context);
             }
@@ -171,7 +171,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
         
 
         @Override
-        public Query termsQuery(List values, QueryShardContext context) {
+        public Query termsQuery(List values, QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
index ca792b8..70c1de6 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
@@ -34,16 +34,8 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.mapper.MergeMappingException;
-import org.elasticsearch.index.mapper.MergeResult;
-import org.elasticsearch.index.mapper.MetadataFieldMapper;
-import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -210,12 +202,12 @@ public class ParentFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             return termsQuery(Collections.singletonList(value), context);
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
index 12e40de..480d2a4 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.List;
@@ -137,7 +137,7 @@ public class TypeFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() == IndexOptions.NONE) {
                 return new ConstantScoreQuery(new PrefixQuery(new Term(UidFieldMapper.NAME, Uid.typePrefixAsBytes(BytesRefs.toBytesRef(value)))));
             }
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index c058a14..9740054 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.indexing.IndexingOperationListener;
@@ -43,7 +42,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.percolator.stats.ShardPercolateService;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
@@ -61,7 +60,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 /**
  * Each shard will have a percolator registry even if there isn't a {@link PercolatorService#TYPE_NAME} document type in the index.
  * For shards with indices that have no {@link PercolatorService#TYPE_NAME} document type, this will hold no percolate queries.
- * <p/>
+ * <p>
  * Once a document type has been created, the real-time percolator will start to listen to write events and update the
  * this registry with queries in real time.
  */
@@ -185,13 +184,12 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
         }
     }
 
-    //norelease this method parses from xcontent to lucene query, need to re-investigate how to split context here
     private Query parseQuery(String type, XContentParser parser) {
         String[] previousTypes = null;
         if (type != null) {
-            QueryShardContext.setTypesWithPrevious(new String[]{type});
+            QueryParseContext.setTypesWithPrevious(new String[]{type});
         }
-        QueryShardContext context = queryParserService.getShardContext();
+        QueryParseContext context = queryParserService.getParseContext();
         try {
             context.reset(parser);
             // This means that fields in the query need to exist in the mapping prior to registering this query
@@ -210,10 +208,10 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
             context.setMapUnmappedFieldAsString(mapUnmappedFieldsAsString ? true : false);
             return queryParserService.parseInnerQuery(context);
         } catch (IOException e) {
-            throw new ParsingException(context.parseContext(), "Failed to parse", e);
+            throw new ParsingException(context, "Failed to parse", e);
         } finally {
             if (type != null) {
-                QueryShardContext.setTypes(previousTypes);
+                QueryParseContext.setTypes(previousTypes);
             }
             context.reset(null);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
deleted file mode 100644
index 91adde0..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
+++ /dev/null
@@ -1,300 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
-
-/**
- * Base class for all classes producing lucene queries.
- * Supports conversion to BytesReference and creation of lucene Query objects.
- */
-public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> extends ToXContentToBytes implements QueryBuilder<QB> {
-
-    /** Default for boost to apply to resulting Lucene query. Defaults to 1.0*/
-    public static final float DEFAULT_BOOST = 1.0f;
-    public static final ParseField NAME_FIELD = new ParseField("_name");
-    public static final ParseField BOOST_FIELD = new ParseField("boost");
-
-    protected String queryName;
-    protected float boost = DEFAULT_BOOST;
-
-    protected AbstractQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        doXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
-
-    protected void printBoostAndQueryName(XContentBuilder builder) throws IOException {
-        builder.field("boost", boost);
-        if (queryName != null) {
-            builder.field("_name", queryName);
-        }
-    }
-
-    @Override
-    public final Query toQuery(QueryShardContext context) throws IOException {
-        Query query = doToQuery(context);
-        if (query != null) {
-            setFinalBoost(query);
-            if (queryName != null) {
-                context.addNamedQuery(queryName, query);
-            }
-        }
-        return query;
-    }
-
-    /**
-     * Sets the main boost to the query obtained by converting the current query into a lucene query.
-     * The default behaviour is to set the main boost, after verifying that we are not overriding any non default boost
-     * value that was previously set to the lucene query. That case would require some manual decision on how to combine
-     * the main boost with the boost coming from lucene by overriding this method.
-     * @throws IllegalStateException if the lucene query boost has already been set
-     */
-    protected void setFinalBoost(Query query) {
-        if (query.getBoost() != AbstractQueryBuilder.DEFAULT_BOOST) {
-            throw new IllegalStateException("lucene query boost is already set, override setFinalBoost to define how to combine lucene boost with main boost");
-        }
-        query.setBoost(boost);
-    }
-
-    @Override
-    public final Query toFilter(QueryShardContext context) throws IOException {
-        Query result = null;
-            final boolean originalIsFilter = context.isFilter;
-            try {
-                context.isFilter = true;
-                result = toQuery(context);
-            } finally {
-                context.isFilter = originalIsFilter;
-            }
-        return result;
-    }
-
-    //norelease to be made abstract once all query builders override doToQuery providing their own specific implementation.
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return context.indexQueryParserService().indicesQueriesRegistry().queryParsers().get(getName()).parse(context);
-    }
-
-    /**
-     * Returns the query name for the query.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB queryName(String queryName) {
-        this.queryName = queryName;
-        return (QB) this;
-    }
-
-    /**
-     * Sets the query name for the query.
-     */
-    @Override
-    public final String queryName() {
-        return queryName;
-    }
-
-    /**
-     * Returns the boost for this query.
-     */
-    @Override
-    public final float boost() {
-        return this.boost;
-    }
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB boost(float boost) {
-        this.boost = boost;
-        return (QB) this;
-    }
-
-    @Override
-    public final QB readFrom(StreamInput in) throws IOException {
-        QB queryBuilder = doReadFrom(in);
-        queryBuilder.boost = in.readFloat();
-        queryBuilder.queryName = in.readOptionalString();
-        return queryBuilder;
-    }
-
-    //norelease make this abstract once all builders implement doReadFrom themselves
-    protected QB doReadFrom(StreamInput in) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        doWriteTo(out);
-        out.writeFloat(boost);
-        out.writeOptionalString(queryName);
-    }
-
-    //norelease make this abstract once all builders implement doWriteTo themselves
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    protected final QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
-        return QueryValidationException.addValidationError(getName(), validationError, validationException);
-    }
-
-    @Override
-    public final boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        @SuppressWarnings("unchecked")
-        QB other = (QB) obj;
-        return Objects.equals(queryName, other.queryName) &&
-                Objects.equals(boost, other.boost) &&
-                doEquals(other);
-    }
-
-    /**
-     * Indicates whether some other {@link QueryBuilder} object of the same type is "equal to" this one.
-     */
-    //norelease to be made abstract once all queries are refactored
-    protected boolean doEquals(QB other) {
-        return super.equals(other);
-    }
-
-    @Override
-    public final int hashCode() {
-        return Objects.hash(getClass(), queryName, boost, doHashCode());
-    }
-
-    //norelease to be made abstract once all queries are refactored
-    protected int doHashCode() {
-        return super.hashCode();
-    }
-
-    /**
-     * This helper method checks if the object passed in is a string, if so it
-     * converts it to a {@link BytesRef}.
-     * @param obj the input object
-     * @return the same input object or a {@link BytesRef} representation if input was of type string
-     */
-    protected static Object convertToBytesRefIfString(Object obj) {
-        if (obj instanceof String) {
-            return BytesRefs.toBytesRef(obj);
-        }
-        return obj;
-    }
-
-    /**
-     * This helper method checks if the object passed in is a {@link BytesRef}, if so it
-     * converts it to a utf8 string.
-     * @param obj the input object
-     * @return the same input object or a utf8 string if input was of type {@link BytesRef}
-     */
-    protected static Object convertToStringIfBytesRef(Object obj) {
-        if (obj instanceof BytesRef) {
-            return ((BytesRef) obj).utf8ToString();
-        }
-        return obj;
-    }
-
-    /**
-     * Helper method to convert collection of {@link QueryBuilder} instances to lucene
-     * {@link Query} instances. {@link QueryBuilder} that return <tt>null</tt> calling
-     * their {@link QueryBuilder#toQuery(QueryShardContext)} method are not added to the
-     * resulting collection.
-     *
-     * @throws IOException
-     * @throws QueryShardException
-     */
-    protected static Collection<Query> toQueries(Collection<QueryBuilder> queryBuilders, QueryShardContext context) throws QueryShardException,
-            IOException {
-        List<Query> queries = new ArrayList<>(queryBuilders.size());
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            Query query = queryBuilder.toQuery(context);
-            if (query != null) {
-                queries.add(query);
-            }
-        }
-        return queries;
-    }
-
-    @Override
-    public String getName() {
-        //default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
-        return getWriteableName();
-    }
-
-    protected final void writeQueries(StreamOutput out, List<? extends QueryBuilder> queries) throws IOException {
-        out.writeVInt(queries.size());
-        for (QueryBuilder query : queries) {
-            out.writeQuery(query);
-        }
-    }
-
-    protected final List<QueryBuilder> readQueries(StreamInput in) throws IOException {
-        List<QueryBuilder> queries = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            queries.add(in.readQuery());
-        }
-        return queries;
-    }
-
-    protected final void writeOptionalQuery(StreamOutput out, QueryBuilder query) throws IOException {
-        if (query == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeQuery(query);
-        }
-    }
-
-    protected final QueryBuilder readOptionalQuery(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            return in.readQuery();
-        }
-        return null;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
deleted file mode 100644
index e732218..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * Class used during the query parsers refactoring. Will be removed once we can parse search requests on the coordinating node.
- * All query parsers that have a refactored "fromXContent" method can be changed to extend this instead of {@link BaseQueryParserTemp}.
- * Keeps old {@link QueryParser#parse(QueryShardContext)} method as a stub delegating to
- * {@link QueryParser#fromXContent(QueryParseContext)} and {@link QueryBuilder#toQuery(QueryShardContext)}}
- */
-//norelease needs to be removed once we parse search requests on the coordinating node, as the parse method is not needed anymore at that point.
-public abstract class BaseQueryParser<QB extends QueryBuilder<QB>> implements QueryParser<QB> {
-
-    @Override
-    public final Query parse(QueryShardContext context) throws IOException {
-        return fromXContent(context.parseContext()).toQuery(context);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
deleted file mode 100644
index a1e9284..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * This class with method impl is an intermediate step in the query parsers refactoring.
- * Provides a fromXContent default implementation for query parsers that don't have yet a
- * specific fromXContent implementation that returns a QueryBuilder.
- */
-//norelease to be removed once all queries are moved over to extend BaseQueryParser
-public abstract class BaseQueryParserTemp implements QueryParser {
-
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
deleted file mode 100644
index 06666b7..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
+++ /dev/null
@@ -1,165 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public abstract class BaseTermQueryBuilder<QB extends BaseTermQueryBuilder<QB>> extends AbstractQueryBuilder<QB> {
-
-    /** Name of field to match against. */
-    protected final String fieldName;
-
-    /** Value to find matches for. */
-    protected final Object value;
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     * In case value is assigned to a string, we internally convert it to a {@link BytesRef}
-     * because in {@link TermQueryParser} and {@link SpanTermQueryParser} string values are parsed to {@link BytesRef}
-     * and we want internal representation of query to be equal regardless of whether it was created from XContent or via Java API.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, Object value) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("value cannot be null");
-        }
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    /**
-     *  Returns the value used in this query.
-     *  If necessary, converts internal {@link BytesRef} representation back to string.
-     */
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value);
-    }
-
-    @Override
-    protected final boolean doEquals(BaseTermQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(value, other.value);
-    }
-
-    @Override
-    protected final QB doReadFrom(StreamInput in) throws IOException {
-        return createBuilder(in.readString(), in.readGenericValue());
-    }
-
-    protected abstract QB createBuilder(String fieldName, Object value);
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeGenericValue(value);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
index 86614ff..d3fa929 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
@@ -19,35 +19,17 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Objects;
-
-import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
  * A Query that matches documents matching boolean combinations of other queries.
  */
-public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
-
-    public static final String NAME = "bool";
-
-    public static final boolean ADJUST_PURE_NEGATIVE_DEFAULT = true;
-
-    public static final boolean DISABLE_COORD_DEFAULT = false;
-
-    static final BoolQueryBuilder PROTOTYPE = new BoolQueryBuilder();
+public class BoolQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> mustClauses = new ArrayList<>();
 
@@ -57,92 +39,63 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> shouldClauses = new ArrayList<>();
 
-    private boolean disableCoord = DISABLE_COORD_DEFAULT;
+    private float boost = -1;
 
-    private boolean adjustPureNegative = ADJUST_PURE_NEGATIVE_DEFAULT;
+    private Boolean disableCoord;
 
     private String minimumShouldMatch;
+    
+    private Boolean adjustPureNegative;
+
+    private String queryName;
 
     /**
      * Adds a query that <b>must</b> appear in the matching documents and will
-     * contribute to scoring. No <tt>null</tt> value allowed.
+     * contribute to scoring.
      */
     public BoolQueryBuilder must(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner bool query clause cannot be null");
-        }
         mustClauses.add(queryBuilder);
         return this;
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> must() {
-        return this.mustClauses;
-    }
-
-    /**
      * Adds a query that <b>must</b> appear in the matching documents but will
-     * not contribute to scoring. No <tt>null</tt> value allowed.
+     * not contribute to scoring.
      */
     public BoolQueryBuilder filter(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner bool query clause cannot be null");
-        }
         filterClauses.add(queryBuilder);
         return this;
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents but don't conntribute to scoring
-     */
-    public List<QueryBuilder> filter() {
-        return this.filterClauses;
-    }
-
-    /**
-     * Adds a query that <b>must not</b> appear in the matching documents.
-     * No <tt>null</tt> value allowed.
+     * Adds a query that <b>must not</b> appear in the matching documents and
+     * will not contribute to scoring.
      */
     public BoolQueryBuilder mustNot(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner bool query clause cannot be null");
-        }
         mustNotClauses.add(queryBuilder);
         return this;
     }
 
     /**
-     * Gets the queries that <b>must not</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> mustNot() {
-        return this.mustNotClauses;
-    }
-
-    /**
-     * Adds a clause that <i>should</i> be matched by the returned documents. For a boolean query with no
+     * Adds a query that <i>should</i> appear in the matching documents. For a boolean query with no
      * <tt>MUST</tt> clauses one or more <code>SHOULD</code> clauses must match a document
-     * for the BooleanQuery to match. No <tt>null</tt> value allowed.
+     * for the BooleanQuery to match.
      *
      * @see #minimumNumberShouldMatch(int)
      */
     public BoolQueryBuilder should(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner bool query clause cannot be null");
-        }
         shouldClauses.add(queryBuilder);
         return this;
     }
 
     /**
-     * Gets the list of clauses that <b>should</b> be matched by the returned documents.
-     *
-     * @see #should(QueryBuilder)
-     *  @see #minimumNumberShouldMatch(int)
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> should() {
-        return this.shouldClauses;
+    @Override
+    public BoolQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -154,20 +107,13 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return whether the <tt>Similarity#coord(int,int)</tt> in scoring are disabled. Defaults to <tt>false</tt>.
-     */
-    public boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    /**
      * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
-     * <p/>
-     * <p>By default no optional clauses are necessary for a match
+     * <p>
+     * By default no optional clauses are necessary for a match
      * (unless there are no required clauses).  If this method is used,
      * then the specified number of clauses is required.
-     * <p/>
-     * <p>Use of this method is totally independent of specifying that
+     * <p>
+     * Use of this method is totally independent of specifying that
      * any specific clauses are required (or prohibited).  This number will
      * only be compared against the number of matching optional clauses.
      *
@@ -178,23 +124,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         return this;
     }
 
-
-    /**
-     * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
-     * @see BoolQueryBuilder#minimumNumberShouldMatch(int)
-     */
-    public BoolQueryBuilder minimumNumberShouldMatch(String minimumNumberShouldMatch) {
-        this.minimumShouldMatch = minimumNumberShouldMatch;
-        return this;
-    }
-
-    /**
-     * @return the string representation of the minimumShouldMatch settings for this query
-     */
-    public String minimumNumberShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Sets the minimum should match using the special syntax (for example, supporting percentage).
      */
@@ -210,7 +139,7 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     public boolean hasClauses() {
         return !(mustClauses.isEmpty() && shouldClauses.isEmpty() && mustNotClauses.isEmpty() && filterClauses.isEmpty());
     }
-
+    
     /**
      * If a boolean query contains only negative ("must not") clauses should the
      * BooleanQuery be enhanced with a {@link MatchAllDocsQuery} in order to act
@@ -222,126 +151,52 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return the setting for the adjust_pure_negative setting in this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean adjustPureNegative() {
-        return this.adjustPureNegative;
+    public BoolQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject("bool");
         doXArrayContent("must", mustClauses, builder, params);
         doXArrayContent("filter", filterClauses, builder, params);
         doXArrayContent("must_not", mustNotClauses, builder, params);
         doXArrayContent("should", shouldClauses, builder, params);
-        builder.field("disable_coord", disableCoord);
-        builder.field("adjust_pure_negative", adjustPureNegative);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        printBoostAndQueryName(builder);
+        if (adjustPureNegative != null) {
+            builder.field("adjust_pure_negative", adjustPureNegative);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
 
-    private static void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
+    private void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
         if (clauses.isEmpty()) {
             return;
         }
-        builder.startArray(field);
-        for (QueryBuilder clause : clauses) {
-            clause.toXContent(builder, params);
-        }
-        builder.endArray();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        BooleanQuery.Builder booleanQueryBuilder = new BooleanQuery.Builder();
-        booleanQueryBuilder.setDisableCoord(disableCoord);
-        addBooleanClauses(context, booleanQueryBuilder, mustClauses, BooleanClause.Occur.MUST);
-        addBooleanClauses(context, booleanQueryBuilder, mustNotClauses, BooleanClause.Occur.MUST_NOT);
-        addBooleanClauses(context, booleanQueryBuilder, shouldClauses, BooleanClause.Occur.SHOULD);
-        addBooleanClauses(context, booleanQueryBuilder, filterClauses, BooleanClause.Occur.FILTER);
-        BooleanQuery booleanQuery = booleanQueryBuilder.build();
-        if (booleanQuery.clauses().isEmpty()) {
-            return new MatchAllDocsQuery();
-        }
-        booleanQuery = Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
-        return adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
-    }
-
-    private void addBooleanClauses(QueryShardContext context, BooleanQuery.Builder booleanQueryBuilder, List<QueryBuilder> clauses, Occur occurs) throws IOException {
-        for (QueryBuilder query : clauses) {
-            Query luceneQuery = null;
-            switch (occurs) {
-            case SHOULD:
-                if (context.isFilter() && minimumShouldMatch == null) {
-                    minimumShouldMatch = "1";
-                }
-                luceneQuery = query.toQuery(context);
-                break;
-            case FILTER:
-            case MUST_NOT:
-                luceneQuery = query.toFilter(context);
-                break;
-            case MUST:
-                luceneQuery = query.toQuery(context);
-            }
-            if (luceneQuery != null) {
-                booleanQueryBuilder.add(new BooleanClause(luceneQuery, occurs));
+        if (clauses.size() == 1) {
+            builder.field(field);
+            clauses.get(0).toXContent(builder, params);
+        } else {
+            builder.startArray(field);
+            for (QueryBuilder clause : clauses) {
+                clause.toXContent(builder, params);
             }
+            builder.endArray();
         }
     }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(adjustPureNegative, disableCoord,
-                minimumShouldMatch, mustClauses, shouldClauses, mustNotClauses, filterClauses);
-    }
-
-    @Override
-    protected boolean doEquals(BoolQueryBuilder other) {
-        return Objects.equals(adjustPureNegative, other.adjustPureNegative) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(mustClauses, other.mustClauses) &&
-                Objects.equals(shouldClauses, other.shouldClauses) &&
-                Objects.equals(mustNotClauses, other.mustNotClauses) &&
-                Objects.equals(filterClauses, other.filterClauses);
-    }
-
-    @Override
-    protected BoolQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        boolQueryBuilder.mustClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.mustNotClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.shouldClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.filterClauses.addAll(queryBuilders);
-        boolQueryBuilder.adjustPureNegative = in.readBoolean();
-        boolQueryBuilder.disableCoord = in.readBoolean();
-        boolQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        return boolQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, mustClauses);
-        writeQueries(out, mustNotClauses);
-        writeQueries(out, shouldClauses);
-        writeQueries(out, filterClauses);
-        out.writeBoolean(adjustPureNegative);
-        out.writeBoolean(disableCoord);
-        out.writeOptionalString(minimumShouldMatch);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
index 474bc7d..9c10acf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
@@ -19,7 +19,10 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -33,9 +36,11 @@ import java.util.List;
 import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
- * Parser for bool query
+ *
  */
-public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
+public class BoolQueryParser implements QueryParser {
+
+    public static final String NAME = "bool";
 
     @Inject
     public BoolQueryParser(Settings settings) {
@@ -44,27 +49,23 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{BoolQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoolQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, ParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        boolean disableCoord = BoolQueryBuilder.DISABLE_COORD_DEFAULT;
-        boolean adjustPureNegative = BoolQueryBuilder.ADJUST_PURE_NEGATIVE_DEFAULT;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
+        float boost = 1.0f;
         String minimumShouldMatch = null;
 
-        final List<QueryBuilder> mustClauses = new ArrayList<>();
-        final List<QueryBuilder> mustNotClauses = new ArrayList<>();
-        final List<QueryBuilder> shouldClauses = new ArrayList<>();
-        final List<QueryBuilder> filterClauses = new ArrayList<>();
+        List<BooleanClause> clauses = new ArrayList<>();
+        boolean adjustPureNegative = true;
         String queryName = null;
-
+        
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder query;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -73,21 +74,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 switch (currentFieldName) {
                 case "must":
-                    query = parseContext.parseInnerQueryBuilder();
-                    mustClauses.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                    }
                     break;
                 case "should":
-                    query = parseContext.parseInnerQueryBuilder();
-                    shouldClauses.add(query);
+                    query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                        if (parseContext.isFilter() && minimumShouldMatch == null) {
+                            minimumShouldMatch = "1";
+                        }
+                    }
                     break;
                 case "filter":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    filterClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                    }
                     break;
                 case "must_not":
                 case "mustNot":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    mustNotClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                    }
                     break;
                 default:
                     throw new ParsingException(parseContext, "[bool] query does not support [" + currentFieldName + "]");
@@ -96,21 +108,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                     switch (currentFieldName) {
                     case "must":
-                        query = parseContext.parseInnerQueryBuilder();
-                        mustClauses.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                        }
                         break;
                     case "should":
-                        query = parseContext.parseInnerQueryBuilder();
-                        shouldClauses.add(query);
+                        query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                            if (parseContext.isFilter() && minimumShouldMatch == null) {
+                                minimumShouldMatch = "1";
+                            }
+                        }
                         break;
                     case "filter":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        filterClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                        }
                         break;
                     case "must_not":
                     case "mustNot":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        mustNotClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                        }
                         break;
                     default:
                         throw new ParsingException(parseContext, "bool query does not support [" + currentFieldName + "]");
@@ -134,29 +157,23 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 }
             }
         }
-        BoolQueryBuilder boolQuery = new BoolQueryBuilder();
-        for (QueryBuilder queryBuilder : mustClauses) {
-            boolQuery.must(queryBuilder);
-        }
-        for (QueryBuilder queryBuilder : mustNotClauses) {
-            boolQuery.mustNot(queryBuilder);
+
+        if (clauses.isEmpty()) {
+            return new MatchAllDocsQuery();
         }
-        for (QueryBuilder queryBuilder : shouldClauses) {
-            boolQuery.should(queryBuilder);
+
+        BooleanQuery.Builder booleanQueryBuilder = new BooleanQuery.Builder();
+        booleanQueryBuilder.setDisableCoord(disableCoord);
+        for (BooleanClause clause : clauses) {
+            booleanQueryBuilder.add(clause);
         }
-        for (QueryBuilder queryBuilder : filterClauses) {
-            boolQuery.filter(queryBuilder);
+        BooleanQuery booleanQuery = booleanQueryBuilder.build();
+        booleanQuery.setBoost(boost);
+        booleanQuery = Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
+        Query query = adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        boolQuery.boost(boost);
-        boolQuery.disableCoord(disableCoord);
-        boolQuery.adjustPureNegative(adjustPureNegative);
-        boolQuery.minimumNumberShouldMatch(minimumShouldMatch);
-        boolQuery.queryName(queryName);
-        return boolQuery;
-    }
-
-    @Override
-    public BoolQueryBuilder getBuilderPrototype() {
-        return BoolQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
new file mode 100644
index 0000000..31572ce
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+/**
+ * Query builder which allow setting some boost
+ */
+public interface BoostableQueryBuilder<B extends BoostableQueryBuilder<B>> {
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    B boost(float boost);
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
index a7cb405..1e2f9c4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
@@ -19,20 +19,15 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.queries.BoostingQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * The BoostingQuery class can be used to effectively demote results that match a given query.
  * Unlike the "NOT" clause, this still selects documents that contain undesirable terms,
  * but reduces their overall score:
- * <p/>
+ * <p>
  * Query balancedQuery = new BoostingQuery(positiveQuery, negativeQuery, 0.01f);
  * In this scenario the positiveQuery contains the mandatory, desirable criteria which is used to
  * select all matching documents, and the negativeQuery contains the undesirable elements which
@@ -40,122 +35,63 @@ import java.util.Objects;
  * multiplied by the supplied "boost" parameter, so this should be less than 1 to achieve a
  * demoting effect
  */
-public class BoostingQueryBuilder extends AbstractQueryBuilder<BoostingQueryBuilder> {
+public class BoostingQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoostingQueryBuilder> {
 
-    public static final String NAME = "boosting";
+    private QueryBuilder positiveQuery;
 
-    private final QueryBuilder positiveQuery;
-
-    private final QueryBuilder negativeQuery;
+    private QueryBuilder negativeQuery;
 
     private float negativeBoost = -1;
 
-    static final BoostingQueryBuilder PROTOTYPE = new BoostingQueryBuilder(EmptyQueryBuilder.PROTOTYPE, EmptyQueryBuilder.PROTOTYPE);
+    private float boost = -1;
+
+    public BoostingQueryBuilder() {
 
-    /**
-     * Create a new {@link BoostingQueryBuilder}
-     *
-     * @param positiveQuery the positive query for this boosting query.
-     * @param negativeQuery the negative query for this boosting query.
-     */
-    public BoostingQueryBuilder(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        if (positiveQuery == null) {
-            throw new IllegalArgumentException("inner clause [positive] cannot be null.");
-        }
-        if (negativeQuery == null) {
-            throw new IllegalArgumentException("inner clause [negative] cannot be null.");
-        }
-        this.positiveQuery = positiveQuery;
-        this.negativeQuery = negativeQuery;
     }
 
-    /**
-     * Get the positive query for this boosting query.
-     */
-    public QueryBuilder positiveQuery() {
-        return this.positiveQuery;
+    public BoostingQueryBuilder positive(QueryBuilder positiveQuery) {
+        this.positiveQuery = positiveQuery;
+        return this;
     }
 
-    /**
-     * Get the negative query for this boosting query.
-     */
-    public QueryBuilder negativeQuery() {
-        return this.negativeQuery;
+    public BoostingQueryBuilder negative(QueryBuilder negativeQuery) {
+        this.negativeQuery = negativeQuery;
+        return this;
     }
 
-    /**
-     * Set the negative boost factor.
-     */
     public BoostingQueryBuilder negativeBoost(float negativeBoost) {
-        if (negativeBoost < 0) {
-            throw new IllegalArgumentException("query requires negativeBoost to be set to positive value");
-        }
         this.negativeBoost = negativeBoost;
         return this;
     }
 
-    /**
-     * Get the negative boost factor.
-     */
-    public float negativeBoost() {
-        return this.negativeBoost;
+    @Override
+    public BoostingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (positiveQuery == null) {
+            throw new IllegalArgumentException("boosting query requires positive query to be set");
+        }
+        if (negativeQuery == null) {
+            throw new IllegalArgumentException("boosting query requires negative query to be set");
+        }
+        if (negativeBoost == -1) {
+            throw new IllegalArgumentException("boosting query requires negativeBoost to be set");
+        }
+        builder.startObject(BoostingQueryParser.NAME);
         builder.field("positive");
         positiveQuery.toXContent(builder, params);
         builder.field("negative");
         negativeQuery.toXContent(builder, params);
-        builder.field("negative_boost", negativeBoost);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        builder.field("negative_boost", negativeBoost);
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query positive = positiveQuery.toQuery(context);
-        Query negative = negativeQuery.toQuery(context);
-        // make upstream queries ignore this query by returning `null`
-        // if either inner query builder returns null
-        if (positive == null || negative == null) {
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-
-        return new BoostingQuery(positive, negative, negativeBoost);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(negativeBoost, positiveQuery, negativeQuery);
-    }
-
-    @Override
-    protected boolean doEquals(BoostingQueryBuilder other) {
-        return Objects.equals(negativeBoost, other.negativeBoost) &&
-                Objects.equals(positiveQuery, other.positiveQuery) &&
-                Objects.equals(negativeQuery, other.negativeQuery);
-    }
-
-    @Override
-    protected BoostingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder positiveQuery = in.readQuery();
-        QueryBuilder negativeQuery = in.readQuery();
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost = in.readFloat();
-        return boostingQuery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(positiveQuery);
-        out.writeQuery(negativeQuery);
-        out.writeFloat(negativeBoost);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
index 1fe485e..b496d2b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.queries.BoostingQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,26 +28,31 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for boosting query
+ *
  */
-public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
+public class BoostingQueryParser implements QueryParser {
+
+    public static final String NAME = "boosting";
+
+    @Inject
+    public BoostingQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{BoostingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoostingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder positiveQuery = null;
+        Query positiveQuery = null;
         boolean positiveQueryFound = false;
-        QueryBuilder negativeQuery = null;
+        Query negativeQuery = null;
         boolean negativeQueryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = -1;
         float negativeBoost = -1;
-        String queryName = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -54,10 +61,10 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("positive".equals(currentFieldName)) {
-                    positiveQuery = parseContext.parseInnerQueryBuilder();
+                    positiveQuery = parseContext.parseInnerQuery();
                     positiveQueryFound = true;
                 } else if ("negative".equals(currentFieldName)) {
-                    negativeQuery = parseContext.parseInnerQueryBuilder();
+                    negativeQuery = parseContext.parseInnerQuery();
                     negativeQueryFound = true;
                 } else {
                     throw new ParsingException(parseContext, "[boosting] query does not support [" + currentFieldName + "]");
@@ -65,8 +72,6 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             } else if (token.isValue()) {
                 if ("negative_boost".equals(currentFieldName) || "negativeBoost".equals(currentFieldName)) {
                     negativeBoost = parser.floatValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
@@ -75,25 +80,25 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             }
         }
 
-        if (!positiveQueryFound) {
+        if (positiveQuery == null && !positiveQueryFound) {
             throw new ParsingException(parseContext, "[boosting] query requires 'positive' query to be set'");
         }
-        if (!negativeQueryFound) {
+        if (negativeQuery == null && !negativeQueryFound) {
             throw new ParsingException(parseContext, "[boosting] query requires 'negative' query to be set'");
         }
-        if (negativeBoost < 0) {
-            throw new ParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set to be a positive value'");
+        if (negativeBoost == -1) {
+            throw new ParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set'");
         }
 
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost(negativeBoost);
-        boostingQuery.boost(boost);
-        boostingQuery.queryName(queryName);
-        return boostingQuery;
-    }
+        // parsers returned null
+        if (positiveQuery == null || negativeQuery == null) {
+            return null;
+        }
 
-    @Override
-    public BoostingQueryBuilder getBuilderPrototype() {
-        return BoostingQueryBuilder.PROTOTYPE;
+        BoostingQuery boostingQuery = new BoostingQuery(positiveQuery, negativeQuery, negativeBoost);
+        if (boost != -1) {
+            boostingQuery.setBoost(boost);
+        }
+        return boostingQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
index cca036e..32b74d0 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
@@ -19,31 +19,18 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * CommonTermsQuery query is a query that executes high-frequency terms in a
  * optional sub-query to prevent slow queries due to "common" terms like
- * stopwords. This query basically builds 2 queries off the
- * {@link org.apache.lucene.queries.CommonTermsQuery#add(Term) added} terms
- * where low-frequency terms are added to a required boolean clause
+ * stopwords. This query basically builds 2 queries off the {@code #add(Term)
+ * added} terms where low-frequency terms are added to a required boolean clause
  * and high-frequency terms are added to an optional boolean clause. The
  * optional clause is only executed if the required "low-frequency' clause
  * matches. Scores produced by this query will be slightly different to plain
@@ -53,60 +40,47 @@ import java.util.Objects;
  * significantly contribute to the document score unless at least one of the
  * low-frequency terms are matched such that this query can improve query
  * execution times significantly if applicable.
- * <p>
  */
-public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQueryBuilder> {
+public class CommonTermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<CommonTermsQueryBuilder> {
 
-    public static final String NAME = "common";
-
-    public static final float DEFAULT_CUTOFF_FREQ = 0.01f;
-
-    public static final Operator DEFAULT_HIGH_FREQ_OCCUR = Operator.OR;
-
-    public static final Operator DEFAULT_LOW_FREQ_OCCUR = Operator.OR;
-
-    public static final boolean DEFAULT_DISABLE_COORD = true;
+    public static enum Operator {
+        OR, AND
+    }
 
-    private final String fieldName;
+    private final String name;
 
     private final Object text;
 
-    private Operator highFreqOperator = DEFAULT_HIGH_FREQ_OCCUR;
+    private Operator highFreqOperator = null;
 
-    private Operator lowFreqOperator = DEFAULT_LOW_FREQ_OCCUR;
+    private Operator lowFreqOperator = null;
 
     private String analyzer = null;
 
+    private Float boost = null;
+
     private String lowFreqMinimumShouldMatch = null;
 
     private String highFreqMinimumShouldMatch = null;
 
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
+    private Boolean disableCoord = null;
 
-    private float cutoffFrequency = DEFAULT_CUTOFF_FREQ;
+    private Float cutoffFrequency = null;
 
-    static final CommonTermsQueryBuilder PROTOTYPE = new CommonTermsQueryBuilder("field", "text");
+    private String queryName;
 
     /**
      * Constructs a new common terms query.
      */
-    public CommonTermsQueryBuilder(String fieldName, Object text) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
+    public CommonTermsQueryBuilder(String name, Object text) {
+        if (name == null) {
+            throw new IllegalArgumentException("Field name must not be null");
         }
         if (text == null) {
-            throw new IllegalArgumentException("text cannot be null.");
+            throw new IllegalArgumentException("Query must not be null");
         }
-        this.fieldName = fieldName;
         this.text = text;
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return this.text;
+        this.name = name;
     }
 
     /**
@@ -115,27 +89,19 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder highFreqOperator(Operator operator) {
-        this.highFreqOperator = (operator == null) ? DEFAULT_HIGH_FREQ_OCCUR : operator;
+        this.highFreqOperator = operator;
         return this;
     }
 
-    public Operator highFreqOperator() {
-        return highFreqOperator;
-    }
-
     /**
      * Sets the operator to use for terms with a low document frequency (less
      * than {@link #cutoffFrequency(float)}. Defaults to <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder lowFreqOperator(Operator operator) {
-        this.lowFreqOperator = (operator == null) ? DEFAULT_LOW_FREQ_OCCUR : operator;
+        this.lowFreqOperator = operator;
         return this;
     }
 
-    public Operator lowFreqOperator() {
-        return lowFreqOperator;
-    }
-
     /**
      * Explicitly set the analyzer to use. Defaults to use explicit mapping
      * config for the field, or, if not set, the default search analyzer.
@@ -145,26 +111,27 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String analyzer() {
-        return this.analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public CommonTermsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
      * Sets the cutoff document frequency for high / low frequent terms. A value
-     * in [0..1] (or absolute number >=1) representing the maximum threshold of
+     * in [0..1] (or absolute number &gt;=1) representing the maximum threshold of
      * a terms document frequency to be considered a low frequency term.
      * Defaults to
-     * <tt>{@value #DEFAULT_CUTOFF_FREQ}</tt>
+     * <tt>{@value CommonTermsQueryParser#DEFAULT_MAX_TERM_DOC_FREQ}</tt>
      */
     public CommonTermsQueryBuilder cutoffFrequency(float cutoffFrequency) {
         this.cutoffFrequency = cutoffFrequency;
         return this;
     }
 
-    public float cutoffFrequency() {
-        return this.cutoffFrequency;
-    }
-
     /**
      * Sets the minimum number of high frequent query terms that need to match in order to
      * produce a hit when there are no low frequen terms.
@@ -174,10 +141,6 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String highFreqMinimumShouldMatch() {
-        return this.highFreqMinimumShouldMatch;
-    }
-
     /**
      * Sets the minimum number of low frequent query terms that need to match in order to
      * produce a hit.
@@ -186,32 +149,44 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         this.lowFreqMinimumShouldMatch = lowFreqMinimumShouldMatch;
         return this;
     }
-
-    public String lowFreqMinimumShouldMatch() {
-        return this.lowFreqMinimumShouldMatch;
-    }
-
+    
     public CommonTermsQueryBuilder disableCoord(boolean disableCoord) {
         this.disableCoord = disableCoord;
         return this;
     }
 
-    public boolean disableCoord() {
-        return this.disableCoord;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public CommonTermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(CommonTermsQueryParser.NAME);
+        builder.startObject(name);
+
         builder.field("query", text);
-        builder.field("disable_coord", disableCoord);
-        builder.field("high_freq_operator", highFreqOperator.toString());
-        builder.field("low_freq_operator", lowFreqOperator.toString());
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
+        if (highFreqOperator != null) {
+            builder.field("high_freq_operator", highFreqOperator.toString());
+        }
+        if (lowFreqOperator != null) {
+            builder.field("low_freq_operator", lowFreqOperator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("cutoff_frequency", cutoffFrequency);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (cutoffFrequency != null) {
+            builder.field("cutoff_frequency", cutoffFrequency);
+        }
         if (lowFreqMinimumShouldMatch != null || highFreqMinimumShouldMatch != null) {
             builder.startObject("minimum_should_match");
             if (lowFreqMinimumShouldMatch != null) {
@@ -222,113 +197,11 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
             }
             builder.endObject();
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            field = fieldType.names().indexName();
-        } else {
-            field = fieldName;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
 
-        Analyzer analyzerObj;
-        if (analyzer == null) {
-            if (fieldType != null) {
-                analyzerObj = context.getSearchAnalyzer(fieldType);
-            } else {
-                analyzerObj = context.mapperService().searchAnalyzer();
-            }
-        } else {
-            analyzerObj = context.mapperService().analysisService().analyzer(analyzer);
-            if (analyzerObj == null) {
-                throw new QueryShardException(context, "[common] analyzer [" + analyzer + "] not found");
-            }
-        }
-
-        Occur highFreqOccur = highFreqOperator.toBooleanClauseOccur();
-        Occur lowFreqOccur = lowFreqOperator.toBooleanClauseOccur();
-
-        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, cutoffFrequency, disableCoord, fieldType);
-        return parseQueryString(commonsQuery, text, field, analyzerObj, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
-    }
-
-    static Query parseQueryString(ExtendedCommonTermsQuery query, Object queryString, String field, Analyzer analyzer,
-                                         String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
-        // Logic similar to QueryParser#getFieldQuery
-        int count = 0;
-        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
-            source.reset();
-            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
-            BytesRefBuilder builder = new BytesRefBuilder();
-            while (source.incrementToken()) {
-                // UTF-8
-                builder.copyChars(termAtt);
-                query.add(new Term(field, builder.toBytesRef()));
-                count++;
-            }
-        }
-
-        if (count == 0) {
-            return null;
-        }
-        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
-        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
-        return query;
-    }
-
-    @Override
-    protected CommonTermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder(in.readString(), in.readGenericValue());
-        commonTermsQueryBuilder.highFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.lowFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.analyzer = in.readOptionalString();
-        commonTermsQueryBuilder.lowFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.highFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.disableCoord = in.readBoolean();
-        commonTermsQueryBuilder.cutoffFrequency = in.readFloat();
-        return commonTermsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.text);
-        highFreqOperator.writeTo(out);
-        lowFreqOperator.writeTo(out);
-        out.writeOptionalString(analyzer);
-        out.writeOptionalString(lowFreqMinimumShouldMatch);
-        out.writeOptionalString(highFreqMinimumShouldMatch);
-        out.writeBoolean(disableCoord);
-        out.writeFloat(cutoffFrequency);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, text, highFreqOperator, lowFreqOperator, analyzer,
-                lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch, disableCoord, cutoffFrequency);
-    }
-
-    @Override
-    protected boolean doEquals(CommonTermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(text, other.text) &&
-                Objects.equals(highFreqOperator, other.highFreqOperator) &&
-                Objects.equals(lowFreqOperator, other.lowFreqOperator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(lowFreqMinimumShouldMatch, other.lowFreqMinimumShouldMatch) &&
-                Objects.equals(highFreqMinimumShouldMatch, other.highFreqMinimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(cutoffFrequency, other.cutoffFrequency);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
index 5c0d3f6..e59d303 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
@@ -19,39 +19,64 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRefBuilder;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for common terms query
+ *
  */
-public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuilder> {
+public class CommonTermsQueryParser implements QueryParser {
+
+    public static final String NAME = "common";
+
+    static final float DEFAULT_MAX_TERM_DOC_FREQ = 0.01f;
+
+    static final Occur DEFAULT_HIGH_FREQ_OCCUR = Occur.SHOULD;
+
+    static final Occur DEFAULT_LOW_FREQ_OCCUR = Occur.SHOULD;
+
+    static final boolean DEFAULT_DISABLE_COORD = true;
+
+
+    @Inject
+    public CommonTermsQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[] { CommonTermsQueryBuilder.NAME };
+        return new String[] { NAME };
     }
 
     @Override
-    public CommonTermsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new ParsingException(parseContext, "[common] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        Object text = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String analyzer = null;
+        Object value = null;
+        float boost = 1.0f;
+        String queryAnalyzer = null;
         String lowFreqMinimumShouldMatch = null;
         String highFreqMinimumShouldMatch = null;
-        boolean disableCoord = CommonTermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        Operator highFreqOperator = CommonTermsQueryBuilder.DEFAULT_HIGH_FREQ_OCCUR;
-        Operator lowFreqOperator = CommonTermsQueryBuilder.DEFAULT_LOW_FREQ_OCCUR;
-        float cutoffFrequency = CommonTermsQueryBuilder.DEFAULT_CUTOFF_FREQ;
+        boolean disableCoord = DEFAULT_DISABLE_COORD;
+        Occur highFreqOccur = DEFAULT_HIGH_FREQ_OCCUR;
+        Occur lowFreqOccur = DEFAULT_LOW_FREQ_OCCUR;
+        float maxTermFrequency = DEFAULT_MAX_TERM_DOC_FREQ;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -81,21 +106,41 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
                     }
                 } else if (token.isValue()) {
                     if ("query".equals(currentFieldName)) {
-                        text = parser.objectText();
+                        value = parser.objectText();
                     } else if ("analyzer".equals(currentFieldName)) {
-                        analyzer = parser.text();
+                        String analyzer = parser.text();
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
+                            throw new ParsingException(parseContext, "[common] analyzer [" + parser.text() + "] not found");
+                        }
+                        queryAnalyzer = analyzer;
                     } else if ("disable_coord".equals(currentFieldName) || "disableCoord".equals(currentFieldName)) {
                         disableCoord = parser.booleanValue();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("high_freq_operator".equals(currentFieldName) || "highFreqOperator".equals(currentFieldName)) {
-                        highFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new ParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("low_freq_operator".equals(currentFieldName) || "lowFreqOperator".equals(currentFieldName)) {
-                        lowFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new ParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         lowFreqMinimumShouldMatch = parser.text();
                     } else if ("cutoff_frequency".equals(currentFieldName)) {
-                        cutoffFrequency = parser.floatValue();
+                        maxTermFrequency = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -105,7 +150,7 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
             parser.nextToken();
         } else {
-            text = parser.objectText();
+            value = parser.objectText();
             // move to the next token
             token = parser.nextToken();
             if (token != XContentParser.Token.END_OBJECT) {
@@ -115,23 +160,66 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
         }
 
-        if (text == null) {
+        if (value == null) {
             throw new ParsingException(parseContext, "No text specified for text query");
         }
-        return new CommonTermsQueryBuilder(fieldName, text)
-                .lowFreqMinimumShouldMatch(lowFreqMinimumShouldMatch)
-                .highFreqMinimumShouldMatch(highFreqMinimumShouldMatch)
-                .analyzer(analyzer)
-                .highFreqOperator(highFreqOperator)
-                .lowFreqOperator(lowFreqOperator)
-                .disableCoord(disableCoord)
-                .cutoffFrequency(cutoffFrequency)
-                .boost(boost)
-                .queryName(queryName);
+        String field;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        } else {
+            field = fieldName;
+        }
+
+        Analyzer analyzer = null;
+        if (queryAnalyzer == null) {
+            if (fieldType != null) {
+                analyzer = fieldType.searchAnalyzer();
+            }
+            if (analyzer == null && fieldType != null) {
+                analyzer = parseContext.getSearchAnalyzer(fieldType);
+            }
+            if (analyzer == null) {
+                analyzer = parseContext.mapperService().searchAnalyzer();
+            }
+        } else {
+            analyzer = parseContext.mapperService().analysisService().analyzer(queryAnalyzer);
+            if (analyzer == null) {
+                throw new IllegalArgumentException("No analyzer found for [" + queryAnalyzer + "]");
+            }
+        }
+
+        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, maxTermFrequency, disableCoord, fieldType);
+        commonsQuery.setBoost(boost);
+        Query query = parseQueryString(commonsQuery, value.toString(), field, parseContext, analyzer, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public CommonTermsQueryBuilder getBuilderPrototype() {
-        return CommonTermsQueryBuilder.PROTOTYPE;
+
+    private final Query parseQueryString(ExtendedCommonTermsQuery query, String queryString, String field, QueryParseContext parseContext,
+            Analyzer analyzer, String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
+        // Logic similar to QueryParser#getFieldQuery
+        int count = 0;
+        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
+            source.reset();
+            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
+            BytesRefBuilder builder = new BytesRefBuilder();
+            while (source.incrementToken()) {
+                // UTF-8
+                builder.copyChars(termAtt);
+                query.add(new Term(field, builder.toBytesRef()));
+                count++;
+            }
+        }
+
+        if (count == 0) {
+            return null;
+        }
+        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
+        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
index 528871c..bdcbe9c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,76 +28,41 @@ import java.util.Objects;
  * A query that wraps a filter and simply returns a constant score equal to the
  * query boost for every document in the filter.
  */
-public class ConstantScoreQueryBuilder extends AbstractQueryBuilder<ConstantScoreQueryBuilder> {
-
-    public static final String NAME = "constant_score";
+public class ConstantScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<ConstantScoreQueryBuilder> {
 
     private final QueryBuilder filterBuilder;
 
-    static final ConstantScoreQueryBuilder PROTOTYPE = new ConstantScoreQueryBuilder(EmptyQueryBuilder.PROTOTYPE);
+    private float boost = -1;
 
     /**
-     * A query that wraps another query and simply returns a constant score equal to the
+     * A query that wraps a query and simply returns a constant score equal to the
      * query boost for every document in the query.
      *
      * @param filterBuilder The query to wrap in a constant score query
      */
     public ConstantScoreQueryBuilder(QueryBuilder filterBuilder) {
-        if (filterBuilder == null) {
-            throw new IllegalArgumentException("inner clause [filter] cannot be null.");
-        }
-        this.filterBuilder = filterBuilder;
+        this.filterBuilder = Objects.requireNonNull(filterBuilder);
     }
 
     /**
-     * @return the query that was wrapped in this constant score query
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public QueryBuilder innerQuery() {
-        return this.filterBuilder;
+    @Override
+    public ConstantScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(ConstantScoreQueryParser.NAME);
         builder.field("filter");
         filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerFilter = filterBuilder.toFilter(context);
-        if (innerFilter == null ) {
-            // return null so that parent queries (e.g. bool) also ignore this
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new ConstantScoreQuery(innerFilter);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filterBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(ConstantScoreQueryBuilder other) {
-        return Objects.equals(filterBuilder, other.filterBuilder);
-    }
-
-    @Override
-    protected ConstantScoreQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerFilterBuilder = in.readQuery();
-        return new ConstantScoreQueryBuilder(innerFilterBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filterBuilder);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
index 84014b1..1df3a54 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
@@ -19,33 +19,40 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for constant_score query
+ *
  */
-public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQueryBuilder> {
+public class ConstantScoreQueryParser implements QueryParser {
 
+    public static final String NAME = "constant_score";
     private static final ParseField INNER_QUERY_FIELD = new ParseField("filter", "query");
 
+    @Inject
+    public ConstantScoreQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{ConstantScoreQueryBuilder.NAME, Strings.toCamelCase(ConstantScoreQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public ConstantScoreQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query filter = null;
         boolean queryFound = false;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -56,15 +63,13 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, INNER_QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    filter = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     throw new ParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
@@ -75,14 +80,12 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
             throw new ParsingException(parseContext, "[constant_score] requires a 'filter' element");
         }
 
-        ConstantScoreQueryBuilder constantScoreBuilder = new ConstantScoreQueryBuilder(query);
-        constantScoreBuilder.boost(boost);
-        constantScoreBuilder.queryName(queryName);
-        return constantScoreBuilder;
-    }
+        if (filter == null) {
+            return null;
+        }
 
-    @Override
-    public ConstantScoreQueryBuilder getBuilderPrototype() {
-        return ConstantScoreQueryBuilder.PROTOTYPE;
+        filter = new ConstantScoreQuery(filter);
+        filter.setBoost(boost);
+        return filter;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
index 8ce9d46..3724a05 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
@@ -19,51 +19,42 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A query that generates the union of documents produced by its sub-queries, and that scores each document
  * with the maximum score for that document as produced by any sub-query, plus a tie breaking increment for any
  * additional matching sub-queries.
  */
-public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder> {
+public class DisMaxQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<DisMaxQueryBuilder> {
 
-    public static final String NAME = "dis_max";
+    private ArrayList<QueryBuilder> queries = new ArrayList<>();
 
-    private final ArrayList<QueryBuilder> queries = new ArrayList<>();
+    private float boost = -1;
 
-    /** Default multiplication factor for breaking ties in document scores.*/
-    public static float DEFAULT_TIE_BREAKER = 0.0f;
-    private float tieBreaker = DEFAULT_TIE_BREAKER;
+    private float tieBreaker = -1;
 
-    static final DisMaxQueryBuilder PROTOTYPE = new DisMaxQueryBuilder();
+    private String queryName;
 
     /**
      * Add a sub-query to this disjunction.
      */
     public DisMaxQueryBuilder add(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner dismax query clause cannot be null");
-        }
         queries.add(queryBuilder);
         return this;
     }
 
     /**
-     * @return an immutable list copy of the current sub-queries of this disjunction
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> innerQueries() {
-        return this.queries;
+    @Override
+    public DisMaxQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -78,65 +69,30 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return the tie breaker score
-     * @see DisMaxQueryBuilder#tieBreaker(float)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public float tieBreaker() {
-        return this.tieBreaker;
+    public DisMaxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("tie_breaker", tieBreaker);
+        builder.startObject(DisMaxQueryParser.NAME);
+        if (tieBreaker != -1) {
+            builder.field("tie_breaker", tieBreaker);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.startArray("queries");
         for (QueryBuilder queryBuilder : queries) {
             queryBuilder.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // return null if there are no queries at all
-        Collection<Query> luceneQueries = toQueries(queries, context);
-        if (luceneQueries.isEmpty()) {
-            return null;
-        }
-
-        return new DisjunctionMaxQuery(luceneQueries, tieBreaker);
-    }
-
-    @Override
-    protected DisMaxQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        DisMaxQueryBuilder disMax = new DisMaxQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        disMax.queries.addAll(queryBuilders);
-        disMax.tieBreaker = in.readFloat();
-        return disMax;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, queries);
-        out.writeFloat(tieBreaker);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queries, tieBreaker);
-    }
-
-    @Override
-    protected boolean doEquals(DisMaxQueryBuilder other) {
-        return Objects.equals(queries, other.queries) &&
-               Objects.equals(tieBreaker, other.tieBreaker);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
index 56aaccf..e978067 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -29,23 +31,29 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for dis_max query
+ *
  */
-public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
+public class DisMaxQueryParser implements QueryParser {
+
+    public static final String NAME = "dis_max";
+
+    @Inject
+    public DisMaxQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{DisMaxQueryBuilder.NAME, Strings.toCamelCase(DisMaxQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public DisMaxQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        float tieBreaker = DisMaxQueryBuilder.DEFAULT_TIE_BREAKER;
+        float boost = 1.0f;
+        float tieBreaker = 0.0f;
 
-        final List<QueryBuilder> queries = new ArrayList<>();
+        List<Query> queries = new ArrayList<>();
         boolean queriesFound = false;
         String queryName = null;
 
@@ -57,8 +65,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    queries.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        queries.add(query);
+                    }
                 } else {
                     throw new ParsingException(parseContext, "[dis_max] query does not support [" + currentFieldName + "]");
                 }
@@ -66,8 +76,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
                     while (token != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        queries.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            queries.add(query);
+                        }
                         token = parser.nextToken();
                     }
                 } else {
@@ -90,18 +102,15 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             throw new ParsingException(parseContext, "[dis_max] requires 'queries' field");
         }
 
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        disMaxQuery.tieBreaker(tieBreaker);
-        disMaxQuery.queryName(queryName);
-        disMaxQuery.boost(boost);
-        for (QueryBuilder query : queries) {
-            disMaxQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        return disMaxQuery;
-    }
 
-    @Override
-    public DisMaxQueryBuilder getBuilderPrototype() {
-        return DisMaxQueryBuilder.PROTOTYPE;
+        DisjunctionMaxQuery query = new DisjunctionMaxQuery(queries, tieBreaker);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
deleted file mode 100644
index 7d1761a..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-
-/**
- * A {@link QueryBuilder} that is a stand in replacement for an empty query clause in the DSL.
- * The current DSL allows parsing inner queries / filters like "{ }", in order to have a
- * valid non-null representation of these clauses that actually do nothing we can use this class.
- *
- * This builder has no corresponding parser and it is not registered under the query name. It is
- * intended to be used internally as a stand-in for nested queries that are left empty and should
- * be ignored upstream.
- */
-public class EmptyQueryBuilder extends ToXContentToBytes implements QueryBuilder<EmptyQueryBuilder> {
-
-    public static final String NAME = "empty_query";
-
-    /** the one and only empty query builder */
-    public static final EmptyQueryBuilder PROTOTYPE = new EmptyQueryBuilder();
-
-    // prevent instances other than prototype
-    private EmptyQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public String getName() {
-        return getWriteableName();
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public Query toQuery(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-    @Override
-    public Query toFilter(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    public EmptyQueryBuilder readFrom(StreamInput in) throws IOException {
-        return EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    @Override
-    public EmptyQueryBuilder queryName(String queryName) {
-        //no-op
-        return this;
-    }
-
-    @Override
-    public String queryName() {
-        return null;
-    }
-
-    @Override
-    public float boost() {
-        return -1;
-    }
-
-    @Override
-    public EmptyQueryBuilder boost(float boost) {
-        //no-op
-        return this;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
index 89a738e..9980d81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
@@ -19,124 +19,38 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
  * Constructs a query that only match on documents that the field has a value in them.
  */
-public class ExistsQueryBuilder extends AbstractQueryBuilder<ExistsQueryBuilder> {
+public class ExistsQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "exists";
+    private String name;
 
-    private final String fieldName;
+    private String queryName;
 
-    static final ExistsQueryBuilder PROTOTYPE = new ExistsQueryBuilder("field");
-
-    public ExistsQueryBuilder(String fieldName) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        this.fieldName = fieldName;
+    public ExistsQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
-     * @return the field name that has to exist for this query to match
+     * Sets the query name for the query that can be used when searching for matched_queries per hit.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public ExistsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldName);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern) {
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
+        builder.startObject(ExistsQueryParser.NAME);
+        builder.field("field", name);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            // no fields exists, so we should not match anything
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        BooleanQuery.Builder boolFilterBuilder = new BooleanQuery.Builder();
-        for (String field : fields) {
-            MappedFieldType fieldType = context.fieldMapper(field);
-            Query filter = null;
-            if (fieldNamesFieldType.isEnabled()) {
-                final String f;
-                if (fieldType != null) {
-                    f = fieldType.names().indexName();
-                } else {
-                    f = field;
-                }
-                filter = fieldNamesFieldType.termQuery(f, context);
-            }
-            // if _field_names are not indexed, we need to go the slow way
-            if (filter == null && fieldType != null) {
-                filter = fieldType.rangeQuery(null, null, true, true);
-            }
-            if (filter == null) {
-                filter = new TermRangeQuery(field, null, null, true, true);
-            }
-            boolFilterBuilder.add(filter, BooleanClause.Occur.SHOULD);
-        }
-        return new ConstantScoreQuery(boolFilterBuilder.build());
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(ExistsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    protected ExistsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ExistsQueryBuilder(in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
index 2ab6481..de2cbe7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
@@ -19,29 +19,40 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.*;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for exists query
+ *
  */
-public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
+public class ExistsQueryParser implements QueryParser {
+
+    public static final String NAME = "exists";
+
+    @Inject
+    public ExistsQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{ExistsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public ExistsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -53,8 +64,6 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
                     fieldPattern = parser.text();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[exists] query does not support [" + currentFieldName + "]");
                 }
@@ -65,14 +74,56 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
             throw new ParsingException(parseContext, "exists must be provided with a [field]");
         }
 
-        ExistsQueryBuilder builder = new ExistsQueryBuilder(fieldPattern);
-        builder.queryName(queryName);
-        builder.boost(boost);
-        return builder;
+        return newFilter(parseContext, fieldPattern, queryName);
     }
 
-    @Override
-    public ExistsQueryBuilder getBuilderPrototype() {
-        return ExistsQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, String queryName) {
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            // no fields exists, so we should not match anything
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        BooleanQuery.Builder boolFilterBuilder = new BooleanQuery.Builder();
+        for (String field : fields) {
+            MappedFieldType fieldType = parseContext.fieldMapper(field);
+            Query filter = null;
+            if (fieldNamesFieldType.isEnabled()) {
+                final String f;
+                if (fieldType != null) {
+                    f = fieldType.names().indexName();
+                } else {
+                    f = field;
+                }
+                filter = fieldNamesFieldType.termQuery(f, parseContext);
+            }
+            // if _field_names are not indexed, we need to go the slow way
+            if (filter == null && fieldType != null) {
+                filter = fieldType.rangeQuery(null, null, true, true);
+            }
+            if (filter == null) {
+                filter = new TermRangeQuery(field, null, null, true, true);
+            }
+            boolFilterBuilder.add(filter, BooleanClause.Occur.SHOULD);
+        }
+
+        BooleanQuery boolFilter = boolFilterBuilder.build();
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, boolFilter);
+        }
+        return new ConstantScoreQuery(boolFilter);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
index 671cfda..c118416 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
@@ -19,106 +19,52 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class FieldMaskingSpanQueryBuilder extends AbstractQueryBuilder<FieldMaskingSpanQueryBuilder> implements SpanQueryBuilder<FieldMaskingSpanQueryBuilder>{
-
-    public static final String NAME = "field_masking_span";
+public class FieldMaskingSpanQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<FieldMaskingSpanQueryBuilder> {
 
     private final SpanQueryBuilder queryBuilder;
 
-    private final String fieldName;
+    private final String field;
 
-    static final FieldMaskingSpanQueryBuilder PROTOTYPE = new FieldMaskingSpanQueryBuilder(new SpanTermQueryBuilder("field", "text"), "field");
+    private float boost = -1;
 
-    /**
-     * Constructs a new {@link FieldMaskingSpanQueryBuilder} given an inner {@link SpanQueryBuilder} for
-     * a given field
-     * @param queryBuilder inner {@link SpanQueryBuilder}
-     * @param fieldName the field name
-     */
-    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String fieldName) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner clause [query] cannot be null.");
-        }
+    private String queryName;
+
+
+    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String field) {
         this.queryBuilder = queryBuilder;
-        this.fieldName = fieldName;
+        this.field = field;
     }
 
-    /**
-     * @return the field name for this query
-     */
-    public String fieldName() {
-        return this.fieldName;
+    @Override
+    public FieldMaskingSpanQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the inner {@link QueryBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public FieldMaskingSpanQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(FieldMaskingSpanQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        String fieldInQuery = fieldName;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
+        builder.field("field", field);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        Query innerQuery = queryBuilder.toQuery(context);
-        assert innerQuery instanceof SpanQuery;
-        return new FieldMaskingSpanQuery((SpanQuery)innerQuery, fieldInQuery);
-    }
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new FieldMaskingSpanQueryBuilder((SpanQueryBuilder) innerQueryBuilder, in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeString(fieldName);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder, fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(FieldMaskingSpanQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-               Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
index 52c2707..9a85791 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
@@ -19,29 +19,40 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+
 import java.io.IOException;
 
 /**
- * Parser for field_masking_span query
+ *
  */
-public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpanQueryBuilder> {
+public class FieldMaskingSpanQueryParser implements QueryParser {
+
+    public static final String NAME = "field_masking_span";
+
+    @Inject
+    public FieldMaskingSpanQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{FieldMaskingSpanQueryBuilder.NAME, Strings.toCamelCase(FieldMaskingSpanQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public FieldMaskingSpanQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder inner = null;
+        SpanQuery inner = null;
         String field = null;
         String queryName = null;
 
@@ -52,11 +63,11 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
-                        throw new ParsingException(parseContext, "[field_masking_span] query must be of type span query");
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
+                        throw new ParsingException(parseContext, "[field_masking_span] query] must be of type span query");
                     }
-                    inner = (SpanQueryBuilder) query;
+                    inner = (SpanQuery) query;
                 } else {
                     throw new ParsingException(parseContext, "[field_masking_span] query does not support ["
                             + currentFieldName + "]");
@@ -80,14 +91,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
             throw new ParsingException(parseContext, "field_masking_span must have [field] set for it");
         }
 
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(inner, field);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
+        MappedFieldType fieldType = parseContext.fieldMapper(field);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        }
 
-    @Override
-    public FieldMaskingSpanQueryBuilder getBuilderPrototype() {
-        return FieldMaskingSpanQueryBuilder.PROTOTYPE;
+        FieldMaskingSpanQuery query = new FieldMaskingSpanQuery(inner, field);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
index 7820f1b..23557b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
@@ -19,273 +19,177 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> implements MultiTermQueryBuilder<FuzzyQueryBuilder> {
+public class FuzzyQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<FuzzyQueryBuilder> {
 
-    public static final String NAME = "fuzzy";
-
-    /** Default maximum edit distance. Defaults to AUTO. */
-    public static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
-
-    /** Default number of initial characters which will not be fuzzified. Defaults to 0. */
-    public static final int DEFAULT_PREFIX_LENGTH = FuzzyQuery.defaultPrefixLength;
-
-    /** Default maximum number of terms that the fuzzy query will expand to. Defaults to 50. */
-    public static final int DEFAULT_MAX_EXPANSIONS = FuzzyQuery.defaultMaxExpansions;
-
-    /** Default as to whether transpositions should be treated as a primitive edit operation,
-     * instead of classic Levenshtein algorithm. Defaults to false. */
-    public static final boolean DEFAULT_TRANSPOSITIONS = false;
-
-    private final String fieldName;
+    private final String name;
 
     private final Object value;
 
-    private Fuzziness fuzziness = DEFAULT_FUZZINESS;
+    private float boost = -1;
 
-    private int prefixLength = DEFAULT_PREFIX_LENGTH;
+    private Fuzziness fuzziness;
 
-    private int maxExpansions = DEFAULT_MAX_EXPANSIONS;
+    private Integer prefixLength;
 
+    private Integer maxExpansions;
+    
     //LUCENE 4 UPGRADE  we need a testcase for this + documentation
-    private boolean transpositions = DEFAULT_TRANSPOSITIONS;
+    private Boolean transpositions;
 
     private String rewrite;
 
-    static final FuzzyQueryBuilder PROTOTYPE = new FuzzyQueryBuilder();
+    private String queryName;
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
+    // NO COMMIT: not sure we should also allow boolean?
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
+     * @param name  The name of the field
+     * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, Object value) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name cannot be null or empty.");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("query value cannot be null");
-        }
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
-    }
-
-    private FuzzyQueryBuilder() {
-        // for protoype
-        this.fieldName = null;
-        this.value = null;
+    public FuzzyQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public FuzzyQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public FuzzyQueryBuilder fuzziness(Fuzziness fuzziness) {
-        this.fuzziness = (fuzziness == null) ? DEFAULT_FUZZINESS : fuzziness;
+        this.fuzziness = fuzziness;
         return this;
     }
 
-    public Fuzziness fuzziness() {
-        return this.fuzziness;
-    }
-
     public FuzzyQueryBuilder prefixLength(int prefixLength) {
         this.prefixLength = prefixLength;
         return this;
     }
 
-    public int prefixLength() {
-        return this.prefixLength;
-    }
-
     public FuzzyQueryBuilder maxExpansions(int maxExpansions) {
         this.maxExpansions = maxExpansions;
         return this;
     }
-
-    public int maxExpansions() {
-        return this.maxExpansions;
-    }
-
+    
     public FuzzyQueryBuilder transpositions(boolean transpositions) {
       this.transpositions = transpositions;
       return this;
     }
 
-    public boolean transpositions() {
-        return this.transpositions;
-    }
-
     public FuzzyQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FuzzyQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        fuzziness.toXContent(builder, params);
-        builder.field("prefix_length", prefixLength);
-        builder.field("max_expansions", maxExpansions);
-        builder.field("transpositions", transpositions);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(FuzzyQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", value);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        if (rewrite == null && context.isFilter()) {
-            rewrite = QueryParsers.CONSTANT_SCORE.getPreferredName();
+        if (transpositions != null) {
+            builder.field("transpositions", transpositions);
         }
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        if (fuzziness != null) {
+            fuzziness.toXContent(builder, params);
         }
-        if (query == null) {
-            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
-            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        if (prefixLength != null) {
+            builder.field("prefix_length", prefixLength);
         }
-        if (query instanceof MultiTermQuery) {
-            MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        if (maxExpansions != null) {
+            builder.field("max_expansions", maxExpansions);
         }
-        return query;
-    }
-
-    @Override
-    public FuzzyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(in.readString(), in.readGenericValue());
-        fuzzyQueryBuilder.fuzziness = Fuzziness.readFuzzinessFrom(in);
-        fuzzyQueryBuilder.prefixLength = in.readVInt();
-        fuzzyQueryBuilder.maxExpansions = in.readVInt();
-        fuzzyQueryBuilder.transpositions = in.readBoolean();
-        fuzzyQueryBuilder.rewrite = in.readOptionalString();
-        return fuzzyQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.value);
-        this.fuzziness.writeTo(out);
-        out.writeVInt(this.prefixLength);
-        out.writeVInt(this.maxExpansions);
-        out.writeBoolean(this.transpositions);
-        out.writeOptionalString(this.rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, fuzziness, prefixLength, maxExpansions, transpositions, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(FuzzyQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(fuzziness, other.fuzziness) &&
-                Objects.equals(prefixLength, other.prefixLength) &&
-                Objects.equals(maxExpansions, other.maxExpansions) &&
-                Objects.equals(transpositions, other.transpositions) &&
-                Objects.equals(rewrite, other.rewrite);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
index d4e3c15..c2013ea 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
@@ -19,45 +19,61 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
-public class FuzzyQueryParser extends BaseQueryParser {
+/**
+ *
+ */
+public class FuzzyQueryParser implements QueryParser {
 
+    public static final String NAME = "fuzzy";
+    private static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("min_similarity");
 
+
+    @Inject
+    public FuzzyQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{ FuzzyQueryBuilder.NAME };
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new ParsingException(parseContext, "[fuzzy] query malformed, no field");
         }
-        
         String fieldName = parser.currentName();
-        Object value = null;
-
-        Fuzziness fuzziness = FuzzyQueryBuilder.DEFAULT_FUZZINESS;
-        int prefixLength = FuzzyQueryBuilder.DEFAULT_PREFIX_LENGTH;
-        int maxExpansions = FuzzyQueryBuilder.DEFAULT_MAX_EXPANSIONS;
-        boolean transpositions = FuzzyQueryBuilder.DEFAULT_TRANSPOSITIONS;
-        String rewrite = null;
 
+        Object value = null;
+        float boost = 1.0f;
+        Fuzziness fuzziness = DEFAULT_FUZZINESS;
+        int prefixLength = FuzzyQuery.defaultPrefixLength;
+        int maxExpansions = FuzzyQuery.defaultMaxExpansions;
+        boolean transpositions = FuzzyQuery.defaultTranspositions;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-
+        MultiTermQuery.RewriteMethod rewriteMethod = null;
+        if (parseContext.isFilter()) {
+            rewriteMethod = MultiTermQuery.CONSTANT_SCORE_REWRITE;
+        }
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
             String currentFieldName = null;
@@ -78,9 +94,9 @@ public class FuzzyQueryParser extends BaseQueryParser {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         maxExpansions = parser.intValue();
                     } else if ("transpositions".equals(currentFieldName)) {
-                        transpositions = parser.booleanValue();
+                      transpositions = parser.booleanValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull(), null);
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -96,20 +112,26 @@ public class FuzzyQueryParser extends BaseQueryParser {
         }
 
         if (value == null) {
-            throw new ParsingException(parseContext, "no value specified for fuzzy query");
+            throw new ParsingException(parseContext, "No value specified for fuzzy query");
         }
-        return new FuzzyQueryBuilder(fieldName, value)
-                .fuzziness(fuzziness)
-                .prefixLength(prefixLength)
-                .maxExpansions(maxExpansions)
-                .transpositions(transpositions)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        }
+        if (query == null) {
+            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
+            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        }
+        if (query instanceof MultiTermQuery) {
+            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        }
+        query.setBoost(boost);
 
-    @Override
-    public FuzzyQueryBuilder getBuilderPrototype() {
-        return FuzzyQueryBuilder.PROTOTYPE;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index c726992..0f08a7c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -19,348 +19,174 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Numbers;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
-import org.elasticsearch.index.search.geo.IndexedGeoBoundingBoxQuery;
 
 import java.io.IOException;
-import java.util.Objects;
 
-/**
- * Creates a Lucene query that will filter for all documents that lie within the specified
- * bounding box.
- *
- * This query can only operate on fields of type geo_point that have latitude and longitude
- * enabled.
- * */
-public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBoundingBoxQueryBuilder> {
-    /** Name of the query. */
-    public static final String NAME = "geo_bbox";
-    /** Default for geo point coerce (as of this writing false). */
-    public static final boolean DEFAULT_COERCE = false;
-    /** Default for skipping geo point validation (as of this writing false). */
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
-    /** Default type for executing this query (memory as of this writing). */
-    public static final GeoExecType DEFAULT_TYPE = GeoExecType.MEMORY;
-    /** Needed for serialization. */
-    static final GeoBoundingBoxQueryBuilder PROTOTYPE = new GeoBoundingBoxQueryBuilder("");
+public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
 
-    /** Name of field holding geo coordinates to compute the bounding box on.*/
-    private final String fieldName;
-    /** Top left corner coordinates of bounding box. */
-    private GeoPoint topLeft = new GeoPoint(Double.NaN, Double.NaN);
-    /** Bottom right corner coordinates of bounding box.*/
-    private GeoPoint bottomRight = new GeoPoint(Double.NaN, Double.NaN);
-    /** Whether or not to infer correct coordinates for wrapping bounding boxes.*/
-    private boolean coerce = DEFAULT_COERCE;
-    /** Whether or not to skip geo point validation. */
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
-    /** How the query should be run. */
-    private GeoExecType type = DEFAULT_TYPE;
+    public static final String TOP_LEFT = GeoBoundingBoxQueryParser.TOP_LEFT;
+    public static final String BOTTOM_RIGHT = GeoBoundingBoxQueryParser.BOTTOM_RIGHT;
 
-    /**
-     * Create new bounding box query.
-     * @param fieldName name of index field containing geo coordinates to operate on.
-     * */
-    public GeoBoundingBoxQueryBuilder(String fieldName) {
-        if (fieldName == null) {
-            throw new IllegalArgumentException("Field name must not be empty.");
-        }
-        this.fieldName = fieldName;
+    private static final int TOP = 0;
+    private static final int LEFT = 1;
+    private static final int BOTTOM = 2;
+    private static final int RIGHT = 3;
+    
+    private final String name;
+
+    private double[] box = {Double.NaN, Double.NaN, Double.NaN, Double.NaN};
+
+    private String queryName;
+    private String type;
+    private Boolean coerce;
+    private Boolean ignoreMalformed;
+
+    public GeoBoundingBoxQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
      * Adds top left point.
-     * @param top The top latitude
-     * @param left The left longitude
-     * @param bottom The bottom latitude
-     * @param right The right longitude
+     *
+     * @param lat The latitude
+     * @param lon The longitude
      */
-    public GeoBoundingBoxQueryBuilder setCorners(double top, double left, double bottom, double right) {
-        if (!ignoreMalformed) {
-            if (Numbers.isValidDouble(top) == false) {
-                throw new IllegalArgumentException("top latitude is invalid: " + top);
-            }
-            if (Numbers.isValidDouble(left) == false) {
-                throw new IllegalArgumentException("left longitude is invalid: " + left);
-            }
-            if (Numbers.isValidDouble(bottom) == false) {
-                throw new IllegalArgumentException("bottom latitude is invalid: " + bottom);
-            }
-            if (Numbers.isValidDouble(right) == false) {
-                throw new IllegalArgumentException("right longitude is invalid: " + right);
-            }
-
-            // all corners are valid after above checks - make sure they are in the right relation
-            if (top < bottom) {
-                throw new IllegalArgumentException("top is below bottom corner: " +
-                            top + " vs. " + bottom);
-                }
-
-                // we do not check longitudes as the query generation code can deal with flipped left/right values
-        }
-        
-        topLeft.reset(top, left);
-        bottomRight.reset(bottom, right);
+    public GeoBoundingBoxQueryBuilder topLeft(double lat, double lon) {
+        box[TOP] = lat;
+        box[LEFT] = lon;
         return this;
     }
 
-    /**
-     * Adds points.
-     * @param topLeft topLeft point to add.
-     * @param bottomRight bottomRight point to add.
-     * */
-    public GeoBoundingBoxQueryBuilder setCorners(GeoPoint topLeft, GeoPoint bottomRight) {
-        return setCorners(topLeft.getLat(), topLeft.getLon(), bottomRight.getLat(), bottomRight.getLon());
+    public GeoBoundingBoxQueryBuilder topLeft(GeoPoint point) {
+        return topLeft(point.lat(), point.lon());
+    }
+
+    public GeoBoundingBoxQueryBuilder topLeft(String geohash) {
+        return topLeft(GeoPoint.fromGeohash(geohash));
     }
 
     /**
-     * Adds points.
-     * @param topLeft topLeft point to add as geohash.
-     * @param bottomRight bottomRight point to add as geohash.
-     * */
-    public GeoBoundingBoxQueryBuilder setCorners(String topLeft, String bottomRight) {
-        return setCorners(GeoPoint.fromGeohash(topLeft), GeoPoint.fromGeohash(bottomRight));
+     * Adds bottom right corner.
+     *
+     * @param lat The latitude
+     * @param lon The longitude
+     */
+    public GeoBoundingBoxQueryBuilder bottomRight(double lat, double lon) {
+        box[BOTTOM] = lat;
+        box[RIGHT] = lon;
+        return this;
     }
 
-    /** Returns the top left corner of the bounding box. */
-    public GeoPoint topLeft() {
-        return topLeft;
+    public GeoBoundingBoxQueryBuilder bottomRight(GeoPoint point) {
+        return bottomRight(point.lat(), point.lon());
     }
-    
-    /** Returns the bottom right corner of the bounding box. */
-    public GeoPoint bottomRight() {
-        return bottomRight;
+
+    public GeoBoundingBoxQueryBuilder bottomRight(String geohash) {
+        return bottomRight(GeoPoint.fromGeohash(geohash));
     }
 
     /**
-     * Adds corners in OGC standard bbox/ envelop format.
+     * Adds bottom left corner.
      *
-     * @param bottomLeft bottom left corner of bounding box.
-     * @param topRight top right corner of bounding box.
+     * @param lat The latitude
+     * @param lon The longitude
      */
-    public GeoBoundingBoxQueryBuilder setCornersOGC(GeoPoint bottomLeft, GeoPoint topRight) {
-        return setCorners(topRight.getLat(), bottomLeft.getLon(), bottomLeft.getLat(), topRight.getLon());
+    public GeoBoundingBoxQueryBuilder bottomLeft(double lat, double lon) {
+        box[BOTTOM] = lat;
+        box[LEFT] = lon;
+        return this;
+    }
+
+    public GeoBoundingBoxQueryBuilder bottomLeft(GeoPoint point) {
+        return bottomLeft(point.lat(), point.lon());
     }
 
+    public GeoBoundingBoxQueryBuilder bottomLeft(String geohash) {
+        return bottomLeft(GeoPoint.fromGeohash(geohash));
+    }
+    
     /**
-     * Adds corners in OGC standard bbox/ envelop format.
+     * Adds top right point.
      *
-     * @param bottomLeft bottom left corner geohash.
-     * @param topRight top right corner geohash.
+     * @param lat The latitude
+     * @param lon The longitude
      */
-    public GeoBoundingBoxQueryBuilder setCornersOGC(String bottomLeft, String topRight) {
-        return setCornersOGC(GeoPoint.fromGeohash(bottomLeft), GeoPoint.fromGeohash(topRight));
+    public GeoBoundingBoxQueryBuilder topRight(double lat, double lon) {
+        box[TOP] = lat;
+        box[RIGHT] = lon;
+        return this;
     }
 
-    /**
-     * Specify whether or not to try and fix broken/wrapping bounding boxes.
-     * If set to true, also enables ignoreMalformed thus disabling geo point
-     * validation altogether.
-     **/
-    public GeoBoundingBoxQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
-        this.coerce = coerce;
-        return this;
+    public GeoBoundingBoxQueryBuilder topRight(GeoPoint point) {
+        return topRight(point.lat(), point.lon());
     }
 
-    /** Returns whether or not to try and fix broken/wrapping bounding boxes. */
-    public boolean coerce() {
-        return this.coerce;
+    public GeoBoundingBoxQueryBuilder topRight(String geohash) {
+        return topRight(GeoPoint.fromGeohash(geohash));
     }
 
     /**
-     * Specify whether or not to ignore validation errors of bounding boxes.
-     * Can only be set if coerce set to false, otherwise calling this
-     * method has no effect.
-     **/
-    public GeoBoundingBoxQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoBoundingBoxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    /** Returns whether or not to skip bounding box validation. */
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
+    public GeoBoundingBoxQueryBuilder coerce(boolean coerce) {
+        this.coerce = coerce;
+        return this;
     }
 
-    /**
-     * Sets the type of executing of the geo bounding box. Can be either `memory` or `indexed`. Defaults
-     * to `memory`.
-     */
-    public GeoBoundingBoxQueryBuilder type(GeoExecType type) {
-        if (type == null) {
-            throw new IllegalArgumentException("Type is not allowed to be null.");
-        }
-        this.type = type;
+    public GeoBoundingBoxQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
+        this.ignoreMalformed = ignoreMalformed;
         return this;
     }
 
     /**
-     * For BWC: Parse type from type name.
-     * */
+     * Sets the type of executing of the geo bounding box. Can be either `memory` or `indexed`. Defaults
+     * to `memory`.
+     */
     public GeoBoundingBoxQueryBuilder type(String type) {
-        this.type = GeoExecType.fromString(type);
+        this.type = type;
         return this;
     }
-    /** Returns the execution type of the geo bounding box.*/
-    public GeoExecType type() {
-        return type;
-    }
-
-    /** Returns the name of the field to base the bounding box computation on. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    QueryValidationException checkLatLon(boolean indexCreatedBeforeV2_0) {
-        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
-        if (ignoreMalformed || indexCreatedBeforeV2_0) {
-            return null;
-        }
-
-        QueryValidationException validationException = null;
-        // For everything post 2.0 validate latitude and longitude unless validation was explicitly turned off
-        if (GeoUtils.isValidLatitude(topLeft.getLat()) == false) {
-            validationException = addValidationError("top latitude is invalid: " + topLeft.getLat(),
-                    validationException);
-        }
-        if (GeoUtils.isValidLongitude(topLeft.getLon()) == false) {
-            validationException = addValidationError("left longitude is invalid: " + topLeft.getLon(),
-                    validationException);
-        }
-        if (GeoUtils.isValidLatitude(bottomRight.getLat()) == false) {
-            validationException = addValidationError("bottom latitude is invalid: " + bottomRight.getLat(),
-                    validationException);
-        }
-        if (GeoUtils.isValidLongitude(bottomRight.getLon()) == false) {
-            validationException = addValidationError("right longitude is invalid: " + bottomRight.getLon(),
-                    validationException);
-        }
-        return validationException;
-    }
 
     @Override
-    public Query doToQuery(QueryShardContext context) {
-        QueryValidationException exception = checkLatLon(context.indexVersionCreated().before(Version.V_2_0_0));
-        if (exception != null) {
-            throw new QueryShardException(context, "couldn't validate latitude/ longitude values", exception);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        // check values
+        if(Double.isNaN(box[TOP])) {
+            throw new IllegalArgumentException("geo_bounding_box requires top latitude to be set");
+        } else if(Double.isNaN(box[BOTTOM])) {
+            throw new IllegalArgumentException("geo_bounding_box requires bottom latitude to be set");
+        } else if(Double.isNaN(box[RIGHT])) {
+            throw new IllegalArgumentException("geo_bounding_box requires right longitude to be set");
+        } else if(Double.isNaN(box[LEFT])) {
+            throw new IllegalArgumentException("geo_bounding_box requires left longitude to be set");
         }
+                
+        builder.startObject(GeoBoundingBoxQueryParser.NAME);
 
-        GeoPoint luceneTopLeft = new GeoPoint(topLeft);
-        GeoPoint luceneBottomRight = new GeoPoint(bottomRight);
-        if (coerce) {
-            // Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for
-            // the complete longitude range so need to set longitude to the complete longditude range
-            double right = luceneBottomRight.getLon();
-            double left = luceneTopLeft.getLon();
+        builder.startObject(name);
+        builder.array(TOP_LEFT, box[LEFT], box[TOP]);
+        builder.array(BOTTOM_RIGHT, box[RIGHT], box[BOTTOM]);
+        builder.endObject();
 
-            boolean completeLonRange = ((right - left) % 360 == 0 && right > left);
-            GeoUtils.normalizePoint(luceneTopLeft, true, !completeLonRange);
-            GeoUtils.normalizePoint(luceneBottomRight, true, !completeLonRange);
-            if (completeLonRange) {
-                luceneTopLeft.resetLon(-180);
-                luceneBottomRight.resetLon(180);
-            }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType == null) {
-            throw new QueryShardException(context, "failed to find geo_point field [" + fieldName + "]");
+        if (type != null) {
+            builder.field("type", type);
         }
-        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
-            throw new QueryShardException(context, "field [" + fieldName + "] is not a geo_point field");
+        if (coerce != null) {
+            builder.field("coerce", coerce);
         }
-        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
-
-        Query result;
-        switch(type) {
-            case INDEXED:
-                result = IndexedGeoBoundingBoxQuery.create(luceneTopLeft, luceneBottomRight, geoFieldType);
-                break;
-            case MEMORY:
-                IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
-                result = new InMemoryGeoBoundingBoxQuery(luceneTopLeft, luceneBottomRight, indexFieldData);
-                break;
-            default:
-                // Someone extended the type enum w/o adjusting this switch statement.
-                throw new IllegalStateException("geo bounding box type [" + type + "] not supported.");
+        if (ignoreMalformed != null) {
+            builder.field("ignore_malformed", ignoreMalformed);
         }
 
-        return result;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-
-        builder.startObject(fieldName);
-        builder.array(GeoBoundingBoxQueryParser.TOP_LEFT, topLeft.getLon(), topLeft.getLat());
-        builder.array(GeoBoundingBoxQueryParser.BOTTOM_RIGHT, bottomRight.getLon(), bottomRight.getLat());
-        builder.endObject();
-        builder.field("coerce", coerce);
-        builder.field("ignore_malformed", ignoreMalformed);
-        builder.field("type", type);
-
-        printBoostAndQueryName(builder);
-
         builder.endObject();
     }
-
-    @Override
-    public boolean doEquals(GeoBoundingBoxQueryBuilder other) {
-        return Objects.equals(topLeft, other.topLeft) &&
-                Objects.equals(bottomRight, other.bottomRight) &&
-                Objects.equals(type, other.type) &&
-                Objects.equals(coerce, other.coerce) &&
-                Objects.equals(ignoreMalformed, other.ignoreMalformed) &&
-                Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(topLeft, bottomRight, type, coerce, ignoreMalformed, fieldName);
-    }
-
-    @Override
-    public GeoBoundingBoxQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        String fieldName = in.readString();
-        GeoBoundingBoxQueryBuilder geo = new GeoBoundingBoxQueryBuilder(fieldName);
-        geo.topLeft = geo.topLeft.readFrom(in);
-        geo.bottomRight = geo.bottomRight.readFrom(in);
-        geo.type = GeoExecType.readTypeFrom(in);
-        geo.coerce = in.readBoolean();
-        geo.ignoreMalformed = in.readBoolean();
-        return geo;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        topLeft.writeTo(out);
-        bottomRight.writeTo(out);
-        type.writeTo(out);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
index d0c752b..3d5d848 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
@@ -19,54 +19,57 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.Version;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
+import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
+import org.elasticsearch.index.search.geo.IndexedGeoBoundingBoxQuery;
 
 import java.io.IOException;
 
-public class GeoBoundingBoxQueryParser extends BaseQueryParser<GeoBoundingBoxQueryBuilder> {
+/**
+ *
+ */
+public class GeoBoundingBoxQueryParser implements QueryParser {
 
     public static final String NAME = "geo_bbox";
 
-    /** Key to refer to the top of the bounding box. */
     public static final String TOP = "top";
-    /** Key to refer to the left of the bounding box. */
     public static final String LEFT = "left";
-    /** Key to refer to the right of the bounding box. */
     public static final String RIGHT = "right";
-    /** Key to refer to the bottom of the bounding box. */
     public static final String BOTTOM = "bottom";
 
-    /** Key to refer to top_left corner of bounding box. */
     public static final String TOP_LEFT = TOP + "_" + LEFT;
-    /** Key to refer to bottom_right corner of bounding box. */
-    public static final String BOTTOM_RIGHT = BOTTOM + "_" + RIGHT;
-    /** Key to refer to top_right corner of bounding box. */
     public static final String TOP_RIGHT = TOP + "_" + RIGHT;
-    /** Key to refer to bottom left corner of bounding box.  */
     public static final String BOTTOM_LEFT = BOTTOM + "_" + LEFT;
+    public static final String BOTTOM_RIGHT = BOTTOM + "_" + RIGHT;
 
-    /** Key to refer to top_left corner of bounding box. */
     public static final String TOPLEFT = "topLeft";
-    /** Key to refer to bottom_right corner of bounding box. */
-    public static final String BOTTOMRIGHT = "bottomRight";
-    /** Key to refer to top_right corner of bounding box. */
     public static final String TOPRIGHT = "topRight";
-    /** Key to refer to bottom left corner of bounding box.  */
     public static final String BOTTOMLEFT = "bottomLeft";
+    public static final String BOTTOMRIGHT = "bottomRight";
 
     public static final String FIELD = "field";
 
+    @Inject
+    public GeoBoundingBoxQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{GeoBoundingBoxQueryBuilder.NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
+        return new String[]{NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
     }
 
     @Override
-    public GeoBoundingBoxQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -75,16 +78,16 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParser<GeoBoundingBoxQue
         double bottom = Double.NaN;
         double left = Double.NaN;
         double right = Double.NaN;
-
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
-        boolean coerce = GeoBoundingBoxQueryBuilder.DEFAULT_COERCE;
-        boolean ignoreMalformed = GeoBoundingBoxQueryBuilder.DEFAULT_IGNORE_MALFORMED;
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
+        boolean coerce = false;
+        boolean ignoreMalformed = false;
 
         GeoPoint sparse = new GeoPoint();
-
+        
         String type = "memory";
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -137,16 +140,14 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParser<GeoBoundingBoxQue
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if ("coerce".equals(currentFieldName) || ("normalize".equals(currentFieldName))) {
+                } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
-                    if (coerce) {
+                    if (coerce == true) {
                         ignoreMalformed = true;
                     }
                 } else if ("type".equals(currentFieldName)) {
                     type = parser.text();
-                } else if ("ignore_malformed".equals(currentFieldName)) {
+                } else if ("ignore_malformed".equals(currentFieldName) && coerce == false) {
                     ignoreMalformed = parser.booleanValue();
                 } else {
                     throw new ParsingException(parseContext, "failed to parse [{}] query. unexpected field [{}]", NAME, currentFieldName);
@@ -156,18 +157,57 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParser<GeoBoundingBoxQue
 
         final GeoPoint topLeft = sparse.reset(top, left);  //just keep the object
         final GeoPoint bottomRight = new GeoPoint(bottom, right);
-        GeoBoundingBoxQueryBuilder builder = new GeoBoundingBoxQueryBuilder(fieldName);
-        builder.setCorners(topLeft, bottomRight);
-        builder.queryName(queryName);
-        builder.boost(boost);
-        builder.type(GeoExecType.fromString(type));
-        builder.coerce(coerce);
-        builder.ignoreMalformed(ignoreMalformed);
-        return builder;
-    }
 
-    @Override
-    public GeoBoundingBoxQueryBuilder getBuilderPrototype() {
-        return GeoBoundingBoxQueryBuilder.PROTOTYPE;
-    }
+        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
+        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+            if (topLeft.lat() > 90.0 || topLeft.lat() < -90.0) {
+                throw new ParsingException(parseContext, "illegal latitude value [{}] for [{}]", topLeft.lat(), NAME);
+            }
+            if (topLeft.lon() > 180.0 || topLeft.lon() < -180) {
+                throw new ParsingException(parseContext, "illegal longitude value [{}] for [{}]", topLeft.lon(), NAME);
+            }
+            if (bottomRight.lat() > 90.0 || bottomRight.lat() < -90.0) {
+                throw new ParsingException(parseContext, "illegal latitude value [{}] for [{}]", bottomRight.lat(), NAME);
+            }
+            if (bottomRight.lon() > 180.0 || bottomRight.lon() < -180) {
+                throw new ParsingException(parseContext, "illegal longitude value [{}] for [{}]", bottomRight.lon(), NAME);
+            }
+        }
+
+        if (coerce) {
+            // Special case: if the difference between the left and right is 360 and the right is greater than the left, we are asking for
+            // the complete longitude range so need to set longitude to the complete longditude range
+            boolean completeLonRange = ((right - left) % 360 == 0 && right > left);
+            GeoUtils.normalizePoint(topLeft, true, !completeLonRange);
+            GeoUtils.normalizePoint(bottomRight, true, !completeLonRange);
+            if (completeLonRange) {
+                topLeft.resetLon(-180);
+                bottomRight.resetLon(180);
+            }
+        }
+
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType == null) {
+            throw new ParsingException(parseContext, "failed to parse [{}] query. could not find [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
+        }
+        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
+            throw new ParsingException(parseContext, "failed to parse [{}] query. field [{}] is expected to be of type [{}], but is of [{}] type instead", NAME, fieldName, GeoPointFieldMapper.CONTENT_TYPE, fieldType.typeName());
+        }
+        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
+
+        Query filter;
+        if ("indexed".equals(type)) {
+            filter = IndexedGeoBoundingBoxQuery.create(topLeft, bottomRight, geoFieldType);
+        } else if ("memory".equals(type)) {
+            IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
+            filter = new InMemoryGeoBoundingBoxQuery(topLeft, bottomRight, indexFieldData);
+        } else {
+            throw new ParsingException(parseContext, "failed to parse [{}] query. geo bounding box type [{}] is not supported. either [indexed] or [memory] are allowed", NAME, type);
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, filter);
+        }
+        return filter;
+    }    
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index ad238c6..77c8f94 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -19,302 +19,122 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 
 import java.io.IOException;
 import java.util.Locale;
-import java.util.Objects;
 
-/**
- * Filter results of a query to include only those within a specific distance to some
- * geo point.
- * */
-public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQueryBuilder> {
+public class GeoDistanceQueryBuilder extends QueryBuilder {
 
-    /** Name of the query in the query dsl. */
-    public static final String NAME = "geo_distance";
-    /** Default for latitude normalization (as of this writing true).*/
-    public static final boolean DEFAULT_NORMALIZE_LAT = true;
-    /** Default for longitude normalization (as of this writing true). */
-    public static final boolean DEFAULT_NORMALIZE_LON = true;
-    /** Default for distance unit computation. */
-    public static final DistanceUnit DEFAULT_DISTANCE_UNIT = DistanceUnit.DEFAULT;
-    /** Default for geo distance computation. */
-    public static final GeoDistance DEFAULT_GEO_DISTANCE = GeoDistance.DEFAULT;
-    /** Default for optimising query through pre computed bounding box query. */
-    public static final String DEFAULT_OPTIMIZE_BBOX = "memory";
-    /** Default for coercing lon/lat values to a standard coordinate system */
-    public static final boolean DEFAULT_COERCE = false;
-    /** Default for accepting accept geo points with invalid latitude or longitude */
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
+    private final String name;
 
-    private final String fieldName;
-    /** Distance from center to cover. */
-    private double distance;
-    /** Point to use as center. */
-    private GeoPoint center = new GeoPoint(Double.NaN, Double.NaN);
-    /** Algorithm to use for distance computation. */
-    private GeoDistance geoDistance = DEFAULT_GEO_DISTANCE;
-    /** Whether or not to use a bbox for pre-filtering. TODO change to enum? */
-    private String optimizeBbox = DEFAULT_OPTIMIZE_BBOX;
-    /** Whether or not to normalize longitude and latitude values to a standard coordinate system */
-    private boolean coerce = DEFAULT_COERCE;
-    /** Whether or not to accept geo points with invalid latitude or longitude */
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
+    private String distance;
 
-    static final GeoDistanceQueryBuilder PROTOTYPE = new GeoDistanceQueryBuilder("_na_");
+    private double lat;
 
-    /**
-     * Construct new GeoDistanceQueryBuilder.
-     * @param fieldName name of indexed geo field to operate distance computation on.
-     * */
-    public GeoDistanceQueryBuilder(String fieldName) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("fieldName must not be null or empty");
-        }
-        this.fieldName = fieldName;
-    }
+    private double lon;
 
-    /** Name of the field this query is operating on. */
-    public String fieldName() {
-        return this.fieldName;
-    }
+    private String geohash;
 
-    /** Sets the center point for the query.
-     * @param point the center of the query
-     **/
-    public GeoDistanceQueryBuilder point(GeoPoint point) {
-        if (point == null) {
-            throw new IllegalArgumentException("center point must not be null");
-        }
-        this.center = point;
-        return this;
+    private GeoDistance geoDistance;
+
+    private String optimizeBbox;
+
+    private String queryName;
+
+    private Boolean coerce;
+
+    private Boolean ignoreMalformed;
+
+    public GeoDistanceQueryBuilder(String name) {
+        this.name = name;
     }
 
-    /**
-     * Sets the center point of the query.
-     * @param lat latitude of center
-     * @param lon longitude of center
-     * */
     public GeoDistanceQueryBuilder point(double lat, double lon) {
-        this.center = new GeoPoint(lat, lon);
+        this.lat = lat;
+        this.lon = lon;
         return this;
     }
 
-    /** Returns the center point of the distance query. */
-    public GeoPoint point() {
-        return this.center;
+    public GeoDistanceQueryBuilder lat(double lat) {
+        this.lat = lat;
+        return this;
     }
 
-    /** Sets the distance from the center using the default distance unit.*/
-    public GeoDistanceQueryBuilder distance(String distance) {
-        return distance(distance, DistanceUnit.DEFAULT);
+    public GeoDistanceQueryBuilder lon(double lon) {
+        this.lon = lon;
+        return this;
     }
 
-    /** Sets the distance from the center for this query. */
-    public GeoDistanceQueryBuilder distance(String distance, DistanceUnit unit) {
-        if (Strings.isEmpty(distance)) {
-            throw new IllegalArgumentException("distance must not be null or empty");
-        }
-        if (unit == null) {
-            throw new IllegalArgumentException("distance unit must not be null");
-        }
-        this.distance = DistanceUnit.parse(distance, unit, DistanceUnit.DEFAULT);
+    public GeoDistanceQueryBuilder distance(String distance) {
+        this.distance = distance;
         return this;
     }
 
-    /** Sets the distance from the center for this query. */
     public GeoDistanceQueryBuilder distance(double distance, DistanceUnit unit) {
-        return distance(Double.toString(distance), unit);
-    }
-
-    /** Returns the distance configured as radius. */
-    public double distance() {
-        return distance;
+        this.distance = unit.toString(distance);
+        return this;
     }
 
-    /** Sets the center point for this query. */
     public GeoDistanceQueryBuilder geohash(String geohash) {
-        if (Strings.isEmpty(geohash)) {
-            throw new IllegalArgumentException("geohash must not be null or empty");
-        }
-        this.center.resetFromGeoHash(geohash);
+        this.geohash = geohash;
         return this;
     }
 
-    /** Which type of geo distance calculation method to use. */
     public GeoDistanceQueryBuilder geoDistance(GeoDistance geoDistance) {
-        if (geoDistance == null) {
-            throw new IllegalArgumentException("geoDistance must not be null");
-        }
         this.geoDistance = geoDistance;
         return this;
     }
 
-    /** Returns geo distance calculation type to use. */
-    public GeoDistance geoDistance() {
-        return this.geoDistance;
-    }
-
-    /**
-     * Set this to memory or indexed if before running the distance
-     * calculation you want to limit the candidates to hits in the
-     * enclosing bounding box.
-     **/
     public GeoDistanceQueryBuilder optimizeBbox(String optimizeBbox) {
-        if (optimizeBbox == null) {
-            throw new IllegalArgumentException("optimizeBox must not be null");
-        }
-        switch (optimizeBbox) {
-            case "none":
-            case "memory":
-            case "indexed":
-                break;
-            default:
-                throw new IllegalArgumentException("optimizeBbox must be one of [none, memory, indexed]");
-        }
         this.optimizeBbox = optimizeBbox;
         return this;
     }
 
     /**
-     * Returns whether or not to run a BoundingBox query prior to
-     * distance query for optimization purposes.*/
-    public String optimizeBbox() {
-        return this.optimizeBbox;
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     public GeoDistanceQueryBuilder coerce(boolean coerce) {
         this.coerce = coerce;
-        if (this.coerce) {
-            this.ignoreMalformed = true;
-        }
         return this;
     }
 
     public GeoDistanceQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+        this.ignoreMalformed = ignoreMalformed;
         return this;
     }
 
     @Override
-    protected Query doToQuery(QueryShardContext shardContext) throws IOException {
-        QueryValidationException exception = checkLatLon(shardContext.indexVersionCreated().before(Version.V_2_0_0));
-        if (exception != null) {
-            throw new QueryShardException(shardContext, "couldn't validate latitude/ longitude values", exception);
-        }
-
-        if (coerce) {
-            GeoUtils.normalizePoint(center, coerce, coerce);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(GeoDistanceQueryParser.NAME);
+        if (geohash != null) {
+            builder.field(name, geohash);
+        } else {
+            builder.startArray(name).value(lon).value(lat).endArray();
         }
-
-        double normDistance = geoDistance.normalize(this.distance, DistanceUnit.DEFAULT);
-
-        MappedFieldType fieldType = shardContext.fieldMapper(fieldName);
-        if (fieldType == null) {
-            throw new QueryShardException(shardContext, "failed to find geo_point field [" + fieldName + "]");
+        builder.field("distance", distance);
+        if (geoDistance != null) {
+            builder.field("distance_type", geoDistance.name().toLowerCase(Locale.ROOT));
         }
-        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
-            throw new QueryShardException(shardContext, "field [" + fieldName + "] is not a geo_point field");
+        if (optimizeBbox != null) {
+            builder.field("optimize_bbox", optimizeBbox);
         }
-        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
-
-        IndexGeoPointFieldData indexFieldData = shardContext.getForField(fieldType);
-        Query query = new GeoDistanceRangeQuery(center, null, normDistance, true, false, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
-        return query;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startArray(fieldName).value(center.lon()).value(center.lat()).endArray();
-        builder.field("distance", distance);
-        builder.field("distance_type", geoDistance.name().toLowerCase(Locale.ROOT));
-        builder.field("optimize_bbox", optimizeBbox);
-        builder.field("coerce", coerce);
-        builder.field("ignore_malformed", ignoreMalformed);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(center, geoDistance, optimizeBbox, distance, coerce, ignoreMalformed);
-    }
-
-    @Override
-    public boolean doEquals(GeoDistanceQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                (distance == other.distance) &&
-                (coerce == other.coerce) &&
-                (ignoreMalformed == other.ignoreMalformed) &&
-                Objects.equals(center, other.center) &&
-                Objects.equals(optimizeBbox, other.optimizeBbox) &&
-                Objects.equals(geoDistance, other.geoDistance);
-    }
-
-    @Override
-    protected GeoDistanceQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        String fieldName = in.readString();
-        GeoDistanceQueryBuilder result = new GeoDistanceQueryBuilder(fieldName);
-        result.distance = in.readDouble();
-        result.coerce = in.readBoolean();
-        result.ignoreMalformed = in.readBoolean();
-        result.center = GeoPoint.readGeoPointFrom(in);
-        result.optimizeBbox = in.readString();
-        result.geoDistance = GeoDistance.readGeoDistanceFrom(in);
-        return result;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeDouble(distance);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
-        center.writeTo(out);
-        out.writeString(optimizeBbox);
-        geoDistance.writeTo(out);
-    }
-
-    /**
-     * @param indexCreatedBeforeV2_0
-     * @return
-     */
-    private QueryValidationException checkLatLon(boolean indexCreatedBeforeV2_0) {
-        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
-        if (ignoreMalformed || indexCreatedBeforeV2_0) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        QueryValidationException validationException = null;
-        // For everything post 2.0, validate latitude and longitude unless validation was explicitly turned off
-        if (GeoUtils.isValidLatitude(center.getLat()) == false) {
-            validationException = addValidationError("center point latitude is invalid: " + center.getLat(), validationException);
+        if (coerce != null) {
+            builder.field("coerce", coerce);
         }
-        if (GeoUtils.isValidLongitude(center.getLon()) == false) {
-            validationException = addValidationError("center point longitude is invalid: " + center.getLon(), validationException);
+        if (ignoreMalformed != null) {
+            builder.field("ignore_malformed", ignoreMalformed);
         }
-        return validationException;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index e0ce628..e0514f4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -19,19 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.elasticsearch.Version;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
+import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 
 import java.io.IOException;
 
 /**
- * Parses a GeoDistanceQuery. See also
- *
  * <pre>
  * {
  *     "name.lat" : 1.1,
@@ -39,31 +43,37 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceQueryParser extends BaseQueryParser {
+public class GeoDistanceQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance";
+
+    @Inject
+    public GeoDistanceQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceQueryBuilder.NAME, "geoDistance"};
+        return new String[]{NAME, "geoDistance"};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
-        GeoPoint point = new GeoPoint(Double.NaN, Double.NaN);
+        GeoPoint point = new GeoPoint();
         String fieldName = null;
+        double distance = 0;
         Object vDistance = null;
-        DistanceUnit unit = GeoDistanceQueryBuilder.DEFAULT_DISTANCE_UNIT;
-        GeoDistance geoDistance = GeoDistanceQueryBuilder.DEFAULT_GEO_DISTANCE;
-        String optimizeBbox = GeoDistanceQueryBuilder.DEFAULT_OPTIMIZE_BBOX;
-        boolean coerce = GeoDistanceQueryBuilder.DEFAULT_COERCE;
-        boolean ignoreMalformed = GeoDistanceQueryBuilder.DEFAULT_IGNORE_MALFORMED;
-
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance geoDistance = GeoDistance.DEFAULT;
+        String optimizeBbox = "memory";
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
+        boolean coerce = false;
+        boolean ignoreMalformed = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -93,15 +103,15 @@ public class GeoDistanceQueryParser extends BaseQueryParser {
                     }
                 }
             } else if (token.isValue()) {
-                if ("distance".equals(currentFieldName)) {
+                if (currentFieldName.equals("distance")) {
                     if (token == XContentParser.Token.VALUE_STRING) {
                         vDistance = parser.text(); // a String
                     } else {
                         vDistance = parser.numberValue(); // a Number
                     }
-                } else if ("unit".equals(currentFieldName)) {
+                } else if (currentFieldName.equals("unit")) {
                     unit = DistanceUnit.fromString(parser.text());
-                } else if ("distance_type".equals(currentFieldName) || "distanceType".equals(currentFieldName)) {
+                } else if (currentFieldName.equals("distance_type") || currentFieldName.equals("distanceType")) {
                     geoDistance = GeoDistance.fromString(parser.text());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.LAT_SUFFIX)) {
                     point.resetLat(parser.doubleValue());
@@ -114,16 +124,14 @@ public class GeoDistanceQueryParser extends BaseQueryParser {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
-                } else if ("coerce".equals(currentFieldName) || ("normalize".equals(currentFieldName))) {
+                } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
                     if (coerce == true) {
                         ignoreMalformed = true;
                     }
-                } else if ("ignore_malformed".equals(currentFieldName)) {
+                } else if ("ignore_malformed".equals(currentFieldName) && coerce == false) {
                     ignoreMalformed = parser.booleanValue();
                 } else {
                     point.resetFromString(parser.text());
@@ -132,28 +140,44 @@ public class GeoDistanceQueryParser extends BaseQueryParser {
             }
         }
 
+        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
+        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+            if (point.lat() > 90.0 || point.lat() < -90.0) {
+                throw new ParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
+            }
+            if (point.lon() > 180.0 || point.lon() < -180) {
+                throw new ParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
+            }
+        }
+
+        if (coerce) {
+            GeoUtils.normalizePoint(point, coerce, coerce);
+        }
+
         if (vDistance == null) {
             throw new ParsingException(parseContext, "geo_distance requires 'distance' to be specified");
+        } else if (vDistance instanceof Number) {
+            distance = DistanceUnit.DEFAULT.convert(((Number) vDistance).doubleValue(), unit);
+        } else {
+            distance = DistanceUnit.parse((String) vDistance, unit, DistanceUnit.DEFAULT);
         }
+        distance = geoDistance.normalize(distance, DistanceUnit.DEFAULT);
 
-        GeoDistanceQueryBuilder qb = new GeoDistanceQueryBuilder(fieldName);
-        if (vDistance instanceof Number) {
-            qb.distance(((Number) vDistance).doubleValue(), unit);
-        } else {
-            qb.distance((String) vDistance, unit);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType == null) {
+            throw new ParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
-        qb.point(point);
-        qb.coerce(coerce);
-        qb.ignoreMalformed(ignoreMalformed);
-        qb.optimizeBbox(optimizeBbox);
-        qb.geoDistance(geoDistance);
-        qb.boost(boost);
-        qb.queryName(queryName);
-        return qb;
-    }
+        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
+            throw new ParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
+        }
+        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
-    @Override
-    public GeoDistanceQueryBuilder getBuilderPrototype() {
-        return GeoDistanceQueryBuilder.PROTOTYPE;
+
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
+        Query query = new GeoDistanceRangeQuery(point, null, distance, true, false, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index bc58057..6aa6f0f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -19,122 +19,89 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 
 import java.io.IOException;
 import java.util.Locale;
-import java.util.Objects;
 
-public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistanceRangeQueryBuilder> {
+public class GeoDistanceRangeQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "geo_distance_range";
-    public static final boolean DEFAULT_INCLUDE_LOWER = true;
-    public static final boolean DEFAULT_INCLUDE_UPPER = true;
-    public static final GeoDistance DEFAULT_GEO_DISTANCE = GeoDistance.DEFAULT;
-    public static final DistanceUnit DEFAULT_UNIT = DistanceUnit.DEFAULT;
-    public static final String DEFAULT_OPTIMIZE_BBOX = "memory";
-    public static final boolean DEFAULT_COERCE = false;
-    public static final boolean DEFAULT_IGNORE_MALFORMED = false;
-
-    private final String fieldName;
+    private final String name;
 
     private Object from;
     private Object to;
-    private boolean includeLower = DEFAULT_INCLUDE_LOWER;
-    private boolean includeUpper = DEFAULT_INCLUDE_UPPER;
+    private boolean includeLower = true;
+    private boolean includeUpper = true;
 
-    private final GeoPoint point;
+    private double lat;
 
-    private GeoDistance geoDistance = DEFAULT_GEO_DISTANCE;
+    private double lon;
 
-    private DistanceUnit unit = DEFAULT_UNIT;
+    private String geohash;
 
-    private String optimizeBbox = DEFAULT_OPTIMIZE_BBOX;
+    private GeoDistance geoDistance;
 
-    private boolean coerce = DEFAULT_COERCE;
+    private String queryName;
 
-    private boolean ignoreMalformed = DEFAULT_IGNORE_MALFORMED;
+    private String optimizeBbox;
 
-    static final GeoDistanceRangeQueryBuilder PROTOTYPE = new GeoDistanceRangeQueryBuilder("_na_", new GeoPoint());
+    private Boolean coerce;
 
-    public GeoDistanceRangeQueryBuilder(String fieldName, GeoPoint point) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("fieldName must not be null");
-        }
-        if (point == null) {
-            throw new IllegalArgumentException("point must not be null");
-        }
-        this.fieldName = fieldName;
-        this.point = point;
-    }
+    private Boolean ignoreMalformed;
 
-    public GeoDistanceRangeQueryBuilder(String fieldName, double lat, double lon) {
-        this(fieldName, new GeoPoint(lat, lon));
+    public GeoDistanceRangeQueryBuilder(String name) {
+        this.name = name;
     }
 
-    public GeoDistanceRangeQueryBuilder(String fieldName, String geohash) {
-        this(fieldName, geohash == null ? null : new GeoPoint().resetFromGeoHash(geohash));
+    public GeoDistanceRangeQueryBuilder point(double lat, double lon) {
+        this.lat = lat;
+        this.lon = lon;
+        return this;
     }
 
-    public String fieldName() {
-        return fieldName;
+    public GeoDistanceRangeQueryBuilder lat(double lat) {
+        this.lat = lat;
+        return this;
     }
 
-    public GeoPoint point() {
-        return point;
+    public GeoDistanceRangeQueryBuilder lon(double lon) {
+        this.lon = lon;
+        return this;
     }
 
-    public GeoDistanceRangeQueryBuilder from(String from) {
-        if (from == null) {
-            throw new IllegalArgumentException("[from] must not be null");
-        }
+    public GeoDistanceRangeQueryBuilder from(Object from) {
         this.from = from;
         return this;
     }
 
-    public GeoDistanceRangeQueryBuilder from(Number from) {
-        if (from == null) {
-            throw new IllegalArgumentException("[from] must not be null");
-        }
-        this.from = from;
+    public GeoDistanceRangeQueryBuilder to(Object to) {
+        this.to = to;
         return this;
     }
 
-    public Object from() {
-        return from;
+    public GeoDistanceRangeQueryBuilder gt(Object from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
-    public GeoDistanceRangeQueryBuilder to(String to) {
-        if (to == null) {
-            throw new IllegalArgumentException("[to] must not be null");
-        }
-        this.to = to;
+    public GeoDistanceRangeQueryBuilder gte(Object from) {
+        this.from = from;
+        this.includeLower = true;
         return this;
     }
 
-    public GeoDistanceRangeQueryBuilder to(Number to) {
-        if (to == null) {
-            throw new IllegalArgumentException("[to] must not be null");
-        }
+    public GeoDistanceRangeQueryBuilder lt(Object to) {
         this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
-    public Object to() {
-        return to;
+    public GeoDistanceRangeQueryBuilder lte(Object to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
     public GeoDistanceRangeQueryBuilder includeLower(boolean includeLower) {
@@ -142,228 +109,71 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         return this;
     }
 
-    public boolean includeLower() {
-        return includeLower;
-    }
-
     public GeoDistanceRangeQueryBuilder includeUpper(boolean includeUpper) {
         this.includeUpper = includeUpper;
         return this;
     }
 
-    public boolean includeUpper() {
-        return includeUpper;
+    public GeoDistanceRangeQueryBuilder geohash(String geohash) {
+        this.geohash = geohash;
+        return this;
     }
 
     public GeoDistanceRangeQueryBuilder geoDistance(GeoDistance geoDistance) {
-        if (geoDistance == null) {
-            throw new IllegalArgumentException("geoDistance calculation mode must not be null");
-        }
         this.geoDistance = geoDistance;
         return this;
     }
 
-    public GeoDistance geoDistance() {
-        return geoDistance;
-    }
-
-    public GeoDistanceRangeQueryBuilder unit(DistanceUnit unit) {
-        if (unit == null) {
-            throw new IllegalArgumentException("distance unit must not be null");
-        }
-        this.unit = unit;
-        return this;
-    }
-
-    public DistanceUnit unit() {
-        return unit;
-    }
-
     public GeoDistanceRangeQueryBuilder optimizeBbox(String optimizeBbox) {
-        if (optimizeBbox == null) {
-            throw new IllegalArgumentException("optimizeBox must not be null");
-        }
-        switch (optimizeBbox) {
-            case "none":
-            case "memory":
-            case "indexed":
-                break;
-            default:
-                throw new IllegalArgumentException("optimizeBbox must be one of [none, memory, indexed]");
-        }
         this.optimizeBbox = optimizeBbox;
         return this;
     }
 
-    public String optimizeBbox() {
-        return optimizeBbox;
-    }
-
     public GeoDistanceRangeQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
         this.coerce = coerce;
         return this;
     }
 
-    public boolean coerce() {
-        return this.coerce;
-    }
-
     public GeoDistanceRangeQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+        this.ignoreMalformed = ignoreMalformed;
         return this;
     }
 
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        final boolean indexCreatedBeforeV2_0 = context.indexVersionCreated().before(Version.V_2_0_0);
-        // validation was not available prior to 2.x, so to support bwc
-        // percolation queries we only ignore_malformed on 2.x created indexes
-        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
-            if (!GeoUtils.isValidLatitude(point.lat())) {
-                throw new QueryShardException(context, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
-            }
-            if (!GeoUtils.isValidLongitude(point.lon())) {
-                throw new QueryShardException(context, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
-            }
-        }
-
-        if (coerce) {
-            GeoUtils.normalizePoint(point, coerce, coerce);
-        }
-
-        Double fromValue = null;
-        Double toValue = null;
-        if (from != null) {
-            if (from instanceof Number) {
-                fromValue = unit.toMeters(((Number) from).doubleValue());
-            } else {
-                fromValue = DistanceUnit.parse((String) from, unit, DistanceUnit.DEFAULT);
-            }
-            fromValue = geoDistance.normalize(fromValue, DistanceUnit.DEFAULT);
-        }
-        if (to != null) {
-            if (to instanceof Number) {
-                toValue = unit.toMeters(((Number) to).doubleValue());
-            } else {
-                toValue = DistanceUnit.parse((String) to, unit, DistanceUnit.DEFAULT);
-            }
-            toValue = geoDistance.normalize(toValue, DistanceUnit.DEFAULT);
-        }
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType == null) {
-            throw new QueryShardException(context, "failed to find geo_point field [" + fieldName + "]");
-        }
-        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
-            throw new QueryShardException(context, "field [" + fieldName + "] is not a geo_point field");
-        }
-        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
-
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
-        return new GeoDistanceRangeQuery(point, fromValue, toValue, includeLower, includeUpper, geoDistance, geoFieldType,
-                indexFieldData, optimizeBbox);
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceRangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startArray(fieldName).value(point.lon()).value(point.lat()).endArray();
-        builder.field(GeoDistanceRangeQueryParser.FROM_FIELD.getPreferredName(), from);
-        builder.field(GeoDistanceRangeQueryParser.TO_FIELD.getPreferredName(), to);
-        builder.field(GeoDistanceRangeQueryParser.INCLUDE_LOWER_FIELD.getPreferredName(), includeLower);
-        builder.field(GeoDistanceRangeQueryParser.INCLUDE_UPPER_FIELD.getPreferredName(), includeUpper);
-        builder.field(GeoDistanceRangeQueryParser.UNIT_FIELD.getPreferredName(), unit);
-        builder.field(GeoDistanceRangeQueryParser.DISTANCE_TYPE_FIELD.getPreferredName(), geoDistance.name().toLowerCase(Locale.ROOT));
-        builder.field(GeoDistanceRangeQueryParser.OPTIMIZE_BBOX_FIELD.getPreferredName(), optimizeBbox);
-        builder.field(GeoDistanceRangeQueryParser.COERCE_FIELD.getPreferredName(), coerce);
-        builder.field(GeoDistanceRangeQueryParser.IGNORE_MALFORMED_FIELD.getPreferredName(), ignoreMalformed);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected GeoDistanceRangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        GeoDistanceRangeQueryBuilder queryBuilder = new GeoDistanceRangeQueryBuilder(in.readString(), GeoPoint.readGeoPointFrom(in));
-        queryBuilder.from = in.readGenericValue();
-        queryBuilder.to = in.readGenericValue();
-        queryBuilder.includeLower = in.readBoolean();
-        queryBuilder.includeUpper = in.readBoolean();
-        queryBuilder.unit = DistanceUnit.valueOf(in.readString());
-        queryBuilder.geoDistance = GeoDistance.readGeoDistanceFrom(in);
-        queryBuilder.optimizeBbox = in.readString();
-        queryBuilder.coerce = in.readBoolean();
-        queryBuilder.ignoreMalformed = in.readBoolean();
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        point.writeTo(out);
-        out.writeGenericValue(from);
-        out.writeGenericValue(to);
-        out.writeBoolean(includeLower);
-        out.writeBoolean(includeUpper);
-        out.writeString(unit.name());
-        geoDistance.writeTo(out);;
-        out.writeString(optimizeBbox);
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
-    }
-
-    @Override
-    protected boolean doEquals(GeoDistanceRangeQueryBuilder other) {
-        if (!Objects.equals(fieldName, other.fieldName)) {
-            return false;
-        }
-        if (!Objects.equals(point, other.point)) {
-            return false;
-        }
-        if (!Objects.equals(from, other.from)) {
-            return false;
+        builder.startObject(GeoDistanceRangeQueryParser.NAME);
+        if (geohash != null) {
+            builder.field(name, geohash);
+        } else {
+            builder.startArray(name).value(lon).value(lat).endArray();
         }
-        if (!Objects.equals(to, other.to)) {
-            return false;
+        builder.field("from", from);
+        builder.field("to", to);
+        builder.field("include_lower", includeLower);
+        builder.field("include_upper", includeUpper);
+        if (geoDistance != null) {
+            builder.field("distance_type", geoDistance.name().toLowerCase(Locale.ROOT));
         }
-        if (!Objects.equals(includeUpper, other.includeUpper)) {
-            return false;
+        if (optimizeBbox != null) {
+            builder.field("optimize_bbox", optimizeBbox);
         }
-        if (!Objects.equals(includeLower, other.includeLower)) {
-            return false;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        if (!Objects.equals(geoDistance, other.geoDistance)) {
-            return false;
+        if (coerce != null) {
+            builder.field("coerce", coerce);
         }
-        if (!Objects.equals(optimizeBbox, other.optimizeBbox)) {
-            return false;
+        if (ignoreMalformed != null) {
+            builder.field("ignore_malformed", ignoreMalformed);
         }
-        if (!Objects.equals(coerce, other.coerce)) {
-            return false;
-        }
-        if (!Objects.equals(ignoreMalformed, other.ignoreMalformed)) {
-            return false;
-        }
-        return true;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, point, from, to, includeUpper, includeLower, geoDistance, optimizeBbox, coerce,
-                ignoreMalformed);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index bb209f0..39a0adf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -19,13 +19,19 @@
 
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.common.ParseField;
+import org.apache.lucene.search.Query;
+import org.elasticsearch.Version;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
+import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 
 import java.io.IOException;
 
@@ -37,92 +43,71 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceRangeQueryParser extends BaseQueryParser<GeoDistanceRangeQueryBuilder> {
+public class GeoDistanceRangeQueryParser implements QueryParser {
 
-    public static final ParseField FROM_FIELD = new ParseField("from");
-    public static final ParseField TO_FIELD = new ParseField("to");
-    public static final ParseField INCLUDE_LOWER_FIELD = new ParseField("include_lower");
-    public static final ParseField INCLUDE_UPPER_FIELD = new ParseField("include_upper");
-    public static final ParseField GT_FIELD = new ParseField("gt");
-    public static final ParseField GTE_FIELD = new ParseField("gte", "ge");
-    public static final ParseField LT_FIELD = new ParseField("lt");
-    public static final ParseField LTE_FIELD = new ParseField("lte", "le");
-    public static final ParseField UNIT_FIELD = new ParseField("unit");
-    public static final ParseField DISTANCE_TYPE_FIELD = new ParseField("distance_type");
-    public static final ParseField NAME_FIELD = new ParseField("_name");
-    public static final ParseField BOOST_FIELD = new ParseField("boost");
-    public static final ParseField OPTIMIZE_BBOX_FIELD = new ParseField("optimize_bbox");
-    public static final ParseField COERCE_FIELD = new ParseField("coerce", "normalize");
-    public static final ParseField IGNORE_MALFORMED_FIELD = new ParseField("ignore_malformed");
+    public static final String NAME = "geo_distance_range";
 
-    @Override
-    public String[] names() {
-        return new String[]{GeoDistanceRangeQueryBuilder.NAME, "geoDistanceRange"};
+    @Inject
+    public GeoDistanceRangeQueryParser() {
     }
 
     @Override
-    public GeoDistanceRangeQueryBuilder getBuilderPrototype() {
-        return GeoDistanceRangeQueryBuilder.PROTOTYPE;
+    public String[] names() {
+        return new String[]{NAME, "geoDistanceRange"};
     }
 
     @Override
-    public GeoDistanceRangeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        Float boost = null;
         String queryName = null;
         String currentFieldName = null;
-        GeoPoint point = null;
+        GeoPoint point = new GeoPoint();
         String fieldName = null;
         Object vFrom = null;
         Object vTo = null;
-        Boolean includeLower = null;
-        Boolean includeUpper = null;
-        DistanceUnit unit = null;
-        GeoDistance geoDistance = null;
-        String optimizeBbox = null;
-        Boolean coerce = null;
-        Boolean ignoreMalformed = null;
+        boolean includeLower = true;
+        boolean includeUpper = true;
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance geoDistance = GeoDistance.DEFAULT;
+        String optimizeBbox = "memory";
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
+        boolean coerce = false;
+        boolean ignoreMalformed = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (parseContext.isDeprecatedSetting(currentFieldName)) {
                 // skip
             } else if (token == XContentParser.Token.START_ARRAY) {
-                if (point == null) {
-                    point = new GeoPoint();
-                }
                 GeoUtils.parseGeoPoint(parser, point);
                 fieldName = currentFieldName;
             } else if (token == XContentParser.Token.START_OBJECT) {
                 // the json in the format of -> field : { lat : 30, lon : 12 }
                 fieldName = currentFieldName;
-                if (point == null) {
-                    point = new GeoPoint();
-                }
                 GeoUtils.parseGeoPoint(parser, point);
             } else if (token.isValue()) {
-                if (parseContext.parseFieldMatcher().match(currentFieldName, FROM_FIELD)) {
+                if (currentFieldName.equals("from")) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vFrom = parser.text(); // a String
                     } else {
                         vFrom = parser.numberValue(); // a Number
                     }
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, TO_FIELD)) {
+                } else if (currentFieldName.equals("to")) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vTo = parser.text(); // a String
                     } else {
                         vTo = parser.numberValue(); // a Number
                     }
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, INCLUDE_LOWER_FIELD)) {
+                } else if ("include_lower".equals(currentFieldName) || "includeLower".equals(currentFieldName)) {
                     includeLower = parser.booleanValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, INCLUDE_UPPER_FIELD)) {
+                } else if ("include_upper".equals(currentFieldName) || "includeUpper".equals(currentFieldName)) {
                     includeUpper = parser.booleanValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, GT_FIELD)) {
+                } else if ("gt".equals(currentFieldName)) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vFrom = parser.text(); // a String
@@ -130,7 +115,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParser<GeoDistanceRang
                         vFrom = parser.numberValue(); // a Number
                     }
                     includeLower = false;
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, GTE_FIELD)) {
+                } else if ("gte".equals(currentFieldName) || "ge".equals(currentFieldName)) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vFrom = parser.text(); // a String
@@ -138,7 +123,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParser<GeoDistanceRang
                         vFrom = parser.numberValue(); // a Number
                     }
                     includeLower = true;
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, LT_FIELD)) {
+                } else if ("lt".equals(currentFieldName)) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vTo = parser.text(); // a String
@@ -146,7 +131,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParser<GeoDistanceRang
                         vTo = parser.numberValue(); // a Number
                     }
                     includeUpper = false;
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, LTE_FIELD)) {
+                } else if ("lte".equals(currentFieldName) || "le".equals(currentFieldName)) {
                     if (token == XContentParser.Token.VALUE_NULL) {
                     } else if (token == XContentParser.Token.VALUE_STRING) {
                         vTo = parser.text(); // a String
@@ -154,97 +139,84 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParser<GeoDistanceRang
                         vTo = parser.numberValue(); // a Number
                     }
                     includeUpper = true;
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, UNIT_FIELD)) {
+                } else if (currentFieldName.equals("unit")) {
                     unit = DistanceUnit.fromString(parser.text());
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, DISTANCE_TYPE_FIELD)) {
+                } else if (currentFieldName.equals("distance_type") || currentFieldName.equals("distanceType")) {
                     geoDistance = GeoDistance.fromString(parser.text());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.LAT_SUFFIX)) {
-                    if (point == null) {
-                        point = new GeoPoint();
-                    }
                     point.resetLat(parser.doubleValue());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.LAT_SUFFIX.length());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.LON_SUFFIX)) {
-                    if (point == null) {
-                        point = new GeoPoint();
-                    }
                     point.resetLon(parser.doubleValue());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.LON_SUFFIX.length());
                 } else if (currentFieldName.endsWith(GeoPointFieldMapper.Names.GEOHASH_SUFFIX)) {
-                    point = GeoPoint.fromGeohash(parser.text());
+                    point.resetFromGeoHash(parser.text());
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, NAME_FIELD)) {
+                } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, BOOST_FIELD)) {
-                    boost = parser.floatValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, OPTIMIZE_BBOX_FIELD)) {
+                } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, COERCE_FIELD)) {
+                } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, IGNORE_MALFORMED_FIELD)) {
+                    if (coerce == true) {
+                        ignoreMalformed = true;
+                    }
+                } else if ("ignore_malformed".equals(currentFieldName) && coerce == false) {
                     ignoreMalformed = parser.booleanValue();
                 } else {
-                    if (point == null) {
-                        point = new GeoPoint();
-                    }
                     point.resetFromString(parser.text());
                     fieldName = currentFieldName;
                 }
             }
         }
 
-        GeoDistanceRangeQueryBuilder queryBuilder = new GeoDistanceRangeQueryBuilder(fieldName, point);
-        if (boost != null) {
-            queryBuilder.boost(boost);
+        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
+        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+            if (point.lat() > 90.0 || point.lat() < -90.0) {
+                throw new ParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
+            }
+            if (point.lon() > 180.0 || point.lon() < -180) {
+                throw new ParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
+            }
         }
 
-        if (queryName != null) {
-            queryBuilder.queryName(queryName);
+        if (coerce) {
+            GeoUtils.normalizePoint(point, coerce, coerce);
         }
 
+        Double from = null;
+        Double to = null;
         if (vFrom != null) {
             if (vFrom instanceof Number) {
-                queryBuilder.from((Number) vFrom);
+                from = unit.toMeters(((Number) vFrom).doubleValue());
             } else {
-                queryBuilder.from((String) vFrom);
+                from = DistanceUnit.parse((String) vFrom, unit, DistanceUnit.DEFAULT);
             }
+            from = geoDistance.normalize(from, DistanceUnit.DEFAULT);
         }
-
         if (vTo != null) {
             if (vTo instanceof Number) {
-                queryBuilder.to((Number) vTo);
+                to = unit.toMeters(((Number) vTo).doubleValue());
             } else {
-                queryBuilder.to((String) vTo);
+                to = DistanceUnit.parse((String) vTo, unit, DistanceUnit.DEFAULT);
             }
+            to = geoDistance.normalize(to, DistanceUnit.DEFAULT);
         }
 
-        if (includeUpper != null) {
-            queryBuilder.includeUpper(includeUpper);
-        }
-
-        if (includeLower != null) {
-            queryBuilder.includeLower(includeLower);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType == null) {
+            throw new ParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
-
-        if (unit != null) {
-            queryBuilder.unit(unit);
-        }
-
-        if (geoDistance != null) {
-            queryBuilder.geoDistance(geoDistance);
+        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
+            throw new ParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
         }
+        GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
-        if (optimizeBbox != null) {
-            queryBuilder.optimizeBbox(optimizeBbox);
-        }
-
-        if (coerce != null) {
-            queryBuilder.coerce(coerce);
-        }
-
-        if (ignoreMalformed != null) {
-            queryBuilder.ignoreMalformed(ignoreMalformed);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
+        Query query = new GeoDistanceRangeQuery(point, from, to, includeLower, includeUpper, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        return queryBuilder;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoExecType.java b/core/src/main/java/org/elasticsearch/index/query/GeoExecType.java
deleted file mode 100644
index 11b9941..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/GeoExecType.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-/** Specifies how a geo query should be run. */
-public enum GeoExecType implements Writeable<GeoExecType> {
-    
-    MEMORY(0), INDEXED(1);
-
-    private final int ordinal;
-
-    private static final GeoExecType PROTOTYPE = MEMORY;
-
-    GeoExecType(int ordinal) {
-        this.ordinal = ordinal;
-    }
-
-    @Override
-    public GeoExecType readFrom(StreamInput in) throws IOException {
-        int ord = in.readVInt();
-        switch(ord) {
-            case(0): return MEMORY;
-            case(1): return INDEXED;
-        }
-        throw new ElasticsearchException("unknown serialized type [" + ord + "]");
-    }
-
-    public static GeoExecType readTypeFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal);
-    }
-
-    public static GeoExecType fromString(String typeName) {
-        if (typeName == null) {
-            throw new IllegalArgumentException("cannot parse type from null string");
-        }
-
-        for (GeoExecType type : GeoExecType.values()) {
-            if (type.name().equalsIgnoreCase(typeName)) {
-                return type;
-            }
-        }
-        throw new IllegalArgumentException("no type can be parsed from ordinal " + typeName);
-    }
-}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index e870036..2d486e0 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -19,195 +19,90 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.search.geo.GeoPolygonQuery;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.List;
-import java.util.Objects;
 
-public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQueryBuilder> {
+public class GeoPolygonQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "geo_polygon";
+    public static final String POINTS = GeoPolygonQueryParser.POINTS;
+    
+    private final String name;
 
-    private static final List<GeoPoint> PROTO_SHAPE = Arrays.asList(new GeoPoint[] { new GeoPoint(1.0, 1.0), new GeoPoint(1.0, 2.0),
-            new GeoPoint(2.0, 1.0) });
+    private final List<GeoPoint> shell = new ArrayList<>();
 
-    static final GeoPolygonQueryBuilder PROTOTYPE = new GeoPolygonQueryBuilder("field", PROTO_SHAPE);
+    private String queryName;
 
-    private final String fieldName;
+    private Boolean coerce;
 
-    private final List<GeoPoint> shell;
+    private Boolean ignoreMalformed;
 
-    private boolean coerce = false;
-
-    private boolean ignoreMalformed = false;
+    public GeoPolygonQueryBuilder(String name) {
+        this.name = name;
+    }
 
-    public GeoPolygonQueryBuilder(String fieldName, List<GeoPoint> points) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("fieldName must not be null");
-        }
-        if (points == null || points.isEmpty()) {
-            throw new IllegalArgumentException("polygon must not be null or empty");
-        } else {
-            GeoPoint start = points.get(0);
-            if (start.equals(points.get(points.size() - 1))) {
-                if (points.size() < 4) {
-                    throw new IllegalArgumentException("too few points defined for geo_polygon query");
-                }
-            } else {
-                if (points.size() < 3) {
-                    throw new IllegalArgumentException("too few points defined for geo_polygon query");
-                }
-            }
-        }
-        this.fieldName = fieldName;
-        this.shell = points;
+    /**
+     * Adds a point with lat and lon
+     *
+     * @param lat The latitude
+     * @param lon The longitude
+     */
+    public GeoPolygonQueryBuilder addPoint(double lat, double lon) {
+        return addPoint(new GeoPoint(lat, lon));
     }
 
-    public String fieldName() {
-        return fieldName;
+    public GeoPolygonQueryBuilder addPoint(String geohash) {
+        return addPoint(GeoPoint.fromGeohash(geohash));
     }
 
-    public List<GeoPoint> points() {
-        return shell;
+    public GeoPolygonQueryBuilder addPoint(GeoPoint point) {
+        shell.add(point);
+        return this;
+    }
+    
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoPolygonQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     public GeoPolygonQueryBuilder coerce(boolean coerce) {
-        if (coerce) {
-            this.ignoreMalformed = true;
-        }
         this.coerce = coerce;
         return this;
     }
 
-    public boolean coerce() {
-        return this.coerce;
-    }
-
     public GeoPolygonQueryBuilder ignoreMalformed(boolean ignoreMalformed) {
-        if (coerce == false) {
-            this.ignoreMalformed = ignoreMalformed;
-        }
+        this.ignoreMalformed = ignoreMalformed;
         return this;
     }
 
-    public boolean ignoreMalformed() {
-        return ignoreMalformed;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        if (!shell.get(shell.size() - 1).equals(shell.get(0))) {
-            shell.add(shell.get(0));
-        }
-
-        final boolean indexCreatedBeforeV2_0 = context.parseContext().shardContext().indexVersionCreated().before(Version.V_2_0_0);
-        // validation was not available prior to 2.x, so to support bwc
-        // percolation queries we only ignore_malformed on 2.x created indexes
-        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
-            for (GeoPoint point : shell) {
-                if (!GeoUtils.isValidLatitude(point.lat())) {
-                    throw new QueryShardException(context, "illegal latitude value [{}] for [{}]", point.lat(),
-                            GeoPolygonQueryBuilder.NAME);
-                }
-                if (!GeoUtils.isValidLongitude(point.lat())) {
-                    throw new QueryShardException(context, "illegal longitude value [{}] for [{}]", point.lon(),
-                            GeoPolygonQueryBuilder.NAME);
-                }
-            }
-        }
-
-        if (coerce) {
-            for (GeoPoint point : shell) {
-                GeoUtils.normalizePoint(point, coerce, coerce);
-            }
-        }
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType == null) {
-            throw new QueryShardException(context, "failed to find geo_point field [" + fieldName + "]");
-        }
-        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
-            throw new QueryShardException(context, "field [" + fieldName + "] is not a geo_point field");
-        }
-
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
-        return new GeoPolygonQuery(indexFieldData, shell.toArray(new GeoPoint[shell.size()]));
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoPolygonQueryParser.NAME);
 
-        builder.startObject(fieldName);
-        builder.startArray(GeoPolygonQueryParser.POINTS_FIELD.getPreferredName());
+        builder.startObject(name);
+        builder.startArray(POINTS);
         for (GeoPoint point : shell) {
             builder.startArray().value(point.lon()).value(point.lat()).endArray();
         }
         builder.endArray();
         builder.endObject();
 
-        builder.field(GeoPolygonQueryParser.COERCE_FIELD.getPreferredName(), coerce);
-        builder.field(GeoPolygonQueryParser.IGNORE_MALFORMED_FIELD.getPreferredName(), ignoreMalformed);
-
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected GeoPolygonQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        String fieldName = in.readString();
-        List<GeoPoint> shell = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            shell.add(GeoPoint.readGeoPointFrom(in));
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(fieldName, shell);
-        builder.coerce = in.readBoolean();
-        builder.ignoreMalformed = in.readBoolean();
-        return builder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeVInt(shell.size());
-        for (GeoPoint point : shell) {
-            point.writeTo(out);
+        if (coerce != null) {
+            builder.field("coerce", coerce);
+        }
+        if (ignoreMalformed != null) {
+            builder.field("ignore_malformed", ignoreMalformed);
         }
-        out.writeBoolean(coerce);
-        out.writeBoolean(ignoreMalformed);
-    }
-
-    @Override
-    protected boolean doEquals(GeoPolygonQueryBuilder other) {
-        return Objects.equals(coerce, other.coerce)
-                && Objects.equals(fieldName, other.fieldName)
-                && Objects.equals(ignoreMalformed, other.ignoreMalformed)
-                && Objects.equals(shell, other.shell);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(coerce, fieldName, ignoreMalformed, shell);
-    }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
index 5a3e165..5390322 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
@@ -19,13 +19,18 @@
 
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.common.ParseField;
+import org.apache.lucene.search.Query;
+import org.elasticsearch.Version;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
+import org.elasticsearch.index.search.geo.GeoPolygonQuery;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -43,28 +48,31 @@ import java.util.List;
  * }
  * </pre>
  */
-public class GeoPolygonQueryParser extends BaseQueryParser<GeoPolygonQueryBuilder> {
+public class GeoPolygonQueryParser implements QueryParser {
 
-    public static final ParseField COERCE_FIELD = new ParseField("coerce", "normalize");
-    public static final ParseField IGNORE_MALFORMED_FIELD = new ParseField("ignore_malformed");
-    public static final ParseField POINTS_FIELD = new ParseField("points");
+    public static final String NAME = "geo_polygon";
+    public static final String POINTS = "points";
+
+    @Inject
+    public GeoPolygonQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{GeoPolygonQueryBuilder.NAME, "geoPolygon"};
+        return new String[]{NAME, "geoPolygon"};
     }
 
     @Override
-    public GeoPolygonQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
 
-        List<GeoPoint> shell = null;
+        List<GeoPoint> shell = new ArrayList<>();
 
-        Float boost = null;
-        Boolean coerce = null;
-        Boolean ignoreMalformed = null;
+        final boolean indexCreatedBeforeV2_0 = parseContext.indexVersionCreated().before(Version.V_2_0_0);
+        boolean coerce = false;
+        boolean ignoreMalformed = false;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -81,11 +89,13 @@ public class GeoPolygonQueryParser extends BaseQueryParser<GeoPolygonQueryBuilde
                     if (token == XContentParser.Token.FIELD_NAME) {
                         currentFieldName = parser.currentName();
                     } else if (token == XContentParser.Token.START_ARRAY) {
-                        if (parseContext.parseFieldMatcher().match(currentFieldName, POINTS_FIELD)) {
-                            shell = new ArrayList<GeoPoint>();
+                        if (POINTS.equals(currentFieldName)) {
                             while ((token = parser.nextToken()) != Token.END_ARRAY) {
                                 shell.add(GeoUtils.parseGeoPoint(parser));
                             }
+                            if (!shell.get(shell.size()-1).equals(shell.get(0))) {
+                                shell.add(shell.get(0));
+                            }
                         } else {
                             throw new ParsingException(parseContext, "[geo_polygon] query does not support [" + currentFieldName
                                     + "]");
@@ -98,14 +108,12 @@ public class GeoPolygonQueryParser extends BaseQueryParser<GeoPolygonQueryBuilde
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, COERCE_FIELD)) {
+                } else if ("coerce".equals(currentFieldName) || (indexCreatedBeforeV2_0 && "normalize".equals(currentFieldName))) {
                     coerce = parser.booleanValue();
                     if (coerce == true) {
                         ignoreMalformed = true;
                     }
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, IGNORE_MALFORMED_FIELD)) {
+                } else if ("ignore_malformed".equals(currentFieldName) && coerce == false) {
                     ignoreMalformed = parser.booleanValue();
                 } else {
                     throw new ParsingException(parseContext, "[geo_polygon] query does not support [" + currentFieldName + "]");
@@ -114,24 +122,53 @@ public class GeoPolygonQueryParser extends BaseQueryParser<GeoPolygonQueryBuilde
                 throw new ParsingException(parseContext, "[geo_polygon] unexpected token type [" + token.name() + "]");
             }
         }
-        GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(fieldName, shell);
-        if (coerce != null) {
-            builder.coerce(coerce);
+
+        if (shell.isEmpty()) {
+            throw new ParsingException(parseContext, "no points defined for geo_polygon query");
+        } else {
+            if (shell.size() < 3) {
+                throw new ParsingException(parseContext, "too few points defined for geo_polygon query");
+            }
+            GeoPoint start = shell.get(0);
+            if (!start.equals(shell.get(shell.size() - 1))) {
+                shell.add(start);
+            }
+            if (shell.size() < 4) {
+                throw new ParsingException(parseContext, "too few points defined for geo_polygon query");
+            }
+        }
+
+        // validation was not available prior to 2.x, so to support bwc percolation queries we only ignore_malformed on 2.x created indexes
+        if (!indexCreatedBeforeV2_0 && !ignoreMalformed) {
+            for (GeoPoint point : shell) {
+                if (point.lat() > 90.0 || point.lat() < -90.0) {
+                    throw new ParsingException(parseContext, "illegal latitude value [{}] for [{}]", point.lat(), NAME);
+                }
+                if (point.lon() > 180.0 || point.lon() < -180) {
+                    throw new ParsingException(parseContext, "illegal longitude value [{}] for [{}]", point.lon(), NAME);
+                }
+            }
         }
-        if (ignoreMalformed != null) {
-            builder.ignoreMalformed(ignoreMalformed);
+
+        if (coerce) {
+            for (GeoPoint point : shell) {
+                GeoUtils.normalizePoint(point, coerce, coerce);
+            }
         }
-        if (queryName != null) {
-            builder.queryName(queryName);
+
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType == null) {
+            throw new ParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
-        if (boost != null) {
-            builder.boost(boost);
+        if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
+            throw new ParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
         }
-        return builder;
-    }
 
-    @Override
-    public GeoPolygonQueryBuilder getBuilderPrototype() {
-        return GeoPolygonQueryBuilder.PROTOTYPE;
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
+        Query query = new GeoPolygonQuery(indexFieldData, shell.toArray(new GeoPoint[shell.size()]));
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 6e00b4a..3887874 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -19,167 +19,95 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.spatial.prefix.PrefixTreeStrategy;
-import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
-import org.apache.lucene.spatial.query.SpatialArgs;
-import org.apache.lucene.spatial.query.SpatialOperation;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.geo.ShapeRelation;
-import org.elasticsearch.common.geo.ShapesAvailability;
 import org.elasticsearch.common.geo.SpatialStrategy;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
- * {@link QueryBuilder} that builds a GeoShape Query
+ * {@link QueryBuilder} that builds a GeoShape Filter
  */
-public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuilder> {
+public class GeoShapeQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<GeoShapeQueryBuilder> {
 
-    public static final String NAME = "geo_shape";
-    public static final String DEFAULT_SHAPE_INDEX_NAME = "shapes";
-    public static final String DEFAULT_SHAPE_FIELD_NAME = "shape";
-    public static final ShapeRelation DEFAULT_SHAPE_RELATION = ShapeRelation.INTERSECTS;
+    private final String name;
 
-    static final GeoShapeQueryBuilder PROTOTYPE = new GeoShapeQueryBuilder("field", new BytesArray(new byte[1]));
-
-    private final String fieldName;
-
-    // TODO make the ShapeBuilder and subclasses Writable and implement hashCode
-    // and Equals so ShapeBuilder can be used here
-    private BytesReference shapeBytes;
+    private final ShapeBuilder shape;
 
     private SpatialStrategy strategy = null;
 
+    private String queryName;
+
     private final String indexedShapeId;
     private final String indexedShapeType;
 
-    private String indexedShapeIndex = DEFAULT_SHAPE_INDEX_NAME;
-    private String indexedShapePath = DEFAULT_SHAPE_FIELD_NAME;
+    private String indexedShapeIndex;
+    private String indexedShapePath;
 
-    private ShapeRelation relation = DEFAULT_SHAPE_RELATION;
+    private ShapeRelation relation = null;
 
+    private float boost = -1;
+    
     /**
-     * Creates a new GeoShapeQueryBuilder whose Query will be against the given
-     * field name using the given Shape
+     * Creates a new GeoShapeQueryBuilder whose Filter will be against the
+     * given field name using the given Shape
      *
-     * @param name
-     *            Name of the field that will be queried
-     * @param shape
-     *            Shape used in the Query
+     * @param name  Name of the field that will be filtered
+     * @param shape Shape used in the filter
      */
-    public GeoShapeQueryBuilder(String fieldName, ShapeBuilder shape) throws IOException {
-        this(fieldName, shape, null, null);
+    public GeoShapeQueryBuilder(String name, ShapeBuilder shape) {
+        this(name, shape, null, null, null);
     }
 
     /**
-     * Creates a new GeoShapeQueryBuilder whose Query will be against the given
-     * field name and will use the Shape found with the given ID in the given
-     * type
+     * Creates a new GeoShapeQueryBuilder whose Filter will be against the
+     * given field name using the given Shape
      *
-     * @param fieldName
-     *            Name of the field that will be filtered
-     * @param indexedShapeId
-     *            ID of the indexed Shape that will be used in the Query
-     * @param indexedShapeType
-     *            Index type of the indexed Shapes
+     * @param name  Name of the field that will be filtered
+     * @param relation {@link ShapeRelation} of query and indexed shape
+     * @param shape Shape used in the filter
      */
-    public GeoShapeQueryBuilder(String fieldName, String indexedShapeId, String indexedShapeType) {
-        this(fieldName, (BytesReference) null, indexedShapeId, indexedShapeType);
-    }
-
-    GeoShapeQueryBuilder(String fieldName, BytesReference shapeBytes) {
-        this(fieldName, shapeBytes, null, null);
-    }
-
-    private GeoShapeQueryBuilder(String fieldName, ShapeBuilder shape, String indexedShapeId, String indexedShapeType) throws IOException {
-        this(fieldName, new BytesArray(new byte[1]), indexedShapeId, indexedShapeType);
-        if (shape != null) {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            shape.toXContent(builder, EMPTY_PARAMS);
-            BytesReference bytes = builder.bytes();
-            if (bytes.length() == 0) {
-                throw new IllegalArgumentException("shape must not be empty");
-            }
-            this.shapeBytes = bytes;
-        } else {
-            throw new IllegalArgumentException("shape must not be null");
-        }
-    }
-
-    private GeoShapeQueryBuilder(String fieldName, BytesReference shapeBytes, String indexedShapeId, String indexedShapeType) {
-        if (fieldName == null) {
-            throw new IllegalArgumentException("fieldName is required");
-        }
-        if ((shapeBytes == null || shapeBytes.length() == 0) && indexedShapeId == null) {
-            throw new IllegalArgumentException("either shapeBytes or indexedShapeId and indexedShapeType are required");
-        }
-        if (indexedShapeId != null && indexedShapeType == null) {
-            throw new IllegalArgumentException("indexedShapeType is required if indexedShapeId is specified");
-        }
-        this.fieldName = fieldName;
-        this.shapeBytes = shapeBytes;
-        this.indexedShapeId = indexedShapeId;
-        this.indexedShapeType = indexedShapeType;
-    }
-
-    /**
-     * @return the name of the field that will be queried
-     */
-    public String fieldName() {
-        return fieldName;
+    public GeoShapeQueryBuilder(String name, ShapeBuilder shape, ShapeRelation relation) {
+        this(name, shape, null, null, relation);
     }
 
     /**
-     * @return the JSON bytes for the shape used in the Query
+     * Creates a new GeoShapeQueryBuilder whose Filter will be against the given field name
+     * and will use the Shape found with the given ID in the given type
+     *
+     * @param name             Name of the field that will be filtered
+     * @param indexedShapeId   ID of the indexed Shape that will be used in the Filter
+     * @param indexedShapeType Index type of the indexed Shapes
      */
-    public BytesReference shapeBytes() {
-        return shapeBytes;
+    public GeoShapeQueryBuilder(String name, String indexedShapeId, String indexedShapeType, ShapeRelation relation) {
+        this(name, null, indexedShapeId, indexedShapeType, relation);
     }
 
-    /**
-     * @return the ID of the indexed Shape that will be used in the Query
-     */
-    public String indexedShapeId() {
-        return indexedShapeId;
+    private GeoShapeQueryBuilder(String name, ShapeBuilder shape, String indexedShapeId, String indexedShapeType, ShapeRelation relation) {
+        this.name = name;
+        this.shape = shape;
+        this.indexedShapeId = indexedShapeId;
+        this.relation = relation;
+        this.indexedShapeType = indexedShapeType;
     }
 
     /**
-     * @return the document type of the indexed Shape that will be used in the
-     *         Query
+     * Sets the name of the filter
+     *
+     * @param queryName Name of the filter
+     * @return this
      */
-    public String indexedShapeType() {
-        return indexedShapeType;
+    public GeoShapeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     /**
-     * Defines which spatial strategy will be used for building the geo shape
-     * Query. When not set, the strategy that will be used will be the one that
-     * is associated with the geo shape field in the mappings.
+     * Defines which spatial strategy will be used for building the geo shape filter. When not set, the strategy that
+     * will be used will be the one that is associated with the geo shape field in the mappings.
      *
-     * @param strategy
-     *            The spatial strategy to use for building the geo shape Query
+     * @param strategy The spatial strategy to use for building the geo shape filter
      * @return this
      */
     public GeoShapeQueryBuilder strategy(SpatialStrategy strategy) {
@@ -188,13 +116,6 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
-     * @return The spatial strategy to use for building the geo shape Query
-     */
-    public SpatialStrategy strategy() {
-        return strategy;
-    }
-
-    /**
      * Sets the name of the index where the indexed Shape can be found
      *
      * @param indexedShapeIndex Name of the index where the indexed Shape is
@@ -206,14 +127,6 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
-     * @return the index name for the indexed Shape that will be used in the
-     *         Query
-     */
-    public String indexedShapeIndex() {
-        return indexedShapeIndex;
-    }
-
-    /**
      * Sets the path of the field in the indexed Shape document that has the Shape itself
      *
      * @param indexedShapePath Path of the field where the Shape itself is defined
@@ -225,255 +138,61 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
-     * @return the path of the indexed Shape that will be used in the Query
-     */
-    public String indexedShapePath() {
-        return indexedShapePath;
-    }
-
-    /**
      * Sets the relation of query shape and indexed shape.
      *
      * @param relation relation of the shapes
      * @return this
      */
     public GeoShapeQueryBuilder relation(ShapeRelation relation) {
-        if (relation == null) {
-            throw new IllegalArgumentException("No Shape Relation defined");
-        }
         this.relation = relation;
         return this;
     }
 
-    /**
-     * @return the relation of query shape and indexed shape to use in the Query
-     */
-    public ShapeRelation relation() {
-        return relation;
-    }
-
-    public QueryValidationException validate() {
-        QueryValidationException errors = null;
-        // TODO did we validate this before the refactoring and can we do this in setters at all?
-        if (strategy != null && strategy == SpatialStrategy.TERM && relation != ShapeRelation.INTERSECTS) {
-            errors = QueryValidationException.addValidationError(NAME,
-                    "strategy [" + strategy.getStrategyName() + "] only supports relation ["
-                    + ShapeRelation.INTERSECTS.getRelationName() + "] found relation [" + relation.getRelationName() + "]", errors);
-        }
-        return errors;
-    }
-
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        ShapeBuilder shape;
-        if (shapeBytes == null) {
-            GetRequest getRequest = new GetRequest(indexedShapeIndex, indexedShapeType, indexedShapeId);
-            getRequest.copyContextAndHeadersFrom(SearchContext.current());
-            shape = fetch(context.getClient(), getRequest, indexedShapePath);
-        } else {
-            XContentParser shapeParser = XContentHelper.createParser(shapeBytes);
-            shapeParser.nextToken();
-            shape = ShapeBuilder.parse(shapeParser);
-        }
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType == null) {
-            throw new QueryShardException(context, "Failed to find geo_shape field [" + fieldName + "]");
-        }
-
-        // TODO: This isn't the nicest way to check this
-        if (!(fieldType instanceof GeoShapeFieldMapper.GeoShapeFieldType)) {
-            throw new QueryShardException(context, "Field [" + fieldName + "] is not a geo_shape");
-        }
-
-        GeoShapeFieldMapper.GeoShapeFieldType shapeFieldType = (GeoShapeFieldMapper.GeoShapeFieldType) fieldType;
-
-        PrefixTreeStrategy strategy = shapeFieldType.defaultStrategy();
-        if (this.strategy != null) {
-            strategy = shapeFieldType.resolveStrategy(this.strategy);
-        }
-        Query query;
-        if (strategy instanceof RecursivePrefixTreeStrategy && relation == ShapeRelation.DISJOINT) {
-            // this strategy doesn't support disjoint anymore: but it did
-            // before, including creating lucene fieldcache (!)
-            // in this case, execute disjoint as exists && !intersects
-            BooleanQuery.Builder bool = new BooleanQuery.Builder();
-            Query exists = ExistsQueryBuilder.newFilter(context, fieldName);
-            Filter intersects = strategy.makeFilter(getArgs(shape, ShapeRelation.INTERSECTS));
-            bool.add(exists, BooleanClause.Occur.MUST);
-            bool.add(intersects, BooleanClause.Occur.MUST_NOT);
-            query = new ConstantScoreQuery(bool.build());
-        } else {
-            query = strategy.makeQuery(getArgs(shape, relation));
-        }
-        return query;
-    }
-
-    /**
-     * Fetches the Shape with the given ID in the given type and index.
-     *
-     * @param getRequest
-     *            GetRequest containing index, type and id
-     * @param path
-     *            Name or path of the field in the Shape Document where the
-     *            Shape itself is located
-     * @return Shape with the given ID
-     * @throws IOException
-     *             Can be thrown while parsing the Shape Document and extracting
-     *             the Shape
-     */
-    private ShapeBuilder fetch(Client client, GetRequest getRequest, String path) throws IOException {
-        if (ShapesAvailability.JTS_AVAILABLE == false) {
-            throw new IllegalStateException("JTS not available");
-        }
-        getRequest.preference("_local");
-        getRequest.operationThreaded(false);
-        GetResponse response = client.get(getRequest).actionGet();
-        if (!response.isExists()) {
-            throw new IllegalArgumentException("Shape with ID [" + getRequest.id() + "] in type [" + getRequest.type() + "] not found");
-        }
-
-        String[] pathElements = Strings.splitStringToArray(path, '.');
-        int currentPathSlot = 0;
-
-        XContentParser parser = null;
-        try {
-            parser = XContentHelper.createParser(response.getSourceAsBytesRef());
-            XContentParser.Token currentToken;
-            while ((currentToken = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (currentToken == XContentParser.Token.FIELD_NAME) {
-                    if (pathElements[currentPathSlot].equals(parser.currentName())) {
-                        parser.nextToken();
-                        if (++currentPathSlot == pathElements.length) {
-                            return ShapeBuilder.parse(parser);
-                        }
-                    } else {
-                        parser.nextToken();
-                        parser.skipChildren();
-                    }
-                }
-            }
-            throw new IllegalStateException("Shape with name [" + getRequest.id() + "] found but missing " + path + " field");
-        } finally {
-            if (parser != null) {
-                parser.close();
-            }
-        }
-    }
-
-    public static SpatialArgs getArgs(ShapeBuilder shape, ShapeRelation relation) {
-        switch (relation) {
-        case DISJOINT:
-            return new SpatialArgs(SpatialOperation.IsDisjointTo, shape.build());
-        case INTERSECTS:
-            return new SpatialArgs(SpatialOperation.Intersects, shape.build());
-        case WITHIN:
-            return new SpatialArgs(SpatialOperation.IsWithin, shape.build());
-        default:
-            throw new IllegalArgumentException("invalid relation [" + relation + "]");
-        }
+    public GeoShapeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoShapeQueryParser.NAME);
 
-        builder.startObject(fieldName);
+        builder.startObject(name);
 
         if (strategy != null) {
-            builder.field(GeoShapeQueryParser.STRATEGY_FIELD.getPreferredName(), strategy.getStrategyName());
+            builder.field("strategy", strategy.getStrategyName());
         }
 
-        if (shapeBytes != null) {
-            builder.field(GeoShapeQueryParser.SHAPE_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(shapeBytes);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+        if (shape != null) {
+            builder.field("shape", shape);
         } else {
-            builder.startObject(GeoShapeQueryParser.INDEXED_SHAPE_FIELD.getPreferredName())
-                    .field(GeoShapeQueryParser.SHAPE_ID_FIELD.getPreferredName(), indexedShapeId)
-                    .field(GeoShapeQueryParser.SHAPE_TYPE_FIELD.getPreferredName(), indexedShapeType);
+            builder.startObject("indexed_shape")
+                    .field("id", indexedShapeId)
+                    .field("type", indexedShapeType);
             if (indexedShapeIndex != null) {
-                builder.field(GeoShapeQueryParser.SHAPE_INDEX_FIELD.getPreferredName(), indexedShapeIndex);
+                builder.field("index", indexedShapeIndex);
             }
             if (indexedShapePath != null) {
-                builder.field(GeoShapeQueryParser.SHAPE_PATH_FIELD.getPreferredName(), indexedShapePath);
+                builder.field("path", indexedShapePath);
             }
             builder.endObject();
         }
 
         if(relation != null) {
-            builder.field(GeoShapeQueryParser.RELATION_FIELD.getPreferredName(), relation.getRelationName());
+            builder.field("relation", relation.getRelationName());
         }
 
         builder.endObject();
 
-        printBoostAndQueryName(builder);
-
-        builder.endObject();
-    }
-
-    @Override
-    protected GeoShapeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        String fieldName = in.readString();
-        GeoShapeQueryBuilder builder;
-        if (in.readBoolean()) {
-            BytesReference shapeBytes = in.readBytesReference();
-            builder = new GeoShapeQueryBuilder(fieldName, shapeBytes);
-        } else {
-            String indexedShapeId = in.readOptionalString();
-            String indexedShapeType = in.readOptionalString();
-            String indexedShapeIndex = in.readOptionalString();
-            String indexedShapePath = in.readOptionalString();
-            builder = new GeoShapeQueryBuilder(fieldName, indexedShapeId, indexedShapeType);
-            if (indexedShapeIndex != null) {
-                builder.indexedShapeIndex = indexedShapeIndex;
-            }
-            if (indexedShapePath != null) {
-                builder.indexedShapePath = indexedShapePath;
-            }
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        builder.relation = ShapeRelation.DISJOINT.readFrom(in);
-        builder.strategy = SpatialStrategy.RECURSIVE.readFrom(in);
-        return builder;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        boolean hasShapeBytes = shapeBytes != null;
-        out.writeBoolean(hasShapeBytes);
-        if (hasShapeBytes) {
-            out.writeBytesReference(shapeBytes);
-        } else {
-            out.writeOptionalString(indexedShapeId);
-            out.writeOptionalString(indexedShapeType);
-            out.writeOptionalString(indexedShapeIndex);
-            out.writeOptionalString(indexedShapePath);
+        if (name != null) {
+            builder.field("_name", queryName);
         }
-        relation.writeTo(out);
-        strategy.writeTo(out);
-    }
-
-    @Override
-    protected boolean doEquals(GeoShapeQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName)
-                && Objects.equals(indexedShapeId, other.indexedShapeId)
-                && Objects.equals(indexedShapeIndex, other.indexedShapeIndex)
-                && Objects.equals(indexedShapePath, other.indexedShapePath)
-                && Objects.equals(indexedShapeType, other.indexedShapeType)
-                && Objects.equals(relation, other.relation)
-                && Objects.equals(shapeBytes, other.shapeBytes)
-                && Objects.equals(strategy, other.strategy);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, indexedShapeId, indexedShapeIndex,
-                indexedShapePath, indexedShapeType, relation, shapeBytes, strategy);
-    }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
index a3f7446..9a367ed 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
@@ -19,51 +19,59 @@
 
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.common.ParseField;
+import org.apache.lucene.search.*;
+import org.apache.lucene.spatial.prefix.PrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.geo.ShapeRelation;
-import org.elasticsearch.common.geo.SpatialStrategy;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.geo.builders.ShapeBuilder;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
+import org.elasticsearch.index.search.shape.ShapeFetchService;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
-public class GeoShapeQueryParser extends BaseQueryParser<GeoShapeQueryBuilder> {
+public class GeoShapeQueryParser implements QueryParser {
 
-    public static final ParseField SHAPE_FIELD = new ParseField("shape");
-    public static final ParseField STRATEGY_FIELD = new ParseField("strategy");
-    public static final ParseField RELATION_FIELD = new ParseField("relation");
-    public static final ParseField INDEXED_SHAPE_FIELD = new ParseField("indexed_shape");
-    public static final ParseField SHAPE_ID_FIELD = new ParseField("id");
-    public static final ParseField SHAPE_TYPE_FIELD = new ParseField("type");
-    public static final ParseField SHAPE_INDEX_FIELD = new ParseField("index");
-    public static final ParseField SHAPE_PATH_FIELD = new ParseField("path");
+    public static final String NAME = "geo_shape";
+
+    private ShapeFetchService fetchService;
+
+    public static class DEFAULTS {
+        public static final String INDEX_NAME = "shapes";
+        public static final String SHAPE_FIELD_NAME = "shape";
+    }
 
     @Override
     public String[] names() {
-        return new String[]{GeoShapeQueryBuilder.NAME, Strings.toCamelCase(GeoShapeQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public GeoShapeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
-        ShapeRelation shapeRelation = null;
-        SpatialStrategy strategy = null;
-        BytesReference shape = null;
+        ShapeRelation shapeRelation = ShapeRelation.INTERSECTS;
+        String strategyName = null;
+        ShapeBuilder shape = null;
 
         String id = null;
         String type = null;
-        String index = null;
-        String shapePath = null;
+        String index = DEFAULTS.INDEX_NAME;
+        String shapePath = DEFAULTS.SHAPE_FIELD_NAME;
 
         XContentParser.Token token;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1f;
         String queryName = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -76,78 +84,113 @@ public class GeoShapeQueryParser extends BaseQueryParser<GeoShapeQueryBuilder> {
                     if (token == XContentParser.Token.FIELD_NAME) {
                         currentFieldName = parser.currentName();
                         token = parser.nextToken();
-                        if (parseContext.parseFieldMatcher().match(currentFieldName, SHAPE_FIELD)) {
-                            XContentBuilder builder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                            shape = builder.bytes();
-                        } else if (parseContext.parseFieldMatcher().match(currentFieldName, STRATEGY_FIELD)) {
-                            String strategyName = parser.text();
-                            strategy = SpatialStrategy.fromString(strategyName);
-                            if (strategy == null) {
-                                throw new ParsingException(parseContext, "Unknown strategy [" + strategyName + " ]");
-                            }
-                        } else if (parseContext.parseFieldMatcher().match(currentFieldName, RELATION_FIELD)) {
+                        if ("shape".equals(currentFieldName)) {
+                            shape = ShapeBuilder.parse(parser);
+                        } else if ("strategy".equals(currentFieldName)) {
+                            strategyName = parser.text();
+                        } else if ("relation".equals(currentFieldName)) {
                             shapeRelation = ShapeRelation.getRelationByName(parser.text());
                             if (shapeRelation == null) {
                                 throw new ParsingException(parseContext, "Unknown shape operation [" + parser.text() + " ]");
                             }
-                        } else if (parseContext.parseFieldMatcher().match(currentFieldName, INDEXED_SHAPE_FIELD)) {
+                        } else if ("indexed_shape".equals(currentFieldName) || "indexedShape".equals(currentFieldName)) {
                             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                                 if (token == XContentParser.Token.FIELD_NAME) {
                                     currentFieldName = parser.currentName();
                                 } else if (token.isValue()) {
-                                    if (parseContext.parseFieldMatcher().match(currentFieldName, SHAPE_ID_FIELD)) {
+                                    if ("id".equals(currentFieldName)) {
                                         id = parser.text();
-                                    } else if (parseContext.parseFieldMatcher().match(currentFieldName, SHAPE_TYPE_FIELD)) {
+                                    } else if ("type".equals(currentFieldName)) {
                                         type = parser.text();
-                                    } else if (parseContext.parseFieldMatcher().match(currentFieldName, SHAPE_INDEX_FIELD)) {
+                                    } else if ("index".equals(currentFieldName)) {
                                         index = parser.text();
-                                    } else if (parseContext.parseFieldMatcher().match(currentFieldName, SHAPE_PATH_FIELD)) {
+                                    } else if ("path".equals(currentFieldName)) {
                                         shapePath = parser.text();
                                     }
                                 }
                             }
+                            if (id == null) {
+                                throw new ParsingException(parseContext, "ID for indexed shape not provided");
+                            } else if (type == null) {
+                                throw new ParsingException(parseContext, "Type for indexed shape not provided");
+                            }
+                            GetRequest getRequest = new GetRequest(index, type, id);
+                            getRequest.copyContextAndHeadersFrom(SearchContext.current());
+                            shape = fetchService.fetch(getRequest, shapePath);
                         } else {
                             throw new ParsingException(parseContext, "[geo_shape] query does not support [" + currentFieldName + "]");
                         }
                     }
                 }
             } else if (token.isValue()) {
-                if (parseContext.parseFieldMatcher().match(currentFieldName, AbstractQueryBuilder.BOOST_FIELD)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, AbstractQueryBuilder.NAME_FIELD)) {
+                } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else {
                     throw new ParsingException(parseContext, "[geo_shape] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        GeoShapeQueryBuilder builder;
-        if (shape != null) {
-            builder = new GeoShapeQueryBuilder(fieldName, shape);
-        } else {
-            builder = new GeoShapeQueryBuilder(fieldName, id, type);
+
+        if (shape == null) {
+            throw new ParsingException(parseContext, "No Shape defined");
+        } else if (shapeRelation == null) {
+            throw new ParsingException(parseContext, "No Shape Relation defined");
         }
-        if (index != null) {
-            builder.indexedShapeIndex(index);
+
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType == null) {
+            throw new ParsingException(parseContext, "Failed to find geo_shape field [" + fieldName + "]");
         }
-        if (shapePath != null) {
-            builder.indexedShapePath(shapePath);
+
+        // TODO: This isn't the nicest way to check this
+        if (!(fieldType instanceof GeoShapeFieldMapper.GeoShapeFieldType)) {
+            throw new ParsingException(parseContext, "Field [" + fieldName + "] is not a geo_shape");
         }
-        if (shapeRelation != null) {
-            builder.relation(shapeRelation);
+
+        GeoShapeFieldMapper.GeoShapeFieldType shapeFieldType = (GeoShapeFieldMapper.GeoShapeFieldType) fieldType;
+
+        PrefixTreeStrategy strategy = shapeFieldType.defaultStrategy();
+        if (strategyName != null) {
+            strategy = shapeFieldType.resolveStrategy(strategyName);
         }
-        if (strategy != null) {
-            builder.strategy(strategy);
+        Query query;
+        if (strategy instanceof RecursivePrefixTreeStrategy && shapeRelation == ShapeRelation.DISJOINT) {
+            // this strategy doesn't support disjoint anymore: but it did before, including creating lucene fieldcache (!)
+            // in this case, execute disjoint as exists && !intersects
+            BooleanQuery.Builder bool = new BooleanQuery.Builder();
+            Query exists = ExistsQueryParser.newFilter(parseContext, fieldName, null);
+            Filter intersects = strategy.makeFilter(getArgs(shape, ShapeRelation.INTERSECTS));
+            bool.add(exists, BooleanClause.Occur.MUST);
+            bool.add(intersects, BooleanClause.Occur.MUST_NOT);
+            query = new ConstantScoreQuery(bool.build());
+        } else {
+            query = strategy.makeQuery(getArgs(shape, shapeRelation));
         }
+        query.setBoost(boost);
         if (queryName != null) {
-            builder.queryName(queryName);
+            parseContext.addNamedQuery(queryName, query);
         }
-            builder.boost(boost);
-        return builder;
+        return query;
     }
 
-    @Override
-    public GeoShapeQueryBuilder getBuilderPrototype() {
-        return GeoShapeQueryBuilder.PROTOTYPE;
+    @Inject(optional = true)
+    public void setFetchService(@Nullable ShapeFetchService fetchService) {
+        this.fetchService = fetchService;
+    }
+
+    public static SpatialArgs getArgs(ShapeBuilder shape, ShapeRelation relation) {
+        switch(relation) {
+        case DISJOINT:
+            return new SpatialArgs(SpatialOperation.IsDisjointTo, shape.build());
+        case INTERSECTS:
+            return new SpatialArgs(SpatialOperation.Intersects, shape.build());
+        case WITHIN:
+            return new SpatialArgs(SpatialOperation.IsWithin, shape.build());
+        default:
+            throw new IllegalArgumentException("");
+
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
index 876bb19..84d3857 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
@@ -23,13 +23,11 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.util.XGeoHashUtils;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -40,14 +38,13 @@ import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Objects;
 
 /**
  * A geohash cell filter that filters {@link GeoPoint}s by their geohashes. Basically the a
  * Geohash prefix is defined by the filter and all geohashes that are matching this
  * prefix will be returned. The <code>neighbors</code> flag allows to filter
  * geohashes that surround the given geohash. In general the neighborhood of a
- * geohash is defined by its eight adjacent cells.<br />
+ * geohash is defined by its eight adjacent cells.<br>
  * The structure of the {@link GeohashCellQuery} is defined as:
  * <pre>
  * &quot;geohash_bbox&quot; {
@@ -60,9 +57,8 @@ import java.util.Objects;
 public class GeohashCellQuery {
 
     public static final String NAME = "geohash_cell";
-    public static final ParseField NEIGHBORS_FIELD = new ParseField("neighbors");
-    public static final ParseField PRECISION_FIELD = new ParseField("precision");
-    public static final boolean DEFAULT_NEIGHBORS = false;
+    public static final String NEIGHBORS = "neighbors";
+    public static final String PRECISION = "precision";
 
     /**
      * Create a new geohash filter for a given set of geohashes. In general this method
@@ -74,7 +70,7 @@ public class GeohashCellQuery {
      * @param geohashes   optional array of additional geohashes
      * @return a new GeoBoundinboxfilter
      */
-    public static Query create(QueryShardContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
+    public static Query create(QueryParseContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
         MappedFieldType geoHashMapper = fieldType.geohashFieldType();
         if (geoHashMapper == null) {
             throw new IllegalArgumentException("geohash filter needs geohash_prefix to be enabled");
@@ -93,20 +89,23 @@ public class GeohashCellQuery {
      * <code>geohash</code> to be set. the default for a neighbor filteing is
      * <code>false</code>.
      */
-    public static class Builder extends AbstractQueryBuilder<Builder> {
+    public static class Builder extends QueryBuilder {
         // we need to store the geohash rather than the corresponding point,
         // because a transformation from a geohash to a point an back to the
         // geohash will extend the accuracy of the hash to max precision
         // i.e. by filing up with z's.
-        private String fieldName;
+        private String field;
         private String geohash;
-        private Integer levels = null;
-        private boolean neighbors = DEFAULT_NEIGHBORS;
-        private static final Builder PROTOTYPE = new Builder("field", new GeoPoint());
+        private int levels = -1;
+        private boolean neighbors;
 
 
+        public Builder(String field) {
+            this(field, null, false);
+        }
+
         public Builder(String field, GeoPoint point) {
-            this(field, point == null ? null : point.geohash(), false);
+            this(field, point.geohash(), false);
         }
 
         public Builder(String field, String geohash) {
@@ -114,13 +113,8 @@ public class GeohashCellQuery {
         }
 
         public Builder(String field, String geohash, boolean neighbors) {
-            if (Strings.isEmpty(field)) {
-                throw new IllegalArgumentException("fieldName must not be null");
-            }
-            if (Strings.isEmpty(geohash)) {
-                throw new IllegalArgumentException("geohash or point must be defined");
-            }
-            this.fieldName = field;
+            super();
+            this.field = field;
             this.geohash = geohash;
             this.neighbors = neighbors;
         }
@@ -140,22 +134,11 @@ public class GeohashCellQuery {
             return this;
         }
 
-        public String geohash() {
-            return geohash;
-        }
-
         public Builder precision(int levels) {
-            if (levels <= 0) {
-                throw new IllegalArgumentException("precision must be greater than 0. Found [" + levels + "]");
-            }
             this.levels = levels;
             return this;
         }
 
-        public Integer precision() {
-            return levels;
-        }
-
         public Builder precision(String precision) {
             double meters = DistanceUnit.parse(precision, DistanceUnit.DEFAULT, DistanceUnit.METERS);
             return precision(GeoUtils.geoHashLevelsForPrecision(meters));
@@ -166,107 +149,27 @@ public class GeohashCellQuery {
             return this;
         }
 
-        public boolean neighbors() {
-            return neighbors;
-        }
-
-        public Builder fieldName(String fieldName) {
-            this.fieldName = fieldName;
+        public Builder field(String field) {
+            this.field = field;
             return this;
         }
 
-        public String fieldName() {
-            return fieldName;
-        }
-
-        @Override
-        protected Query doToQuery(QueryShardContext context) throws IOException {
-            MappedFieldType fieldType = context.fieldMapper(fieldName);
-            if (fieldType == null) {
-                throw new QueryShardException(context, "failed to parse [{}] query. missing [{}] field [{}]", NAME,
-                        GeoPointFieldMapper.CONTENT_TYPE, fieldName);
-            }
-
-            if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
-                throw new QueryShardException(context, "failed to parse [{}] query. field [{}] is not a geo_point field", NAME, fieldName);
-            }
-
-            GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
-            if (!geoFieldType.isGeohashPrefixEnabled()) {
-                throw new QueryShardException(context, "failed to parse [{}] query. [geohash_prefix] is not enabled for field [{}]", NAME,
-                        fieldName);
-            }
-
-            if (levels != null) {
-                int len = Math.min(levels, geohash.length());
-                geohash = geohash.substring(0, len);
-            }
-
-            Query query;
-            if (neighbors) {
-                query = create(context, geoFieldType, geohash, XGeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
-            } else {
-                query = create(context, geoFieldType, geohash, null);
-            }
-            return query;
-        }
-
         @Override
         protected void doXContent(XContentBuilder builder, Params params) throws IOException {
             builder.startObject(NAME);
-            builder.field(NEIGHBORS_FIELD.getPreferredName(), neighbors);
-            if (levels != null) {
-                builder.field(PRECISION_FIELD.getPreferredName(), levels);
-            }
-            builder.field(fieldName, geohash);
-            printBoostAndQueryName(builder);
-            builder.endObject();
-        }
-
-        @Override
-        protected Builder doReadFrom(StreamInput in) throws IOException {
-            String field = in.readString();
-            String geohash = in.readString();
-            Builder builder = new Builder(field, geohash);
-            if (in.readBoolean()) {
-                builder.precision(in.readVInt());
+            if (neighbors) {
+                builder.field(NEIGHBORS, neighbors);
             }
-            builder.neighbors(in.readBoolean());
-            return builder;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeString(fieldName);
-            out.writeString(geohash);
-            boolean hasLevels = levels != null;
-            out.writeBoolean(hasLevels);
-            if (hasLevels) {
-                out.writeVInt(levels);
+            if(levels > 0) {
+                builder.field(PRECISION, levels);
             }
-            out.writeBoolean(neighbors);
-        }
-
-        @Override
-        protected boolean doEquals(Builder other) {
-            return Objects.equals(fieldName, other.fieldName)
-                    && Objects.equals(geohash, other.geohash)
-                    && Objects.equals(levels, other.levels)
-                    && Objects.equals(neighbors, other.neighbors);
-        }
+            builder.field(field, geohash);
 
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(fieldName, geohash, levels, neighbors);
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.endObject();
         }
     }
 
-    public static class Parser extends BaseQueryParser<Builder> {
+    public static class Parser implements QueryParser {
 
         @Inject
         public Parser() {
@@ -278,15 +181,14 @@ public class GeohashCellQuery {
         }
 
         @Override
-        public Builder fromXContent(QueryParseContext parseContext) throws IOException {
+        public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
             XContentParser parser = parseContext.parser();
 
             String fieldName = null;
             String geohash = null;
-            Integer levels = null;
-            Boolean neighbors = null;
-            String queryName = null;
-            Float boost = null;
+            int levels = -1;
+            boolean neighbors = false;
+
 
             XContentParser.Token token;
             if ((token = parser.currentToken()) != Token.START_OBJECT) {
@@ -299,31 +201,24 @@ public class GeohashCellQuery {
 
                     if (parseContext.isDeprecatedSetting(field)) {
                         // skip
-                    } else if (parseContext.parseFieldMatcher().match(field, PRECISION_FIELD)) {
+                    } else if (PRECISION.equals(field)) {
                         token = parser.nextToken();
-                        if (token == Token.VALUE_NUMBER) {
+                        if(token == Token.VALUE_NUMBER) {
                             levels = parser.intValue();
-                        } else if (token == Token.VALUE_STRING) {
+                        } else if(token == Token.VALUE_STRING) {
                             double meters = DistanceUnit.parse(parser.text(), DistanceUnit.DEFAULT, DistanceUnit.METERS);
                             levels = GeoUtils.geoHashLevelsForPrecision(meters);
                         }
-                    } else if (parseContext.parseFieldMatcher().match(field, NEIGHBORS_FIELD)) {
+                    } else if (NEIGHBORS.equals(field)) {
                         parser.nextToken();
                         neighbors = parser.booleanValue();
-                    } else if (parseContext.parseFieldMatcher().match(field, AbstractQueryBuilder.NAME_FIELD)) {
-                        parser.nextToken();
-                        queryName = parser.text();
-                    } else if (parseContext.parseFieldMatcher().match(field, AbstractQueryBuilder.BOOST_FIELD)) {
-                        parser.nextToken();
-                        boost = parser.floatValue();
                     } else {
                         fieldName = field;
                         token = parser.nextToken();
-                        if (token == Token.VALUE_STRING) {
-                            // A string indicates either a geohash or a lat/lon
-                            // string
+                        if(token == Token.VALUE_STRING) {
+                            // A string indicates either a gehash or a lat/lon string
                             String location = parser.text();
-                            if (location.indexOf(",") > 0) {
+                            if(location.indexOf(",")>0) {
                                 geohash = GeoUtils.parseGeoPoint(parser).geohash();
                             } else {
                                 geohash = location;
@@ -336,25 +231,38 @@ public class GeohashCellQuery {
                     throw new ElasticsearchParseException("failed to parse [{}] query. unexpected token [{}]", NAME, token);
                 }
             }
-            Builder builder = new Builder(fieldName, geohash);
-            if (levels != null) {
-                builder.precision(levels);
+
+            if (geohash == null) {
+                throw new ParsingException(parseContext, "failed to parse [{}] query. missing geohash value", NAME);
             }
-            if (neighbors != null) {
-                builder.neighbors(neighbors);
+
+            MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+            if (fieldType == null) {
+                throw new ParsingException(parseContext, "failed to parse [{}] query. missing [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
             }
-            if (queryName != null) {
-                builder.queryName(queryName);
+
+            if (!(fieldType instanceof GeoPointFieldMapper.GeoPointFieldType)) {
+                throw new ParsingException(parseContext, "failed to parse [{}] query. field [{}] is not a geo_point field", NAME, fieldName);
             }
-            if (boost != null) {
-                builder.boost(boost);
+
+            GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
+            if (!geoFieldType.isGeohashPrefixEnabled()) {
+                throw new ParsingException(parseContext, "failed to parse [{}] query. [geohash_prefix] is not enabled for field [{}]", NAME, fieldName);
             }
-            return builder;
-        }
 
-        @Override
-        public GeohashCellQuery.Builder getBuilderPrototype() {
-            return Builder.PROTOTYPE;
+            if(levels > 0) {
+                int len = Math.min(levels, geohash.length());
+                geohash = geohash.substring(0, len);
+            }
+
+            Query filter;
+            if (neighbors) {
+                filter = create(parseContext, geoFieldType, geohash, XGeoHashUtils.addNeighbors(geohash, new ArrayList<>(8)));
+            } else {
+                filter = create(parseContext, geoFieldType, geohash, null);
+            }
+
+            return filter;
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
index 3439d88..58af4c4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
@@ -18,92 +18,48 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.JoinUtil;
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.fielddata.IndexParentChildFieldData;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
+import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
-import java.util.Locale;
-import java.util.Objects;
 
-/**
- * A query builder for <tt>has_child</tt> queries.
- */
-public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuilder> {
-
-    /**
-     * The queries name
-     */
-    public static final String NAME = "has_child";
+public class HasChildQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasChildQueryBuilder> {
 
-    /**
-     * The default maximum number of children that are required to match for the parent to be considered a match.
-     */
-    public static final int DEFAULT_MAX_CHILDREN = Integer.MAX_VALUE;
-    /**
-     * The default minimum number of children that are required to match for the parent to be considered a match.
-     */
-    public static final int DEFAULT_MIN_CHILDREN = 0;
-    /*
-     * The default score mode that is used to combine score coming from multiple parent documents.
-     */
-    public static final ScoreMode DEFAULT_SCORE_MODE = ScoreMode.None;
+    private final QueryBuilder queryBuilder;
 
-    private final QueryBuilder query;
+    private String childType;
 
-    private final String type;
+    private float boost = 1.0f;
 
-    private ScoreMode scoreMode = DEFAULT_SCORE_MODE;
+    private String scoreMode;
 
-    private int minChildren = DEFAULT_MIN_CHILDREN;
+    private Integer minChildren;
 
-    private int maxChildren = DEFAULT_MAX_CHILDREN;
+    private Integer maxChildren;
 
-    private QueryInnerHits queryInnerHits;
+    private String queryName;
 
-    static final HasChildQueryBuilder PROTOTYPE = new HasChildQueryBuilder("", EmptyQueryBuilder.PROTOTYPE);
+    private QueryInnerHitBuilder innerHit = null;
 
-    public HasChildQueryBuilder(String type, QueryBuilder query, int maxChildren, int minChildren, ScoreMode scoreMode, QueryInnerHits queryInnerHits) {
-        this(type, query);
-        scoreMode(scoreMode);
-        this.maxChildren = maxChildren;
-        this.minChildren = minChildren;
-        this.queryInnerHits = queryInnerHits;
+    public HasChildQueryBuilder(String type, QueryBuilder queryBuilder) {
+        this.childType = type;
+        this.queryBuilder = queryBuilder;
     }
 
-    public HasChildQueryBuilder(String type, QueryBuilder query) {
-        if (type == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'type' field");
-        }
-        if (query == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'query' field");
-        }
-        this.type = type;
-        this.query = query;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public HasChildQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
      * Defines how the scores from the matching child documents are mapped into the parent document.
      */
-    public HasChildQueryBuilder scoreMode(ScoreMode scoreMode) {
-        if (scoreMode == null) {
-            throw new IllegalArgumentException("[" + NAME + "]  requires 'score_mode' field");
-        }
+    public HasChildQueryBuilder scoreMode(String scoreMode) {
         this.scoreMode = scoreMode;
         return this;
     }
@@ -112,9 +68,6 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
      * Defines the minimum number of children that are required to match for the parent to be considered a match.
      */
     public HasChildQueryBuilder minChildren(int minChildren) {
-        if (minChildren < 0) {
-            throw new IllegalArgumentException("[" + NAME + "]  requires non-negative 'min_children' field");
-        }
         this.minChildren = minChildren;
         return this;
     }
@@ -123,9 +76,6 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
      * Defines the maximum number of children that are required to match for the parent to be considered a match.
      */
     public HasChildQueryBuilder maxChildren(int maxChildren) {
-        if (maxChildren < 0) {
-            throw new IllegalArgumentException("[" + NAME + "]  requires non-negative 'max_children' field");
-        }
         this.maxChildren = maxChildren;
         return this;
     }
@@ -133,252 +83,45 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     /**
      * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public HasChildQueryBuilder innerHit(QueryInnerHits queryInnerHits) {
-        this.queryInnerHits = queryInnerHits;
+    public HasChildQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
     /**
-     * Returns inner hit definition in the scope of this query and reusing the defined type and query.
-     */
-    public QueryInnerHits innerHit() {
-        return queryInnerHits;
-    }
-
-    /**
-     * Returns the children query to execute.
-     */
-    public QueryBuilder query() {
-        return query;
-    }
-
-    /**
-     * Returns the child type
-     */
-    public String childType() {
-        return type;
-    }
-
-    /**
-     * Returns how the scores from the matching child documents are mapped into the parent document.
+     * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
-    public ScoreMode scoreMode() {
-        return scoreMode;
-    }
-
-    /**
-     * Returns the minimum number of children that are required to match for the parent to be considered a match.
-     * The default is {@value #DEFAULT_MAX_CHILDREN}
-     */
-    public int minChildren() {
-        return minChildren;
+    public HasChildQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
+        this.innerHit = innerHit;
+        return this;
     }
 
-    /**
-     * Returns the maximum number of children that are required to match for the parent to be considered a match.
-     * The default is {@value #DEFAULT_MIN_CHILDREN}
-     */
-    public int maxChildren() { return maxChildren; }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasChildQueryParser.NAME);
         builder.field("query");
-        query.toXContent(builder, params);
-        builder.field("child_type", type);
-        builder.field("score_mode", scoreMode.name().toLowerCase(Locale.ROOT));
-        builder.field("min_children", minChildren);
-        builder.field("max_children", maxChildren);
-        printBoostAndQueryName(builder);
-        if (queryInnerHits != null) {
-            queryInnerHits.toXContent(builder, params);
-        }
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerQuery = query.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        innerQuery.setBoost(boost);
-
-        DocumentMapper childDocMapper = context.mapperService().documentMapper(type);
-        if (childDocMapper == null) {
-            throw new QueryShardException(context, "[" + NAME + "] no mapping found for type [" + type + "]");
-        }
-        ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
-        if (parentFieldMapper.active() == false) {
-            throw new QueryShardException(context, "[" + NAME + "] _parent field has no parent type configured");
-        }
-        if (queryInnerHits != null) {
-            try (XContentParser parser = queryInnerHits.getXcontentParser()) {
-                XContentParser.Token token = parser.nextToken();
-                if (token != XContentParser.Token.START_OBJECT) {
-                    throw new IllegalStateException("start object expected but was: [" + token + "]");
-                }
-                InnerHitsSubSearchContext innerHits = context.indexQueryParserService().getInnerHitsQueryParserHelper().parse(parser);
-                if (innerHits != null) {
-                    ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-                    InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, context.mapperService(), childDocMapper);
-                    String name = innerHits.getName() != null ? innerHits.getName() : type;
-                    context.addInnerHits(name, parentChildInnerHits);
-                }
-            }
-        }
-
-        String parentType = parentFieldMapper.type();
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
-        if (parentDocMapper == null) {
-            throw new QueryShardException(context, "[" + NAME + "] Type [" + type + "] points to a non existent parent type ["
-                    + parentType + "]");
-        }
-
-        if (maxChildren > 0 && maxChildren < minChildren) {
-            throw new QueryShardException(context, "[" + NAME + "] 'max_children' is less than 'min_children'");
-        }
-
-        // wrap the query with type query
-        innerQuery = Queries.filtered(innerQuery, childDocMapper.typeFilter());
-
-        final ParentChildIndexFieldData parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
-        int maxChildren = maxChildren();
-        // 0 in pre 2.x p/c impl means unbounded
-        if (maxChildren == 0) {
-            maxChildren = Integer.MAX_VALUE;
-        }
-        return new LateParsingQuery(parentDocMapper.typeFilter(), innerQuery, minChildren(), maxChildren, parentType, scoreMode, parentChildIndexFieldData);
-    }
-
-    final static class LateParsingQuery extends Query {
-
-        private final Query toQuery;
-        private final Query innerQuery;
-        private final int minChildren;
-        private final int maxChildren;
-        private final String parentType;
-        private final ScoreMode scoreMode;
-        private final ParentChildIndexFieldData parentChildIndexFieldData;
-
-        LateParsingQuery(Query toQuery, Query innerQuery, int minChildren, int maxChildren, String parentType, ScoreMode scoreMode, ParentChildIndexFieldData parentChildIndexFieldData) {
-            this.toQuery = toQuery;
-            this.innerQuery = innerQuery;
-            this.minChildren = minChildren;
-            this.maxChildren = maxChildren;
-            this.parentType = parentType;
-            this.scoreMode = scoreMode;
-            this.parentChildIndexFieldData = parentChildIndexFieldData;
-        }
-
-        @Override
-        public Query rewrite(IndexReader reader) throws IOException {
-            if (getBoost() != 1.0F) {
-                return super.rewrite(reader);
-            }
-            String joinField = ParentFieldMapper.joinField(parentType);
-            IndexSearcher indexSearcher = new IndexSearcher(reader);
-            indexSearcher.setQueryCache(null);
-            IndexParentChildFieldData indexParentChildFieldData = parentChildIndexFieldData.loadGlobal(indexSearcher.getIndexReader());
-            MultiDocValues.OrdinalMap ordinalMap = ParentChildIndexFieldData.getOrdinalMap(indexParentChildFieldData, parentType);
-            return JoinUtil.createJoinQuery(joinField, innerQuery, toQuery, indexSearcher, scoreMode, ordinalMap, minChildren, maxChildren);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-            if (!super.equals(o)) return false;
-
-            LateParsingQuery that = (LateParsingQuery) o;
-
-            if (minChildren != that.minChildren) return false;
-            if (maxChildren != that.maxChildren) return false;
-            if (!toQuery.equals(that.toQuery)) return false;
-            if (!innerQuery.equals(that.innerQuery)) return false;
-            if (!parentType.equals(that.parentType)) return false;
-            return scoreMode == that.scoreMode;
+        queryBuilder.toXContent(builder, params);
+        builder.field("child_type", childType);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
         }
-
-        @Override
-        public int hashCode() {
-            int result = super.hashCode();
-            result = 31 * result + toQuery.hashCode();
-            result = 31 * result + innerQuery.hashCode();
-            result = 31 * result + minChildren;
-            result = 31 * result + maxChildren;
-            result = 31 * result + parentType.hashCode();
-            result = 31 * result + scoreMode.hashCode();
-            return result;
+        if (scoreMode != null) {
+            builder.field("score_mode", scoreMode);
         }
-
-        @Override
-        public String toString(String s) {
-            return "LateParsingQuery {parentType=" + parentType + "}";
+        if (minChildren != null) {
+            builder.field("min_children", minChildren);
         }
-
-        public int getMinChildren() {
-            return minChildren;
+        if (maxChildren != null) {
+            builder.field("max_children", maxChildren);
         }
-
-        public int getMaxChildren() {
-            return maxChildren;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        public ScoreMode getScoreMode() {
-            return scoreMode;
-        }
-    }
-
-    @Override
-    protected boolean doEquals(HasChildQueryBuilder that) {
-        return Objects.equals(query, that.query)
-                && Objects.equals(type, that.type)
-                && Objects.equals(scoreMode, that.scoreMode)
-                && Objects.equals(minChildren, that.minChildren)
-                && Objects.equals(maxChildren, that.maxChildren)
-                && Objects.equals(queryInnerHits, that.queryInnerHits);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(query, type, scoreMode, minChildren, maxChildren, queryInnerHits);
-    }
-
-    protected HasChildQueryBuilder(StreamInput in) throws IOException {
-        type = in.readString();
-        minChildren = in.readInt();
-        maxChildren = in.readInt();
-        final int ordinal = in.readVInt();
-        scoreMode = ScoreMode.values()[ordinal];
-        query = in.readQuery();
-        if (in.readBoolean()) {
-            queryInnerHits = new QueryInnerHits(in);
-        }
-    }
-
-    @Override
-    protected HasChildQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new HasChildQueryBuilder(in);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(type);
-        out.writeInt(minChildren());
-        out.writeInt(maxChildren());
-        out.writeVInt(scoreMode.ordinal());
-        out.writeQuery(query);
-        if (queryInnerHits != null) {
-            out.writeBoolean(true);
-            queryInnerHits.writeTo(out);
-        } else {
-            out.writeBoolean(false);
+        if (innerHit != null) {
+            builder.startObject("inner_hits");
+            builder.value(innerHit);
+            builder.endObject();
         }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
index 44c5a0a..376764a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
@@ -19,50 +19,80 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.join.JoinUtil;
 import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.support.QueryInnerHits;
+import org.elasticsearch.index.fielddata.IndexParentChildFieldData;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.index.query.support.InnerHitsQueryParserHelper;
+import org.elasticsearch.index.query.support.XContentStructure;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
 
 import java.io.IOException;
 
 /**
- * A query parser for <tt>has_child</tt> queries.
+ *
  */
-public class HasChildQueryParser extends BaseQueryParser {
+public class HasChildQueryParser implements QueryParser {
 
+    public static final String NAME = "has_child";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
+    private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
+
+    @Inject
+    public HasChildQueryParser(InnerHitsQueryParserHelper innerHitsQueryParserHelper) {
+        this.innerHitsQueryParserHelper = innerHitsQueryParserHelper;
+    }
+
     @Override
     public String[] names() {
-        return new String[] { HasChildQueryBuilder.NAME, Strings.toCamelCase(HasChildQueryBuilder.NAME) };
+        return new String[] { NAME, Strings.toCamelCase(NAME) };
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        boolean queryFound = false;
+        float boost = 1.0f;
         String childType = null;
-        ScoreMode scoreMode = HasChildQueryBuilder.DEFAULT_SCORE_MODE;
-        int minChildren = HasChildQueryBuilder.DEFAULT_MIN_CHILDREN;
-        int maxChildren = HasChildQueryBuilder.DEFAULT_MAX_CHILDREN;
+        ScoreMode scoreMode = ScoreMode.None;
+        int minChildren = 0;
+        int maxChildren = Integer.MAX_VALUE;
         String queryName = null;
-        QueryInnerHits queryInnerHits = null;
+        InnerHitsSubSearchContext innerHits = null;
+
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder iqb = null;
+        XContentStructure.InnerQuery iq = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (parseContext.isDeprecatedSetting(currentFieldName)) {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
+                // Usually, the query would be parsed here, but the child
+                // type may not have been extracted yet, so use the
+                // XContentStructure.<type> facade to parse if available,
+                // or delay parsing if not.
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    iqb = parseContext.parseInnerQueryBuilder();
+                    iq = new XContentStructure.InnerQuery(parseContext, childType == null ? null : new String[] { childType });
+                    queryFound = true;
                 } else if ("inner_hits".equals(currentFieldName)) {
-                    queryInnerHits = new QueryInnerHits(parser);
+                    innerHits = innerHitsQueryParserHelper.parse(parseContext);
                 } else {
                     throw new ParsingException(parseContext, "[has_child] query does not support [" + currentFieldName + "]");
                 }
@@ -84,10 +114,62 @@ public class HasChildQueryParser extends BaseQueryParser {
                 }
             }
         }
-        HasChildQueryBuilder hasChildQueryBuilder = new HasChildQueryBuilder(childType, iqb, maxChildren, minChildren, scoreMode, queryInnerHits);
-        hasChildQueryBuilder.queryName(queryName);
-        hasChildQueryBuilder.boost(boost);
-        return hasChildQueryBuilder;
+        if (!queryFound) {
+            throw new ParsingException(parseContext, "[has_child] requires 'query' field");
+        }
+        if (childType == null) {
+            throw new ParsingException(parseContext, "[has_child] requires 'type' field");
+        }
+
+        Query innerQuery = iq.asQuery(childType);
+
+        if (innerQuery == null) {
+            return null;
+        }
+        innerQuery.setBoost(boost);
+
+        DocumentMapper childDocMapper = parseContext.mapperService().documentMapper(childType);
+        if (childDocMapper == null) {
+            throw new ParsingException(parseContext, "[has_child] No mapping for for type [" + childType + "]");
+        }
+        ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
+        if (parentFieldMapper.active() == false) {
+            throw new ParsingException(parseContext, "[has_child] _parent field has no parent type configured");
+        }
+
+        if (innerHits != null) {
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, parseContext.mapperService(), childDocMapper);
+            String name = innerHits.getName() != null ? innerHits.getName() : childType;
+            parseContext.addInnerHits(name, parentChildInnerHits);
+        }
+
+        String parentType = parentFieldMapper.type();
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
+        if (parentDocMapper == null) {
+            throw new ParsingException(parseContext, "[has_child]  Type [" + childType + "] points to a non existent parent type ["
+                    + parentType + "]");
+        }
+
+        if (maxChildren > 0 && maxChildren < minChildren) {
+            throw new ParsingException(parseContext, "[has_child] 'max_children' is less than 'min_children'");
+        }
+
+        // wrap the query with type query
+        innerQuery = Queries.filtered(innerQuery, childDocMapper.typeFilter());
+
+        final Query query;
+        final ParentChildIndexFieldData parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
+        query = joinUtilHelper(parentType, parentChildIndexFieldData, parentDocMapper.typeFilter(), scoreMode, innerQuery, minChildren, maxChildren);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        query.setBoost(boost);
+        return query;
+    }
+
+    public static Query joinUtilHelper(String parentType, ParentChildIndexFieldData parentChildIndexFieldData, Query toQuery, ScoreMode scoreMode, Query innerQuery, int minChildren, int maxChildren) throws IOException {
+        return new LateParsingQuery(toQuery, innerQuery, minChildren, maxChildren, parentType, scoreMode, parentChildIndexFieldData);
     }
 
     public static ScoreMode parseScoreMode(String scoreModeString) {
@@ -105,8 +187,64 @@ public class HasChildQueryParser extends BaseQueryParser {
         throw new IllegalArgumentException("No score mode for child query [" + scoreModeString + "] found");
     }
 
-    @Override
-    public HasChildQueryBuilder getBuilderPrototype() {
-        return HasChildQueryBuilder.PROTOTYPE;
+    final static class LateParsingQuery extends Query {
+
+        private final Query toQuery;
+        private final Query innerQuery;
+        private final int minChildren;
+        private final int maxChildren;
+        private final String parentType;
+        private final ScoreMode scoreMode;
+        private final ParentChildIndexFieldData parentChildIndexFieldData;
+        private final Object identity = new Object();
+
+        LateParsingQuery(Query toQuery, Query innerQuery, int minChildren, int maxChildren, String parentType, ScoreMode scoreMode, ParentChildIndexFieldData parentChildIndexFieldData) {
+            this.toQuery = toQuery;
+            this.innerQuery = innerQuery;
+            this.minChildren = minChildren;
+            this.maxChildren = maxChildren;
+            this.parentType = parentType;
+            this.scoreMode = scoreMode;
+            this.parentChildIndexFieldData = parentChildIndexFieldData;
+        }
+
+        @Override
+        public Query rewrite(IndexReader reader) throws IOException {
+            if (getBoost() != 1.0F) {
+                return super.rewrite(reader);
+            }
+            String joinField = ParentFieldMapper.joinField(parentType);
+            IndexSearcher indexSearcher = new IndexSearcher(reader);
+            indexSearcher.setQueryCache(null);
+            IndexParentChildFieldData indexParentChildFieldData = parentChildIndexFieldData.loadGlobal(indexSearcher.getIndexReader());
+            MultiDocValues.OrdinalMap ordinalMap = ParentChildIndexFieldData.getOrdinalMap(indexParentChildFieldData, parentType);
+            return JoinUtil.createJoinQuery(joinField, innerQuery, toQuery, indexSearcher, scoreMode, ordinalMap, minChildren, maxChildren);
+        }
+
+        // Even though we only cache rewritten queries it is good to let all queries implement hashCode() and equals():
+
+        // We can't check for actually equality here, since we need to IndexReader for this, but
+        // that isn't available on all cases during query parse time, so instead rely on identity:
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+            if (!super.equals(o)) return false;
+
+            LateParsingQuery that = (LateParsingQuery) o;
+            return identity.equals(that.identity);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = super.hashCode();
+            result = 31 * result + identity.hashCode();
+            return result;
+        }
+
+        @Override
+        public String toString(String s) {
+            return "LateParsingQuery {parentType=" + parentType + "}";
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
index 248c645..15868fe 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
@@ -18,233 +18,83 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
+import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
-import java.util.HashSet;
-import java.util.Objects;
-import java.util.Set;
 
 /**
  * Builder for the 'has_parent' query.
  */
-public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBuilder> {
+public class HasParentQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasParentQueryBuilder> {
 
-    public static final String NAME = "has_parent";
-    public static final boolean DEFAULT_SCORE = false;
-    private final QueryBuilder query;
-    private final String type;
-    private boolean score = DEFAULT_SCORE;
-    private QueryInnerHits innerHit;
+    private final QueryBuilder queryBuilder;
+    private final String parentType;
+    private String scoreMode;
+    private float boost = 1.0f;
+    private String queryName;
+    private QueryInnerHitBuilder innerHit = null;
 
     /**
-     * @param type  The parent type
-     * @param query The query that will be matched with parent documents
+     * @param parentType  The parent type
+     * @param parentQuery The query that will be matched with parent documents
      */
-    public HasParentQueryBuilder(String type, QueryBuilder query) {
-        if (type == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'parent_type' field");
-        }
-        if (query == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'query' field");
-        }
-        this.type = type;
-        this.query = query;
-    }
-
-    public HasParentQueryBuilder(String type, QueryBuilder query, boolean score, QueryInnerHits innerHits) {
-        this(type, query);
-        this.score = score;
-        this.innerHit = innerHits;
+    public HasParentQueryBuilder(String parentType, QueryBuilder parentQuery) {
+        this.parentType = parentType;
+        this.queryBuilder = parentQuery;
     }
 
-    /**
-     * Defines if the parent score is mapped into the child documents.
-     */
-    public HasParentQueryBuilder score(boolean score) {
-        this.score = score;
+    @Override
+    public HasParentQueryBuilder boost(float boost) {
+        this.boost = boost;
         return this;
     }
 
     /**
-     * Sets inner hit definition in the scope of this query and reusing the defined type and query.
+     * Defines how the parent score is mapped into the child documents.
      */
-    public HasParentQueryBuilder innerHit(QueryInnerHits innerHit) {
-        this.innerHit = innerHit;
+    public HasParentQueryBuilder scoreMode(String scoreMode) {
+        this.scoreMode = scoreMode;
         return this;
     }
 
     /**
-     * Returns the query to execute.
-     */
-    public QueryBuilder query() {
-        return query;
-    }
-
-    /**
-     * Returns <code>true</code> if the parent score is mapped into the child documents
-     */
-    public boolean score() {
-        return score;
-    }
-
-    /**
-     * Returns the parents type name
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public String type() {
-        return type;
+    public HasParentQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     /**
-     *  Returns inner hit definition in the scope of this query and reusing the defined type and query.
+     * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
-    public QueryInnerHits innerHit() {
-        return innerHit;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerQuery = query.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        innerQuery.setBoost(boost);
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(type);
-        if (parentDocMapper == null) {
-            throw new ParsingException(context.parseContext(), "[has_parent] query configured 'parent_type' [" + type
-                    + "] is not a valid type");
-        }
-
-        if (innerHit != null) {
-            try (XContentParser parser = innerHit.getXcontentParser()) {
-                XContentParser.Token token = parser.nextToken();
-                if (token != XContentParser.Token.START_OBJECT) {
-                    throw new IllegalStateException("start object expected but was: [" + token + "]");
-                }
-                InnerHitsSubSearchContext innerHits = context.indexQueryParserService().getInnerHitsQueryParserHelper().parse(parser);
-                if (innerHits != null) {
-                    ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-                    InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, context.mapperService(), parentDocMapper);
-                    String name = innerHits.getName() != null ? innerHits.getName() : type;
-                    context.addInnerHits(name, parentChildInnerHits);
-                }
-            }
-        }
-
-        Set<String> parentTypes = new HashSet<>(5);
-        parentTypes.add(parentDocMapper.type());
-        ParentChildIndexFieldData parentChildIndexFieldData = null;
-        for (DocumentMapper documentMapper : context.mapperService().docMappers(false)) {
-            ParentFieldMapper parentFieldMapper = documentMapper.parentFieldMapper();
-            if (parentFieldMapper.active()) {
-                DocumentMapper parentTypeDocumentMapper = context.mapperService().documentMapper(parentFieldMapper.type());
-                parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
-                if (parentTypeDocumentMapper == null) {
-                    // Only add this, if this parentFieldMapper (also a parent)  isn't a child of another parent.
-                    parentTypes.add(parentFieldMapper.type());
-                }
-            }
-        }
-        if (parentChildIndexFieldData == null) {
-            throw new ParsingException(context.parseContext(), "[has_parent] no _parent field configured");
-        }
-
-        Query parentTypeQuery = null;
-        if (parentTypes.size() == 1) {
-            DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypes.iterator().next());
-            if (documentMapper != null) {
-                parentTypeQuery = documentMapper.typeFilter();
-            }
-        } else {
-            BooleanQuery.Builder parentsFilter = new BooleanQuery.Builder();
-            for (String parentTypeStr : parentTypes) {
-                DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypeStr);
-                if (documentMapper != null) {
-                    parentsFilter.add(documentMapper.typeFilter(), BooleanClause.Occur.SHOULD);
-                }
-            }
-            parentTypeQuery = parentsFilter.build();
-        }
-
-        if (parentTypeQuery == null) {
-            return null;
-        }
-
-        // wrap the query with type query
-        innerQuery = Queries.filtered(innerQuery, parentDocMapper.typeFilter());
-        Query childrenFilter = Queries.not(parentTypeQuery);
-        return new HasChildQueryBuilder.LateParsingQuery(childrenFilter, innerQuery, HasChildQueryBuilder.DEFAULT_MIN_CHILDREN, HasChildQueryBuilder.DEFAULT_MAX_CHILDREN, type, score ? ScoreMode.Max : ScoreMode.None, parentChildIndexFieldData);
+    public HasParentQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
+        this.innerHit = innerHit;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasParentQueryParser.NAME);
         builder.field("query");
-        query.toXContent(builder, params);
-        builder.field("parent_type", type);
-        builder.field("score", score);
-        printBoostAndQueryName(builder);
-        if (innerHit != null) {
-           innerHit.toXContent(builder, params);
+        queryBuilder.toXContent(builder, params);
+        builder.field("parent_type", parentType);
+        if (scoreMode != null) {
+            builder.field("score_mode", scoreMode);
         }
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    protected HasParentQueryBuilder(StreamInput in) throws IOException {
-        type = in.readString();
-        score = in.readBoolean();
-        query = in.readQuery();
-        if (in.readBoolean()) {
-            innerHit = new QueryInnerHits(in);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-    }
-
-    @Override
-    protected HasParentQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new HasParentQueryBuilder(in);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(type);
-        out.writeBoolean(score);
-        out.writeQuery(query);
         if (innerHit != null) {
-            out.writeBoolean(true);
-            innerHit.writeTo(out);
-        } else {
-            out.writeBoolean(false);
+            builder.startObject("inner_hits");
+            builder.value(innerHit);
+            builder.endObject();
         }
-    }
-
-    @Override
-    protected boolean doEquals(HasParentQueryBuilder that) {
-        return Objects.equals(query, that.query)
-                && Objects.equals(type, that.type)
-                && Objects.equals(score, that.score)
-                && Objects.equals(innerHit, that.innerHit);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(query, type, score, innerHit);
+        builder.endObject();
     }
 }
+
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
index 922079d..432dd07 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
@@ -18,65 +18,85 @@
  */
 package org.elasticsearch.index.query;
 
-
+import org.apache.lucene.search.*;
+import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.support.QueryInnerHits;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.index.query.support.InnerHitsQueryParserHelper;
+import org.elasticsearch.index.query.support.XContentStructure;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
 
 import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.elasticsearch.index.query.HasChildQueryParser.joinUtilHelper;
 
-public class HasParentQueryParser extends BaseQueryParser  {
+public class HasParentQueryParser implements QueryParser {
 
-    private static final HasParentQueryBuilder PROTOTYPE = new HasParentQueryBuilder("", EmptyQueryBuilder.PROTOTYPE);
+    public static final String NAME = "has_parent";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
-    private static final ParseField SCORE_FIELD = new ParseField("score_mode").withAllDeprecated("score");
-    private static final ParseField TYPE_FIELD = new ParseField("parent_type", "type");
+
+    private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
+
+    @Inject
+    public HasParentQueryParser(InnerHitsQueryParserHelper innerHitsQueryParserHelper) {
+        this.innerHitsQueryParserHelper = innerHitsQueryParserHelper;
+    }
 
     @Override
     public String[] names() {
-        return new String[]{HasParentQueryBuilder.NAME, Strings.toCamelCase(HasParentQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean queryFound = false;
+        float boost = 1.0f;
         String parentType = null;
-        boolean score = HasParentQueryBuilder.DEFAULT_SCORE;
+        boolean score = false;
         String queryName = null;
-        QueryInnerHits innerHits = null;
+        InnerHitsSubSearchContext innerHits = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder iqb = null;
+        XContentStructure.InnerQuery iq = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
+                // Usually, the query would be parsed here, but the child
+                // type may not have been extracted yet, so use the
+                // XContentStructure.<type> facade to parse if available,
+                // or delay parsing if not.
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    iqb = parseContext.parseInnerQueryBuilder();
+                    iq = new XContentStructure.InnerQuery(parseContext, parentType == null ? null : new String[] {parentType});
+                    queryFound = true;
                 } else if ("inner_hits".equals(currentFieldName)) {
-                    innerHits = new QueryInnerHits(parser);
+                    innerHits = innerHitsQueryParserHelper.parse(parseContext);
                 } else {
                     throw new ParsingException(parseContext, "[has_parent] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
-                if (parseContext.parseFieldMatcher().match(currentFieldName, TYPE_FIELD)) {
+                if ("type".equals(currentFieldName) || "parent_type".equals(currentFieldName) || "parentType".equals(currentFieldName)) {
                     parentType = parser.text();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, SCORE_FIELD)) {
+                } else if ("score_mode".equals(currentFieldName) || "scoreMode".equals(currentFieldName)) {
                     String scoreModeValue = parser.text();
                     if ("score".equals(scoreModeValue)) {
                         score = true;
                     } else if ("none".equals(scoreModeValue)) {
                         score = false;
-                    } else {
-                        throw new ParsingException(parseContext, "[has_parent] query does not support [" + scoreModeValue + "] as an option for score_mode");
                     }
-                } else if ("score".equals(currentFieldName)) {
-                    score = parser.booleanValue();
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("_name".equals(currentFieldName)) {
@@ -86,11 +106,90 @@ public class HasParentQueryParser extends BaseQueryParser  {
                 }
             }
         }
-        return new HasParentQueryBuilder(parentType, iqb, score, innerHits).queryName(queryName).boost(boost);
+        if (!queryFound) {
+            throw new ParsingException(parseContext, "[has_parent] query requires 'query' field");
+        }
+        if (parentType == null) {
+            throw new ParsingException(parseContext, "[has_parent] query requires 'parent_type' field");
+        }
+
+        Query innerQuery = iq.asQuery(parentType);
+
+        if (innerQuery == null) {
+            return null;
+        }
+
+        innerQuery.setBoost(boost);
+        Query query = createParentQuery(innerQuery, parentType, score, parseContext, innerHits);
+        if (query == null) {
+            return null;
+        }
+
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public HasParentQueryBuilder getBuilderPrototype() {
-        return PROTOTYPE;
+    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryParseContext parseContext, InnerHitsSubSearchContext innerHits) throws IOException {
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
+        if (parentDocMapper == null) {
+            throw new ParsingException(parseContext, "[has_parent] query configured 'parent_type' [" + parentType
+                    + "] is not a valid type");
+        }
+
+        if (innerHits != null) {
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, parseContext.mapperService(), parentDocMapper);
+            String name = innerHits.getName() != null ? innerHits.getName() : parentType;
+            parseContext.addInnerHits(name, parentChildInnerHits);
+        }
+
+        Set<String> parentTypes = new HashSet<>(5);
+        parentTypes.add(parentDocMapper.type());
+        ParentChildIndexFieldData parentChildIndexFieldData = null;
+        for (DocumentMapper documentMapper : parseContext.mapperService().docMappers(false)) {
+            ParentFieldMapper parentFieldMapper = documentMapper.parentFieldMapper();
+            if (parentFieldMapper.active()) {
+                DocumentMapper parentTypeDocumentMapper = parseContext.mapperService().documentMapper(parentFieldMapper.type());
+                parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
+                if (parentTypeDocumentMapper == null) {
+                    // Only add this, if this parentFieldMapper (also a parent)  isn't a child of another parent.
+                    parentTypes.add(parentFieldMapper.type());
+                }
+            }
+        }
+        if (parentChildIndexFieldData == null) {
+            throw new ParsingException(parseContext, "[has_parent] no _parent field configured");
+        }
+
+        Query parentTypeQuery = null;
+        if (parentTypes.size() == 1) {
+            DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypes.iterator().next());
+            if (documentMapper != null) {
+                parentTypeQuery = documentMapper.typeFilter();
+            }
+        } else {
+            BooleanQuery.Builder parentsFilter = new BooleanQuery.Builder();
+            for (String parentTypeStr : parentTypes) {
+                DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypeStr);
+                if (documentMapper != null) {
+                    parentsFilter.add(documentMapper.typeFilter(), BooleanClause.Occur.SHOULD);
+                }
+            }
+            parentTypeQuery = parentsFilter.build();
+        }
+
+        if (parentTypeQuery == null) {
+            return null;
+        }
+
+        // wrap the query with type query
+        innerQuery = Queries.filtered(innerQuery, parentDocMapper.typeFilter());
+        Query childrenFilter = Queries.not(parentTypeQuery);
+        ScoreMode scoreMode = score ? ScoreMode.Max : ScoreMode.None;
+        return joinUtilHelper(parentType, parentChildIndexFieldData, childrenFilter, scoreMode, innerQuery, 0, Integer.MAX_VALUE);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index b85db4b..02c2a17 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -19,60 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
  */
-public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
+public class IdsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<IdsQueryBuilder> {
 
-    public static final String NAME = "ids";
+    private final List<String> types;
 
-    private final Set<String> ids = new HashSet<>();
+    private List<String> values = new ArrayList<>();
 
-    private final String[] types;
+    private float boost = -1;
 
-    static final IdsQueryBuilder PROTOTYPE = new IdsQueryBuilder();
+    private String queryName;
 
-    /**
-     * Creates a new IdsQueryBuilder by optionally providing the types of the documents to look for
-     */
-    public IdsQueryBuilder(@Nullable String... types) {
-        this.types = types;
-    }
-
-    /**
-     * Returns the types used in this query
-     */
-    public String[] types() {
-        return this.types;
+    public IdsQueryBuilder(String... types) {
+        this.types = types == null ? null : Arrays.asList(types);
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(String... ids) {
-        Collections.addAll(this.ids, ids);
+        values.addAll(Arrays.asList(ids));
         return this;
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(Collection<String> ids) {
-        this.ids.addAll(ids);
+        values.addAll(ids);
         return this;
     }
 
@@ -91,78 +75,48 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
     }
 
     /**
-     * Returns the ids for the query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public IdsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public Set<String> ids() {
-        return this.ids;
+    public IdsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IdsQueryParser.NAME);
         if (types != null) {
-            if (types.length == 1) {
-                builder.field("type", types[0]);
+            if (types.size() == 1) {
+                builder.field("type", types.get(0));
             } else {
-                builder.array("types", types);
+                builder.startArray("types");
+                for (Object type : types) {
+                    builder.value(type);
+                }
+                builder.endArray();
             }
         }
         builder.startArray("values");
-        for (String value : ids) {
+        for (Object value : values) {
             builder.value(value);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query;
-        if (this.ids.isEmpty()) {
-             query = Queries.newMatchNoDocsQuery();
-        } else {
-            Collection<String> typesForQuery;
-            if (types == null || types.length == 0) {
-                typesForQuery = context.queryTypes();
-            } else if (types.length == 1 && MetaData.ALL.equals(types[0])) {
-                typesForQuery = context.mapperService().types();
-            } else {
-                typesForQuery = new HashSet<>();
-                Collections.addAll(typesForQuery, types);
-            }
-
-            query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(typesForQuery, ids));
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
-        idsQueryBuilder.addIds(in.readStringArray());
-        return idsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeStringArray(types);
-        out.writeStringArray(ids.toArray(new String[ids.size()]));
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(ids, Arrays.hashCode(types));
-    }
-
-    @Override
-    protected boolean doEquals(IdsQueryBuilder other) {
-        return Objects.equals(ids, other.ids) &&
-               Arrays.equals(types, other.types);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
index 3f7c6c0..37c8053 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
@@ -19,36 +19,48 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
 /**
- * Parser for ids query
+ *
  */
-public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
+public class IdsQueryParser implements QueryParser {
+
+    public static final String NAME = "ids";
+
+    @Inject
+    public IdsQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{IdsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
-    /**
-     * @return a QueryBuilder representation of the query passed in as XContent in the parse context
-     */
     @Override
-    public IdsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        List<String> ids = new ArrayList<>();
-        List<String> types = new ArrayList<>();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
 
+        List<BytesRef> ids = new ArrayList<>();
+        Collection<String> types = null;
         String currentFieldName = null;
+        float boost = 1.0f;
+        String queryName = null;
         XContentParser.Token token;
         boolean idsProvided = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -60,17 +72,18 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         if ((token == XContentParser.Token.VALUE_STRING) ||
                                 (token == XContentParser.Token.VALUE_NUMBER)) {
-                            String id = parser.textOrNull();
-                            if (id == null) {
+                            BytesRef value = parser.utf8BytesOrNull();
+                            if (value == null) {
                                 throw new ParsingException(parseContext, "No value specified for term filter");
                             }
-                            ids.add(id);
+                            ids.add(value);
                         } else {
                             throw new ParsingException(parseContext, "Illegal value for id, expecting a string or number, got: "
                                     + token);
                         }
                     }
                 } else if ("types".equals(currentFieldName) || "type".equals(currentFieldName)) {
+                    types = new ArrayList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -93,18 +106,26 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                 }
             }
         }
+
         if (!idsProvided) {
             throw new ParsingException(parseContext, "[ids] query, no ids values provided");
         }
 
-        IdsQueryBuilder query = new IdsQueryBuilder(types.toArray(new String[types.size()]));
-        query.addIds(ids.toArray(new String[ids.size()]));
-        query.boost(boost).queryName(queryName);
-        return query;
-    }
+        if (ids.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
 
-    @Override
-    public IdsQueryBuilder getBuilderPrototype() {
-        return IdsQueryBuilder.PROTOTYPE;
+        if (types == null || types.isEmpty()) {
+            types = parseContext.queryTypes();
+        } else if (types.size() == 1 && Iterables.getFirst(types, null).equals("_all")) {
+            types = parseContext.mapperService().types();
+        }
+
+        TermsQuery query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(types, ids));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 603d577..dd8abbd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -22,17 +22,12 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.CloseableThreadLocal;
 import org.elasticsearch.Version;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
@@ -45,7 +40,6 @@ import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.AllFieldMapper;
-import org.elasticsearch.index.query.support.InnerHitsQueryParserHelper;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
@@ -57,16 +51,13 @@ public class IndexQueryParserService extends AbstractIndexComponent {
 
     public static final String DEFAULT_FIELD = "index.query.default_field";
     public static final String QUERY_STRING_LENIENT = "index.query_string.lenient";
-    public static final String QUERY_STRING_ANALYZE_WILDCARD = "indices.query.query_string.analyze_wildcard";
-    public static final String QUERY_STRING_ALLOW_LEADING_WILDCARD = "indices.query.query_string.allowLeadingWildcard";
     public static final String PARSE_STRICT = "index.query.parse.strict";
     public static final String ALLOW_UNMAPPED = "index.query.parse.allow_unmapped_fields";
-    private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
 
-    private CloseableThreadLocal<QueryShardContext> cache = new CloseableThreadLocal<QueryShardContext>() {
+    private CloseableThreadLocal<QueryParseContext> cache = new CloseableThreadLocal<QueryParseContext>() {
         @Override
-        protected QueryShardContext initialValue() {
-            return new QueryShardContext(index, IndexQueryParserService.this);
+        protected QueryParseContext initialValue() {
+            return new QueryParseContext(index, IndexQueryParserService.this);
         }
     };
 
@@ -80,34 +71,24 @@ public class IndexQueryParserService extends AbstractIndexComponent {
 
     final IndexCache indexCache;
 
-    protected IndexFieldDataService fieldDataService;
-
-    final ClusterService clusterService;
-
-    final IndexNameExpressionResolver indexNameExpressionResolver;
+    final IndexFieldDataService fieldDataService;
 
     final BitsetFilterCache bitsetFilterCache;
 
     private final IndicesQueriesRegistry indicesQueriesRegistry;
 
-    private final String defaultField;
-    private final boolean queryStringLenient;
-    private final boolean queryStringAnalyzeWildcard;
-    private final boolean queryStringAllowLeadingWildcard;
+    private String defaultField;
+    private boolean queryStringLenient;
     private final ParseFieldMatcher parseFieldMatcher;
     private final boolean defaultAllowUnmappedFields;
 
-    private Client client;
-
     @Inject
-    public IndexQueryParserService(Index index, @IndexSettings Settings indexSettings, Settings settings,
+    public IndexQueryParserService(Index index, @IndexSettings Settings indexSettings,
                                    IndicesQueriesRegistry indicesQueriesRegistry,
                                    ScriptService scriptService, AnalysisService analysisService,
                                    MapperService mapperService, IndexCache indexCache, IndexFieldDataService fieldDataService,
                                    BitsetFilterCache bitsetFilterCache,
-                                   @Nullable SimilarityService similarityService, ClusterService clusterService,
-                                   IndexNameExpressionResolver indexNameExpressionResolver,
-                                   InnerHitsQueryParserHelper innerHitsQueryParserHelper, Client client) {
+                                   @Nullable SimilarityService similarityService) {
         super(index, indexSettings);
         this.scriptService = scriptService;
         this.analysisService = analysisService;
@@ -116,18 +97,12 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         this.indexCache = indexCache;
         this.fieldDataService = fieldDataService;
         this.bitsetFilterCache = bitsetFilterCache;
-        this.clusterService = clusterService;
-        this.indexNameExpressionResolver = indexNameExpressionResolver;
 
         this.defaultField = indexSettings.get(DEFAULT_FIELD, AllFieldMapper.NAME);
         this.queryStringLenient = indexSettings.getAsBoolean(QUERY_STRING_LENIENT, false);
-        this.queryStringAnalyzeWildcard = settings.getAsBoolean(QUERY_STRING_ANALYZE_WILDCARD, false);
-        this.queryStringAllowLeadingWildcard = settings.getAsBoolean(QUERY_STRING_ALLOW_LEADING_WILDCARD, true);
         this.parseFieldMatcher = new ParseFieldMatcher(indexSettings);
         this.defaultAllowUnmappedFields = indexSettings.getAsBoolean(ALLOW_UNMAPPED, true);
         this.indicesQueriesRegistry = indicesQueriesRegistry;
-        this.innerHitsQueryParserHelper = innerHitsQueryParserHelper;
-        this.client = client;
     }
 
     public void close() {
@@ -138,20 +113,12 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.defaultField;
     }
 
-    public boolean queryStringAnalyzeWildcard() {
-        return this.queryStringAnalyzeWildcard;
-    }
-
-    public boolean queryStringAllowLeadingWildcard() {
-        return this.queryStringAllowLeadingWildcard;
-    }
-
     public boolean queryStringLenient() {
         return this.queryStringLenient;
     }
 
-    IndicesQueriesRegistry indicesQueriesRegistry() {
-        return indicesQueriesRegistry;
+    public QueryParser queryParser(String name) {
+        return indicesQueriesRegistry.queryParsers().get(name);
     }
 
     public ParsedQuery parse(QueryBuilder queryBuilder) {
@@ -160,10 +127,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
             BytesReference bytes = queryBuilder.buildAsBytes();
             parser = XContentFactory.xContent(bytes).createParser(bytes);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (ParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new ParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new ParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -180,10 +147,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         try {
             parser = XContentFactory.xContent(source, offset, length).createParser(source, offset, length);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (ParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new ParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new ParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -195,8 +162,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), source);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, BytesReference source) {
+    public ParsedQuery parse(QueryParseContext context, BytesReference source) {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
@@ -204,7 +170,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         } catch (ParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new ParsingException(context.parseContext(), "Failed to parse", e);
+            throw new ParsingException(context, "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -212,15 +178,15 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         }
     }
 
-    public ParsedQuery parse(String source) throws ParsingException, QueryShardException {
+    public ParsedQuery parse(String source) throws ParsingException {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
             return innerParse(cache.get(), parser);
-        } catch (QueryShardException|ParsingException e) {
+        } catch (ParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new ParsingException(getShardContext().parseContext(), "Failed to parse [" + source + "]", e);
+            throw new ParsingException(getParseContext(), "Failed to parse [" + source + "]", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -232,12 +198,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), parser);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, XContentParser parser) {
+    public ParsedQuery parse(QueryParseContext context, XContentParser parser) {
         try {
             return innerParse(context, parser);
         } catch (IOException e) {
-            throw new ParsingException(context.parseContext(), "Failed to parse", e);
+            throw new ParsingException(context, "Failed to parse", e);
         }
     }
 
@@ -245,12 +210,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
      * Parses an inner filter, returning null if the filter should be ignored.
      */
     @Nullable
-    //norelease
     public ParsedQuery parseInnerFilter(XContentParser parser) throws IOException {
-        QueryShardContext context = cache.get();
+        QueryParseContext context = cache.get();
         context.reset(parser);
         try {
-            Query filter = context.parseContext().parseInnerFilter();
+            Query filter = context.parseInnerFilter();
             if (filter == null) {
                 return null;
             }
@@ -261,22 +225,27 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     }
 
     @Nullable
-    public QueryBuilder parseInnerQueryBuilder(QueryParseContext parseContext) throws IOException {
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        return parseContext.parseInnerQueryBuilder();
+    public Query parseInnerQuery(XContentParser parser) throws IOException {
+        QueryParseContext context = cache.get();
+        context.reset(parser);
+        try {
+            return context.parseInnerQuery();
+        } finally {
+            context.reset(null);
+        }
     }
 
     @Nullable
-    //norelease
-    public Query parseInnerQuery(QueryShardContext context) throws IOException {
-        Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
+    public Query parseInnerQuery(QueryParseContext parseContext) throws IOException {
+        parseContext.parseFieldMatcher(parseFieldMatcher);
+        Query query = parseContext.parseInnerQuery();
         if (query == null) {
             query = Queries.newMatchNoDocsQuery();
         }
         return query;
     }
 
-    public QueryShardContext getShardContext() {
+    public QueryParseContext getParseContext() {
         return cache.get();
     }
 
@@ -308,60 +277,37 @@ public class IndexQueryParserService extends AbstractIndexComponent {
                         XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource);
                         parsedQuery = parse(qSourceParser);
                     } else {
-                        throw new ParsingException(getShardContext().parseContext(), "request does not support [" + fieldName + "]");
+                        throw new ParsingException(getParseContext(), "request does not support [" + fieldName + "]");
                     }
                 }
             }
             if (parsedQuery != null) {
                 return parsedQuery;
             }
-        } catch (QueryShardException e) {
+        } catch (ParsingException e) {
             throw e;
         } catch (Throwable e) {
-            throw new ParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new ParsingException(getParseContext(), "Failed to parse", e);
         }
 
-        throw new ParsingException(getShardContext().parseContext(), "Required query is missing");
+        throw new ParsingException(getParseContext(), "Required query is missing");
     }
 
-    //norelease
-    private ParsedQuery innerParse(QueryShardContext context, XContentParser parser) throws IOException, QueryShardException {
-        context.reset(parser);
+    private ParsedQuery innerParse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException {
+        parseContext.reset(parser);
         try {
-            context.parseFieldMatcher(parseFieldMatcher);
-            return innerParse(context, context.parseContext().parseInnerQueryBuilder());
+            parseContext.parseFieldMatcher(parseFieldMatcher);
+            Query query = parseContext.parseInnerQuery();
+            if (query == null) {
+                query = Queries.newMatchNoDocsQuery();
+            }
+            return new ParsedQuery(query, parseContext.copyNamedQueries());
         } finally {
-            context.reset(null);
-        }
-    }
-
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
+            parseContext.reset(null);
         }
-        return new ParsedQuery(query, context.copyNamedQueries());
     }
 
     public ParseFieldMatcher parseFieldMatcher() {
         return parseFieldMatcher;
     }
-
-    public boolean matchesIndices(String... indices) {
-        final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(clusterService.state(), IndicesOptions.lenientExpandOpen(), indices);
-        for (String index : concreteIndices) {
-            if (Regex.simpleMatch(index, this.index.name())) {
-                return true;
-            }
-        }
-        return false;
-    }
-
-    public InnerHitsQueryParserHelper getInnerHitsQueryParserHelper() {
-        return innerHitsQueryParserHelper;
-    }
-
-    public Client getClient() {
-        return client;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
index b4c7b53..7c2af81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
@@ -19,133 +19,69 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Objects;
 
 /**
  * A query that will execute the wrapped query only for the specified indices, and "match_all" when
  * it does not match those indices (by default).
  */
-public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilder> {
+public class IndicesQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "indices";
-
-    private final QueryBuilder innerQuery;
+    private final QueryBuilder queryBuilder;
 
     private final String[] indices;
 
-    private QueryBuilder noMatchQuery = defaultNoMatchQuery();
+    private String sNoMatchQuery;
+    private QueryBuilder noMatchQuery;
 
-    static final IndicesQueryBuilder PROTOTYPE = new IndicesQueryBuilder(EmptyQueryBuilder.PROTOTYPE, "index");
+    private String queryName;
 
-    public IndicesQueryBuilder(QueryBuilder innerQuery, String... indices) {
-        if (innerQuery == null) {
-            throw new IllegalArgumentException("inner query cannot be null");
-        }
-        if (indices == null || indices.length == 0) {
-            throw new IllegalArgumentException("list of indices cannot be null or empty");
-        }
-        this.innerQuery = Objects.requireNonNull(innerQuery);
+    public IndicesQueryBuilder(QueryBuilder queryBuilder, String... indices) {
+        this.queryBuilder = queryBuilder;
         this.indices = indices;
     }
 
-    public QueryBuilder innerQuery() {
-        return this.innerQuery;
-    }
-
-    public String[] indices() {
-        return this.indices;
+    /**
+     * Sets the no match query, can either be <tt>all</tt> or <tt>none</tt>.
+     */
+    public IndicesQueryBuilder noMatchQuery(String type) {
+        this.sNoMatchQuery = type;
+        return this;
     }
 
     /**
      * Sets the query to use when it executes on an index that does not match the indices provided.
      */
     public IndicesQueryBuilder noMatchQuery(QueryBuilder noMatchQuery) {
-        if (noMatchQuery == null) {
-            throw new IllegalArgumentException("noMatch query cannot be null");
-        }
         this.noMatchQuery = noMatchQuery;
         return this;
     }
 
     /**
-     * Sets the no match query, can either be <tt>all</tt> or <tt>none</tt>.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public IndicesQueryBuilder noMatchQuery(String type) {
-        this.noMatchQuery = IndicesQueryParser.parseNoMatchQuery(type);
+    public IndicesQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    public QueryBuilder noMatchQuery() {
-        return this.noMatchQuery;
-    }
-
-    static QueryBuilder defaultNoMatchQuery() {
-        return QueryBuilders.matchAllQuery();
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IndicesQueryParser.NAME);
         builder.field("indices", indices);
         builder.field("query");
-        innerQuery.toXContent(builder, params);
-        builder.field("no_match_query");
-        noMatchQuery.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (context.matchesIndices(indices)) {
-            return innerQuery.toQuery(context);
+        queryBuilder.toXContent(builder, params);
+        if (noMatchQuery != null) {
+            builder.field("no_match_query");
+            noMatchQuery.toXContent(builder, params);
+        } else if (sNoMatchQuery != null) {
+            builder.field("no_match_query", sNoMatchQuery);
         }
-        return noMatchQuery.toQuery(context);
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        if (boost != DEFAULT_BOOST) {
-            //if both the wrapped query and the wrapper hold a boost, the main one coming from the wrapper wins
-            query.setBoost(boost);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
+        builder.endObject();
     }
-
-    @Override
-    protected IndicesQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IndicesQueryBuilder indicesQueryBuilder = new IndicesQueryBuilder(in.readQuery(), in.readStringArray());
-        indicesQueryBuilder.noMatchQuery = in.readQuery();
-        return indicesQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(innerQuery);
-        out.writeStringArray(indices);
-        out.writeQuery(noMatchQuery);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(innerQuery, noMatchQuery, Arrays.hashCode(indices));
-    }
-
-    @Override
-    protected boolean doEquals(IndicesQueryBuilder other) {
-        return Objects.equals(innerQuery, other.innerQuery) &&
-                Arrays.equals(indices, other.indices) &&  // otherwise we are comparing pointers
-                Objects.equals(noMatchQuery, other.noMatchQuery);
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
index b2305dd..b0a23b4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
@@ -19,57 +19,79 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.support.XContentStructure;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 
 /**
- * Parser for {@link IndicesQueryBuilder}.
  */
-public class IndicesQueryParser extends BaseQueryParser {
+public class IndicesQueryParser implements QueryParser {
 
+    public static final String NAME = "indices";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
     private static final ParseField NO_MATCH_QUERY = new ParseField("no_match_query", "no_match_filter");
 
+    @Nullable
+    private final ClusterService clusterService;
+    private final IndexNameExpressionResolver indexNameExpressionResolver;
+
+    @Inject
+    public IndicesQueryParser(@Nullable ClusterService clusterService, IndexNameExpressionResolver indexNameExpressionResolver) {
+        this.clusterService = clusterService;
+        this.indexNameExpressionResolver = indexNameExpressionResolver;
+    }
+
     @Override
     public String[] names() {
-        return new String[]{IndicesQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, ParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder innerQuery = null;
-        Collection<String> indices = new ArrayList<>();
-        QueryBuilder noMatchQuery = IndicesQueryBuilder.defaultNoMatchQuery();
-
+        Query noMatchQuery = null;
+        boolean queryFound = false;
+        boolean indicesFound = false;
+        boolean currentIndexMatchesIndices = false;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         String currentFieldName = null;
         XContentParser.Token token;
+        XContentStructure.InnerQuery innerQuery = null;
+        XContentStructure.InnerQuery innerNoMatchQuery = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    innerQuery = parseContext.parseInnerQueryBuilder();
+                    innerQuery = new XContentStructure.InnerQuery(parseContext, (String[])null);
+                    queryFound = true;
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_QUERY)) {
-                    noMatchQuery = parseContext.parseInnerQueryBuilder();
+                    innerNoMatchQuery = new XContentStructure.InnerQuery(parseContext, (String[])null);
                 } else {
                     throw new ParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("indices".equals(currentFieldName)) {
-                    if (indices.isEmpty() == false) {
+                    if (indicesFound) {
                         throw new ParsingException(parseContext, "[indices] indices or index already specified");
                     }
+                    indicesFound = true;
+                    Collection<String> indices = new ArrayList<>();
                     while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -77,50 +99,67 @@ public class IndicesQueryParser extends BaseQueryParser {
                         }
                         indices.add(value);
                     }
+                    currentIndexMatchesIndices = matchesIndices(parseContext.index().name(), indices.toArray(new String[indices.size()]));
                 } else {
                     throw new ParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("index".equals(currentFieldName)) {
-                    if (indices.isEmpty() == false) {
+                    if (indicesFound) {
                         throw new ParsingException(parseContext, "[indices] indices or index already specified");
                     }
-                    indices.add(parser.text());
+                    indicesFound = true;
+                    currentIndexMatchesIndices = matchesIndices(parseContext.index().name(), parser.text());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_QUERY)) {
-                    noMatchQuery = parseNoMatchQuery(parser.text());
+                    String type = parser.text();
+                    if ("all".equals(type)) {
+                        noMatchQuery = Queries.newMatchAllQuery();
+                    } else if ("none".equals(type)) {
+                        noMatchQuery = Queries.newMatchNoDocsQuery();
+                    }
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        
-        if (innerQuery == null) {
+        if (!queryFound) {
             throw new ParsingException(parseContext, "[indices] requires 'query' element");
         }
-        if (indices.isEmpty()) {
+        if (!indicesFound) {
             throw new ParsingException(parseContext, "[indices] requires 'indices' or 'index' element");
         }
-        return new IndicesQueryBuilder(innerQuery, indices.toArray(new String[indices.size()]))
-                .noMatchQuery(noMatchQuery)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    static QueryBuilder parseNoMatchQuery(String type) {
-        if ("all".equals(type)) {
-            return QueryBuilders.matchAllQuery();
-        } else if ("none".equals(type)) {
-            return new MatchNoneQueryBuilder();
+        Query chosenQuery;
+        if (currentIndexMatchesIndices) {
+            chosenQuery = innerQuery.asQuery();
+        } else {
+            // If noMatchQuery is set, it means "no_match_query" was "all" or "none"
+            if (noMatchQuery != null) {
+                chosenQuery = noMatchQuery;
+            } else {
+                // There might be no "no_match_query" set, so default to the match_all if not set
+                if (innerNoMatchQuery == null) {
+                    chosenQuery = Queries.newMatchAllQuery();
+                } else {
+                    chosenQuery = innerNoMatchQuery.asQuery();
+                }
+            }
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, chosenQuery);
         }
-        throw new IllegalArgumentException("query type can only be [all] or [none] but not " + "[" + type + "]");
+        return chosenQuery;
     }
 
-    @Override
-    public IndicesQueryBuilder getBuilderPrototype() {
-        return IndicesQueryBuilder.PROTOTYPE;
+    protected boolean matchesIndices(String currentIndex, String... indices) {
+        final String[] concreteIndices = indexNameExpressionResolver.concreteIndices(clusterService.state(), IndicesOptions.lenientExpandOpen(), indices);
+        for (String index : concreteIndices) {
+            if (Regex.simpleMatch(index, currentIndex)) {
+                return true;
+            }
+        }
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
index 934d32f..b09bc9f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -30,46 +26,26 @@ import java.io.IOException;
 /**
  * A query that matches on all documents.
  */
-public class MatchAllQueryBuilder extends AbstractQueryBuilder<MatchAllQueryBuilder> {
+public class MatchAllQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchAllQueryBuilder> {
 
-    public static final String NAME = "match_all";
-
-    static final MatchAllQueryBuilder PROTOTYPE = new MatchAllQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    protected boolean doEquals(MatchAllQueryBuilder other) {
-        return true;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return 0;
-    }
+    private float boost = -1;
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected MatchAllQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchAllQueryBuilder();
+    public MatchAllQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(MatchAllQueryParser.NAME);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
index 57afc06..8582f54 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
@@ -19,52 +19,58 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for match_all query
+ *
  */
-public class MatchAllQueryParser extends BaseQueryParser<MatchAllQueryBuilder> {
+public class MatchAllQueryParser implements QueryParser {
+
+    public static final String NAME = "match_all";
+
+    @Inject
+    public MatchAllQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{MatchAllQueryBuilder.NAME, Strings.toCamelCase(MatchAllQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public MatchAllQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
+        float boost = 1.0f;
         String currentFieldName = null;
+
         XContentParser.Token token;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         while (((token = parser.nextToken()) != XContentParser.Token.END_OBJECT && token != XContentParser.Token.END_ARRAY)) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[match_all] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        MatchAllQueryBuilder queryBuilder = new MatchAllQueryBuilder();
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public MatchAllQueryBuilder getBuilderPrototype() {
-        return MatchAllQueryBuilder.PROTOTYPE;
+        if (boost == 1.0f) {
+            return Queries.newMatchAllQuery();
+        }
+
+        MatchAllDocsQuery query = new MatchAllDocsQuery();
+        query.setBoost(boost);
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java
deleted file mode 100644
index 0c466a4..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryBuilder.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * A query that matches no document.
- */
-public class MatchNoneQueryBuilder extends AbstractQueryBuilder<MatchNoneQueryBuilder> {
-
-    public static final String NAME = "match_none";
-
-    public static final MatchNoneQueryBuilder PROTOTYPE = new MatchNoneQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchNoDocsQuery();
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //no-op this query doesn't support boost
-    }
-
-    @Override
-    protected boolean doEquals(MatchNoneQueryBuilder other) {
-        return true;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    protected MatchNoneQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java
deleted file mode 100644
index 24a82c1..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MatchNoneQueryParser.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentParser;
-
-import java.io.IOException;
-
-public class MatchNoneQueryParser extends BaseQueryParser {
-
-    @Override
-    public String[] names() {
-        return new String[]{MatchNoneQueryBuilder.NAME, Strings.toCamelCase(MatchNoneQueryBuilder.NAME)};
-    }
-
-    @Override
-    public MatchNoneQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        XContentParser parser = parseContext.parser();
-
-        XContentParser.Token token = parser.nextToken();
-        if (token != XContentParser.Token.END_OBJECT) {
-            throw new ParsingException(parseContext, "[match_none] query malformed");
-        }
-
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    public MatchNoneQueryBuilder getBuilderPrototype() {
-        return MatchNoneQueryBuilder.PROTOTYPE;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index fdbfd33..c7c530b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -19,112 +19,97 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.elasticsearch.index.search.MatchQuery;
 
 import java.io.IOException;
 import java.util.Locale;
-import java.util.Objects;
 
 /**
  * Match query is a query that analyzes the text and constructs a query as the result of the analysis. It
  * can construct different queries based on the type provided.
  */
-public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
+public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchQueryBuilder> {
 
-    /** The default name for the match query */
-    public static final String NAME = "match";
+    public enum Operator {
+        OR,
+        AND
+    }
 
-    /** The default mode terms are combined in a match query */
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
+    public enum Type {
+        /**
+         * The text is analyzed and terms are added to a boolean query.
+         */
+        BOOLEAN,
+        /**
+         * The text is analyzed and used as a phrase query.
+         */
+        PHRASE,
+        /**
+         * The text is analyzed and used in a phrase query, with the last term acting as a prefix.
+         */
+        PHRASE_PREFIX
+    }
 
-    /** The default mode match query type */
-    public static final MatchQuery.Type DEFAULT_TYPE = MatchQuery.Type.BOOLEAN;
+    public enum ZeroTermsQuery {
+        NONE,
+        ALL
+    }
 
-    private final String fieldName;
+    private final String name;
 
-    private final Object value;
+    private final Object text;
 
-    private MatchQuery.Type type = DEFAULT_TYPE;
+    private Type type;
 
-    private Operator operator = DEFAULT_OPERATOR;
+    private Operator operator;
 
     private String analyzer;
 
-    private int slop = MatchQuery.DEFAULT_PHRASE_SLOP;
+    private Float boost;
 
-    private Fuzziness fuzziness = null;
+    private Integer slop;
 
-    private int prefixLength = FuzzyQuery.defaultPrefixLength;
+    private Fuzziness fuzziness;
 
-    private int  maxExpansions = FuzzyQuery.defaultMaxExpansions;
+    private Integer prefixLength;
 
-    private boolean fuzzyTranspositions = FuzzyQuery.defaultTranspositions;
+    private Integer maxExpansions;
 
     private String minimumShouldMatch;
 
     private String fuzzyRewrite = null;
 
-    private boolean lenient = MatchQuery.DEFAULT_LENIENCY;
+    private Boolean lenient;
+
+    private Boolean fuzzyTranspositions = null;
 
-    private MatchQuery.ZeroTermsQuery zeroTermsQuery = MatchQuery.DEFAULT_ZERO_TERMS_QUERY;
+    private ZeroTermsQuery zeroTermsQuery;
 
-    private Float cutoffFrequency = null;
+    private Float cutoff_Frequency = null;
 
-    static final MatchQueryBuilder PROTOTYPE = new MatchQueryBuilder("","");
+    private String queryName;
 
     /**
-     * Constructs a new match query.
+     * Constructs a new text query.
      */
-    public MatchQueryBuilder(String fieldName, Object value) {
-        if (fieldName == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires fieldName");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires query value");
-        }
-        this.fieldName = fieldName;
-        this.value = value;
+    public MatchQueryBuilder(String name, Object text) {
+        this.name = name;
+        this.text = text;
     }
 
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    /** Returns the value used in this query. */
-    public Object value() {
-        return this.value;
-    }
-
-    /** Sets the type of the text query. */
-    public MatchQueryBuilder type(MatchQuery.Type type) {
-        if (type == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires type to be non-null");
-        }
+    /**
+     * Sets the type of the text query.
+     */
+    public MatchQueryBuilder type(Type type) {
         this.type = type;
         return this;
     }
 
-    /** Get the type of the query. */
-    public MatchQuery.Type type() {
-        return this.type;
-    }
-
-    /** Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>. */
+    /**
+     * Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>.
+     */
     public MatchQueryBuilder operator(Operator operator) {
-        if (operator == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires operator to be non-null");
-        }
         this.operator = operator;
         return this;
     }
@@ -138,326 +123,147 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         return this;
     }
 
-    /** Get the analyzer to use, if previously set, otherwise <tt>null</tt> */
-    public String analyzer() {
-        return this.analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    /** Sets a slop factor for phrase queries */
+    /**
+     * Set the phrase slop if evaluated to a phrase query type.
+     */
     public MatchQueryBuilder slop(int slop) {
-        if (slop < 0 ) {
-            throw new IllegalArgumentException("No negative slop allowed.");
-        }
         this.slop = slop;
         return this;
     }
 
-    /** Get the slop factor for phrase queries. */
-    public int slop() {
-        return this.slop;
-    }
-
-    /** Sets the fuzziness used when evaluated to a fuzzy query type. Defaults to "AUTO". */
+    /**
+     * Sets the fuzziness used when evaluated to a fuzzy query type. Defaults to "AUTO".
+     */
     public MatchQueryBuilder fuzziness(Object fuzziness) {
         this.fuzziness = Fuzziness.build(fuzziness);
         return this;
     }
 
-    /**  Gets the fuzziness used when evaluated to a fuzzy query type. */
-    public Fuzziness fuzziness() {
-        return this.fuzziness;
-    }
-
-    /**
-     * Sets the length of a length of common (non-fuzzy) prefix for fuzzy match queries
-     * @param prefixLength non-negative length of prefix
-     * @throws IllegalArgumentException in case the prefix is negative
-     */
     public MatchQueryBuilder prefixLength(int prefixLength) {
-        if (prefixLength < 0 ) {
-            throw new IllegalArgumentException("No negative prefix length allowed.");
-        }
         this.prefixLength = prefixLength;
         return this;
     }
 
     /**
-     * Gets the length of a length of common (non-fuzzy) prefix for fuzzy match queries
-     */
-    public int prefixLength() {
-        return this.prefixLength;
-    }
-
-    /**
-     * When using fuzzy or prefix type query, the number of term expansions to use.
+     * When using fuzzy or prefix type query, the number of term expansions to use. Defaults to unbounded
+     * so its recommended to set it to a reasonable value for faster execution.
      */
     public MatchQueryBuilder maxExpansions(int maxExpansions) {
-        if (maxExpansions < 0 ) {
-            throw new IllegalArgumentException("No negative maxExpansions allowed.");
-        }
         this.maxExpansions = maxExpansions;
         return this;
     }
 
     /**
-     * Get the (optional) number of term expansions when using fuzzy or prefix type query.
-     */
-    public int maxExpansions() {
-        return this.maxExpansions;
-    }
-
-    /**
-     * Sets an optional cutoff value in [0..1] (or absolute number >=1) representing the
+     * Set a cutoff value in [0..1] (or absolute number &gt;=1) representing the
      * maximum threshold of a terms document frequency to be considered a low
      * frequency term.
      */
     public MatchQueryBuilder cutoffFrequency(float cutoff) {
-        this.cutoffFrequency = cutoff;
+        this.cutoff_Frequency = cutoff;
         return this;
     }
 
-    /** Gets the optional cutoff value, can be <tt>null</tt> if not set previously */
-    public Float cutoffFrequency() {
-        return this.cutoffFrequency;
-    }
-
-    /** Sets optional minimumShouldMatch value to apply to the query */
     public MatchQueryBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    /** Gets the minimumShouldMatch value */
-    public String minimumShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
-    /** Sets the fuzzy_rewrite parameter controlling how the fuzzy query will get rewritten */
     public MatchQueryBuilder fuzzyRewrite(String fuzzyRewrite) {
         this.fuzzyRewrite = fuzzyRewrite;
         return this;
     }
 
-    /**
-     * Get the fuzzy_rewrite parameter
-     * @see #fuzzyRewrite(String)
-     */
-    public String fuzzyRewrite() {
-        return this.fuzzyRewrite;
-    }
-
-    /**
-     * Sets whether transpositions are supported in fuzzy queries.<p>
-     * The default metric used by fuzzy queries to determine a match is the Damerau-Levenshtein
-     * distance formula which supports transpositions. Setting transposition to false will
-     * switch to classic Levenshtein distance.<br>
-     * If not set, Damerau-Levenshtein distance metric will be used.
-     */
     public MatchQueryBuilder fuzzyTranspositions(boolean fuzzyTranspositions) {
+        //LUCENE 4 UPGRADE add documentation
         this.fuzzyTranspositions = fuzzyTranspositions;
         return this;
     }
 
-    /** Gets the fuzzy query transposition setting. */
-    public boolean fuzzyTranspositions() {
-        return this.fuzzyTranspositions;
-    }
-
     /**
      * Sets whether format based failures will be ignored.
-     * @deprecated use #lenient() instead
      */
-    @Deprecated
     public MatchQueryBuilder setLenient(boolean lenient) {
-        return lenient(lenient);
-    }
-
-    /**
-     * Sets whether format based failures will be ignored.
-     */
-    public MatchQueryBuilder lenient(boolean lenient) {
         this.lenient = lenient;
         return this;
     }
 
-    /**
-     * Gets leniency setting that controls if format based failures will be ignored.
-     */
-    public boolean lenient() {
-        return this.lenient;
-    }
-
-    /**
-     * Sets query to use in case no query terms are available, e.g. after analysis removed them.
-     * Defaults to {@link MatchQuery.ZeroTermsQuery#NONE}, but can be set to
-     * {@link MatchQuery.ZeroTermsQuery#ALL} instead.
-     */
-    public MatchQueryBuilder zeroTermsQuery(MatchQuery.ZeroTermsQuery zeroTermsQuery) {
-        if (zeroTermsQuery == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires zeroTermsQuery to be non-null");
-        }
+    public MatchQueryBuilder zeroTermsQuery(ZeroTermsQuery zeroTermsQuery) {
         this.zeroTermsQuery = zeroTermsQuery;
         return this;
     }
 
     /**
-     * Get the setting for handling zero terms queries.
-     * @see #zeroTermsQuery(ZeroTermsQuery)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public MatchQuery.ZeroTermsQuery zeroTermsQuery() {
-        return this.zeroTermsQuery;
+    public MatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
+        builder.startObject(MatchQueryParser.NAME);
+        builder.startObject(name);
 
-        builder.field("query", value);
-        builder.field("type", type.toString().toLowerCase(Locale.ENGLISH));
-        builder.field("operator", operator.toString());
+        builder.field("query", text);
+        if (type != null) {
+            builder.field("type", type.toString().toLowerCase(Locale.ENGLISH));
+        }
+        if (operator != null) {
+            builder.field("operator", operator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("slop", slop);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (slop != null) {
+            builder.field("slop", slop);
+        }
         if (fuzziness != null) {
             fuzziness.toXContent(builder, params);
         }
-        builder.field("prefix_length", prefixLength);
-        builder.field("max_expansions", maxExpansions);
+        if (prefixLength != null) {
+            builder.field("prefix_length", prefixLength);
+        }
+        if (maxExpansions != null) {
+            builder.field("max_expansions", maxExpansions);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
         if (fuzzyRewrite != null) {
             builder.field("fuzzy_rewrite", fuzzyRewrite);
         }
-        // LUCENE 4 UPGRADE we need to document this & test this
-        builder.field("fuzzy_transpositions", fuzzyTranspositions);
-        builder.field("lenient", lenient);
-        builder.field("zero_terms_query", zeroTermsQuery.toString());
-        if (cutoffFrequency != null) {
-            builder.field("cutoff_frequency", cutoffFrequency);
+        if (fuzzyTranspositions != null) {
+            //LUCENE 4 UPGRADE we need to document this & test this
+            builder.field("fuzzy_transpositions", fuzzyTranspositions);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // validate context specific fields
-        if (analyzer != null && context.analysisService().analyzer(analyzer) == null) {
-            throw new QueryShardException(context, "[match] analyzer [" + analyzer + "] not found");
-        }
-
-        MatchQuery matchQuery = new MatchQuery(context);
-        matchQuery.setOccur(operator.toBooleanClauseOccur());
-        matchQuery.setAnalyzer(analyzer);
-        matchQuery.setPhraseSlop(slop);
-        matchQuery.setFuzziness(fuzziness);
-        matchQuery.setFuzzyPrefixLength(prefixLength);
-        matchQuery.setMaxExpansions(maxExpansions);
-        matchQuery.setTranspositions(fuzzyTranspositions);
-        matchQuery.setFuzzyRewriteMethod(QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), fuzzyRewrite, null));
-        matchQuery.setLenient(lenient);
-        matchQuery.setCommonTermsCutoff(cutoffFrequency);
-        matchQuery.setZeroTermsQuery(zeroTermsQuery);
-
-        Query query = matchQuery.parse(type, fieldName, value);
-        if (query == null) {
-            return null;
+        if (lenient != null) {
+            builder.field("lenient", lenient);
         }
-
-        if (query instanceof BooleanQuery) {
-            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
-        } else if (query instanceof ExtendedCommonTermsQuery) {
-            ((ExtendedCommonTermsQuery)query).setLowFreqMinimumNumberShouldMatch(minimumShouldMatch);
+        if (zeroTermsQuery != null) {
+            builder.field("zero_terms_query", zeroTermsQuery.toString());
         }
-        return query;
-    }
-
-    @Override
-    protected boolean doEquals(MatchQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(value, other.value) &&
-               Objects.equals(type, other.type) &&
-               Objects.equals(operator, other.operator) &&
-               Objects.equals(analyzer, other.analyzer) &&
-               Objects.equals(slop, other.slop) &&
-               Objects.equals(fuzziness, other.fuzziness) &&
-               Objects.equals(prefixLength, other.prefixLength) &&
-               Objects.equals(maxExpansions, other.maxExpansions) &&
-               Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-               Objects.equals(fuzzyRewrite, other.fuzzyRewrite) &&
-               Objects.equals(lenient, other.lenient) &&
-               Objects.equals(fuzzyTranspositions, other.fuzzyTranspositions) &&
-               Objects.equals(zeroTermsQuery, other.zeroTermsQuery) &&
-               Objects.equals(cutoffFrequency, other.cutoffFrequency);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, value, type, operator, analyzer, slop,
-                fuzziness, prefixLength, maxExpansions, minimumShouldMatch,
-                fuzzyRewrite, lenient, fuzzyTranspositions, zeroTermsQuery, cutoffFrequency);
-    }
-
-    @Override
-    protected MatchQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MatchQueryBuilder matchQuery = new MatchQueryBuilder(in.readString(), in.readGenericValue());
-        matchQuery.type = MatchQuery.Type.readTypeFrom(in);
-        matchQuery.operator = Operator.readOperatorFrom(in);
-        matchQuery.slop = in.readVInt();
-        matchQuery.prefixLength = in.readVInt();
-        matchQuery.maxExpansions = in.readVInt();
-        matchQuery.fuzzyTranspositions = in.readBoolean();
-        matchQuery.lenient = in.readBoolean();
-        matchQuery.zeroTermsQuery = MatchQuery.ZeroTermsQuery.readZeroTermsQueryFrom(in);
-        // optional fields
-        matchQuery.analyzer = in.readOptionalString();
-        matchQuery.minimumShouldMatch = in.readOptionalString();
-        matchQuery.fuzzyRewrite = in.readOptionalString();
-        if (in.readBoolean()) {
-            matchQuery.fuzziness = Fuzziness.readFuzzinessFrom(in);
+        if (cutoff_Frequency != null) {
+            builder.field("cutoff_frequency", cutoff_Frequency);
         }
-        if (in.readBoolean()) {
-            matchQuery.cutoffFrequency = in.readFloat();
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return matchQuery;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeGenericValue(value);
-        type.writeTo(out);
-        operator.writeTo(out);
-        out.writeVInt(slop);
-        out.writeVInt(prefixLength);
-        out.writeVInt(maxExpansions);
-        out.writeBoolean(fuzzyTranspositions);
-        out.writeBoolean(lenient);
-        zeroTermsQuery.writeTo(out);
-        // optional fields
-        out.writeOptionalString(analyzer);
-        out.writeOptionalString(minimumShouldMatch);
-        out.writeOptionalString(fuzzyRewrite);
-        if (fuzziness == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            fuzziness.writeTo(out);
-        }
-        if (cutoffFrequency == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeFloat(cutoffFrequency);
-        }
-    }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
index 9bd2998..5e8a516 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
@@ -19,30 +19,40 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.support.QueryParsers;
 import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MatchQuery.ZeroTermsQuery;
 
 import java.io.IOException;
 
 /**
  *
  */
-public class MatchQueryParser extends BaseQueryParser {
+public class MatchQueryParser implements QueryParser {
+
+    public static final String NAME = "match";
+
+    @Inject
+    public MatchQueryParser() {
+    }
 
     @Override
     public String[] names() {
         return new String[]{
-                MatchQueryBuilder.NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
+                NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
         };
     }
 
     @Override
-    public MatchQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         MatchQuery.Type type = MatchQuery.Type.BOOLEAN;
@@ -61,19 +71,9 @@ public class MatchQueryParser extends BaseQueryParser {
         String fieldName = parser.currentName();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
+        MatchQuery matchQuery = new MatchQuery(parseContext);
         String minimumShouldMatch = null;
-        String analyzer = null;
-        Operator operator = MatchQueryBuilder.DEFAULT_OPERATOR;
-        int slop = MatchQuery.DEFAULT_PHRASE_SLOP;
-        Fuzziness fuzziness = null;
-        int prefixLength = FuzzyQuery.defaultPrefixLength;
-        int maxExpansion = FuzzyQuery.defaultMaxExpansions;
-        boolean fuzzyTranspositions = FuzzyQuery.defaultTranspositions;
-        String fuzzyRewrite = null;
-        boolean lenient = MatchQuery.DEFAULT_LENIENCY;
-        Float cutOffFrequency = null;
-        ZeroTermsQuery zeroTermsQuery = MatchQuery.DEFAULT_ZERO_TERMS_QUERY;
         String queryName = null;
 
         token = parser.nextToken();
@@ -97,35 +97,47 @@ public class MatchQueryParser extends BaseQueryParser {
                             throw new ParsingException(parseContext, "[match] query does not support type " + tStr);
                         }
                     } else if ("analyzer".equals(currentFieldName)) {
-                        analyzer = parser.text();
+                        String analyzer = parser.text();
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
+                            throw new ParsingException(parseContext, "[match] analyzer [" + parser.text() + "] not found");
+                        }
+                        matchQuery.setAnalyzer(analyzer);
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("slop".equals(currentFieldName) || "phrase_slop".equals(currentFieldName) || "phraseSlop".equals(currentFieldName)) {
-                        slop = parser.intValue();
+                        matchQuery.setPhraseSlop(parser.intValue());
                     } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fuzziness.FIELD)) {
-                        fuzziness = Fuzziness.parse(parser);
+                        matchQuery.setFuzziness(Fuzziness.parse(parser));
                     } else if ("prefix_length".equals(currentFieldName) || "prefixLength".equals(currentFieldName)) {
-                        prefixLength = parser.intValue();
+                        matchQuery.setFuzzyPrefixLength(parser.intValue());
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
-                        maxExpansion = parser.intValue();
+                        matchQuery.setMaxExpansions(parser.intValue());
                     } else if ("operator".equals(currentFieldName)) {
-                        operator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.MUST);
+                        } else {
+                            throw new ParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not ["
+                                    + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         minimumShouldMatch = parser.textOrNull();
                     } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
-                        fuzzyRewrite = parser.textOrNull();
+                        matchQuery.setFuzzyRewriteMethod(QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull(), null));
                     } else if ("fuzzy_transpositions".equals(currentFieldName)) {
-                        fuzzyTranspositions = parser.booleanValue();
+                        matchQuery.setTranspositions(parser.booleanValue());
                     } else if ("lenient".equals(currentFieldName)) {
-                        lenient = parser.booleanValue();
+                        matchQuery.setLenient(parser.booleanValue());
                     } else if ("cutoff_frequency".equals(currentFieldName)) {
-                        cutOffFrequency = parser.floatValue();
+                        matchQuery.setCommonTermsCutoff(parser.floatValue());
                     } else if ("zero_terms_query".equals(currentFieldName)) {
                         String zeroTermsDocs = parser.text();
                         if ("none".equalsIgnoreCase(zeroTermsDocs)) {
-                            zeroTermsQuery = MatchQuery.ZeroTermsQuery.NONE;
+                            matchQuery.setZeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE);
                         } else if ("all".equalsIgnoreCase(zeroTermsDocs)) {
-                            zeroTermsQuery = MatchQuery.ZeroTermsQuery.ALL;
+                            matchQuery.setZeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL);
                         } else {
                             throw new ParsingException(parseContext, "Unsupported zero_terms_docs value [" + zeroTermsDocs + "]");
                         }
@@ -151,31 +163,20 @@ public class MatchQueryParser extends BaseQueryParser {
             throw new ParsingException(parseContext, "No text specified for text query");
         }
 
-        MatchQueryBuilder matchQuery = new MatchQueryBuilder(fieldName, value);
-        matchQuery.operator(operator);
-        matchQuery.type(type);
-        matchQuery.analyzer(analyzer);
-        matchQuery.slop(slop);
-        matchQuery.minimumShouldMatch(minimumShouldMatch);
-        if (fuzziness != null) {
-            matchQuery.fuzziness(fuzziness);
+        Query query = matchQuery.parse(type, fieldName, value);
+        if (query == null) {
+            return null;
         }
-        matchQuery.fuzzyRewrite(fuzzyRewrite);
-        matchQuery.prefixLength(prefixLength);
-        matchQuery.fuzzyTranspositions(fuzzyTranspositions);
-        matchQuery.maxExpansions(maxExpansion);
-        matchQuery.lenient(lenient);
-        if (cutOffFrequency != null) {
-            matchQuery.cutoffFrequency(cutOffFrequency);
-        }
-        matchQuery.zeroTermsQuery(zeroTermsQuery);
-        matchQuery.queryName(queryName);
-        matchQuery.boost(boost);
-        return matchQuery;
-    }
 
-    @Override
-    public MatchQueryBuilder getBuilderPrototype() {
-        return MatchQueryBuilder.PROTOTYPE;
+        if (query instanceof BooleanQuery) {
+            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        } else if (query instanceof ExtendedCommonTermsQuery) {
+            ((ExtendedCommonTermsQuery)query).setLowFreqMinimumNumberShouldMatch(minimumShouldMatch);
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
index c3d0ab7..ac3f279 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
@@ -19,216 +19,66 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
- * Constructs a filter that have only null values or no value in the original field.
+ * Constructs a filter that only match on documents that the field has a value in them.
  */
-public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilder> {
+public class MissingQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "missing";
+    private String name;
 
-    public static final boolean DEFAULT_NULL_VALUE = false;
+    private String queryName;
 
-    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
+    private Boolean nullValue;
 
-    private final String fieldPattern;
+    private Boolean existence;
 
-    private final boolean nullValue;
-
-    private final boolean existence;
-
-    static final MissingQueryBuilder PROTOTYPE = new MissingQueryBuilder("field", DEFAULT_NULL_VALUE, DEFAULT_EXISTENCE_VALUE);
+    public MissingQueryBuilder(String name) {
+        this.name = name;
+    }
 
     /**
-     * Constructs a filter that returns documents with only null values or no value in the original field.
-     * @param fieldPattern the field to query
-     * @param nullValue should the missing filter automatically include fields with null value configured in the
+     * Should the missing filter automatically include fields with null value configured in the
      * mappings. Defaults to <tt>false</tt>.
-     * @param existance should the missing filter include documents where the field doesn't exist in the docs.
-     * Defaults to <tt>true</tt>.
-     * @throws IllegalArgumentException when both <tt>existence</tt> and <tt>nullValue</tt> are set to false
      */
-    public MissingQueryBuilder(String fieldPattern, boolean nullValue, boolean existence) {
-        if (Strings.isEmpty(fieldPattern)) {
-            throw new IllegalArgumentException("missing query must be provided with a [field]");
-        }
-        if (nullValue == false && existence == false) {
-            throw new IllegalArgumentException("missing query must have either 'existence', or 'null_value', or both set to true");
-        }
-        this.fieldPattern = fieldPattern;
+    public MissingQueryBuilder nullValue(boolean nullValue) {
         this.nullValue = nullValue;
-        this.existence = existence;
-    }
-
-    public MissingQueryBuilder(String fieldPattern) {
-        this(fieldPattern, DEFAULT_NULL_VALUE, DEFAULT_EXISTENCE_VALUE);
-    }
-
-    public String fieldPattern() {
-        return this.fieldPattern;
+        return this;
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field contains a null value, otherwise
-     * these documents will not be included.
+     * Should the missing filter include documents where the field doesn't exists in the docs.
+     * Defaults to <tt>true</tt>.
      */
-    public boolean nullValue() {
-        return this.nullValue;
+    public MissingQueryBuilder existence(boolean existence) {
+        this.existence = existence;
+        return this;
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field has no values, otherwise
-     * these documents will not be included.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean existence() {
-        return this.existence;
+    public MissingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldPattern);
-        builder.field("null_value", nullValue);
-        builder.field("existence", existence);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldPattern, existence, nullValue);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern, boolean existence, boolean nullValue) {
-        if (!existence && !nullValue) {
-            throw new QueryShardException(context, "missing must have either existence, or null_value, or both set to true");
-        }
-
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
+        builder.startObject(MissingQueryParser.NAME);
+        builder.field("field", name);
+        if (nullValue != null) {
+            builder.field("null_value", nullValue);
         }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            if (existence) {
-                // if we ask for existence of fields, and we found none, then we should match on all
-                return Queries.newMatchAllQuery();
-            }
-            return null;
+        if (existence != null) {
+            builder.field("existence", existence);
         }
-
-        Query existenceFilter = null;
-        Query nullFilter = null;
-
-        if (existence) {
-            BooleanQuery.Builder boolFilter = new BooleanQuery.Builder();
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                Query filter = null;
-                if (fieldNamesFieldType.isEnabled()) {
-                    final String f;
-                    if (fieldType != null) {
-                        f = fieldType.names().indexName();
-                    } else {
-                        f = field;
-                    }
-                    filter = fieldNamesFieldType.termQuery(f, context);
-                }
-                // if _field_names are not indexed, we need to go the slow way
-                if (filter == null && fieldType != null) {
-                    filter = fieldType.rangeQuery(null, null, true, true);
-                }
-                if (filter == null) {
-                    filter = new TermRangeQuery(field, null, null, true, true);
-                }
-                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
-            }
-
-            existenceFilter = boolFilter.build();
-            existenceFilter = Queries.not(existenceFilter);;
-        }
-
-        if (nullValue) {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                if (fieldType != null) {
-                    nullFilter = fieldType.nullValueQuery();
-                }
-            }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        Query filter;
-        if (nullFilter != null) {
-            if (existenceFilter != null) {
-                filter = new BooleanQuery.Builder()
-                        .add(existenceFilter, BooleanClause.Occur.SHOULD)
-                        .add(nullFilter, BooleanClause.Occur.SHOULD)
-                        .build();
-            } else {
-                filter = nullFilter;
-            }
-        } else {
-            filter = existenceFilter;
-        }
-
-        if (filter == null) {
-            return null;
-        }
-
-        return new ConstantScoreQuery(filter);
-    }
-
-    @Override
-    protected MissingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MissingQueryBuilder(in.readString(), in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldPattern);
-        out.writeBoolean(nullValue);
-        out.writeBoolean(existence);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldPattern, nullValue, existence);
-    }
-
-    @Override
-    protected boolean doEquals(MissingQueryBuilder other) {
-        return Objects.equals(fieldPattern, other.fieldPattern) &&
-                Objects.equals(nullValue, other.nullValue) &&
-                Objects.equals(existence, other.existence);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
index c8fa5ff..df849ff 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
@@ -19,30 +19,48 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for missing query
+ *
  */
-public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
+public class MissingQueryParser implements QueryParser {
+
+    public static final String NAME = "missing";
+    public static final boolean DEFAULT_NULL_VALUE = false;
+    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
+
+    @Inject
+    public MissingQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{MissingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public MissingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean nullValue = MissingQueryBuilder.DEFAULT_NULL_VALUE;
-        boolean existence = MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE;
+        boolean nullValue = DEFAULT_NULL_VALUE;
+        boolean existence = DEFAULT_EXISTENCE_VALUE;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -58,8 +76,6 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
                     existence = parser.booleanValue();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[missing] query does not support [" + currentFieldName + "]");
                 }
@@ -69,13 +85,97 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
         if (fieldPattern == null) {
             throw new ParsingException(parseContext, "missing must be provided with a [field]");
         }
-        return new MissingQueryBuilder(fieldPattern, nullValue, existence)
-                .boost(boost)
-                .queryName(queryName);
+
+        return newFilter(parseContext, fieldPattern, existence, nullValue, queryName);
     }
 
-    @Override
-    public MissingQueryBuilder getBuilderPrototype() {
-        return MissingQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, boolean existence, boolean nullValue, String queryName) {
+        if (!existence && !nullValue) {
+            throw new ParsingException(parseContext, "missing must have either existence, or null_value, or both set to true");
+        }
+
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            if (existence) {
+                // if we ask for existence of fields, and we found none, then we should match on all
+                return Queries.newMatchAllQuery();
+            }
+            return null;
+        }
+
+        Query existenceFilter = null;
+        Query nullFilter = null;
+
+        if (existence) {
+            BooleanQuery.Builder boolFilter = new BooleanQuery.Builder();
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                Query filter = null;
+                if (fieldNamesFieldType.isEnabled()) {
+                    final String f;
+                    if (fieldType != null) {
+                        f = fieldType.names().indexName();
+                    } else {
+                        f = field;
+                    }
+                    filter = fieldNamesFieldType.termQuery(f, parseContext);
+                }
+                // if _field_names are not indexed, we need to go the slow way
+                if (filter == null && fieldType != null) {
+                    filter = fieldType.rangeQuery(null, null, true, true);
+                }
+                if (filter == null) {
+                    filter = new TermRangeQuery(field, null, null, true, true);
+                }
+                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+            }
+
+            existenceFilter = boolFilter.build();
+            existenceFilter = Queries.not(existenceFilter);;
+        }
+
+        if (nullValue) {
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                if (fieldType != null) {
+                    nullFilter = fieldType.nullValueQuery();
+                }
+            }
+        }
+
+        Query filter;
+        if (nullFilter != null) {
+            if (existenceFilter != null) {
+                filter = new BooleanQuery.Builder()
+                    .add(existenceFilter, BooleanClause.Occur.SHOULD)
+                    .add(nullFilter, BooleanClause.Occur.SHOULD)
+                    .build();
+            } else {
+                filter = nullFilter;
+            }
+        } else {
+            filter = existenceFilter;
+        }
+
+        if (filter == null) {
+            return null;
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, existenceFilter);
+        }
+        return new ConstantScoreQuery(filter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
index 51072e7..5c7e24b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
@@ -19,92 +19,33 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.action.termvectors.*;
-import org.elasticsearch.client.Client;
+import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.lucene.search.MoreLikeThisQuery;
-import org.elasticsearch.common.lucene.search.XMoreLikeThis;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.analysis.Analysis;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.*;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.mapper.Uid.createUidAsBytes;
 
 /**
  * A more like this query that finds documents that are "like" the provided set of document(s).
  *
  * The documents are provided as a set of strings and/or a list of {@link Item}.
  */
-public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQueryBuilder> {
-
-    public static final String NAME = "mlt";
-
-    public static final int DEFAULT_MAX_QUERY_TERMS = XMoreLikeThis.DEFAULT_MAX_QUERY_TERMS;
-    public static final int DEFAULT_MIN_TERM_FREQ = XMoreLikeThis.DEFAULT_MIN_TERM_FREQ;
-    public static final int DEFAULT_MIN_DOC_FREQ = XMoreLikeThis.DEFAULT_MIN_DOC_FREQ;
-    public static final int DEFAULT_MAX_DOC_FREQ = XMoreLikeThis.DEFAULT_MAX_DOC_FREQ;
-    public static final int DEFAULT_MIN_WORD_LENGTH = XMoreLikeThis.DEFAULT_MIN_WORD_LENGTH;
-    public static final int DEFAULT_MAX_WORD_LENGTH = XMoreLikeThis.DEFAULT_MAX_WORD_LENGTH;
-    public static final String DEFAULT_MINIMUM_SHOULD_MATCH = MoreLikeThisQuery.DEFAULT_MINIMUM_SHOULD_MATCH;
-    public static final float DEFAULT_BOOST_TERMS = 0;  // no boost terms
-    public static final boolean DEFAULT_INCLUDE = false;
-    public static final boolean DEFAULT_FAIL_ON_UNSUPPORTED_FIELDS = true;
-
-    // document inputs
-    private final List<String> fields;
-    private List<String> likeTexts = new ArrayList<>();
-    private List<String> unlikeTexts = new ArrayList<>();
-    private List<Item> likeItems = new ArrayList<>();
-    private List<Item> unlikeItems = new ArrayList<>();
-
-    // term selection parameters
-    private int maxQueryTerms = DEFAULT_MAX_QUERY_TERMS;
-    private int minTermFreq = DEFAULT_MIN_TERM_FREQ;
-    private int minDocFreq = DEFAULT_MIN_DOC_FREQ;
-    private int maxDocFreq = DEFAULT_MAX_DOC_FREQ;
-    private int minWordLength = DEFAULT_MIN_WORD_LENGTH;
-    private int maxWordLength = DEFAULT_MAX_WORD_LENGTH;
-    private String[] stopWords;
-    private String analyzer;
-
-    // query formation parameters
-    private String minimumShouldMatch = DEFAULT_MINIMUM_SHOULD_MATCH;
-    private float boostTerms = DEFAULT_BOOST_TERMS;
-    private boolean include = DEFAULT_INCLUDE;
-
-    // other parameters
-    private boolean failOnUnsupportedField = DEFAULT_FAIL_ON_UNSUPPORTED_FIELDS;
-
-    static final MoreLikeThisQueryBuilder PROTOTYPE = new MoreLikeThisQueryBuilder();
+public class MoreLikeThisQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MoreLikeThisQueryBuilder> {
 
     /**
      * A single item to be used for a {@link MoreLikeThisQueryBuilder}.
      */
-    public static final class Item implements ToXContent, Writeable<Item> {
+    public static final class Item implements ToXContent {
         public static final Item[] EMPTY_ARRAY = new Item[0];
 
         public interface Field {
@@ -129,8 +70,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         private long version = Versions.MATCH_ANY;
         private VersionType versionType = VersionType.INTERNAL;
 
-        static final Item PROTOTYPE = new Item();
-
         public Item() {
 
         }
@@ -142,10 +81,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
          * @param type the type of the document
          * @param id and its id
          */
-        public Item(@Nullable String index, @Nullable String type, String id) {
-            if (id == null) {
-                throw new IllegalArgumentException("Item requires id to be non-null");
-            }
+        public Item(String index, @Nullable String type, String id) {
             this.index = index;
             this.type = type;
             this.id = id;
@@ -158,13 +94,10 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
          * @param type the type to be used for parsing the doc
          * @param doc the document specification
          */
-        public Item(@Nullable String index, @Nullable String type, XContentBuilder doc) {
-            if (doc == null) {
-                throw new IllegalArgumentException("Item requires doc to be non-null");
-            }
+        public Item(String index, String type, XContentBuilder doc) {
             this.index = index;
             this.type = type;
-            this.doc = doc.bytes();
+            this.doc(doc);
         }
 
         public String index() {
@@ -189,10 +122,30 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
             return id;
         }
 
+        public Item id(String id) {
+            this.id = id;
+            return this;
+        }
+
         public BytesReference doc() {
             return doc;
         }
 
+        /**
+         * Sets to a given artificial document, that is a document that is not present in the index.
+         */
+        public Item doc(BytesReference doc) {
+            this.doc = doc;
+            return this;
+        }
+
+        /**
+         * Sets to a given artificial document, that is a document that is not present in the index.
+         */
+        public Item doc(XContentBuilder doc) {
+            return this.doc(doc.bytes());
+        }
+
         public String[] fields() {
             return fields;
         }
@@ -260,7 +213,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
             // for artificial docs to make sure that the id has changed in the item too
             if (doc != null) {
                 termVectorsRequest.doc(doc, true);
-                this.id = termVectorsRequest.id();
+                this.id(termVectorsRequest.id());
             }
             return termVectorsRequest;
         }
@@ -282,7 +235,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
                     } else if (parseFieldMatcher.match(currentFieldName, Field.ID)) {
                         item.id = parser.text();
                     } else if (parseFieldMatcher.match(currentFieldName, Field.DOC)) {
-                        item.doc = jsonBuilder().copyCurrentStructure(parser).bytes();
+                        item.doc(jsonBuilder().copyCurrentStructure(parser));
                     } else if (parseFieldMatcher.match(currentFieldName, Field.FIELDS)) {
                         if (token == XContentParser.Token.START_ARRAY) {
                             List<String> fields = new ArrayList<>();
@@ -313,10 +266,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
                 throw new ElasticsearchParseException(
                         "failed to parse More Like This item. either [id] or [doc] can be specified, but not both!");
             }
-            if (item.id == null && item.doc == null) {
-                throw new ElasticsearchParseException(
-                        "failed to parse More Like This item. neither [id] nor [doc] is specified!");
-            }
             return item;
         }
 
@@ -329,7 +278,7 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
             if (this.type != null) {
                 builder.field(Field.TYPE.getPreferredName(), this.type);
             }
-            if (this.id != null) {
+            if (this.id != null && this.doc == null) {
                 builder.field(Field.ID.getPreferredName(), this.id);
             }
             if (this.doc != null) {
@@ -374,45 +323,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         }
 
         @Override
-        public Item readFrom(StreamInput in) throws IOException {
-            Item item = new Item();
-            item.index = in.readOptionalString();
-            item.type = in.readOptionalString();
-            if (in.readBoolean()) {
-                item.doc = (BytesReference) in.readGenericValue();
-            } else {
-                item.id = in.readString();
-            }
-            item.fields = in.readOptionalStringArray();
-            item.perFieldAnalyzer = (Map<String, String>) in.readGenericValue();
-            item.routing = in.readOptionalString();
-            item.version = in.readLong();
-            item.versionType = VersionType.readVersionTypeFrom(in);
-            return item;
-        }
-
-        public static Item readItemFrom(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(index);
-            out.writeOptionalString(type);
-            out.writeBoolean(doc != null);
-            if (doc != null) {
-                out.writeGenericValue(doc);
-            } else {
-                out.writeString(id);
-            }
-            out.writeOptionalStringArray(fields);
-            out.writeGenericValue(perFieldAnalyzer);
-            out.writeOptionalString(routing);
-            out.writeLong(version);
-            versionType.writeTo(out);
-        }
-
-        @Override
         public int hashCode() {
             return Objects.hash(index, type, id, doc, Arrays.hashCode(fields), perFieldAnalyzer, routing,
                     version, versionType);
@@ -435,6 +345,33 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         }
     }
 
+    // document inputs
+    private List<String> likeTexts = new ArrayList<>();
+    private List<String> unlikeTexts = new ArrayList<>();
+    private List<Item> likeItems = new ArrayList<>();
+    private List<Item> unlikeItems = new ArrayList<>();
+    private final String[] fields;
+
+    // term selection parameters
+    private int maxQueryTerms = -1;
+    private int minTermFreq = -1;
+    private int minDocFreq = -1;
+    private int maxDocFreq = -1;
+    private int minWordLength = -1;
+    private int maxWordLength = -1;
+    private String[] stopWords = null;
+    private String analyzer;
+
+    // query formation parameters
+    private String minimumShouldMatch = null;
+    private float boostTerms = -1;
+    private Boolean include = null;
+
+    // other parameters
+    private Boolean failOnUnsupportedField;
+    private float boost = -1;
+    private String queryName;
+
     /**
      * Constructs a new more like this query which uses the "_all" field.
      */
@@ -448,34 +385,17 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
      * @param fields the field names that will be used when generating the 'More Like This' query.
      */
     public MoreLikeThisQueryBuilder(String... fields) {
-        this(Collections.unmodifiableList(Arrays.asList(fields)));
-    }
-
-    /**
-     * Sets the field names that will be used when generating the 'More Like This' query.
-     *
-     * @param fields the field names that will be used when generating the 'More Like This' query.
-     */
-    public MoreLikeThisQueryBuilder(List<String> fields) {
         this.fields = fields;
     }
 
-    public List<String> fields() {
-        return fields;
-    }
-
     /**
      * Sets the text to use in order to find documents that are "like" this.
      *
      * @param likeTexts the text to use when generating the 'More Like This' query.
      */
     public MoreLikeThisQueryBuilder like(String... likeTexts) {
-        this.likeTexts = Collections.unmodifiableList(Arrays.asList(likeTexts));
-        return this;
-    }
-
-    public List<String> likeTexts() {
-        return likeTexts;
+        this.likeTexts = new ArrayList<>();
+        return addLikeText(likeTexts);
     }
 
     /**
@@ -484,36 +404,56 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
      * @param likeItems the documents to use when generating the 'More Like This' query.
      */
     public MoreLikeThisQueryBuilder like(Item... likeItems) {
-        this.likeItems = Collections.unmodifiableList(Arrays.asList(likeItems));
-        return this;
+        this.likeItems = new ArrayList<>();
+        return addLikeItem(likeItems);
     }
 
-    public List<Item> likeItems() {
-        return likeItems;
+    /**
+     * Adds some text to use in order to find documents that are "like" this.
+     */
+    public MoreLikeThisQueryBuilder addLikeText(String... likeTexts) {
+        Collections.addAll(this.likeTexts, likeTexts);
+        return this;
     }
 
     /**
-     * Sets the text from which the terms should not be selected from.
+     * Adds a document to use in order to find documents that are "like" this.
      */
-    public MoreLikeThisQueryBuilder unlike(String... unlikeTexts) {
-        this.unlikeTexts = Collections.unmodifiableList(Arrays.asList(unlikeTexts));
+    public MoreLikeThisQueryBuilder addLikeItem(Item... likeItems) {
+        Collections.addAll(this.likeItems, likeItems);
         return this;
     }
 
-    public List<String> unlikeTexts() {
-        return unlikeTexts;
+    /**
+     * Sets the text from which the terms should not be selected from.
+     */
+    public MoreLikeThisQueryBuilder unlike(String... unlikeTexts) {
+        this.unlikeTexts = new ArrayList<>();
+        return addUnlikeText(unlikeTexts);
     }
 
     /**
      * Sets the documents from which the terms should not be selected from.
      */
     public MoreLikeThisQueryBuilder unlike(Item... unlikeItems) {
-        this.unlikeItems = Collections.unmodifiableList(Arrays.asList(unlikeItems));
+        this.unlikeItems = new ArrayList<>();
+        return addUnlikeItem(unlikeItems);
+    }
+
+    /**
+     * Adds some text to use in order to find documents that are "unlike" this.
+     */
+    public MoreLikeThisQueryBuilder addUnlikeText(String... unlikeTexts) {
+        Collections.addAll(this.unlikeTexts, unlikeTexts);
         return this;
     }
 
-    public List<Item> unlikeItems() {
-        return unlikeItems;
+    /**
+     * Adds a document to use in order to find documents that are "unlike" this.
+     */
+    public MoreLikeThisQueryBuilder addUnlikeItem(Item... unlikeItems) {
+        Collections.addAll(this.unlikeItems, unlikeItems);
+        return this;
     }
 
     /**
@@ -525,10 +465,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int maxQueryTerms() {
-        return maxQueryTerms;
-    }
-
     /**
      * The frequency below which terms will be ignored in the source doc. The default
      * frequency is <tt>2</tt>.
@@ -538,10 +474,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int minTermFreq() {
-        return minTermFreq;
-    }
-
     /**
      * Sets the frequency at which words will be ignored which do not occur in at least this
      * many docs. Defaults to <tt>5</tt>.
@@ -551,10 +483,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int minDocFreq() {
-        return minDocFreq;
-    }
-
     /**
      * Set the maximum frequency in which words may still appear. Words that appear
      * in more than this many docs will be ignored. Defaults to unbounded.
@@ -564,10 +492,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int maxDocFreq() {
-        return maxDocFreq;
-    }
-
     /**
      * Sets the minimum word length below which words will be ignored. Defaults
      * to <tt>0</tt>.
@@ -577,10 +501,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int minWordLength() {
-        return minWordLength;
-    }
-
     /**
      * Sets the maximum word length above which words will be ignored. Defaults to
      * unbounded (<tt>0</tt>).
@@ -590,14 +510,10 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public int maxWordLength() {
-        return maxWordLength;
-    }
-
     /**
      * Set the set of stopwords.
-     * <p/>
-     * <p>Any word in this set is considered "uninteresting" and ignored. Even if your Analyzer allows stopwords, you
+     * <p>
+     * Any word in this set is considered "uninteresting" and ignored. Even if your Analyzer allows stopwords, you
      * might want to tell the MoreLikeThis code to ignore them, as for the purposes of document similarity it seems
      * reasonable to assume that "a stop word is never interesting".
      */
@@ -606,18 +522,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public MoreLikeThisQueryBuilder stopWords(List<String> stopWords) {
-        if (stopWords == null) {
-            throw new IllegalArgumentException("requires stopwords to be non-null");
-        }
-        this.stopWords = stopWords.toArray(new String[stopWords.size()]);
-        return this;
-    }
-
-    public String[] stopWords() {
-        return stopWords;
-    }
-
     /**
      * The analyzer that will be used to analyze the text. Defaults to the analyzer associated with the fied.
      */
@@ -626,10 +530,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public String analyzer() {
-        return analyzer;
-    }
-
     /**
      * Number of terms that must match the generated query expressed in the
      * common syntax for minimum should match. Defaults to <tt>30%</tt>.
@@ -637,29 +537,18 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
      * @see    org.elasticsearch.common.lucene.search.Queries#calculateMinShouldMatch(int, String)
      */
     public MoreLikeThisQueryBuilder minimumShouldMatch(String minimumShouldMatch) {
-        if (minimumShouldMatch == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires minimum should match to be non-null");
-        }
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
     /**
-     * Sets the boost factor to use when boosting terms. Defaults to <tt>0</tt> (deactivated).
+     * Sets the boost factor to use when boosting terms. Defaults to <tt>1</tt>.
      */
     public MoreLikeThisQueryBuilder boostTerms(float boostTerms) {
         this.boostTerms = boostTerms;
         return this;
     }
 
-    public float boostTerms() {
-        return boostTerms;
-    }
-
     /**
      * Whether to include the input documents. Defaults to <tt>false</tt>
      */
@@ -668,20 +557,26 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
-    public boolean include() {
-        return include;
-    }
-
     /**
      * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.
      */
     public MoreLikeThisQueryBuilder failOnUnsupportedField(boolean fail) {
-        this.failOnUnsupportedField = fail;
+        failOnUnsupportedField = fail;
         return this;
     }
 
-    public boolean failOnUnsupportedField() {
-        return failOnUnsupportedField;
+    @Override
+    public MoreLikeThisQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MoreLikeThisQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     /**
@@ -701,343 +596,106 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return like(items);
     }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        if (fields != null) {
-            builder.field(MoreLikeThisQueryParser.Field.FIELDS.getPreferredName(), fields);
-        }
-        buildLikeField(builder, MoreLikeThisQueryParser.Field.LIKE.getPreferredName(), likeTexts, likeItems);
-        if (!unlikeTexts.isEmpty() || !unlikeItems.isEmpty()) {
-            buildLikeField(builder, MoreLikeThisQueryParser.Field.UNLIKE.getPreferredName(), unlikeTexts, unlikeItems);
-        }
-        builder.field(MoreLikeThisQueryParser.Field.MAX_QUERY_TERMS.getPreferredName(), maxQueryTerms);
-        builder.field(MoreLikeThisQueryParser.Field.MIN_TERM_FREQ.getPreferredName(), minTermFreq);
-        builder.field(MoreLikeThisQueryParser.Field.MIN_DOC_FREQ.getPreferredName(), minDocFreq);
-        builder.field(MoreLikeThisQueryParser.Field.MAX_DOC_FREQ.getPreferredName(), maxDocFreq);
-        builder.field(MoreLikeThisQueryParser.Field.MIN_WORD_LENGTH.getPreferredName(), minWordLength);
-        builder.field(MoreLikeThisQueryParser.Field.MAX_WORD_LENGTH.getPreferredName(), maxWordLength);
-        if (stopWords != null) {
-            builder.field(MoreLikeThisQueryParser.Field.STOP_WORDS.getPreferredName(), stopWords);
-        }
-        if (analyzer != null) {
-            builder.field(MoreLikeThisQueryParser.Field.ANALYZER.getPreferredName(), analyzer);
-        }
-        builder.field(MoreLikeThisQueryParser.Field.MINIMUM_SHOULD_MATCH.getPreferredName(), minimumShouldMatch);
-        builder.field(MoreLikeThisQueryParser.Field.BOOST_TERMS.getPreferredName(), boostTerms);
-        builder.field(MoreLikeThisQueryParser.Field.INCLUDE.getPreferredName(), include);
-        builder.field(MoreLikeThisQueryParser.Field.FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    @Deprecated
+    public MoreLikeThisQueryBuilder docs(Item... docs) {
+        return like(docs);
     }
 
-    private static void buildLikeField(XContentBuilder builder, String fieldName, List<String> texts, List<Item> items) throws IOException {
-        builder.startArray(fieldName);
-        for (String text : texts) {
-            builder.value(text);
-        }
-        for (Item item : items) {
-            builder.value(item);
-        }
-        builder.endArray();
+    /**
+     * Sets the documents from which the terms should not be selected from.
+     *
+     * @Deprecated Use {@link #unlike(Item...)} instead
+     */
+    @Deprecated
+    public MoreLikeThisQueryBuilder ignoreLike(Item... docs) {
+        return unlike(docs);
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * Sets the text from which the terms should not be selected from.
+     *
+     * @Deprecated Use {@link #unlike(String...)} instead.
+     */
+    @Deprecated
+    public MoreLikeThisQueryBuilder ignoreLike(String... likeText) {
+        return unlike(likeText);
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MoreLikeThisQuery mltQuery = new MoreLikeThisQuery();
-
-        // set similarity
-        mltQuery.setSimilarity(context.searchSimilarity());
-
-        // set query parameters
-        mltQuery.setMaxQueryTerms(maxQueryTerms);
-        mltQuery.setMinTermFrequency(minTermFreq);
-        mltQuery.setMinDocFreq(minDocFreq);
-        mltQuery.setMaxDocFreq(maxDocFreq);
-        mltQuery.setMinWordLen(minWordLength);
-        mltQuery.setMaxWordLen(maxWordLength);
-        mltQuery.setMinimumShouldMatch(minimumShouldMatch);
-        if (stopWords != null) {
-            mltQuery.setStopWords(new HashSet<>(Arrays.asList(stopWords)));
-        }
-
-        // sets boost terms
-        if (boostTerms != 0) {
-            mltQuery.setBoostTerms(true);
-            mltQuery.setBoostTermsFactor(boostTerms);
-        }
-
-        // set analyzer
-        Analyzer analyzerObj = context.analysisService().analyzer(analyzer);
-        if (analyzerObj == null) {
-            analyzerObj = context.mapperService().searchAnalyzer();
-        }
-        mltQuery.setAnalyzer(analyzerObj);
-
-        // set like text fields
-        boolean useDefaultField = (fields == null);
-        List<String> moreLikeFields = new ArrayList<>();
-        if (useDefaultField) {
-            moreLikeFields = Collections.singletonList(context.defaultField());
-        } else {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName());
-            }
-        }
-
-        // possibly remove unsupported fields
-        removeUnsupportedFields(moreLikeFields, analyzerObj, failOnUnsupportedField);
-        if (moreLikeFields.isEmpty()) {
-            return null;
-        }
-        mltQuery.setMoreLikeFields(moreLikeFields.toArray(Strings.EMPTY_ARRAY));
+    /**
+     * Adds a document to use in order to find documents that are "like" this.
+     */
+    @Deprecated
+    public MoreLikeThisQueryBuilder addItem(Item... likeItems) {
+        return addLikeItem(likeItems);
+    }
 
-        // handle like texts
-        if (likeTexts.isEmpty() == false) {
-            mltQuery.setLikeText(likeTexts);
-        }
-        if (unlikeTexts.isEmpty() == false) {
-            mltQuery.setUnlikeText(unlikeTexts);
+    @Override
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(MoreLikeThisQueryParser.NAME);
+        if (fields != null) {
+            builder.field(MoreLikeThisQueryParser.Field.FIELDS.getPreferredName(), fields);
         }
-
-        // handle items
-        if (likeItems.isEmpty() == false) {
-            return handleItems(context, mltQuery, likeItems, unlikeItems, include, moreLikeFields, useDefaultField);
+        if (this.likeTexts.isEmpty() && this.likeItems.isEmpty()) {
+            throw new IllegalArgumentException("more_like_this requires '" + MoreLikeThisQueryParser.Field.LIKE.getPreferredName() + "' to be provided");
         } else {
-            return mltQuery;
-        }
-    }
-
-    private static List<String> removeUnsupportedFields(List<String> moreLikeFields, Analyzer analyzer, boolean failOnUnsupportedField) throws IOException {
-        for (Iterator<String> it = moreLikeFields.iterator(); it.hasNext(); ) {
-            final String fieldName = it.next();
-            if (!Analysis.generatesCharacterTokenStream(analyzer, fieldName)) {
-                if (failOnUnsupportedField) {
-                    throw new IllegalArgumentException("more_like_this doesn't support binary/numeric fields: [" + fieldName + "]");
-                } else {
-                    it.remove();
-                }
-            }
+            buildLikeField(builder, MoreLikeThisQueryParser.Field.LIKE.getPreferredName(), likeTexts, likeItems);
         }
-        return moreLikeFields;
-    }
-
-    private Query handleItems(QueryShardContext context, MoreLikeThisQuery mltQuery, List<Item> likeItems, List<Item> unlikeItems,
-                              boolean include, List<String> moreLikeFields, boolean useDefaultField) throws IOException {
-        // set default index, type and fields if not specified
-        for (Item item : likeItems) {
-            setDefaultIndexTypeFields(context, item, moreLikeFields, useDefaultField);
+        if (!unlikeTexts.isEmpty() || !unlikeItems.isEmpty()) {
+            buildLikeField(builder, MoreLikeThisQueryParser.Field.UNLIKE.getPreferredName(), unlikeTexts, unlikeItems);
         }
-        for (Item item : unlikeItems) {
-            setDefaultIndexTypeFields(context, item, moreLikeFields, useDefaultField);
+        if (maxQueryTerms != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MAX_QUERY_TERMS.getPreferredName(), maxQueryTerms);
         }
-
-        // fetching the items with multi-termvectors API
-        MultiTermVectorsResponse responses = fetchResponse(context.getClient(), likeItems, unlikeItems, SearchContext.current());
-
-        // getting the Fields for liked items
-        mltQuery.setLikeText(getFieldsFor(responses, likeItems));
-
-        // getting the Fields for unliked items
-        if (!unlikeItems.isEmpty()) {
-            org.apache.lucene.index.Fields[] unlikeFields = getFieldsFor(responses, unlikeItems);
-            if (unlikeFields.length > 0) {
-                mltQuery.setUnlikeText(unlikeFields);
-            }
+        if (minTermFreq != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MIN_TERM_FREQ.getPreferredName(), minTermFreq);
         }
-
-        BooleanQuery boolQuery = new BooleanQuery();
-        boolQuery.add(mltQuery, BooleanClause.Occur.SHOULD);
-
-        // exclude the items from the search
-        if (!include) {
-            handleExclude(boolQuery, likeItems);
+        if (minDocFreq != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MIN_DOC_FREQ.getPreferredName(), minDocFreq);
         }
-        return boolQuery;
-    }
-
-    private static void setDefaultIndexTypeFields(QueryShardContext context, Item item, List<String> moreLikeFields,
-                                                  boolean useDefaultField) {
-        if (item.index() == null) {
-            item.index(context.index().name());
+        if (maxDocFreq != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MAX_DOC_FREQ.getPreferredName(), maxDocFreq);
         }
-        if (item.type() == null) {
-            if (context.queryTypes().size() > 1) {
-                throw new QueryShardException(context,
-                        "ambiguous type for item with id: " + item.id() + " and index: " + item.index());
-            } else {
-                item.type(context.queryTypes().iterator().next());
-            }
+        if (minWordLength != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MIN_WORD_LENGTH.getPreferredName(), minWordLength);
         }
-        // default fields if not present but don't override for artificial docs
-        if ((item.fields() == null || item.fields().length == 0) && item.doc() == null) {
-            if (useDefaultField) {
-                item.fields("*");
-            } else {
-                item.fields(moreLikeFields.toArray(new String[moreLikeFields.size()]));
-            }
+        if (maxWordLength != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.MAX_WORD_LENGTH.getPreferredName(), maxWordLength);
         }
-    }
-
-    private MultiTermVectorsResponse fetchResponse(Client client, List<Item> likeItems, @Nullable List<Item> unlikeItems,
-                                                   SearchContext searchContext) throws IOException {
-        MultiTermVectorsRequest request = new MultiTermVectorsRequest();
-        for (Item item : likeItems) {
-            request.add(item.toTermVectorsRequest());
+        if (stopWords != null && stopWords.length > 0) {
+            builder.field(MoreLikeThisQueryParser.Field.STOP_WORDS.getPreferredName(), stopWords);
         }
-        if (unlikeItems != null) {
-            for (Item item : unlikeItems) {
-                request.add(item.toTermVectorsRequest());
-            }
+        if (analyzer != null) {
+            builder.field(MoreLikeThisQueryParser.Field.ANALYZER.getPreferredName(), analyzer);
         }
-        request.copyContextAndHeadersFrom(searchContext);
-        return client.multiTermVectors(request).actionGet();
-    }
-
-    private static Fields[] getFieldsFor(MultiTermVectorsResponse responses, List<Item> items) throws IOException {
-        List<Fields> likeFields = new ArrayList<>();
-
-        Set<Item> selectedItems = new HashSet<>();
-        for (Item request : items) {
-            selectedItems.add(new Item(request.index(), request.type(), request.id()));
+        if (minimumShouldMatch != null) {
+            builder.field(MoreLikeThisQueryParser.Field.MINIMUM_SHOULD_MATCH.getPreferredName(), minimumShouldMatch);
         }
-
-        for (MultiTermVectorsItemResponse response : responses) {
-            if (!hasResponseFromRequest(response, selectedItems)) {
-                continue;
-            }
-            if (response.isFailed()) {
-                continue;
-            }
-            TermVectorsResponse getResponse = response.getResponse();
-            if (!getResponse.isExists()) {
-                continue;
-            }
-            likeFields.add(getResponse.getFields());
+        if (boostTerms != -1) {
+            builder.field(MoreLikeThisQueryParser.Field.BOOST_TERMS.getPreferredName(), boostTerms);
         }
-        return likeFields.toArray(Fields.EMPTY_ARRAY);
-    }
-
-    private static boolean hasResponseFromRequest(MultiTermVectorsItemResponse response, Set<Item> selectedItems) {
-        return selectedItems.contains(new Item(response.getIndex(), response.getType(), response.getId()));
-    }
-
-    private static void handleExclude(BooleanQuery boolQuery, List<Item> likeItems) {
-        // artificial docs get assigned a random id and should be disregarded
-        List<BytesRef> uids = new ArrayList<>();
-        for (Item item : likeItems) {
-            if (item.doc() != null) {
-                continue;
-            }
-            uids.add(createUidAsBytes(item.type(), item.id()));
+        if (include != null) {
+            builder.field(MoreLikeThisQueryParser.Field.INCLUDE.getPreferredName(), include);
         }
-        if (!uids.isEmpty()) {
-            TermsQuery query = new TermsQuery(UidFieldMapper.NAME, uids.toArray(new BytesRef[0]));
-            boolQuery.add(query, BooleanClause.Occur.MUST_NOT);
+        if (failOnUnsupportedField != null) {
+            builder.field(MoreLikeThisQueryParser.Field.FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField);
         }
-    }
-
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (likeTexts.isEmpty() && likeItems.isEmpty()) {
-            validationException = addValidationError("requires 'like' to be specified.", validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (fields != null && fields.isEmpty()) {
-            validationException = addValidationError("requires 'fields' to be specified", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected MoreLikeThisQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MoreLikeThisQueryBuilder moreLikeThisQueryBuilder = new MoreLikeThisQueryBuilder((List<String>) in.readGenericValue());
-        moreLikeThisQueryBuilder.likeTexts = (List<String>) in.readGenericValue();
-        moreLikeThisQueryBuilder.unlikeTexts = (List<String>) in.readGenericValue();
-        moreLikeThisQueryBuilder.likeItems = readItems(in);
-        moreLikeThisQueryBuilder.unlikeItems = readItems(in);
-        moreLikeThisQueryBuilder.maxQueryTerms = in.readVInt();
-        moreLikeThisQueryBuilder.minTermFreq = in.readVInt();
-        moreLikeThisQueryBuilder.minDocFreq = in.readVInt();
-        moreLikeThisQueryBuilder.maxDocFreq = in.readVInt();
-        moreLikeThisQueryBuilder.minWordLength = in.readVInt();
-        moreLikeThisQueryBuilder.maxWordLength = in.readVInt();
-        moreLikeThisQueryBuilder.stopWords = in.readOptionalStringArray();
-        moreLikeThisQueryBuilder.analyzer = in.readOptionalString();
-        moreLikeThisQueryBuilder.minimumShouldMatch = in.readString();
-        moreLikeThisQueryBuilder.boostTerms = (Float) in.readGenericValue();
-        moreLikeThisQueryBuilder.include = in.readBoolean();
-        moreLikeThisQueryBuilder.failOnUnsupportedField = in.readBoolean();
-        return moreLikeThisQueryBuilder;
+        builder.endObject();
     }
 
-    private static List<Item> readItems(StreamInput in) throws IOException {
-        List<Item> items = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            items.add(Item.readItemFrom(in));
+    private static void buildLikeField(XContentBuilder builder, String fieldName, List<String> texts, List<Item> items) throws IOException {
+        builder.startArray(fieldName);
+        for (String text : texts) {
+            builder.value(text);
         }
-        return items;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeGenericValue(fields);
-        out.writeGenericValue(likeTexts);
-        out.writeGenericValue(unlikeTexts);
-        writeItems(likeItems, out);
-        writeItems(unlikeItems, out);
-        out.writeVInt(maxQueryTerms);
-        out.writeVInt(minTermFreq);
-        out.writeVInt(minDocFreq);
-        out.writeVInt(maxDocFreq);
-        out.writeVInt(minWordLength);
-        out.writeVInt(maxWordLength);
-        out.writeOptionalStringArray(stopWords);
-        out.writeOptionalString(analyzer);
-        out.writeString(minimumShouldMatch);
-        out.writeGenericValue(boostTerms);
-        out.writeBoolean(include);
-        out.writeBoolean(failOnUnsupportedField);
-    }
-
-    private static void writeItems(List<Item> items, StreamOutput out) throws IOException {
-        out.writeVInt(items.size());
         for (Item item : items) {
-            item.writeTo(out);
+            builder.value(item);
         }
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fields, likeTexts, unlikeTexts, likeItems, unlikeItems, maxQueryTerms, minTermFreq,
-                minDocFreq, maxDocFreq, minWordLength, maxWordLength, Arrays.hashCode(stopWords), analyzer, minimumShouldMatch,
-                boostTerms, include, failOnUnsupportedField);
-    }
-
-    @Override
-    protected boolean doEquals(MoreLikeThisQueryBuilder other) {
-        return Objects.equals(fields, other.fields) &&
-                Objects.equals(likeTexts, other.likeTexts) &&
-                Objects.equals(unlikeTexts, other.unlikeTexts) &&
-                Objects.equals(likeItems, other.likeItems) &&
-                Objects.equals(unlikeItems, other.unlikeItems) &&
-                Objects.equals(maxQueryTerms, other.maxQueryTerms) &&
-                Objects.equals(minTermFreq, other.minTermFreq) &&
-                Objects.equals(minDocFreq, other.minDocFreq) &&
-                Objects.equals(maxDocFreq, other.maxDocFreq) &&
-                Objects.equals(minWordLength, other.minWordLength) &&
-                Objects.equals(maxWordLength, other.maxWordLength) &&
-                Arrays.equals(stopWords, other.stopWords) &&  // otherwise we are comparing pointers
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(boostTerms, other.boostTerms) &&
-                Objects.equals(include, other.include) &&
-                Objects.equals(failOnUnsupportedField, other.failOnUnsupportedField);
+        builder.endArray();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
index 849b9c8..692d2f7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
@@ -19,21 +19,47 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.MoreLikeThisQuery;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.analysis.Analysis;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
+import org.elasticsearch.index.search.morelikethis.MoreLikeThisFetchService;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
+import java.util.Set;
+
+import static org.elasticsearch.index.mapper.Uid.createUidAsBytes;
 
 /**
  * Parser for the The More Like This Query (MLT Query) which finds documents that are "like" a given set of documents.
  *
  * The documents are provided as a set of strings and/or a list of {@link Item}.
  */
-public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBuilder> {
+public class MoreLikeThisQueryParser implements QueryParser {
+
+    public static final String NAME = "mlt";
+    private MoreLikeThisFetchService fetchService = null;
 
     public interface Field {
         ParseField FIELDS = new ParseField("fields");
@@ -56,40 +82,37 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
         ParseField FAIL_ON_UNSUPPORTED_FIELD = new ParseField("fail_on_unsupported_field");
     }
 
+    public MoreLikeThisQueryParser() {
+
+    }
+
+    @Inject(optional = true)
+    public void setFetchService(@Nullable MoreLikeThisFetchService fetchService) {
+        this.fetchService = fetchService;
+    }
+
     @Override
     public String[] names() {
-        return new String[]{MoreLikeThisQueryBuilder.NAME, "more_like_this", "moreLikeThis"};
+        return new String[]{NAME, "more_like_this", "moreLikeThis"};
     }
 
     @Override
-    public MoreLikeThisQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        // document inputs
-        List<String> fields = null;
+        MoreLikeThisQuery mltQuery = new MoreLikeThisQuery();
+        mltQuery.setSimilarity(parseContext.searchSimilarity());
+
         List<String> likeTexts = new ArrayList<>();
         List<String> unlikeTexts = new ArrayList<>();
         List<Item> likeItems = new ArrayList<>();
         List<Item> unlikeItems = new ArrayList<>();
 
-        // term selection parameters
-        int maxQueryTerms = MoreLikeThisQueryBuilder.DEFAULT_MAX_QUERY_TERMS;
-        int minTermFreq = MoreLikeThisQueryBuilder.DEFAULT_MIN_TERM_FREQ;
-        int minDocFreq = MoreLikeThisQueryBuilder.DEFAULT_MIN_DOC_FREQ;
-        int maxDocFreq = MoreLikeThisQueryBuilder.DEFAULT_MAX_DOC_FREQ;
-        int minWordLength = MoreLikeThisQueryBuilder.DEFAULT_MIN_WORD_LENGTH;
-        int maxWordLength = MoreLikeThisQueryBuilder.DEFAULT_MAX_WORD_LENGTH;
-        List<String> stopWords = null;
-        String analyzer = null;
-
-        // query formation parameters
-        String minimumShouldMatch = MoreLikeThisQueryBuilder.DEFAULT_MINIMUM_SHOULD_MATCH;
-        float boostTerms = MoreLikeThisQueryBuilder.DEFAULT_BOOST_TERMS;
-        boolean include = MoreLikeThisQueryBuilder.DEFAULT_INCLUDE;
-
-        // other parameters
-        boolean failOnUnsupportedField = MoreLikeThisQueryBuilder.DEFAULT_FAIL_ON_UNSUPPORTED_FIELDS;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        List<String> moreLikeFields = null;
+        Analyzer analyzer = null;
+        boolean include = false;
+
+        boolean failOnUnsupportedField = true;
         String queryName = null;
 
         XContentParser.Token token;
@@ -105,29 +128,33 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.LIKE_TEXT)) {
                     likeTexts.add(parser.text());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MAX_QUERY_TERMS)) {
-                    maxQueryTerms = parser.intValue();
+                    mltQuery.setMaxQueryTerms(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MIN_TERM_FREQ)) {
-                    minTermFreq =parser.intValue();
+                    mltQuery.setMinTermFrequency(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MIN_DOC_FREQ)) {
-                    minDocFreq = parser.intValue();
+                    mltQuery.setMinDocFreq(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MAX_DOC_FREQ)) {
-                    maxDocFreq = parser.intValue();
+                    mltQuery.setMaxDocFreq(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MIN_WORD_LENGTH)) {
-                    minWordLength = parser.intValue();
+                    mltQuery.setMinWordLen(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MAX_WORD_LENGTH)) {
-                    maxWordLength = parser.intValue();
+                    mltQuery.setMaxWordLen(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.ANALYZER)) {
-                    analyzer = parser.text();
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.MINIMUM_SHOULD_MATCH)) {
-                    minimumShouldMatch = parser.text();
+                    mltQuery.setMinimumShouldMatch(parser.text());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.BOOST_TERMS)) {
-                    boostTerms = parser.floatValue();
+                    float boostFactor = parser.floatValue();
+                    if (boostFactor != 0) {
+                        mltQuery.setBoostTerms(true);
+                        mltQuery.setBoostTermsFactor(boostFactor);
+                    }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.INCLUDE)) {
                     include = parser.booleanValue();
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.FAIL_ON_UNSUPPORTED_FIELD)) {
                     failOnUnsupportedField = parser.booleanValue();
                 } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
+                    mltQuery.setBoost(parser.floatValue());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else {
@@ -135,9 +162,11 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, Field.FIELDS)) {
-                    fields = new ArrayList<>();
+                    moreLikeFields = new LinkedList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        fields.add(parser.text());
+                        String field = parser.text();
+                        MappedFieldType fieldType = parseContext.fieldMapper(field);
+                        moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName());
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.LIKE)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
@@ -162,10 +191,11 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
                         likeItems.add(Item.parse(parser, parseContext.parseFieldMatcher(), new Item()));
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Field.STOP_WORDS)) {
-                    stopWords = new ArrayList<>();
+                    Set<String> stopWords = new HashSet<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         stopWords.add(parser.text());
                     }
+                    mltQuery.setStopWords(stopWords);
                 } else {
                     throw new ParsingException(parseContext, "[mlt] query does not support [" + currentFieldName + "]");
                 }
@@ -183,32 +213,48 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
         if (likeTexts.isEmpty() && likeItems.isEmpty()) {
             throw new ParsingException(parseContext, "more_like_this requires 'like' to be specified");
         }
-        if (fields != null && fields.isEmpty()) {
+        if (moreLikeFields != null && moreLikeFields.isEmpty()) {
             throw new ParsingException(parseContext, "more_like_this requires 'fields' to be non-empty");
         }
 
-        MoreLikeThisQueryBuilder moreLikeThisQueryBuilder = new MoreLikeThisQueryBuilder(fields)
-                .like(likeTexts.toArray(new String[likeTexts.size()]))
-                .unlike(unlikeTexts.toArray(new String[unlikeTexts.size()]))
-                .like(likeItems.toArray(new Item[likeItems.size()]))
-                .unlike(unlikeItems.toArray(new Item[unlikeItems.size()]))
-                .maxQueryTerms(maxQueryTerms)
-                .minTermFreq(minTermFreq)
-                .minDocFreq(minDocFreq)
-                .maxDocFreq(maxDocFreq)
-                .minWordLength(minWordLength)
-                .maxWordLength(maxWordLength)
-                .analyzer(analyzer)
-                .minimumShouldMatch(minimumShouldMatch)
-                .boostTerms(boostTerms)
-                .include(include)
-                .failOnUnsupportedField(failOnUnsupportedField)
-                .boost(boost)
-                .queryName(queryName);
-        if (stopWords != null) {
-            moreLikeThisQueryBuilder.stopWords(stopWords);
-        }
-        return moreLikeThisQueryBuilder;
+        // set analyzer
+        if (analyzer == null) {
+            analyzer = parseContext.mapperService().searchAnalyzer();
+        }
+        mltQuery.setAnalyzer(analyzer);
+
+        // set like text fields
+        boolean useDefaultField = (moreLikeFields == null);
+        if (useDefaultField) {
+            moreLikeFields = Collections.singletonList(parseContext.defaultField());
+        }
+
+        // possibly remove unsupported fields
+        removeUnsupportedFields(moreLikeFields, analyzer, failOnUnsupportedField);
+        if (moreLikeFields.isEmpty()) {
+            return null;
+        }
+        mltQuery.setMoreLikeFields(moreLikeFields.toArray(Strings.EMPTY_ARRAY));
+
+        // support for named query
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, mltQuery);
+        }
+
+        // handle like texts
+        if (!likeTexts.isEmpty()) {
+            mltQuery.setLikeText(likeTexts);
+        }
+        if (!unlikeTexts.isEmpty()) {
+            mltQuery.setUnlikeText(unlikeTexts);
+        }
+
+        // handle items
+        if (!likeItems.isEmpty()) {
+            return handleItems(parseContext, mltQuery, likeItems, unlikeItems, include, moreLikeFields, useDefaultField);
+        } else {
+            return mltQuery;
+        }
     }
 
     private static void parseLikeField(QueryParseContext parseContext, List<String> texts, List<Item> items) throws IOException {
@@ -222,8 +268,89 @@ public class MoreLikeThisQueryParser extends BaseQueryParser<MoreLikeThisQueryBu
         }
     }
 
-    @Override
-    public MoreLikeThisQueryBuilder getBuilderPrototype() {
-        return MoreLikeThisQueryBuilder.PROTOTYPE;
+    private static List<String> removeUnsupportedFields(List<String> moreLikeFields, Analyzer analyzer, boolean failOnUnsupportedField) throws IOException {
+        for (Iterator<String> it = moreLikeFields.iterator(); it.hasNext(); ) {
+            final String fieldName = it.next();
+            if (!Analysis.generatesCharacterTokenStream(analyzer, fieldName)) {
+                if (failOnUnsupportedField) {
+                    throw new IllegalArgumentException("more_like_this doesn't support binary/numeric fields: [" + fieldName + "]");
+                } else {
+                    it.remove();
+                }
+            }
+        }
+        return moreLikeFields;
+    }
+
+    private Query handleItems(QueryParseContext parseContext, MoreLikeThisQuery mltQuery, List<Item> likeItems, List<Item> unlikeItems,
+                              boolean include, List<String> moreLikeFields, boolean useDefaultField) throws IOException {
+        // set default index, type and fields if not specified
+        for (Item item : likeItems) {
+            setDefaultIndexTypeFields(parseContext, item, moreLikeFields, useDefaultField);
+        }
+        for (Item item : unlikeItems) {
+            setDefaultIndexTypeFields(parseContext, item, moreLikeFields, useDefaultField);
+        }
+
+        // fetching the items with multi-termvectors API
+        MultiTermVectorsResponse responses = fetchService.fetchResponse(likeItems, unlikeItems, SearchContext.current());
+
+        // getting the Fields for liked items
+        mltQuery.setLikeText(MoreLikeThisFetchService.getFieldsFor(responses, likeItems));
+
+        // getting the Fields for unliked items
+        if (!unlikeItems.isEmpty()) {
+            org.apache.lucene.index.Fields[] unlikeFields = MoreLikeThisFetchService.getFieldsFor(responses, unlikeItems);
+            if (unlikeFields.length > 0) {
+                mltQuery.setUnlikeText(unlikeFields);
+            }
+        }
+
+        BooleanQuery boolQuery = new BooleanQuery();
+        boolQuery.add(mltQuery, BooleanClause.Occur.SHOULD);
+
+        // exclude the items from the search
+        if (!include) {
+            handleExclude(boolQuery, likeItems);
+        }
+        return boolQuery;
+    }
+
+    private static void setDefaultIndexTypeFields(QueryParseContext parseContext, Item item, List<String> moreLikeFields,
+                                                  boolean useDefaultField) {
+        if (item.index() == null) {
+            item.index(parseContext.index().name());
+        }
+        if (item.type() == null) {
+            if (parseContext.queryTypes().size() > 1) {
+                throw new ParsingException(parseContext,
+                            "ambiguous type for item with id: " + item.id() + " and index: " + item.index());
+            } else {
+                item.type(parseContext.queryTypes().iterator().next());
+            }
+        }
+        // default fields if not present but don't override for artificial docs
+        if ((item.fields() == null || item.fields().length == 0) && item.doc() == null) {
+            if (useDefaultField) {
+                item.fields("*");
+            } else {
+                item.fields(moreLikeFields.toArray(new String[moreLikeFields.size()]));
+            }
+        }
+    }
+
+    private static void handleExclude(BooleanQuery boolQuery, List<Item> likeItems) {
+        // artificial docs get assigned a random id and should be disregarded
+        List<BytesRef> uids = new ArrayList<>();
+        for (Item item : likeItems) {
+            if (item.doc() != null) {
+                continue;
+            }
+            uids.add(createUidAsBytes(item.type(), item.id()));
+        }
+        if (!uids.isEmpty()) {
+            TermsQuery query = new TermsQuery(UidFieldMapper.NAME, uids.toArray(new BytesRef[0]));
+            boolQuery.add(query, BooleanClause.Occur.MUST_NOT);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index 05161c7..d42f0c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -19,64 +19,64 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.Query;
+import com.carrotsearch.hppc.ObjectFloatHashMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.support.QueryParsers;
 import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MultiMatchQuery;
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
 import java.util.Locale;
-import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
  * Same as {@link MatchQueryBuilder} but supports multiple fields.
  */
-public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQueryBuilder> {
-
-    public static final String NAME = "multi_match";
-
-    public static final MultiMatchQueryBuilder.Type DEFAULT_TYPE = MultiMatchQueryBuilder.Type.BEST_FIELDS;
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    public static final int DEFAULT_PHRASE_SLOP = MatchQuery.DEFAULT_PHRASE_SLOP;
-    public static final int DEFAULT_PREFIX_LENGTH = FuzzyQuery.defaultPrefixLength;
-    public static final int DEFAULT_MAX_EXPANSIONS = FuzzyQuery.defaultMaxExpansions;
-    public static final boolean DEFAULT_LENIENCY = MatchQuery.DEFAULT_LENIENCY;
-    public static final MatchQuery.ZeroTermsQuery DEFAULT_ZERO_TERMS_QUERY = MatchQuery.DEFAULT_ZERO_TERMS_QUERY;
-
-    private final Object value;
-    private final Map<String, Float> fieldsBoosts;
-    private MultiMatchQueryBuilder.Type type = DEFAULT_TYPE;
-    private Operator operator = DEFAULT_OPERATOR;
+public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MultiMatchQueryBuilder> {
+
+    private final Object text;
+
+    private final List<String> fields;
+    private ObjectFloatHashMap<String> fieldsBoosts;
+
+    private MultiMatchQueryBuilder.Type type;
+
+    private MatchQueryBuilder.Operator operator;
+
     private String analyzer;
-    private int slop = DEFAULT_PHRASE_SLOP;
+
+    private Float boost;
+
+    private Integer slop;
+
     private Fuzziness fuzziness;
-    private int prefixLength = DEFAULT_PREFIX_LENGTH;
-    private int maxExpansions = DEFAULT_MAX_EXPANSIONS;
+
+    private Integer prefixLength;
+
+    private Integer maxExpansions;
+
     private String minimumShouldMatch;
+
     private String fuzzyRewrite = null;
+
     private Boolean useDisMax;
+
     private Float tieBreaker;
-    private boolean lenient = DEFAULT_LENIENCY;
+
+    private Boolean lenient;
+
     private Float cutoffFrequency = null;
-    private MatchQuery.ZeroTermsQuery zeroTermsQuery = DEFAULT_ZERO_TERMS_QUERY;
 
-    static final MultiMatchQueryBuilder PROTOTYPE = new MultiMatchQueryBuilder("");
+    private MatchQueryBuilder.ZeroTermsQuery zeroTermsQuery = null;
+
+    private String queryName;
+
 
-    public enum Type implements Writeable<Type> {
+    public enum Type {
 
         /**
          * Uses the best matching boolean field as main score and uses
@@ -109,8 +109,6 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
          */
         PHRASE_PREFIX(MatchQuery.Type.PHRASE_PREFIX, 0.0f, new ParseField("phrase_prefix"));
 
-        private static final Type PROTOTYPE = BEST_FIELDS;
-
         private MatchQuery.Type matchQueryType;
         private final float tieBreaker;
         private final ParseField parseField;
@@ -143,26 +141,12 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
                 }
             }
             if (type == null) {
-                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", NAME, value);
+                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", MultiMatchQueryParser.NAME, value);
             }
             return type;
         }
-
-        @Override
-        public Type readFrom(StreamInput in) throws IOException {
-            return Type.values()[in.readVInt()];
-        }
-
-        public static Type readTypeFrom(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(this.ordinal());
-        }
     }
-
+    
     /**
      * Returns the type (for testing)
      */
@@ -173,32 +157,17 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     /**
      * Constructs a new text query.
      */
-    public MultiMatchQueryBuilder(Object value, String... fields) {
-        if (value == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires query value");
-        }
-        if (fields == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires fields at initalization time");
-        }
-        this.value = value;
-        this.fieldsBoosts = new TreeMap<>();
-        for (String field : fields) {
-            field(field);
-        }
-    }
-
-    public Object value() {
-        return value;
+    public MultiMatchQueryBuilder(Object text, String... fields) {
+        this.fields = new ArrayList<>();
+        this.fields.addAll(Arrays.asList(fields));
+        this.text = text;
     }
 
     /**
      * Adds a field to run the multi match against.
      */
     public MultiMatchQueryBuilder field(String field) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsBoosts.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        fields.add(field);
         return this;
     }
 
@@ -206,32 +175,18 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
      * Adds a field to run the multi match against with a specific boost.
      */
     public MultiMatchQueryBuilder field(String field, float boost) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
+        fields.add(field);
+        if (fieldsBoosts == null) {
+            fieldsBoosts = new ObjectFloatHashMap<>();
         }
-        this.fieldsBoosts.put(field, boost);
-        return this;
-    }
-
-    /**
-     * Add several fields to run the query against with a specific boost.
-     */
-    public MultiMatchQueryBuilder fields(Map<String, Float> fields) {
-        this.fieldsBoosts.putAll(fields);
+        fieldsBoosts.put(field, boost);
         return this;
     }
 
-    public Map<String, Float> fields() {
-        return fieldsBoosts;
-    }
-
     /**
      * Sets the type of the text query.
      */
     public MultiMatchQueryBuilder type(MultiMatchQueryBuilder.Type type) {
-        if (type == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires type to be non-null");
-        }
         this.type = type;
         return this;
     }
@@ -240,32 +195,18 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
      * Sets the type of the text query.
      */
     public MultiMatchQueryBuilder type(Object type) {
-        if (type == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires type to be non-null");
-        }
-        this.type = Type.parse(type.toString().toLowerCase(Locale.ROOT), ParseFieldMatcher.EMPTY);
+        this.type = type == null ? null : Type.parse(type.toString().toLowerCase(Locale.ROOT), ParseFieldMatcher.EMPTY);
         return this;
     }
 
-    public Type type() {
-        return type;
-    }
-
     /**
      * Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>.
      */
-    public MultiMatchQueryBuilder operator(Operator operator) {
-        if (operator == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires operator to be non-null");
-        }
+    public MultiMatchQueryBuilder operator(MatchQueryBuilder.Operator operator) {
         this.operator = operator;
         return this;
     }
 
-    public Operator operator() {
-        return operator;
-    }
-
     /**
      * Explicitly set the analyzer to use. Defaults to use explicit mapping config for the field, or, if not
      * set, the default search analyzer.
@@ -275,99 +216,65 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
-    public String analyzer() {
-        return analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MultiMatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MultiMatchQueryBuilder slop(int slop) {
-        if (slop < 0) {
-            throw new IllegalArgumentException("No negative slop allowed.");
-        }
         this.slop = slop;
         return this;
     }
 
-    public int slop() {
-        return slop;
-    }
-
     /**
      * Sets the fuzziness used when evaluated to a fuzzy query type. Defaults to "AUTO".
      */
     public MultiMatchQueryBuilder fuzziness(Object fuzziness) {
-        if (fuzziness != null) {
-            this.fuzziness = Fuzziness.build(fuzziness);
-        }
+        this.fuzziness = Fuzziness.build(fuzziness);
         return this;
     }
 
-    public Fuzziness fuzziness() {
-        return fuzziness;
-    }
-
     public MultiMatchQueryBuilder prefixLength(int prefixLength) {
-        if (prefixLength < 0) {
-            throw new IllegalArgumentException("No negative prefix length allowed.");
-        }
         this.prefixLength = prefixLength;
         return this;
     }
 
-    public int prefixLength() {
-        return prefixLength;
-    }
-
     /**
      * When using fuzzy or prefix type query, the number of term expansions to use. Defaults to unbounded
      * so its recommended to set it to a reasonable value for faster execution.
      */
     public MultiMatchQueryBuilder maxExpansions(int maxExpansions) {
-        if (maxExpansions <= 0) {
-            throw new IllegalArgumentException("Max expansions must be strictly great than zero.");
-        }
         this.maxExpansions = maxExpansions;
         return this;
     }
 
-    public int maxExpansions() {
-        return maxExpansions;
-    }
-
     public MultiMatchQueryBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
     public MultiMatchQueryBuilder fuzzyRewrite(String fuzzyRewrite) {
         this.fuzzyRewrite = fuzzyRewrite;
         return this;
     }
 
-    public String fuzzyRewrite() {
-        return fuzzyRewrite;
-    }
-
     /**
      * @deprecated use a tieBreaker of 1.0f to disable "dis-max"
      * query or select the appropriate {@link Type}
      */
     @Deprecated
-    public MultiMatchQueryBuilder useDisMax(Boolean useDisMax) {
+    public MultiMatchQueryBuilder useDisMax(boolean useDisMax) {
         this.useDisMax = useDisMax;
         return this;
     }
 
-    public Boolean useDisMax() {
-        return useDisMax;
-    }
-
     /**
      * <p>Tie-Breaker for "best-match" disjunction queries (OR-Queries).
      * The tie breaker capability allows documents that match more than one query clause
@@ -386,27 +293,6 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     }
 
     /**
-     * <p>Tie-Breaker for "best-match" disjunction queries (OR-Queries).
-     * The tie breaker capability allows documents that match more than one query clause
-     * (in this case on more than one field) to be scored better than documents that
-     * match only the best of the fields, without confusing this with the better case of
-     * two distinct matches in the multiple fields.</p>
-     *
-     * <p>A tie-breaker value of <tt>1.0</tt> is interpreted as a signal to score queries as
-     * "most-match" queries where all matching query clauses are considered for scoring.</p>
-     *
-     * @see Type
-     */
-    public MultiMatchQueryBuilder tieBreaker(Float tieBreaker) {
-        this.tieBreaker = tieBreaker;
-        return this;
-    }
-
-    public Float tieBreaker() {
-        return tieBreaker;
-    }
-
-    /**
      * Sets whether format based failures will be ignored.
      */
     public MultiMatchQueryBuilder lenient(boolean lenient) {
@@ -414,12 +300,9 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
-    public boolean lenient() {
-        return lenient;
-    }
 
     /**
-     * Set a cutoff value in [0..1] (or absolute number >=1) representing the
+     * Set a cutoff value in [0..1] (or absolute number &gt;=1) representing the
      * maximum threshold of a terms document frequency to be considered a low
      * frequency term.
      */
@@ -428,227 +311,91 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
-    /**
-     * Set a cutoff value in [0..1] (or absolute number >=1) representing the
-     * maximum threshold of a terms document frequency to be considered a low
-     * frequency term.
-     */
-    public MultiMatchQueryBuilder cutoffFrequency(Float cutoff) {
-        this.cutoffFrequency = cutoff;
-        return this;
-    }
 
-    public Float cutoffFrequency() {
-        return cutoffFrequency;
-    }
-
-    public MultiMatchQueryBuilder zeroTermsQuery(MatchQuery.ZeroTermsQuery zeroTermsQuery) {
-        if (zeroTermsQuery == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires zero terms query to be non-null");
-        }
+    public MultiMatchQueryBuilder zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery zeroTermsQuery) {
         this.zeroTermsQuery = zeroTermsQuery;
         return this;
     }
 
-    public MatchQuery.ZeroTermsQuery zeroTermsQuery() {
-        return zeroTermsQuery;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MultiMatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query", value);
+        builder.startObject(MultiMatchQueryParser.NAME);
+
+        builder.field("query", text);
         builder.startArray("fields");
-        for (Map.Entry<String, Float> fieldEntry : this.fieldsBoosts.entrySet()) {
-            builder.value(fieldEntry.getKey() + "^" + fieldEntry.getValue());
+        for (String field : fields) {
+            final int keySlot;
+            if (fieldsBoosts != null && ((keySlot = fieldsBoosts.indexOf(field)) >= 0)) {
+                field += "^" + fieldsBoosts.indexGet(keySlot);
+            }
+            builder.value(field);
         }
         builder.endArray();
-        builder.field("type", type.toString().toLowerCase(Locale.ENGLISH));
-        builder.field("operator", operator.toString());
+
+        if (type != null) {
+            builder.field("type", type.toString().toLowerCase(Locale.ENGLISH));
+        }
+        if (operator != null) {
+            builder.field("operator", operator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("slop", slop);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (slop != null) {
+            builder.field("slop", slop);
+        }
         if (fuzziness != null) {
             fuzziness.toXContent(builder, params);
         }
-        builder.field("prefix_length", prefixLength);
-        builder.field("max_expansions", maxExpansions);
+        if (prefixLength != null) {
+            builder.field("prefix_length", prefixLength);
+        }
+        if (maxExpansions != null) {
+            builder.field("max_expansions", maxExpansions);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
         if (fuzzyRewrite != null) {
             builder.field("fuzzy_rewrite", fuzzyRewrite);
         }
+
         if (useDisMax != null) {
             builder.field("use_dis_max", useDisMax);
         }
-        if (tieBreaker != null) {
-            builder.field("tie_breaker", tieBreaker);
-        }
-        builder.field("lenient", lenient);
-        if (cutoffFrequency != null) {
-            builder.field("cutoff_frequency", cutoffFrequency);
-        }
-        builder.field("zero_terms_query", zeroTermsQuery.toString());
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(context);
-        if (analyzer != null) {
-            if (context.analysisService().analyzer(analyzer) == null) {
-                throw new QueryShardException(context, "[" + NAME + "] analyzer [" + analyzer + "] not found");
-            }
-            multiMatchQuery.setAnalyzer(analyzer);
-        }
-        multiMatchQuery.setPhraseSlop(slop);
-        if (fuzziness != null) {
-            multiMatchQuery.setFuzziness(fuzziness);
-        }
-        multiMatchQuery.setFuzzyPrefixLength(prefixLength);
-        multiMatchQuery.setMaxExpansions(maxExpansions);
-        multiMatchQuery.setOccur(operator.toBooleanClauseOccur());
-        if (fuzzyRewrite != null) {
-            multiMatchQuery.setFuzzyRewriteMethod(QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), fuzzyRewrite, null));
-        }
         if (tieBreaker != null) {
-            multiMatchQuery.setTieBreaker(tieBreaker);
-        }
-        if (cutoffFrequency != null) {
-            multiMatchQuery.setCommonTermsCutoff(cutoffFrequency);
-        }
-        multiMatchQuery.setLenient(lenient);
-        multiMatchQuery.setZeroTermsQuery(zeroTermsQuery);
-
-        if (useDisMax != null) { // backwards foobar
-            boolean typeUsesDismax = type.tieBreaker() != 1.0f;
-            if (typeUsesDismax != useDisMax) {
-                if (useDisMax && tieBreaker == null) {
-                    multiMatchQuery.setTieBreaker(0.0f);
-                } else {
-                    multiMatchQuery.setTieBreaker(1.0f);
-                }
-            }
+            builder.field("tie_breaker", tieBreaker);
         }
 
-        Map<String, Float> newFieldsBoosts = handleFieldsMatchPattern(context.mapperService(), fieldsBoosts);
-
-        Query query = multiMatchQuery.parse(type, newFieldsBoosts, value, minimumShouldMatch);
-        if (query == null) {
-            return null;
+        if (lenient != null) {
+            builder.field("lenient", lenient);
         }
-        return query;
-    }
 
-    @Override
-    protected void setFinalBoost(Query query) {
-        // we need to preserve the boost that came out of the parsing phase
-        query.setBoost(boost * query.getBoost());
-    }
-
-    private static Map<String, Float> handleFieldsMatchPattern(MapperService mapperService, Map<String, Float> fieldsBoosts) {
-        Map<String, Float> newFieldsBoosts = new TreeMap<>();
-        for (Map.Entry<String, Float> fieldBoost : fieldsBoosts.entrySet()) {
-            String fField = fieldBoost.getKey();
-            Float fBoost = fieldBoost.getValue();
-            if (Regex.isSimpleMatchPattern(fField)) {
-                for (String field : mapperService.simpleMatchToIndexNames(fField)) {
-                    newFieldsBoosts.put(field, fBoost);
-                }
-            } else {
-                newFieldsBoosts.put(fField, fBoost);
-            }
+        if (cutoffFrequency != null) {
+            builder.field("cutoff_frequency", cutoffFrequency);
         }
-        return newFieldsBoosts;
-    }
 
-    @Override
-    protected MultiMatchQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MultiMatchQueryBuilder multiMatchQuery = new MultiMatchQueryBuilder(in.readGenericValue());
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            multiMatchQuery.fieldsBoosts.put(in.readString(), in.readFloat());
+        if (zeroTermsQuery != null) {
+            builder.field("zero_terms_query", zeroTermsQuery.toString());
         }
-        multiMatchQuery.type = MultiMatchQueryBuilder.Type.readTypeFrom(in);
-        multiMatchQuery.operator = Operator.readOperatorFrom(in);
-        multiMatchQuery.analyzer = in.readOptionalString();
-        multiMatchQuery.slop = in.readVInt();
-        if (in.readBoolean()) {
-            multiMatchQuery.fuzziness = Fuzziness.readFuzzinessFrom(in);
-        }
-        multiMatchQuery.prefixLength = in.readVInt();
-        multiMatchQuery.maxExpansions = in.readVInt();
-        multiMatchQuery.minimumShouldMatch = in.readOptionalString();
-        multiMatchQuery.fuzzyRewrite = in.readOptionalString();
-        multiMatchQuery.useDisMax = in.readOptionalBoolean();
-        multiMatchQuery.tieBreaker = (Float) in.readGenericValue();
-        multiMatchQuery.lenient = in.readBoolean();
-        multiMatchQuery.cutoffFrequency = (Float) in.readGenericValue();
-        multiMatchQuery.zeroTermsQuery = MatchQuery.ZeroTermsQuery.readZeroTermsQueryFrom(in);
-        return multiMatchQuery;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeGenericValue(value);
-        out.writeVInt(fieldsBoosts.size());
-        for (Map.Entry<String, Float> fieldsEntry : fieldsBoosts.entrySet()) {
-            out.writeString(fieldsEntry.getKey());
-            out.writeFloat(fieldsEntry.getValue());
-        }
-        type.writeTo(out);
-        operator.writeTo(out);
-        out.writeOptionalString(analyzer);
-        out.writeVInt(slop);
-        if (fuzziness != null) {
-            out.writeBoolean(true);
-            fuzziness.writeTo(out);
-        } else {
-            out.writeBoolean(false);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        out.writeVInt(prefixLength);
-        out.writeVInt(maxExpansions);
-        out.writeOptionalString(minimumShouldMatch);
-        out.writeOptionalString(fuzzyRewrite);
-        out.writeOptionalBoolean(useDisMax);
-        out.writeGenericValue(tieBreaker);
-        out.writeBoolean(lenient);
-        out.writeGenericValue(cutoffFrequency);
-        zeroTermsQuery.writeTo(out);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(value, fieldsBoosts, type, operator, analyzer, slop, fuzziness,
-                prefixLength, maxExpansions, minimumShouldMatch, fuzzyRewrite, useDisMax, tieBreaker, lenient,
-                cutoffFrequency, zeroTermsQuery);
+        builder.endObject();
     }
 
-    @Override
-    protected boolean doEquals(MultiMatchQueryBuilder other) {
-        return Objects.equals(value, other.value) &&
-                Objects.equals(fieldsBoosts, other.fieldsBoosts) &&
-                Objects.equals(type, other.type) &&
-                Objects.equals(operator, other.operator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(slop, other.slop) &&
-                Objects.equals(fuzziness, other.fuzziness) &&
-                Objects.equals(prefixLength, other.prefixLength) &&
-                Objects.equals(maxExpansions, other.maxExpansions) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(fuzzyRewrite, other.fuzzyRewrite) &&
-                Objects.equals(useDisMax, other.useDisMax) &&
-                Objects.equals(tieBreaker, other.tieBreaker) &&
-                Objects.equals(lenient, other.lenient) &&
-                Objects.equals(cutoffFrequency, other.cutoffFrequency) &&
-                Objects.equals(zeroTermsQuery, other.zeroTermsQuery);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
index b02d26c..6ac3e4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
@@ -19,10 +19,16 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.support.QueryParsers;
 import org.elasticsearch.index.search.MatchQuery;
+import org.elasticsearch.index.search.MultiMatchQuery;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -31,53 +37,48 @@ import java.util.Map;
 /**
  * Same as {@link MatchQueryParser} but has support for multiple fields.
  */
-public class MultiMatchQueryParser extends BaseQueryParser<MultiMatchQueryBuilder> {
+public class MultiMatchQueryParser implements QueryParser {
+
+    public static final String NAME = "multi_match";
+
+    @Inject
+    public MultiMatchQueryParser() {
+    }
 
     @Override
     public String[] names() {
         return new String[]{
-                MultiMatchQueryBuilder.NAME, "multiMatch"
+                NAME, "multiMatch"
         };
     }
 
     @Override
-    public MultiMatchQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         Object value = null;
-        Map<String, Float> fieldsBoosts = new HashMap<>();
-        MultiMatchQueryBuilder.Type type = MultiMatchQueryBuilder.DEFAULT_TYPE;
-        String analyzer = null;
-        int slop = MultiMatchQueryBuilder.DEFAULT_PHRASE_SLOP;
-        Fuzziness fuzziness = null;
-        int prefixLength = MultiMatchQueryBuilder.DEFAULT_PREFIX_LENGTH;
-        int maxExpansions = MultiMatchQueryBuilder.DEFAULT_MAX_EXPANSIONS;
-        Operator operator = MultiMatchQueryBuilder.DEFAULT_OPERATOR;
-        String minimumShouldMatch = null;
-        String fuzzyRewrite = null;
-        Boolean useDisMax = null;
+        float boost = 1.0f;
         Float tieBreaker = null;
-        Float cutoffFrequency = null;
-        boolean lenient = MultiMatchQueryBuilder.DEFAULT_LENIENCY;
-        MatchQuery.ZeroTermsQuery zeroTermsQuery = MultiMatchQueryBuilder.DEFAULT_ZERO_TERMS_QUERY;
-
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        MultiMatchQueryBuilder.Type type = null;
+        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(parseContext);
+        String minimumShouldMatch = null;
+        Map<String, Float> fieldNameWithBoosts = new HashMap<>();
         String queryName = null;
-
         XContentParser.Token token;
         String currentFieldName = null;
+        Boolean useDisMax = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if ("fields".equals(currentFieldName)) {
                 if (token == XContentParser.Token.START_ARRAY) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        parseFieldAndBoost(parser, fieldsBoosts);
+                        extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                     }
                 } else if (token.isValue()) {
-                    parseFieldAndBoost(parser, fieldsBoosts);
+                    extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                 } else {
-                    throw new ParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new ParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -85,37 +86,49 @@ public class MultiMatchQueryParser extends BaseQueryParser<MultiMatchQueryBuilde
                 } else if ("type".equals(currentFieldName)) {
                     type = MultiMatchQueryBuilder.Type.parse(parser.text(), parseContext.parseFieldMatcher());
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzer = parser.text();
+                    String analyzer = parser.text();
+                    if (parseContext.analysisService().analyzer(analyzer) == null) {
+                        throw new ParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
+                    }
+                    multiMatchQuery.setAnalyzer(analyzer);
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("slop".equals(currentFieldName) || "phrase_slop".equals(currentFieldName) || "phraseSlop".equals(currentFieldName)) {
-                    slop = parser.intValue();
+                    multiMatchQuery.setPhraseSlop(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fuzziness.FIELD)) {
-                    fuzziness = Fuzziness.parse(parser);
+                    multiMatchQuery.setFuzziness(Fuzziness.parse(parser));
                 } else if ("prefix_length".equals(currentFieldName) || "prefixLength".equals(currentFieldName)) {
-                    prefixLength = parser.intValue();
+                    multiMatchQuery.setFuzzyPrefixLength(parser.intValue());
                 } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
-                    maxExpansions = parser.intValue();
+                    multiMatchQuery.setMaxExpansions(parser.intValue());
                 } else if ("operator".equals(currentFieldName)) {
-                    operator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.MUST);
+                    } else {
+                        throw new ParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not [" + op
+                                + "]");
+                    }
                 } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
-                    fuzzyRewrite = parser.textOrNull();
+                    multiMatchQuery.setFuzzyRewriteMethod(QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull(), null));
                 } else if ("use_dis_max".equals(currentFieldName) || "useDisMax".equals(currentFieldName)) {
                     useDisMax = parser.booleanValue();
                 } else if ("tie_breaker".equals(currentFieldName) || "tieBreaker".equals(currentFieldName)) {
-                    tieBreaker = parser.floatValue();
+                    multiMatchQuery.setTieBreaker(tieBreaker = parser.floatValue());
                 }  else if ("cutoff_frequency".equals(currentFieldName)) {
-                    cutoffFrequency = parser.floatValue();
+                    multiMatchQuery.setCommonTermsCutoff(parser.floatValue());
                 } else if ("lenient".equals(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    multiMatchQuery.setLenient(parser.booleanValue());
                 } else if ("zero_terms_query".equals(currentFieldName)) {
                     String zeroTermsDocs = parser.text();
                     if ("none".equalsIgnoreCase(zeroTermsDocs)) {
-                        zeroTermsQuery = MatchQuery.ZeroTermsQuery.NONE;
+                        multiMatchQuery.setZeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE);
                     } else if ("all".equalsIgnoreCase(zeroTermsDocs)) {
-                        zeroTermsQuery = MatchQuery.ZeroTermsQuery.ALL;
+                        multiMatchQuery.setZeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL);
                     } else {
                         throw new ParsingException(parseContext, "Unsupported zero_terms_docs value [" + zeroTermsDocs + "]");
                     }
@@ -131,33 +144,37 @@ public class MultiMatchQueryParser extends BaseQueryParser<MultiMatchQueryBuilde
             throw new ParsingException(parseContext, "No text specified for multi_match query");
         }
 
-        if (fieldsBoosts.isEmpty()) {
+        if (fieldNameWithBoosts.isEmpty()) {
             throw new ParsingException(parseContext, "No fields specified for multi_match query");
         }
+        if (type == null) {
+            type = MultiMatchQueryBuilder.Type.BEST_FIELDS;
+        }
+        if (useDisMax != null) { // backwards foobar
+            boolean typeUsesDismax = type.tieBreaker() != 1.0f;
+            if (typeUsesDismax != useDisMax) {
+                if (useDisMax && tieBreaker == null) {
+                    multiMatchQuery.setTieBreaker(0.0f);
+                } else {
+                    multiMatchQuery.setTieBreaker(1.0f);
+                }
+            }
+        }
+        Query query = multiMatchQuery.parse(type, fieldNameWithBoosts, value, minimumShouldMatch);
+        if (query == null) {
+            return null;
+        }
 
-        return new MultiMatchQueryBuilder(value)
-                .fields(fieldsBoosts)
-                .type(type)
-                .analyzer(analyzer)
-                .cutoffFrequency(cutoffFrequency)
-                .fuzziness(fuzziness)
-                .fuzzyRewrite(fuzzyRewrite)
-                .useDisMax(useDisMax)
-                .lenient(lenient)
-                .maxExpansions(maxExpansions)
-                .minimumShouldMatch(minimumShouldMatch)
-                .operator(operator)
-                .prefixLength(prefixLength)
-                .slop(slop)
-                .tieBreaker(tieBreaker)
-                .zeroTermsQuery(zeroTermsQuery)
-                .boost(boost)
-                .queryName(queryName);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    private void parseFieldAndBoost(XContentParser parser, Map<String, Float> fieldsBoosts) throws IOException {
+    private void extractFieldAndBoost(QueryParseContext parseContext, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
         String fField = null;
-        Float fBoost = AbstractQueryBuilder.DEFAULT_BOOST;
+        Float fBoost = null;
         char[] fieldText = parser.textCharacters();
         int end = parser.textOffset() + parser.textLength();
         for (int i = parser.textOffset(); i < end; i++) {
@@ -171,11 +188,13 @@ public class MultiMatchQueryParser extends BaseQueryParser<MultiMatchQueryBuilde
         if (fField == null) {
             fField = parser.text();
         }
-        fieldsBoosts.put(fField, fBoost);
-    }
 
-    @Override
-    public MultiMatchQueryBuilder getBuilderPrototype() {
-        return MultiMatchQueryBuilder.PROTOTYPE;
+        if (Regex.isSimpleMatchPattern(fField)) {
+            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                fieldNameWithBoosts.put(field, fBoost);
+            }
+        } else {
+            fieldNameWithBoosts.put(fField, fBoost);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
index 0e946d6..9c7383d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
@@ -18,6 +18,6 @@
  */
 package org.elasticsearch.index.query;
 
-public interface MultiTermQueryBuilder<QB extends MultiTermQueryBuilder<QB>> extends QueryBuilder<QB> {
+public abstract class MultiTermQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
index e012c52..63b40dc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
@@ -19,211 +19,85 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitSetProducer;
-import org.apache.lucene.search.join.ScoreMode;
-import org.apache.lucene.search.join.ToParentBlockJoinQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
+import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
-import java.util.Locale;
 import java.util.Objects;
 
-public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder> {
+public class NestedQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<NestedQueryBuilder> {
 
-    /**
-     * The default score move for nested queries.
-     */
-    public static final ScoreMode DEFAULT_SCORE_MODE = ScoreMode.Avg;
+    private final QueryBuilder queryBuilder;
 
-    /**
-     * The queries name used while parsing
-     */
-    public static final String NAME = "nested";
+    private final String path;
 
-    private final QueryBuilder query;
+    private String scoreMode;
 
-    private final String path;
+    private float boost = 1.0f;
 
-    private ScoreMode scoreMode = DEFAULT_SCORE_MODE;
+    private String queryName;
 
-    private QueryInnerHits queryInnerHits;
+    private QueryInnerHitBuilder innerHit;
 
-    public NestedQueryBuilder(String path, QueryBuilder query) {
-        if (path == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'path' field");
-        }
-        if (query == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'query' field");
-        }
+    public NestedQueryBuilder(String path, QueryBuilder queryBuilder) {
         this.path = path;
-        this.query = query;
-    }
-
-    public NestedQueryBuilder(String path, QueryBuilder query, ScoreMode scoreMode, QueryInnerHits queryInnerHits) {
-        this(path, query);
-        scoreMode(scoreMode);
-        this.queryInnerHits = queryInnerHits;
+        this.queryBuilder = Objects.requireNonNull(queryBuilder);
     }
-
     /**
-     * The score mode how the scores from the matching child documents are mapped into the nested parent document.
+     * The score mode.
      */
-    public NestedQueryBuilder scoreMode(ScoreMode scoreMode) {
-        if (scoreMode == null) {
-            throw new IllegalArgumentException("[" + NAME + "] requires 'score_mode' field");
-        }
+    public NestedQueryBuilder scoreMode(String scoreMode) {
         this.scoreMode = scoreMode;
         return this;
     }
 
     /**
-     * Sets inner hit definition in the scope of this nested query and reusing the defined path and query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public NestedQueryBuilder innerHit(QueryInnerHits innerHit) {
-        this.queryInnerHits = innerHit;
+    @Override
+    public NestedQueryBuilder boost(float boost) {
+        this.boost = boost;
         return this;
     }
 
     /**
-     * Returns the nested query to execute.
-     */
-    public QueryBuilder query() {
-        return query;
-    }
-
-    /**
-     * Returns inner hit definition in the scope of this query and reusing the defined type and query.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public QueryInnerHits innerHit() {
-        return queryInnerHits;
+    public NestedQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     /**
-     * Returns how the scores from the matching child documents are mapped into the nested parent document.
+     * Sets inner hit definition in the scope of this nested query and reusing the defined path and query.
      */
-    public ScoreMode scoreMode() {
-        return scoreMode;
+    public NestedQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
+        this.innerHit = innerHit;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NestedQueryParser.NAME);
         builder.field("query");
-        query.toXContent(builder, params);
+        queryBuilder.toXContent(builder, params);
         builder.field("path", path);
         if (scoreMode != null) {
-            builder.field("score_mode", scoreMode.name().toLowerCase(Locale.ROOT));
-        }
-        printBoostAndQueryName(builder);
-        if (queryInnerHits != null) {
-            queryInnerHits.toXContent(builder, params);
-        }
-        builder.endObject();
-    }
-
-    @Override
-    public final String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected boolean doEquals(NestedQueryBuilder that) {
-        return Objects.equals(query, that.query)
-                && Objects.equals(path, that.path)
-                && Objects.equals(scoreMode, that.scoreMode)
-                && Objects.equals(queryInnerHits, that.queryInnerHits);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(query, path, scoreMode, queryInnerHits);
-    }
-
-    private NestedQueryBuilder(StreamInput in) throws IOException {
-        path = in.readString();
-        final int ordinal = in.readVInt();
-        scoreMode = ScoreMode.values()[ordinal];
-        query = in.readQuery();
-        if (in.readBoolean()) {
-            queryInnerHits = new QueryInnerHits(in);
-        }
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(path);
-        out.writeVInt(scoreMode.ordinal());
-        out.writeQuery(query);
-        if (queryInnerHits != null) {
-            out.writeBoolean(true);
-            queryInnerHits.writeTo(out);
-        } else {
-            out.writeBoolean(false);
-        }
-    }
-
-    @Override
-    protected NestedQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new NestedQueryBuilder(in);
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        ObjectMapper nestedObjectMapper = context.getObjectMapper(path);
-        if (nestedObjectMapper == null) {
-            throw new IllegalStateException("[" + NAME + "] failed to find nested object under path [" + path + "]");
+            builder.field("score_mode", scoreMode);
         }
-        if (!nestedObjectMapper.nested().isNested()) {
-            throw new IllegalStateException("[" + NAME + "] nested object under path [" + path + "] is not of nested type");
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
         }
-        final BitSetProducer parentFilter;
-        final Filter childFilter;
-        final ObjectMapper parentObjectMapper;
-        final Query innerQuery;
-        ObjectMapper objectMapper = context.nestedScope().getObjectMapper();
-        try {
-            if (objectMapper == null) {
-                parentFilter = context.bitsetFilter(Queries.newNonNestedFilter());
-            } else {
-                parentFilter = context.bitsetFilter(objectMapper.nestedTypeFilter());
-            }
-            childFilter = nestedObjectMapper.nestedTypeFilter();
-            parentObjectMapper = context.nestedScope().nextLevel(nestedObjectMapper);
-            innerQuery = this.query.toQuery(context);
-            if (innerQuery == null) {
-                return null;
-            }
-        } finally {
-            context.nestedScope().previousLevel();
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        if (queryInnerHits != null) {
-            try (XContentParser parser = queryInnerHits.getXcontentParser()) {
-                XContentParser.Token token = parser.nextToken();
-                if (token != XContentParser.Token.START_OBJECT) {
-                    throw new IllegalStateException("start object expected but was: [" + token + "]");
-                }
-                InnerHitsSubSearchContext innerHits = context.indexQueryParserService().getInnerHitsQueryParserHelper().parse(parser);
-                if (innerHits != null) {
-                    ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-
-                    InnerHitsContext.NestedInnerHits nestedInnerHits = new InnerHitsContext.NestedInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, parentObjectMapper, nestedObjectMapper);
-                    String name = innerHits.getName() != null ? innerHits.getName() : path;
-                    context.addInnerHits(name, nestedInnerHits);
-                }
-            }
+        if (innerHit != null) {
+            builder.startObject("inner_hits");
+            builder.value(innerHit);
+            builder.endObject();
         }
-        return new ToParentBlockJoinQuery(Queries.filtered(innerQuery, childFilter), parentFilter, scoreMode);
+        builder.endObject();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
index c6d2716..5709d9b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
@@ -32,49 +32,55 @@ import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.support.InnerHitsQueryParserHelper;
 import org.elasticsearch.index.query.support.NestedInnerQueryParseSupport;
-import org.elasticsearch.index.query.support.QueryInnerHits;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsSubSearchContext;
 
 import java.io.IOException;
 
-public class NestedQueryParser extends BaseQueryParser<NestedQueryBuilder> {
+public class NestedQueryParser implements QueryParser {
 
+    public static final String NAME = "nested";
     private static final ParseField FILTER_FIELD = new ParseField("filter").withAllDeprecated("query");
-    private static final NestedQueryBuilder PROTOTYPE = new NestedQueryBuilder("", EmptyQueryBuilder.PROTOTYPE);
+
+    private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
+
+    @Inject
+    public NestedQueryParser(InnerHitsQueryParserHelper innerHitsQueryParserHelper) {
+        this.innerHitsQueryParserHelper = innerHitsQueryParserHelper;
+    }
 
     @Override
     public String[] names() {
-        return new String[]{NestedQueryBuilder.NAME, Strings.toCamelCase(NestedQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public NestedQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        ScoreMode scoreMode = NestedQueryBuilder.DEFAULT_SCORE_MODE;
+        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(parseContext);
+
+        float boost = 1.0f;
+        ScoreMode scoreMode = ScoreMode.Avg;
         String queryName = null;
-        QueryBuilder query = null;
-        String path = null;
+
         String currentFieldName = null;
-        QueryInnerHits queryInnerHits = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    query = parseContext.parseInnerQueryBuilder();
+                    builder.query();
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, FILTER_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    builder.filter();
                 } else if ("inner_hits".equals(currentFieldName)) {
-                    queryInnerHits = new QueryInnerHits(parser);
+                    builder.setInnerHits(innerHitsQueryParserHelper.parse(parseContext));
                 } else {
                     throw new ParsingException(parseContext, "[nested] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("path".equals(currentFieldName)) {
-                    path = parser.text();
+                    builder.setPath(parser.text());
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("score_mode".equals(currentFieldName) || "scoreMode".equals(currentFieldName)) {
@@ -99,11 +105,64 @@ public class NestedQueryParser extends BaseQueryParser<NestedQueryBuilder> {
                 }
             }
         }
-        return new NestedQueryBuilder(path, query, scoreMode, queryInnerHits).queryName(queryName).boost(boost);
+
+        builder.setScoreMode(scoreMode);
+        ToParentBlockJoinQuery joinQuery = builder.build();
+        if (joinQuery != null) {
+            joinQuery.setBoost(boost);
+            if (queryName != null) {
+                parseContext.addNamedQuery(queryName, joinQuery);
+            }
+        }
+        return joinQuery;
     }
 
-    @Override
-    public NestedQueryBuilder getBuilderPrototype() {
-        return PROTOTYPE;
+    public static class ToBlockJoinQueryBuilder extends NestedInnerQueryParseSupport {
+
+        private ScoreMode scoreMode;
+        private InnerHitsSubSearchContext innerHits;
+
+        public ToBlockJoinQueryBuilder(QueryParseContext parseContext) throws IOException {
+            super(parseContext);
+        }
+
+        public void setScoreMode(ScoreMode scoreMode) {
+            this.scoreMode = scoreMode;
+        }
+
+        public void setInnerHits(InnerHitsSubSearchContext innerHits) {
+            this.innerHits = innerHits;
+        }
+
+        @Nullable
+        public ToParentBlockJoinQuery build() throws IOException {
+            Query innerQuery;
+            if (queryFound) {
+                innerQuery = getInnerQuery();
+            } else if (filterFound) {
+                Query innerFilter = getInnerFilter();
+                if (innerFilter != null) {
+                    innerQuery = new ConstantScoreQuery(getInnerFilter());
+                } else {
+                    innerQuery = null;
+                }
+            } else {
+                throw new ParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
+            }
+
+            if (innerHits != null) {
+                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+                InnerHitsContext.NestedInnerHits nestedInnerHits = new InnerHitsContext.NestedInnerHits(innerHits.getSubSearchContext(), parsedQuery, null, getParentObjectMapper(), nestedObjectMapper);
+                String name = innerHits.getName() != null ? innerHits.getName() : path;
+                parseContext.addInnerHits(name, nestedInnerHits);
+            }
+
+            if (innerQuery != null) {
+                return new ToParentBlockJoinQuery(Queries.filtered(innerQuery, childFilter), parentFilter, scoreMode);
+            } else {
+                return null;
+            }
+        }
+
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
index 72b70a7..c16cf64 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -31,69 +27,29 @@ import java.util.Objects;
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  */
-public class NotQueryBuilder extends AbstractQueryBuilder<NotQueryBuilder> {
-
-    public static final String NAME = "not";
+public class NotQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder filter;
 
-    static final NotQueryBuilder PROTOTYPE = new NotQueryBuilder(EmptyQueryBuilder.PROTOTYPE);
+    private String queryName;
 
     public NotQueryBuilder(QueryBuilder filter) {
-        if (filter == null) {
-            throw new IllegalArgumentException("inner filter cannot be null");
-        }
-        this.filter = filter;
+        this.filter = Objects.requireNonNull(filter);
     }
 
-    /**
-     * @return the query added to "not".
-     */
-    public QueryBuilder innerQuery() {
-        return this.filter;
+    public NotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NotQueryParser.NAME);
         builder.field("query");
         filter.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query luceneQuery = filter.toFilter(context);
-        if (luceneQuery == null) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return Queries.not(luceneQuery);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filter);
-    }
-
-    @Override
-    protected boolean doEquals(NotQueryBuilder other) {
-        return Objects.equals(filter, other.filter);
-    }
-
-    @Override
-    protected NotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder queryBuilder = in.readQuery();
-        return new NotQueryBuilder(queryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filter);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
index 1fb32bd..80884a8 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
@@ -19,35 +19,41 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for not query
+ *
  */
-public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
+public class NotQueryParser implements QueryParser {
+
+    public static final String NAME = "not";
+    private static final ParseField QUERY_FIELD = new ParseField("filter", "query");
 
-    private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
+    @Inject
+    public NotQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{NotQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public NotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query query = null;
         boolean queryFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -56,18 +62,16 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    query = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     queryFound = true;
                     // its the filter, and the name is the field
-                    query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                    query = parseContext.parseInnerFilter(currentFieldName);
                 }
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new ParsingException(parseContext, "[not] query does not support [" + currentFieldName + "]");
                 }
@@ -75,17 +79,17 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
         }
 
         if (!queryFound) {
-            throw new ParsingException(parseContext, "query is required when using `not` query");
+            throw new ParsingException(parseContext, "filter is required when using `not` query");
         }
 
-        NotQueryBuilder notQueryBuilder = new NotQueryBuilder(query);
-        notQueryBuilder.queryName(queryName);
-        notQueryBuilder.boost(boost);
-        return notQueryBuilder;
-    }
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public NotQueryBuilder getBuilderPrototype() {
-        return NotQueryBuilder.PROTOTYPE;
+        Query notQuery = Queries.not(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, notQuery);
+        }
+        return notQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/Operator.java b/core/src/main/java/org/elasticsearch/index/query/Operator.java
deleted file mode 100644
index 22b5469..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/Operator.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queryparser.classic.QueryParser;
-import org.apache.lucene.search.BooleanClause;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.util.CollectionUtils;
-
-import java.io.IOException;
-
-public enum Operator implements Writeable<Operator> {
-    OR, AND;
-
-    private static final Operator PROTOTYPE = OR;
-
-    public BooleanClause.Occur toBooleanClauseOccur() {
-        switch (this) {
-            case OR:
-                return BooleanClause.Occur.SHOULD;
-            case AND:
-                return BooleanClause.Occur.MUST;
-            default:
-                throw Operator.newOperatorException(this.toString());
-        }
-    }
-
-    public QueryParser.Operator toQueryParserOperator() {
-        switch (this) {
-            case OR:
-                return QueryParser.Operator.OR;
-            case AND:
-                return QueryParser.Operator.AND;
-            default:
-                throw Operator.newOperatorException(this.toString());
-        }
-    }
-
-    @Override
-    public Operator readFrom(StreamInput in) throws IOException {
-        return Operator.values()[in.readVInt()];
-    }
-
-    public static Operator readOperatorFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal());
-    }
-
-    public static Operator fromString(String op) {
-        for (Operator operator : Operator.values()) {
-            if (operator.name().equalsIgnoreCase(op)) {
-                return operator;
-            }
-        }
-        throw Operator.newOperatorException(op);
-    }
-
-    private static IllegalArgumentException newOperatorException(String op) {
-        return new IllegalArgumentException("operator needs to be either " + CollectionUtils.arrayAsArrayList(Operator.values()) + ", but not [" + op + "]");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
index f5ca136..e0e5b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
@@ -19,59 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents containing terms with a specified prefix.
  */
-public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder> implements MultiTermQueryBuilder<PrefixQueryBuilder> {
+public class PrefixQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<PrefixQueryBuilder> {
 
-    public static final String NAME = "prefix";
+    private final String name;
 
-    private final String fieldName;
+    private final String prefix;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final PrefixQueryBuilder PROTOTYPE = new PrefixQueryBuilder("field", "value");
+    private String queryName;
 
     /**
      * A Query that matches documents containing terms with a specified prefix.
      *
-     * @param fieldName The name of the field
-     * @param value The prefix query
+     * @param name   The name of the field
+     * @param prefix The prefix query
      */
-    public PrefixQueryBuilder(String fieldName, String value) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("value cannot be null.");
-        }
-        this.fieldName = fieldName;
-        this.value = value;
+    public PrefixQueryBuilder(String name, String prefix) {
+        this.name = name;
+        this.prefix = prefix;
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public String value() {
-        return this.value;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public PrefixQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public PrefixQueryBuilder rewrite(String rewrite) {
@@ -79,71 +64,33 @@ public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder>
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public PrefixQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("prefix", this.value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.prefixQuery(value, method, context);
-        }
-        if (query == null) {
-            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
-            if (method != null) {
-                prefixQuery.setRewriteMethod(method);
+        builder.startObject(PrefixQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, prefix);
+        } else {
+            builder.startObject(name);
+            builder.field("prefix", prefix);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
             }
-            query = prefixQuery;
+            builder.endObject();
         }
-
-        return query;
-    }
-
-    @Override
-    protected PrefixQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder(in.readString(), in.readString());
-        prefixQueryBuilder.rewrite = in.readOptionalString();
-        return prefixQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(PrefixQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
index 3529ec4..c99f268 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
@@ -19,35 +19,48 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for prefix query
+ *
  */
-public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
+public class PrefixQueryParser implements QueryParser {
+
+    public static final String NAME = "prefix";
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of prefix query");
 
+    @Inject
+    public PrefixQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{PrefixQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public PrefixQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String value = null;
-        String rewrite = null;
-
+        String rewriteMethod = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        String value = null;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -68,7 +81,7 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else {
                             throw new ParsingException(parseContext, "[regexp] query does not support [" + currentFieldName + "]");
                         }
@@ -87,14 +100,25 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
         if (value == null) {
             throw new ParsingException(parseContext, "No value specified for prefix query");
         }
-        return new PrefixQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public PrefixQueryBuilder getBuilderPrototype() {
-        return PrefixQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.prefixQuery(value, method, parseContext);
+        }
+        if (query == null) {
+            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+            if (method != null) {
+                prefixQuery.setRewriteMethod(method);
+            }
+            query = prefixQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return  query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
index e66c95e..fa11d32 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
@@ -19,72 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
 
-public interface QueryBuilder<QB extends QueryBuilder> extends NamedWriteable<QB>, ToXContent {
+public abstract class QueryBuilder extends ToXContentToBytes {
 
-    /**
-     * Converts this QueryBuilder to a lucene {@link Query}.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toQuery(QueryShardContext context) throws IOException;
+    protected QueryBuilder() {
+        super(XContentType.JSON);
+    }
 
-    /**
-     * Converts this QueryBuilder to an unscored lucene {@link Query} that acts as a filter.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toFilter(QueryShardContext context) throws IOException;
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        doXContent(builder, params);
+        builder.endObject();
+        return builder;
+    }
 
-    /**
-     * Returns a {@link org.elasticsearch.common.bytes.BytesReference}
-     * containing the {@link ToXContent} output in binary format.
-     * Builds the request based on the default {@link XContentType}, either {@link Requests#CONTENT_TYPE} or provided as a constructor argument
-     */
-    //norelease once we move to serializing queries over the wire in Streamable format, this method shouldn't be needed anymore
-    BytesReference buildAsBytes();
-
-    /**
-     * Sets the arbitrary name to be assigned to the query (see named queries).
-     */
-    QB queryName(String queryName);
-
-    /**
-     * Returns the arbitrary name assigned to the query (see named queries).
-     */
-    String queryName();
-
-    /**
-     * Returns the boost for this query.
-     */
-    float boost();
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    QB boost(float boost);
-
-    /**
-     * Returns the name that identifies uniquely the query
-     */
-    String getName();
+    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 82f6395..f042056 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -26,15 +26,11 @@ import org.elasticsearch.common.geo.ShapeRelation;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
 
-import java.io.IOException;
 import java.util.Collection;
-import java.util.List;
 import java.util.Map;
 
 /**
@@ -43,7 +39,7 @@ import java.util.Map;
 public abstract class QueryBuilders {
 
     /**
-     * A query that matches on all documents.
+     * A query that match on all documents.
      */
     public static MatchAllQueryBuilder matchAllQuery() {
         return new MatchAllQueryBuilder();
@@ -56,17 +52,17 @@ public abstract class QueryBuilders {
      * @param text The query text (to be analyzed).
      */
     public static MatchQueryBuilder matchQuery(String name, Object text) {
-        return new MatchQueryBuilder(name, text).type(MatchQuery.Type.BOOLEAN);
+        return new MatchQueryBuilder(name, text).type(MatchQueryBuilder.Type.BOOLEAN);
     }
 
     /**
      * Creates a common query for the provided field name and text.
      *
-     * @param fieldName The field name.
+     * @param name The field name.
      * @param text The query text (to be analyzed).
      */
-    public static CommonTermsQueryBuilder commonTermsQuery(String fieldName, Object text) {
-        return new CommonTermsQueryBuilder(fieldName, text);
+    public static CommonTermsQueryBuilder commonTermsQuery(String name, Object text) {
+        return new CommonTermsQueryBuilder(name, text);
     }
 
     /**
@@ -86,7 +82,7 @@ public abstract class QueryBuilders {
      * @param text The query text (to be analyzed).
      */
     public static MatchQueryBuilder matchPhraseQuery(String name, Object text) {
-        return new MatchQueryBuilder(name, text).type(MatchQuery.Type.PHRASE);
+        return new MatchQueryBuilder(name, text).type(MatchQueryBuilder.Type.PHRASE);
     }
 
     /**
@@ -96,7 +92,7 @@ public abstract class QueryBuilders {
      * @param text The query text (to be analyzed).
      */
     public static MatchQueryBuilder matchPhrasePrefixQuery(String name, Object text) {
-        return new MatchQueryBuilder(name, text).type(MatchQuery.Type.PHRASE_PREFIX);
+        return new MatchQueryBuilder(name, text).type(MatchQueryBuilder.Type.PHRASE_PREFIX);
     }
 
     /**
@@ -280,8 +276,8 @@ public abstract class QueryBuilders {
      * Unlike the "NOT" clause, this still selects documents that contain undesirable terms,
      * but reduces their overall score:
      */
-    public static BoostingQueryBuilder boostingQuery(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        return new BoostingQueryBuilder(positiveQuery, negativeQuery);
+    public static BoostingQueryBuilder boostingQuery() {
+        return new BoostingQueryBuilder();
     }
 
     /**
@@ -315,33 +311,26 @@ public abstract class QueryBuilders {
         return new SpanFirstQueryBuilder(match, end);
     }
 
-    public static SpanNearQueryBuilder spanNearQuery(SpanQueryBuilder initialClause, int slop) {
-        return new SpanNearQueryBuilder(initialClause, slop);
+    public static SpanNearQueryBuilder spanNearQuery() {
+        return new SpanNearQueryBuilder();
     }
 
-    public static SpanNotQueryBuilder spanNotQuery(SpanQueryBuilder include, SpanQueryBuilder exclude) {
-        return new SpanNotQueryBuilder(include, exclude);
+    public static SpanNotQueryBuilder spanNotQuery() {
+        return new SpanNotQueryBuilder();
     }
 
-    public static SpanOrQueryBuilder spanOrQuery(SpanQueryBuilder initialClause) {
-        return new SpanOrQueryBuilder(initialClause);
+    public static SpanOrQueryBuilder spanOrQuery() {
+        return new SpanOrQueryBuilder();
     }
 
-    /** Creates a new {@code span_within} builder.
-    * @param big the big clause, it must enclose {@code little} for a match.
-    * @param little the little clause, it must be contained within {@code big} for a match.
-    */
-    public static SpanWithinQueryBuilder spanWithinQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanWithinQueryBuilder(big, little);
+    /** Creates a new {@code span_within} builder. */
+    public static SpanWithinQueryBuilder spanWithinQuery() {
+        return new SpanWithinQueryBuilder();
     }
 
-    /**
-     * Creates a new {@code span_containing} builder.
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
-     */
-    public static SpanContainingQueryBuilder spanContainingQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanContainingQueryBuilder(big, little);
+    /** Creates a new {@code span_containing} builder. */
+    public static SpanContainingQueryBuilder spanContainingQuery() {
+        return new SpanContainingQueryBuilder();
     }
 
     /**
@@ -351,7 +340,6 @@ public abstract class QueryBuilders {
      *
      * @param multiTermQueryBuilder The {@link MultiTermQueryBuilder} that
      *                              backs the created builder.
-     * @return
      */
 
     public static SpanMultiTermQueryBuilder spanMultiTermQueryBuilder(MultiTermQueryBuilder multiTermQueryBuilder) {
@@ -546,8 +534,19 @@ public abstract class QueryBuilders {
     /**
      * A Query builder which allows building a query thanks to a JSON string or binary data.
      */
-    public static WrapperQueryBuilder wrapperQuery(byte[] source) {
-        return new WrapperQueryBuilder(source);
+    public static WrapperQueryBuilder wrapperQuery(byte[] source, int offset, int length) {
+        return new WrapperQueryBuilder(source, offset, length);
+    }
+
+    /**
+     * Query that matches Documents based on the relationship between the given shape and
+     * indexed shapes
+     *
+     * @param name  The shape field name
+     * @param shape Shape to use in the Query
+     */
+    public static GeoShapeQueryBuilder geoShapeQuery(String name, ShapeBuilder shape) {
+        return new GeoShapeQueryBuilder(name, shape);
     }
 
     /**
@@ -579,10 +578,11 @@ public abstract class QueryBuilders {
     }
 
     /**
-     * A terms query that can extract the terms from another doc in an index.
+     * A terms lookup filter for the provided field name. A lookup terms filter can
+     * extract the terms to filter by from another doc in an index.
      */
-    public static TermsQueryBuilder termsLookupQuery(String name, TermsLookup termsLookup) {
-        return new TermsQueryBuilder(name, termsLookup);
+    public static TermsLookupQueryBuilder termsLookupQuery(String name) {
+        return new TermsLookupQueryBuilder(name);
     }
 
     /**
@@ -608,40 +608,29 @@ public abstract class QueryBuilders {
      * A filter to filter based on a specific range from a specific geo location / point.
      *
      * @param name The location field name.
-     * @param point The point
-     */
-    public static GeoDistanceRangeQueryBuilder geoDistanceRangeQuery(String name, GeoPoint point) {
-        return new GeoDistanceRangeQueryBuilder(name, point);
-    }
-
-    /**
-     * A filter to filter based on a specific range from a specific geo location / point.
-     *
-     * @param name The location field name.
-     * @param point The point as geohash
      */
-    public static GeoDistanceRangeQueryBuilder geoDistanceRangeQuery(String name, String geohash) {
-        return new GeoDistanceRangeQueryBuilder(name, geohash);
+    public static GeoDistanceRangeQueryBuilder geoDistanceRangeQuery(String name) {
+        return new GeoDistanceRangeQueryBuilder(name);
     }
 
     /**
-     * A filter to filter based on a specific range from a specific geo location / point.
+     * A filter to filter based on a bounding box defined by top left and bottom right locations / points
      *
      * @param name The location field name.
-     * @param lat The points latitude
-     * @param lon The points longitude
      */
-    public static GeoDistanceRangeQueryBuilder geoDistanceRangeQuery(String name, double lat, double lon) {
-        return new GeoDistanceRangeQueryBuilder(name, lat, lon);
+    public static GeoBoundingBoxQueryBuilder geoBoundingBoxQuery(String name) {
+        return new GeoBoundingBoxQueryBuilder(name);
     }
 
     /**
-     * A filter to filter based on a bounding box defined by top left and bottom right locations / points
+     * A filter based on a bounding box defined by geohash. The field this filter is applied to
+     * must have <code>{&quot;type&quot;:&quot;geo_point&quot;, &quot;geohash&quot;:true}</code>
+     * to work.
      *
-     * @param name The location field name.
+     * @param name The geo point field name.
      */
-    public static GeoBoundingBoxQueryBuilder geoBoundingBoxQuery(String name) {
-        return new GeoBoundingBoxQueryBuilder(name);
+    public static GeohashCellQuery.Builder geoHashCellQuery(String name) {
+        return new GeohashCellQuery.Builder(name);
     }
 
     /**
@@ -680,14 +669,14 @@ public abstract class QueryBuilders {
     public static GeohashCellQuery.Builder geoHashCellQuery(String name, String geohash, boolean neighbors) {
         return new GeohashCellQuery.Builder(name, geohash, neighbors);
     }
-
+    
     /**
      * A filter to filter based on a polygon defined by a set of locations  / points.
      *
      * @param name The location field name.
      */
-    public static GeoPolygonQueryBuilder geoPolygonQuery(String name, List<GeoPoint> points) {
-        return new GeoPolygonQueryBuilder(name, points);
+    public static GeoPolygonQueryBuilder geoPolygonQuery(String name) {
+        return new GeoPolygonQueryBuilder(name);
     }
 
     /**
@@ -697,12 +686,16 @@ public abstract class QueryBuilders {
      * @param shape Shape to use in the filter
      * @param relation relation of the shapes
      */
-    public static GeoShapeQueryBuilder geoShapeQuery(String name, ShapeBuilder shape) throws IOException {
-        return new GeoShapeQueryBuilder(name, shape);
+    public static GeoShapeQueryBuilder geoShapeQuery(String name, ShapeBuilder shape, ShapeRelation relation) {
+        return new GeoShapeQueryBuilder(name, shape, relation);
+    }
+
+    public static GeoShapeQueryBuilder geoShapeQuery(String name, String indexedShapeId, String indexedShapeType, ShapeRelation relation) {
+        return new GeoShapeQueryBuilder(name, indexedShapeId, indexedShapeType, relation);
     }
 
     public static GeoShapeQueryBuilder geoShapeQuery(String name, String indexedShapeId, String indexedShapeType) {
-        return new GeoShapeQueryBuilder(name, indexedShapeId, indexedShapeType);
+        return geoShapeQuery(name, indexedShapeId, indexedShapeType, null);
     }
 
     /**
@@ -711,16 +704,12 @@ public abstract class QueryBuilders {
      * @param name  The shape field name
      * @param shape Shape to use in the filter
      */
-    public static GeoShapeQueryBuilder geoIntersectionQuery(String name, ShapeBuilder shape) throws IOException {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, shape);
-        builder.relation(ShapeRelation.INTERSECTS);
-        return builder;
+    public static GeoShapeQueryBuilder geoIntersectionQuery(String name, ShapeBuilder shape) {
+        return geoShapeQuery(name, shape, ShapeRelation.INTERSECTS);
     }
 
     public static GeoShapeQueryBuilder geoIntersectionQuery(String name, String indexedShapeId, String indexedShapeType) {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, indexedShapeId, indexedShapeType);
-        builder.relation(ShapeRelation.INTERSECTS);
-        return builder;
+        return geoShapeQuery(name, indexedShapeId, indexedShapeType, ShapeRelation.INTERSECTS);
     }
 
     /**
@@ -729,16 +718,12 @@ public abstract class QueryBuilders {
      * @param name  The shape field name
      * @param shape Shape to use in the filter
      */
-    public static GeoShapeQueryBuilder geoWithinQuery(String name, ShapeBuilder shape) throws IOException {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, shape);
-        builder.relation(ShapeRelation.WITHIN);
-        return builder;
+    public static GeoShapeQueryBuilder geoWithinQuery(String name, ShapeBuilder shape) {
+        return geoShapeQuery(name, shape, ShapeRelation.WITHIN);
     }
 
     public static GeoShapeQueryBuilder geoWithinQuery(String name, String indexedShapeId, String indexedShapeType) {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, indexedShapeId, indexedShapeType);
-        builder.relation(ShapeRelation.WITHIN);
-        return builder;
+        return geoShapeQuery(name, indexedShapeId, indexedShapeType, ShapeRelation.WITHIN);
     }
 
     /**
@@ -747,16 +732,12 @@ public abstract class QueryBuilders {
      * @param name  The shape field name
      * @param shape Shape to use in the filter
      */
-    public static GeoShapeQueryBuilder geoDisjointQuery(String name, ShapeBuilder shape) throws IOException {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, shape);
-        builder.relation(ShapeRelation.DISJOINT);
-        return builder;
+    public static GeoShapeQueryBuilder geoDisjointQuery(String name, ShapeBuilder shape) {
+        return geoShapeQuery(name, shape, ShapeRelation.DISJOINT);
     }
 
     public static GeoShapeQueryBuilder geoDisjointQuery(String name, String indexedShapeId, String indexedShapeType) {
-        GeoShapeQueryBuilder builder = geoShapeQuery(name, indexedShapeId, indexedShapeType);
-        builder.relation(ShapeRelation.DISJOINT);
-        return builder;
+        return geoShapeQuery(name, indexedShapeId, indexedShapeType, ShapeRelation.DISJOINT);
     }
 
     /**
@@ -770,23 +751,11 @@ public abstract class QueryBuilders {
 
     /**
      * A filter to filter only documents where a field does not exists in them.
-     * @param fieldPattern the field to query
+     *
+     * @param name The name of the field
      */
     public static MissingQueryBuilder missingQuery(String name) {
-        return missingQuery(name, MissingQueryBuilder.DEFAULT_NULL_VALUE, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE);
-    }
-
-    /**
-     * A filter to filter only documents where a field does not exists in them.
-     * @param fieldPattern the field to query
-     * @param nullValue should the missing filter automatically include fields with null value configured in the
-     * mappings. Defaults to <tt>false</tt>.
-     * @param existence should the missing filter include documents where the field doesn't exist in the docs.
-     * Defaults to <tt>true</tt>.
-     * @throws IllegalArgumentException when both <tt>existence</tt> and <tt>nullValue</tt> are set to false
-     */
-    public static MissingQueryBuilder missingQuery(String name, boolean nullValue, boolean existence) {
-        return new MissingQueryBuilder(name, nullValue, existence);
+        return new MissingQueryBuilder(name);
     }
 
     public static NotQueryBuilder notQuery(QueryBuilder filter) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
index 4ca9e15..1e03ef1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A filter that simply wraps a query.
@@ -35,77 +30,22 @@ import java.util.Objects;
  */
 //TODO: remove when https://github.com/elastic/elasticsearch/issues/13326 is fixed
 @Deprecated
-public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder> {
-
-    public static final String NAME = "query";
+public class QueryFilterBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
-    static final QueryFilterBuilder PROTOTYPE = new QueryFilterBuilder(EmptyQueryBuilder.PROTOTYPE);
-
     /**
      * A filter that simply wraps a query.
      *
      * @param queryBuilder The query to wrap as a filter
      */
     public QueryFilterBuilder(QueryBuilder queryBuilder) {
-        if (queryBuilder == null) {
-            throw new IllegalArgumentException("inner query cannot be null");
-        }
         this.queryBuilder = queryBuilder;
     }
 
-    /**
-     * @return the query builder that is wrapped by this {@link QueryFilterBuilder}
-     */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(NAME);
+        builder.field(QueryFilterParser.NAME);
         queryBuilder.toXContent(builder, params);
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // inner query builder can potentially be `null`, in that case we ignore it
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //no-op this query doesn't support boost
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(QueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected QueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new QueryFilterBuilder(innerQueryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
index 44aa926..4d67eaf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
@@ -19,30 +19,29 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
 
 import java.io.IOException;
 
-/**
- * Parser for query filter
- * @deprecated use any query instead directly, possible since queries and filters are merged.
- */
 // TODO: remove when https://github.com/elastic/elasticsearch/issues/13326 is fixed
 @Deprecated
-public class QueryFilterParser extends BaseQueryParser<QueryFilterBuilder> {
+public class QueryFilterParser implements QueryParser {
 
-    @Override
-    public String[] names() {
-        return new String[]{QueryFilterBuilder.NAME};
+    public static final String NAME = "query";
+
+    @Inject
+    public QueryFilterParser() {
     }
 
     @Override
-    public QueryFilterBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        return new QueryFilterBuilder(parseContext.parseInnerQueryBuilder());
+    public String[] names() {
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryFilterBuilder getBuilderPrototype() {
-        return QueryFilterBuilder.PROTOTYPE;
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
+        return parseContext.parseInnerQuery();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
index 9a9fa4c..6237124 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
@@ -19,16 +19,43 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryparser.classic.MapperQueryParser;
+import org.apache.lucene.queryparser.classic.QueryParserSettings;
+import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.join.BitSetProducer;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.query.support.NestedScope;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
 import java.util.Map;
 
 public class QueryParseContext {
@@ -36,90 +63,175 @@ public class QueryParseContext {
     private static final ParseField CACHE = new ParseField("_cache").withAllDeprecated("Elasticsearch makes its own caching decisions");
     private static final ParseField CACHE_KEY = new ParseField("_cache_key").withAllDeprecated("Filters are always used as cache keys");
 
-    private XContentParser parser;
+    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
+
+    public static void setTypes(String[] types) {
+        typesContext.set(types);
+    }
+
+    public static String[] getTypes() {
+        return typesContext.get();
+    }
+
+    public static String[] setTypesWithPrevious(String[] types) {
+        String[] old = typesContext.get();
+        setTypes(types);
+        return old;
+    }
+
+    public static void removeTypes() {
+        typesContext.remove();
+    }
+
     private final Index index;
-    //norelease this flag is also used in the QueryShardContext, we need to make sure we set it there correctly in doToQuery()
-    private ParseFieldMatcher parseFieldMatcher;
 
-    //norelease this can eventually be deleted when context() method goes away
-    private final QueryShardContext shardContext;
-    private IndicesQueriesRegistry indicesQueriesRegistry;
+    private final Version indexVersionCreated;
+
+    private final IndexQueryParserService indexQueryParser;
+
+    private final Map<String, Query> namedQueries = new HashMap<>();
+
+    private final MapperQueryParser queryParser = new MapperQueryParser(this);
+
+    private XContentParser parser;
 
-    public QueryParseContext(Index index, IndicesQueriesRegistry registry) {
+    private ParseFieldMatcher parseFieldMatcher = ParseFieldMatcher.EMPTY;
+
+    private boolean allowUnmappedFields;
+
+    private boolean mapUnmappedFieldAsString;
+
+    private NestedScope nestedScope;
+
+    private boolean isFilter;
+
+    public QueryParseContext(Index index, IndexQueryParserService indexQueryParser) {
         this.index = index;
-        this.indicesQueriesRegistry = registry;
-        this.shardContext = null;
+        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
+        this.indexQueryParser = indexQueryParser;
+    }
+
+    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
+        if (parseFieldMatcher == null) {
+            throw new IllegalArgumentException("parseFieldMatcher must not be null");
+        }
+        this.parseFieldMatcher = parseFieldMatcher;
     }
 
-    QueryParseContext(QueryShardContext context) {
-        this.shardContext = context;
-        this.index = context.index();
-        this.indicesQueriesRegistry = context.indexQueryParserService().indicesQueriesRegistry();
+    public ParseFieldMatcher parseFieldMatcher() {
+        return parseFieldMatcher;
     }
 
     public void reset(XContentParser jp) {
+        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
         this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
+        this.lookup = null;
         this.parser = jp;
+        if (parser != null) {
+            this.parser.setParseFieldMatcher(parseFieldMatcher);
+        }
+        this.namedQueries.clear();
+        this.nestedScope = new NestedScope();
+        this.isFilter = false;
     }
 
-    //norelease this is still used in BaseQueryParserTemp and FunctionScoreQueryParser, remove if not needed there anymore
-    @Deprecated
-    public QueryShardContext shardContext() {
-        return this.shardContext;
+    public Index index() {
+        return this.index;
+    }
+
+    public void parser(XContentParser parser) {
+        this.parser = parser;
     }
 
     public XContentParser parser() {
-        return this.parser;
+        return parser;
+    }
+    
+    public IndexQueryParserService indexQueryParserService() {
+        return indexQueryParser;
     }
 
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
+    public AnalysisService analysisService() {
+        return indexQueryParser.analysisService;
     }
 
-    public boolean isDeprecatedSetting(String setting) {
-        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    public ScriptService scriptService() {
+        return indexQueryParser.scriptService;
     }
 
-    public Index index() {
-        return this.index;
+    public MapperService mapperService() {
+        return indexQueryParser.mapperService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder() for the resulting queries
-     */
     @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter() throws QueryShardException, IOException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public SimilarityService similarityService() {
+        return indexQueryParser.similarityService;
+    }
+
+    public Similarity searchSimilarity() {
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+    }
+
+    public String defaultField() {
+        return indexQueryParser.defaultField();
+    }
+
+    public boolean queryStringLenient() {
+        return indexQueryParser.queryStringLenient();
+    }
+
+    public MapperQueryParser queryParser(QueryParserSettings settings) {
+        queryParser.reset(settings);
+        return queryParser;
+    }
+
+    public BitSetProducer bitsetFilter(Filter filter) {
+        return indexQueryParser.bitsetFilterCache.getBitSetProducer(filter);
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
+        return indexQueryParser.fieldDataService.getForField(mapper);
+    }
+
+    public void addNamedQuery(String name, Query query) {
+        if (query != null) {
+            namedQueries.put(name, query);
         }
-        return result;
+    }
+
+    public ImmutableMap<String, Query> copyNamedQueries() {
+        return ImmutableMap.copyOf(namedQueries);
+    }
+
+    public void combineNamedQueries(QueryParseContext context) {
+        namedQueries.putAll(context.namedQueries);
     }
 
     /**
-     * @deprecated replaced by calls to parseInnerQueryBuilder() for the resulting queries
+     * Return whether we are currently parsing a filter or a query.
      */
-    @Nullable
-    @Deprecated
-    //norelease this method will be removed once all queries are refactored
-    public Query parseInnerQuery() throws IOException, QueryShardException {
-        QueryBuilder builder = parseInnerQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public boolean isFilter() {
+        return isFilter;
+    }
+
+    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
+        SearchContext sc = SearchContext.current();
+        if (sc == null) {
+            throw new ParsingException(this, "inner_hits unsupported");
         }
-        return result;
+
+        InnerHitsContext innerHitsContext;
+        if (sc.innerHits() == null) {
+            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
+            sc.innerHits(innerHitsContext);
+        } else {
+            innerHitsContext = sc.innerHits();
+        }
+        innerHitsContext.addInnerHitDefinition(name, context);
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser
-     * @throws IOException
-     */
-    public QueryBuilder parseInnerQueryBuilder() throws IOException {
+    @Nullable
+    public Query parseInnerQuery() throws ParsingException, IOException {
         // move to START object
         XContentParser.Token token;
         if (parser.currentToken() != XContentParser.Token.START_OBJECT) {
@@ -131,7 +243,7 @@ public class QueryParseContext {
         token = parser.nextToken();
         if (token == XContentParser.Token.END_OBJECT) {
             // empty query
-            return EmptyQueryBuilder.PROTOTYPE;
+            return null;
         }
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new ParsingException(this, "[_na] query malformed, no field after start_object");
@@ -143,11 +255,11 @@ public class QueryParseContext {
             throw new ParsingException(this, "[_na] query malformed, no field after start_object");
         }
 
-        QueryParser queryParser = queryParser(queryName);
+        QueryParser queryParser = indexQueryParser.queryParser(queryName);
         if (queryParser == null) {
             throw new ParsingException(this, "No query registered for [" + queryName + "]");
         }
-        QueryBuilder result = queryParser.fromXContent(this);
+        Query result = queryParser.parse(this);
         if (parser.currentToken() == XContentParser.Token.END_OBJECT || parser.currentToken() == XContentParser.Token.END_ARRAY) {
             // if we are at END_OBJECT, move to the next one...
             parser.nextToken();
@@ -155,46 +267,137 @@ public class QueryParseContext {
         return result;
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser, but does so that the inner query
-     * is parsed to a filter
-     * @throws IOException
-     */
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    public QueryBuilder parseInnerFilterToQueryBuilder() throws IOException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    @Nullable
+    public Query parseInnerFilter() throws ParsingException, IOException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            return parseInnerQueryBuilder();
+            isFilter = true;
+            return parseInnerQuery();
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    public QueryBuilder parseInnerFilterToQueryBuilder(String queryName) throws IOException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    public Query parseInnerFilter(String queryName) throws IOException, ParsingException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            QueryParser queryParser = queryParser(queryName);
+            isFilter = true;
+            QueryParser queryParser = indexQueryParser.queryParser(queryName);
             if (queryParser == null) {
                 throw new ParsingException(this, "No query registered for [" + queryName + "]");
             }
-            return queryParser.fromXContent(this);
+            return queryParser.parse(this);
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
+    public Collection<String> simpleMatchToIndexNames(String pattern) {
+        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
+    }
+
+    public MappedFieldType fieldMapper(String name) {
+        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
+    }
+
+    public ObjectMapper getObjectMapper(String name) {
+        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
+    }
+
+    /** Gets the search analyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchAnalyzer() != null) {
+            return fieldType.searchAnalyzer();
+        }
+        return mapperService().searchAnalyzer();
+    }
+
+    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchQuoteAnalyzer() != null) {
+            return fieldType.searchQuoteAnalyzer();
+        }
+        return mapperService().searchQuoteAnalyzer();
     }
 
-    public void parser(XContentParser innerParser) {
-        this.parser = innerParser;
+    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
+        this.allowUnmappedFields = allowUnmappedFields;
+    }
+
+    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
+        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
+    }
+
+    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
+        if (allowUnmappedFields) {
+            return fieldMapping;
+        } else if (mapUnmappedFieldAsString){
+            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+            // it would be better to pass the real index settings, but they are not easily accessible from here...
+            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
+            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
+        } else {
+            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
+            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
+                throw new ParsingException(this, "Strict field resolution and no field mapping can be found for the field with name ["
+                        + name + "]");
+            } else {
+                return fieldMapping;
+            }
+        }
+    }
+
+    /**
+     * Returns the narrowed down explicit types, or, if not set, all types.
+     */
+    public Collection<String> queryTypes() {
+        String[] types = getTypes();
+        if (types == null || types.length == 0) {
+            return mapperService().types();
+        }
+        if (types.length == 1 && types[0].equals("_all")) {
+            return mapperService().types();
+        }
+        return Arrays.asList(types);
+    }
+
+    private SearchLookup lookup = null;
+
+    public SearchLookup lookup() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.lookup();
+        }
+        if (lookup == null) {
+            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
+        }
+        return lookup;
+    }
+
+    public long nowInMillis() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.nowInMillis();
+        }
+        return System.currentTimeMillis();
+    }
+
+    public NestedScope nestedScope() {
+        return nestedScope;
+    }
+
+    /**
+     * Return whether the setting is deprecated.
+     */
+    public boolean isDeprecatedSetting(String setting) {
+        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
     }
 
-    QueryParser queryParser(String name) {
-        return indicesQueriesRegistry.queryParsers().get(name);
+    public Version indexVersionCreated() {
+        return indexVersionCreated;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
index 564298f..9553d93 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
@@ -21,14 +21,14 @@ package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.ParsingException;
 
 import java.io.IOException;
 
 /**
- * Defines a query parser that is able to read and parse a query object in {@link org.elasticsearch.common.xcontent.XContent}
- * format and create an internal object representing the query, implementing {@link QueryBuilder}, which can be streamed to other nodes.
+ *
  */
-public interface QueryParser<QB extends QueryBuilder<QB>> {
+public interface QueryParser {
 
     /**
      * The names this query parser is registered under.
@@ -36,32 +36,11 @@ public interface QueryParser<QB extends QueryBuilder<QB>> {
     String[] names();
 
     /**
-     * Parses the into a query from the current parser location. Will be at
-     * "START_OBJECT" location, and should end when the token is at the matching
-     * "END_OBJECT".
-     * <p/>
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * the DSL.
+     * Parses the into a query from the current parser location. Will be at "START_OBJECT" location,
+     * and should end when the token is at the matching "END_OBJECT".
+     * <p>
+     * Returns <tt>null</tt> if this query should be ignored in the context of the DSL.
      */
-    //norelease can be removed in favour of fromXContent once search requests can be parsed on the coordinating node
     @Nullable
-    Query parse(QueryShardContext context) throws IOException;
-
-    /**
-     * Creates a new {@link QueryBuilder} from the query held by the {@link QueryShardContext}
-     * in {@link org.elasticsearch.common.xcontent.XContent} format
-     *
-     * @param parseContext
-     *            the input parse context. The state on the parser contained in
-     *            this context will be changed as a side effect of this method
-     *            call
-     * @return the new QueryBuilder
-     * @throws IOException
-     */
-    QB fromXContent(QueryParseContext parseContext) throws IOException;
-
-    /**
-     * @return an empty {@link QueryBuilder} instance for this parser that can be used for deserialization
-     */
-    QB getBuilderPrototype();
+    Query parse(QueryParseContext parseContext) throws IOException, ParsingException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
deleted file mode 100644
index 6dd2e49..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ /dev/null
@@ -1,348 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.ImmutableMap;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryparser.classic.MapperQueryParser;
-import org.apache.lucene.queryparser.classic.QueryParserSettings;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitSetProducer;
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.Version;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.*;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.support.NestedScope;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Context object used to create lucene queries on the shard level.
- */
-public class QueryShardContext {
-
-    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
-
-    public static void setTypes(String[] types) {
-        typesContext.set(types);
-    }
-
-    public static String[] getTypes() {
-        return typesContext.get();
-    }
-
-    public static String[] setTypesWithPrevious(String[] types) {
-        String[] old = typesContext.get();
-        setTypes(types);
-        return old;
-    }
-
-    public static void removeTypes() {
-        typesContext.remove();
-    }
-
-    private final Index index;
-
-    private final Version indexVersionCreated;
-
-    private final IndexQueryParserService indexQueryParser;
-
-    private final Map<String, Query> namedQueries = new HashMap<>();
-
-    private final MapperQueryParser queryParser = new MapperQueryParser(this);
-
-    private ParseFieldMatcher parseFieldMatcher;
-
-    private boolean allowUnmappedFields;
-
-    private boolean mapUnmappedFieldAsString;
-
-    private NestedScope nestedScope;
-
-    //norelease this should be possible to remove once query context are completely separated
-    private QueryParseContext parseContext;
-
-    boolean isFilter;
-
-    public QueryShardContext(Index index, IndexQueryParserService indexQueryParser) {
-        this.index = index;
-        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
-        this.indexQueryParser = indexQueryParser;
-        this.parseContext = new QueryParseContext(this);
-    }
-
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
-    }
-
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
-    }
-
-    public void reset() {
-        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
-        this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
-        this.lookup = null;
-        this.namedQueries.clear();
-        this.nestedScope = new NestedScope();
-    }
-
-    //norelease remove parser argument once query contexts are separated
-    public void reset(XContentParser jp) {
-        this.reset();
-        this.parseContext.reset(jp);
-    }
-
-    public Index index() {
-        return this.index;
-    }
-
-    //norelease we might be able to avoid exposing the service to the outside world once all queries are refactored
-    public IndexQueryParserService indexQueryParserService() {
-        return indexQueryParser;
-    }
-
-    public AnalysisService analysisService() {
-        return indexQueryParser.analysisService;
-    }
-
-    public ScriptService scriptService() {
-        return indexQueryParser.scriptService;
-    }
-
-    public MapperService mapperService() {
-        return indexQueryParser.mapperService;
-    }
-
-    public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
-    }
-
-    public String defaultField() {
-        return indexQueryParser.defaultField();
-    }
-
-    public boolean queryStringLenient() {
-        return indexQueryParser.queryStringLenient();
-    }
-
-    public boolean queryStringAnalyzeWildcard() {
-        return indexQueryParser.queryStringAnalyzeWildcard();
-    }
-
-    public boolean queryStringAllowLeadingWildcard() {
-        return indexQueryParser.queryStringAllowLeadingWildcard();
-    }
-
-    public MapperQueryParser queryParser(QueryParserSettings settings) {
-        queryParser.reset(settings);
-        return queryParser;
-    }
-
-    public BitSetProducer bitsetFilter(Filter filter) {
-        return indexQueryParser.bitsetFilterCache.getBitSetProducer(filter);
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
-        return indexQueryParser.fieldDataService.getForField(mapper);
-    }
-
-    public void addNamedQuery(String name, Query query) {
-        if (query != null) {
-            namedQueries.put(name, query);
-        }
-    }
-
-    public ImmutableMap<String, Query> copyNamedQueries() {
-        return ImmutableMap.copyOf(namedQueries);
-    }
-
-    public void combineNamedQueries(QueryShardContext context) {
-        namedQueries.putAll(context.namedQueries);
-    }
-
-    /**
-     * Return whether we are currently parsing a filter or a query.
-     */
-    public boolean isFilter() {
-        return isFilter;
-    }
-
-    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
-        SearchContext sc = SearchContext.current();
-        if (sc == null) {
-            throw new QueryShardException(this, "inner_hits unsupported");
-        }
-
-        InnerHitsContext innerHitsContext;
-        if (sc.innerHits() == null) {
-            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
-            sc.innerHits(innerHitsContext);
-        } else {
-            innerHitsContext = sc.innerHits();
-        }
-        innerHitsContext.addInnerHitDefinition(name, context);
-    }
-
-    public Collection<String> simpleMatchToIndexNames(String pattern) {
-        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern);
-    }
-
-    public MappedFieldType fieldMapper(String name) {
-        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
-    }
-
-    public ObjectMapper getObjectMapper(String name) {
-        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
-    }
-
-    /**
-     * Gets the search analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchAnalyzer() != null) {
-            return fieldType.searchAnalyzer();
-        }
-        return mapperService().searchAnalyzer();
-    }
-
-    /**
-     * Gets the search quote analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchQuoteAnalyzer() != null) {
-            return fieldType.searchQuoteAnalyzer();
-        }
-        return mapperService().searchQuoteAnalyzer();
-    }
-
-    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
-        this.allowUnmappedFields = allowUnmappedFields;
-    }
-
-    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
-        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
-    }
-
-    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
-        if (allowUnmappedFields) {
-            return fieldMapping;
-        } else if (mapUnmappedFieldAsString) {
-            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
-            // it would be better to pass the real index settings, but they are not easily accessible from here...
-            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
-            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
-        } else {
-            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
-            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
-                throw new QueryShardException(this, "Strict field resolution and no field mapping can be found for the field with name ["
-                        + name + "]");
-            } else {
-                return fieldMapping;
-            }
-        }
-    }
-
-    /**
-     * Returns the narrowed down explicit types, or, if not set, all types.
-     */
-    public Collection<String> queryTypes() {
-        String[] types = getTypes();
-        if (types == null || types.length == 0) {
-            return mapperService().types();
-        }
-        if (types.length == 1 && types[0].equals("_all")) {
-            return mapperService().types();
-        }
-        return Arrays.asList(types);
-    }
-
-    private SearchLookup lookup = null;
-
-    public SearchLookup lookup() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.lookup();
-        }
-        if (lookup == null) {
-            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
-        }
-        return lookup;
-    }
-
-    public long nowInMillis() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.nowInMillis();
-        }
-        return System.currentTimeMillis();
-    }
-
-    public NestedScope nestedScope() {
-        return nestedScope;
-    }
-
-    public Version indexVersionCreated() {
-        return indexVersionCreated;
-    }
-
-    public QueryParseContext parseContext() {
-        return this.parseContext;
-    }
-
-    public boolean matchesIndices(String... indices) {
-        return this.indexQueryParser.matchesIndices(indices);
-    }
-
-    /*
-    * Executes the given template, and returns the response.
-    */
-    public BytesReference executeQueryTemplate(Template template, SearchContext searchContext) {
-        ExecutableScript executable = scriptService().executable(template, ScriptContext.Standard.SEARCH, searchContext);
-        return (BytesReference) executable.run();
-    }
-
-    public Client getClient() {
-        return indexQueryParser.getClient();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
deleted file mode 100644
index 1e31c7c..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- * Exception that is thrown when creating lucene queries on the shard
- */
-public class QueryShardException extends ElasticsearchException {
-
-    public QueryShardException(QueryShardContext context, String msg, Object... args) {
-        this(context, msg, null, args);
-    }
-
-    public QueryShardException(QueryShardContext context, String msg, Throwable cause, Object... args) {
-        super(msg, cause, args);
-        setIndex(context.index());
-    }
-
-    /**
-     * This constructor is provided for use in unit tests where a
-     * {@link QueryShardContext} may not be available
-     */
-    public QueryShardException(Index index, String msg, Throwable cause) {
-        super(msg, cause);
-        setIndex(index);
-    }
-
-    public QueryShardException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.BAD_REQUEST;
-    }
-
-    @Override
-    protected void innerToXContent(XContentBuilder builder, Params params) throws IOException {
-        super.innerToXContent(builder, params);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index 5c0f679..78bfac7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -19,27 +19,14 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.queryparser.classic.MapperQueryParser;
-import org.apache.lucene.queryparser.classic.QueryParserSettings;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.automaton.Operations;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
+import com.carrotsearch.hppc.ObjectFloatHashMap;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.analysis.NamedAnalyzer;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Locale;
-import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
  * A query that parses a query string and runs it. There are two modes that this operates. The first,
@@ -47,98 +34,73 @@ import java.util.TreeMap;
  * will use the {@link #defaultField(String)} set. The second, when one or more fields are added
  * (using {@link #field(String)}), will run the parsed query against the provided fields, and combine
  * them either using DisMax or a plain boolean query (see {@link #useDisMax(boolean)}).
- * <p/>
  */
-public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQueryBuilder> {
-
-    public static final String NAME = "query_string";
-
-    public static final boolean DEFAULT_AUTO_GENERATE_PHRASE_QUERIES = false;
-    public static final int DEFAULT_MAX_DETERMINED_STATES = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
-    public static final boolean DEFAULT_LOWERCASE_EXPANDED_TERMS = true;
-    public static final boolean DEFAULT_ENABLE_POSITION_INCREMENTS = true;
-    public static final boolean DEFAULT_ESCAPE = false;
-    public static final boolean DEFAULT_USE_DIS_MAX = true;
-    public static final int DEFAULT_FUZZY_PREFIX_LENGTH = FuzzyQuery.defaultPrefixLength;
-    public static final int DEFAULT_FUZZY_MAX_EXPANSIONS = FuzzyQuery.defaultMaxExpansions;
-    public static final int DEFAULT_PHRASE_SLOP = 0;
-    public static final float DEFAULT_TIE_BREAKER = 0.0f;
-    public static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    public static final Locale DEFAULT_LOCALE = Locale.ROOT;
-
-    static final QueryStringQueryBuilder PROTOTYPE = new QueryStringQueryBuilder("");
+public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<QueryStringQueryBuilder> {
+
+    public enum Operator {
+        OR,
+        AND
+    }
 
     private final String queryString;
 
     private String defaultField;
-    /**
-     * Fields to query against. If left empty will query default field,
-     * currently _ALL. Uses a TreeMap to hold the fields so boolean clauses are
-     * always sorted in same order for generated Lucene query for easier
-     * testing.
-     *
-     * Can be changed back to HashMap once https://issues.apache.org/jira/browse/LUCENE-6305 is fixed.
-     */
-    private final Map<String, Float> fieldsAndWeights = new TreeMap<>();
 
-    private Operator defaultOperator = DEFAULT_OPERATOR;
+    private Operator defaultOperator;
 
     private String analyzer;
     private String quoteAnalyzer;
 
     private String quoteFieldSuffix;
 
-    private boolean autoGeneratePhraseQueries = DEFAULT_AUTO_GENERATE_PHRASE_QUERIES;
+    private Boolean autoGeneratePhraseQueries;
 
     private Boolean allowLeadingWildcard;
 
-    private Boolean analyzeWildcard;
+    private Boolean lowercaseExpandedTerms;
 
-    private boolean lowercaseExpandedTerms = DEFAULT_LOWERCASE_EXPANDED_TERMS;
+    private Boolean enablePositionIncrements;
 
-    private boolean enablePositionIncrements = DEFAULT_ENABLE_POSITION_INCREMENTS;
+    private Boolean analyzeWildcard;
 
-    private Locale locale = DEFAULT_LOCALE;
+    private Locale locale;
 
-    private Fuzziness fuzziness = DEFAULT_FUZZINESS;
+    private float boost = -1;
 
-    private int fuzzyPrefixLength = DEFAULT_FUZZY_PREFIX_LENGTH;
+    private Fuzziness fuzziness;
+    private int fuzzyPrefixLength = -1;
+    private int fuzzyMaxExpansions = -1;
+    private String fuzzyRewrite;
 
-    private int fuzzyMaxExpansions = DEFAULT_FUZZY_MAX_EXPANSIONS;
+    private int phraseSlop = -1;
 
-    private String rewrite;
+    private List<String> fields;
 
-    private String fuzzyRewrite;
+    private ObjectFloatHashMap<String> fieldsBoosts;
 
-    private boolean escape = DEFAULT_ESCAPE;
+    private Boolean useDisMax;
 
-    private int phraseSlop = DEFAULT_PHRASE_SLOP;
+    private float tieBreaker = -1;
 
-    private boolean useDisMax = DEFAULT_USE_DIS_MAX;
-
-    private float tieBreaker = DEFAULT_TIE_BREAKER;
+    private String rewrite = null;
 
     private String minimumShouldMatch;
 
     private Boolean lenient;
 
-    private DateTimeZone timeZone;
+    private String queryName;
+
+    private String timeZone;
 
     /** To limit effort spent determinizing regexp queries. */
-    private int maxDeterminizedStates = DEFAULT_MAX_DETERMINED_STATES;
+    private Integer maxDeterminizedStates;
+
+    private Boolean escape;
 
     public QueryStringQueryBuilder(String queryString) {
-        if (queryString == null) {
-            throw new IllegalArgumentException("query text missing");
-        }
         this.queryString = queryString;
     }
 
-    public String queryString() {
-        return this.queryString;
-    }
-
     /**
      * The default field to run against when no prefix field is specified. Only relevant when
      * not explicitly adding fields the query string will run against.
@@ -148,16 +110,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public String defaultField() {
-        return this.defaultField;
-    }
-
     /**
-     * Adds a field to run the query string against. The field will be associated with the default boost of {@link AbstractQueryBuilder#DEFAULT_BOOST}.
-     * Use {@link #field(String, float)} to set a specific boost for the field.
+     * Adds a field to run the query string against.
      */
     public QueryStringQueryBuilder field(String field) {
-        this.fieldsAndWeights.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        if (fields == null) {
+            fields = new ArrayList<>();
+        }
+        fields.add(field);
         return this;
     }
 
@@ -165,23 +125,17 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
      * Adds a field to run the query string against with a specific boost.
      */
     public QueryStringQueryBuilder field(String field, float boost) {
-        this.fieldsAndWeights.put(field, boost);
-        return this;
-    }
-
-    /**
-     * Add several fields to run the query against with a specific boost.
-     */
-    public QueryStringQueryBuilder fields(Map<String, Float> fields) {
-        this.fieldsAndWeights.putAll(fields);
+        if (fields == null) {
+            fields = new ArrayList<>();
+        }
+        fields.add(field);
+        if (fieldsBoosts == null) {
+            fieldsBoosts = new ObjectFloatHashMap<>();
+        }
+        fieldsBoosts.put(field, boost);
         return this;
     }
 
-    /** Returns the fields including their respective boosts to run the query against. */
-    public Map<String, Float> fields() {
-        return this.fieldsAndWeights;
-    }
-
     /**
      * When more than one field is used with the query string, should queries be combined using
      * dis max, or boolean query. Defaults to dis max (<tt>true</tt>).
@@ -191,10 +145,6 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public boolean useDisMax() {
-        return this.useDisMax;
-    }
-
     /**
      * When more than one field is used with the query string, and combined queries are using
      * dis max, control the tie breaker for it.
@@ -204,29 +154,21 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public float tieBreaker() {
-        return this.tieBreaker;
-    }
-
     /**
      * Sets the boolean operator of the query parser used to parse the query string.
-     * <p/>
-     * <p>In default mode ({@link Operator#OR}) terms without any modifiers
+     * <p>
+     * In default mode ({@link Operator#OR}) terms without any modifiers
      * are considered optional: for example <code>capital of Hungary</code> is equal to
      * <code>capital OR of OR Hungary</code>.
-     * <p/>
-     * <p>In {@link Operator#AND} mode terms are considered to be in conjunction: the
+     * <p>
+     * In {@link Operator#AND} mode terms are considered to be in conjunction: the
      * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
      */
     public QueryStringQueryBuilder defaultOperator(Operator defaultOperator) {
-        this.defaultOperator = defaultOperator == null ? DEFAULT_OPERATOR : defaultOperator;
+        this.defaultOperator = defaultOperator;
         return this;
     }
 
-    public Operator defaultOperator() {
-        return this.defaultOperator;
-    }
-
     /**
      * The optional analyzer used to analyze the query string. Note, if a field has search analyzer
      * defined for it, then it will be used automatically. Defaults to the smart search analyzer.
@@ -240,17 +182,18 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
      * The optional analyzer used to analyze the query string for phrase searches. Note, if a field has search (quote) analyzer
      * defined for it, then it will be used automatically. Defaults to the smart search analyzer.
      */
-    public QueryStringQueryBuilder quoteAnalyzer(String quoteAnalyzer) {
-        this.quoteAnalyzer = quoteAnalyzer;
+    public QueryStringQueryBuilder quoteAnalyzer(String analyzer) {
+        this.quoteAnalyzer = analyzer;
         return this;
     }
 
+
     /**
      * Set to true if phrase queries will be automatically generated
      * when the analyzer returns more than one term from whitespace
      * delimited text.
      * NOTE: this behavior may not be suitable for all languages.
-     * <p/>
+     * <p>
      * Set to false if phrase queries should only be generated when
      * surrounded by double quotes.
      */
@@ -259,10 +202,6 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public boolean autoGeneratePhraseQueries() {
-        return this.autoGeneratePhraseQueries;
-    }
-
     /**
      * Protects against too-difficult regular expression queries.
      */
@@ -271,22 +210,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public int maxDeterminizedStates() {
-        return this.maxDeterminizedStates;
-    }
-
     /**
      * Should leading wildcards be allowed or not. Defaults to <tt>true</tt>.
      */
-    public QueryStringQueryBuilder allowLeadingWildcard(Boolean allowLeadingWildcard) {
+    public QueryStringQueryBuilder allowLeadingWildcard(boolean allowLeadingWildcard) {
         this.allowLeadingWildcard = allowLeadingWildcard;
         return this;
     }
 
-    public Boolean allowLeadingWildcard() {
-        return this.allowLeadingWildcard;
-    }
-
     /**
      * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
      * lower-cased or not.  Default is <tt>true</tt>.
@@ -296,15 +227,11 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public boolean lowercaseExpandedTerms() {
-        return this.lowercaseExpandedTerms;
-    }
-
     /**
      * Set to <tt>true</tt> to enable position increments in result query. Defaults to
      * <tt>true</tt>.
-     * <p/>
-     * <p>When set, result phrase and multi-phrase queries will be aware of position increments.
+     * <p>
+     * When set, result phrase and multi-phrase queries will be aware of position increments.
      * Useful when e.g. a StopFilter increases the position increment of the token that follows an omitted token.
      */
     public QueryStringQueryBuilder enablePositionIncrements(boolean enablePositionIncrements) {
@@ -312,22 +239,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public boolean enablePositionIncrements() {
-        return this.enablePositionIncrements;
-    }
-
     /**
      * Set the edit distance for fuzzy queries. Default is "AUTO".
      */
     public QueryStringQueryBuilder fuzziness(Fuzziness fuzziness) {
-        this.fuzziness = fuzziness == null ? DEFAULT_FUZZINESS : fuzziness;
+        this.fuzziness = fuzziness;
         return this;
     }
 
-    public Fuzziness fuzziness() {
-        return this.fuzziness;
-    }
-
     /**
      * Set the minimum prefix length for fuzzy queries. Default is 1.
      */
@@ -336,28 +255,16 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public int fuzzyPrefixLength() {
-        return fuzzyPrefixLength;
-    }
-
     public QueryStringQueryBuilder fuzzyMaxExpansions(int fuzzyMaxExpansions) {
         this.fuzzyMaxExpansions = fuzzyMaxExpansions;
         return this;
     }
 
-    public int fuzzyMaxExpansions() {
-        return fuzzyMaxExpansions;
-    }
-
     public QueryStringQueryBuilder fuzzyRewrite(String fuzzyRewrite) {
         this.fuzzyRewrite = fuzzyRewrite;
         return this;
     }
 
-    public String fuzzyRewrite() {
-        return fuzzyRewrite;
-    }
-
     /**
      * Sets the default slop for phrases.  If zero, then exact phrase matches
      * are required. Default value is zero.
@@ -367,38 +274,32 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public int phraseSlop() {
-        return phraseSlop;
-    }
-
     /**
      * Set to <tt>true</tt> to enable analysis on wildcard and prefix queries.
      */
-    public QueryStringQueryBuilder analyzeWildcard(Boolean analyzeWildcard) {
+    public QueryStringQueryBuilder analyzeWildcard(boolean analyzeWildcard) {
         this.analyzeWildcard = analyzeWildcard;
         return this;
     }
 
-    public Boolean analyzeWildcard() {
-        return this.analyzeWildcard;
-    }
-
     public QueryStringQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
-    }
-
     public QueryStringQueryBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    public String minimumShouldMatch() {
-        return this.minimumShouldMatch;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public QueryStringQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -409,10 +310,6 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public String quoteFieldSuffix() {
-        return this.quoteFieldSuffix;
-    }
-
     /**
      * Sets the query string parser to be lenient when parsing field values, defaults to the index
      * setting and if not set, defaults to false.
@@ -422,40 +319,27 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public Boolean lenient() {
-        return this.lenient;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public QueryStringQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     public QueryStringQueryBuilder locale(Locale locale) {
-        this.locale = locale == null ? DEFAULT_LOCALE : locale;
+        this.locale = locale;
         return this;
     }
 
-    public Locale locale() {
-        return this.locale;
-    }
-
     /**
      * In case of date field, we can adjust the from/to fields using a timezone
      */
     public QueryStringQueryBuilder timeZone(String timeZone) {
-        if (timeZone != null) {
-            this.timeZone = DateTimeZone.forID(timeZone);
-        } else {
-            this.timeZone = null;
-        }
-        return this;
-    }
-
-    public QueryStringQueryBuilder timeZone(DateTimeZone timeZone) {
         this.timeZone = timeZone;
         return this;
     }
 
-    public DateTimeZone timeZone() {
-        return this.timeZone;
-    }
-
     /**
      * Set to <tt>true</tt> to enable escaping of the query string
      */
@@ -464,275 +348,98 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
-    public boolean escape() {
-        return this.escape;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query", this.queryString);
-        if (this.defaultField != null) {
-            builder.field("default_field", this.defaultField);
+        builder.startObject(QueryStringQueryParser.NAME);
+        builder.field("query", queryString);
+        if (defaultField != null) {
+            builder.field("default_field", defaultField);
         }
-        builder.startArray("fields");
-        for (Map.Entry<String, Float> fieldEntry : this.fieldsAndWeights.entrySet()) {
-            builder.value(fieldEntry.getKey() + "^" + fieldEntry.getValue());
+        if (fields != null) {
+            builder.startArray("fields");
+            for (String field : fields) {
+                if (fieldsBoosts != null && fieldsBoosts.containsKey(field)) {
+                    field += "^" + fieldsBoosts.get(field);
+                }
+                builder.value(field);
+            }
+            builder.endArray();
         }
-        builder.endArray();
-        builder.field("use_dis_max", this.useDisMax);
-        builder.field("tie_breaker", this.tieBreaker);
-        builder.field("default_operator", this.defaultOperator.name().toLowerCase(Locale.ROOT));
-        if (this.analyzer != null) {
-            builder.field("analyzer", this.analyzer);
+        if (useDisMax != null) {
+            builder.field("use_dis_max", useDisMax);
         }
-        if (this.quoteAnalyzer != null) {
-            builder.field("quote_analyzer", this.quoteAnalyzer);
+        if (tieBreaker != -1) {
+            builder.field("tie_breaker", tieBreaker);
         }
-        builder.field("auto_generate_phrase_queries", this.autoGeneratePhraseQueries);
-        builder.field("max_determinized_states", this.maxDeterminizedStates);
-        if (this.allowLeadingWildcard != null) {
-            builder.field("allow_leading_wildcard", this.allowLeadingWildcard);
+        if (defaultOperator != null) {
+            builder.field("default_operator", defaultOperator.name().toLowerCase(Locale.ROOT));
         }
-        builder.field("lowercase_expanded_terms", this.lowercaseExpandedTerms);
-        builder.field("enable_position_increments", this.enablePositionIncrements);
-        this.fuzziness.toXContent(builder, params);
-        builder.field("fuzzy_prefix_length", this.fuzzyPrefixLength);
-        builder.field("fuzzy_max_expansions", this.fuzzyMaxExpansions);
-        if (this.fuzzyRewrite != null) {
-            builder.field("fuzzy_rewrite", this.fuzzyRewrite);
+        if (analyzer != null) {
+            builder.field("analyzer", analyzer);
         }
-        builder.field("phrase_slop", this.phraseSlop);
-        if (this.analyzeWildcard != null) {
-            builder.field("analyze_wildcard", this.analyzeWildcard);
+        if (quoteAnalyzer != null) {
+            builder.field("quote_analyzer", quoteAnalyzer);
         }
-        if (this.rewrite != null) {
-            builder.field("rewrite", this.rewrite);
+        if (autoGeneratePhraseQueries != null) {
+            builder.field("auto_generate_phrase_queries", autoGeneratePhraseQueries);
         }
-        if (this.minimumShouldMatch != null) {
-            builder.field("minimum_should_match", this.minimumShouldMatch);
+        if (maxDeterminizedStates != null) {
+            builder.field("max_determinized_states", maxDeterminizedStates);
         }
-        if (this.quoteFieldSuffix != null) {
-            builder.field("quote_field_suffix", this.quoteFieldSuffix);
+        if (allowLeadingWildcard != null) {
+            builder.field("allow_leading_wildcard", allowLeadingWildcard);
         }
-        if (this.lenient != null) {
-            builder.field("lenient", this.lenient);
+        if (lowercaseExpandedTerms != null) {
+            builder.field("lowercase_expanded_terms", lowercaseExpandedTerms);
         }
-        builder.field("locale", this.locale.toLanguageTag());
-        if (this.timeZone != null) {
-            builder.field("time_zone", this.timeZone.getID());
+        if (enablePositionIncrements != null) {
+            builder.field("enable_position_increments", enablePositionIncrements);
         }
-        builder.field("escape", this.escape);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected QueryStringQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryStringQueryBuilder queryStringQueryBuilder = new QueryStringQueryBuilder(in.readString());
-        queryStringQueryBuilder.defaultField = in.readOptionalString();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            queryStringQueryBuilder.fieldsAndWeights.put(in.readString(), in.readFloat());
+        if (fuzziness != null) {
+            fuzziness.toXContent(builder, params);
         }
-        queryStringQueryBuilder.defaultOperator = Operator.readOperatorFrom(in);
-        queryStringQueryBuilder.analyzer = in.readOptionalString();
-        queryStringQueryBuilder.quoteAnalyzer = in.readOptionalString();
-        queryStringQueryBuilder.quoteFieldSuffix = in.readOptionalString();
-        queryStringQueryBuilder.autoGeneratePhraseQueries = in.readBoolean();
-        queryStringQueryBuilder.allowLeadingWildcard = in.readOptionalBoolean();
-        queryStringQueryBuilder.analyzeWildcard = in.readOptionalBoolean();
-        queryStringQueryBuilder.lowercaseExpandedTerms = in.readBoolean();
-        queryStringQueryBuilder.enablePositionIncrements = in.readBoolean();
-        queryStringQueryBuilder.locale = Locale.forLanguageTag(in.readString());
-        queryStringQueryBuilder.fuzziness = Fuzziness.readFuzzinessFrom(in);
-        queryStringQueryBuilder.fuzzyPrefixLength = in.readVInt();
-        queryStringQueryBuilder.fuzzyMaxExpansions = in.readVInt();
-        queryStringQueryBuilder.fuzzyRewrite = in.readOptionalString();
-        queryStringQueryBuilder.phraseSlop = in.readVInt();
-        queryStringQueryBuilder.useDisMax = in.readBoolean();
-        queryStringQueryBuilder.tieBreaker = in.readFloat();
-        queryStringQueryBuilder.rewrite = in.readOptionalString();
-        queryStringQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        queryStringQueryBuilder.lenient = in.readOptionalBoolean();
-        if (in.readBoolean()) {
-            queryStringQueryBuilder.timeZone = DateTimeZone.forID(in.readString());
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        queryStringQueryBuilder.escape = in.readBoolean();
-        queryStringQueryBuilder.maxDeterminizedStates = in.readVInt();
-        return queryStringQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.queryString);
-        out.writeOptionalString(this.defaultField);
-        out.writeVInt(this.fieldsAndWeights.size());
-        for (Map.Entry<String, Float> fieldsEntry : this.fieldsAndWeights.entrySet()) {
-            out.writeString(fieldsEntry.getKey());
-            out.writeFloat(fieldsEntry.getValue());
+        if (fuzzyPrefixLength != -1) {
+            builder.field("fuzzy_prefix_length", fuzzyPrefixLength);
         }
-        this.defaultOperator.writeTo(out);
-        out.writeOptionalString(this.analyzer);
-        out.writeOptionalString(this.quoteAnalyzer);
-        out.writeOptionalString(this.quoteFieldSuffix);
-        out.writeBoolean(this.autoGeneratePhraseQueries);
-        out.writeOptionalBoolean(this.allowLeadingWildcard);
-        out.writeOptionalBoolean(this.analyzeWildcard);
-        out.writeBoolean(this.lowercaseExpandedTerms);
-        out.writeBoolean(this.enablePositionIncrements);
-        out.writeString(this.locale.toLanguageTag());
-        this.fuzziness.writeTo(out);
-        out.writeVInt(this.fuzzyPrefixLength);
-        out.writeVInt(this.fuzzyMaxExpansions);
-        out.writeOptionalString(this.fuzzyRewrite);
-        out.writeVInt(this.phraseSlop);
-        out.writeBoolean(this.useDisMax);
-        out.writeFloat(this.tieBreaker);
-        out.writeOptionalString(this.rewrite);
-        out.writeOptionalString(this.minimumShouldMatch);
-        out.writeOptionalBoolean(this.lenient);
-        if (this.timeZone == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeString(this.timeZone.getID());
+        if (fuzzyMaxExpansions != -1) {
+            builder.field("fuzzy_max_expansions", fuzzyMaxExpansions);
         }
-        out.writeBoolean(this.escape);
-        out.writeVInt(this.maxDeterminizedStates);
-    }
-
-    @Override
-    protected boolean doEquals(QueryStringQueryBuilder other) {
-        return Objects.equals(queryString, other.queryString) &&
-                Objects.equals(defaultField, other.defaultField) &&
-                Objects.equals(fieldsAndWeights, other.fieldsAndWeights) &&
-                Objects.equals(defaultOperator, other.defaultOperator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(quoteAnalyzer, other.quoteAnalyzer) &&
-                Objects.equals(quoteFieldSuffix, other.quoteFieldSuffix) &&
-                Objects.equals(autoGeneratePhraseQueries, other.autoGeneratePhraseQueries) &&
-                Objects.equals(allowLeadingWildcard, other.allowLeadingWildcard) &&
-                Objects.equals(lowercaseExpandedTerms, other.lowercaseExpandedTerms) &&
-                Objects.equals(enablePositionIncrements, other.enablePositionIncrements) &&
-                Objects.equals(analyzeWildcard, other.analyzeWildcard) &&
-                Objects.equals(locale.toLanguageTag(), other.locale.toLanguageTag()) &&
-                Objects.equals(fuzziness, other.fuzziness) &&
-                Objects.equals(fuzzyPrefixLength, other.fuzzyPrefixLength) &&
-                Objects.equals(fuzzyMaxExpansions, other.fuzzyMaxExpansions) &&
-                Objects.equals(fuzzyRewrite, other.fuzzyRewrite) &&
-                Objects.equals(phraseSlop, other.phraseSlop) &&
-                Objects.equals(useDisMax, other.useDisMax) &&
-                Objects.equals(tieBreaker, other.tieBreaker) &&
-                Objects.equals(rewrite, other.rewrite) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(lenient, other.lenient) &&
-                timeZone == null ? other.timeZone == null : other.timeZone != null && Objects.equals(timeZone.getID(), other.timeZone.getID()) &&
-                Objects.equals(escape, other.escape) &&
-                Objects.equals(maxDeterminizedStates, other.maxDeterminizedStates);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryString, defaultField, fieldsAndWeights, defaultOperator, analyzer, quoteAnalyzer,
-                quoteFieldSuffix, autoGeneratePhraseQueries, allowLeadingWildcard, lowercaseExpandedTerms,
-                enablePositionIncrements, analyzeWildcard, locale.toLanguageTag(), fuzziness, fuzzyPrefixLength,
-                fuzzyMaxExpansions, fuzzyRewrite, phraseSlop, useDisMax, tieBreaker, rewrite, minimumShouldMatch, lenient,
-                timeZone == null ? 0 : timeZone.getID(), escape, maxDeterminizedStates);
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        //TODO would be nice to have all the settings in one place: some change though at query execution time
-        //e.g. field names get expanded to concrete names, defaults get resolved sometimes to settings values etc.
-        QueryParserSettings qpSettings;
-        if (this.escape) {
-            qpSettings = new QueryParserSettings(org.apache.lucene.queryparser.classic.QueryParser.escape(this.queryString));
-        } else {
-            qpSettings = new QueryParserSettings(this.queryString);
+        if (fuzzyRewrite != null) {
+            builder.field("fuzzy_rewrite", fuzzyRewrite);
         }
-        qpSettings.defaultField(this.defaultField == null ? context.defaultField() : this.defaultField);
-        Map<String, Float> resolvedFields = new TreeMap<>();
-        for (Map.Entry<String, Float> fieldsEntry : fieldsAndWeights.entrySet()) {
-            String fieldName = fieldsEntry.getKey();
-            Float weight = fieldsEntry.getValue();
-            if (Regex.isSimpleMatchPattern(fieldName)) {
-                for (String resolvedFieldName : context.mapperService().simpleMatchToIndexNames(fieldName)) {
-                    resolvedFields.put(resolvedFieldName, weight);
-                }
-            } else {
-                resolvedFields.put(fieldName, weight);
-            }
+        if (phraseSlop != -1) {
+            builder.field("phrase_slop", phraseSlop);
         }
-        qpSettings.fieldsAndWeights(resolvedFields);
-        qpSettings.defaultOperator(defaultOperator.toQueryParserOperator());
-
-        if (analyzer == null) {
-            qpSettings.defaultAnalyzer(context.mapperService().searchAnalyzer());
-        } else {
-            NamedAnalyzer namedAnalyzer = context.analysisService().analyzer(analyzer);
-            if (namedAnalyzer == null) {
-                throw new QueryShardException(context, "[query_string] analyzer [" + analyzer + "] not found");
-            }
-            qpSettings.forceAnalyzer(namedAnalyzer);
+        if (analyzeWildcard != null) {
+            builder.field("analyze_wildcard", analyzeWildcard);
         }
-        if (quoteAnalyzer != null) {
-            NamedAnalyzer namedAnalyzer = context.analysisService().analyzer(quoteAnalyzer);
-            if (namedAnalyzer == null) {
-                throw new QueryShardException(context, "[query_string] quote_analyzer [" + quoteAnalyzer + "] not found");
-            }
-            qpSettings.forceQuoteAnalyzer(namedAnalyzer);
-        } else if (analyzer != null) {
-            qpSettings.forceQuoteAnalyzer(qpSettings.analyzer());
-        } else {
-            qpSettings.defaultQuoteAnalyzer(context.mapperService().searchQuoteAnalyzer());
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-
-        qpSettings.quoteFieldSuffix(quoteFieldSuffix);
-        qpSettings.autoGeneratePhraseQueries(autoGeneratePhraseQueries);
-        qpSettings.allowLeadingWildcard(allowLeadingWildcard == null ? context.queryStringAllowLeadingWildcard() : allowLeadingWildcard);
-        qpSettings.analyzeWildcard(analyzeWildcard == null ? context.queryStringAnalyzeWildcard() : analyzeWildcard);
-        qpSettings.lowercaseExpandedTerms(lowercaseExpandedTerms);
-        qpSettings.enablePositionIncrements(enablePositionIncrements);
-        qpSettings.locale(locale);
-        qpSettings.fuzziness(fuzziness);
-        qpSettings.fuzzyPrefixLength(fuzzyPrefixLength);
-        qpSettings.fuzzyMaxExpansions(fuzzyMaxExpansions);
-        qpSettings.fuzzyRewriteMethod(QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), this.fuzzyRewrite));
-        qpSettings.phraseSlop(phraseSlop);
-        qpSettings.useDisMax(useDisMax);
-        qpSettings.tieBreaker(tieBreaker);
-        qpSettings.rewriteMethod(QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), this.rewrite));
-        qpSettings.lenient(lenient == null ? context.queryStringLenient() : lenient);
-        qpSettings.timeZone(timeZone);
-        qpSettings.maxDeterminizedStates(maxDeterminizedStates);
-
-        MapperQueryParser queryParser = context.queryParser(qpSettings);
-        Query query;
-        try {
-            query = queryParser.parse(queryString);
-        } catch (org.apache.lucene.queryparser.classic.ParseException e) {
-            throw new QueryShardException(context, "Failed to parse query [" + this.queryString + "]", e);
+        if (minimumShouldMatch != null) {
+            builder.field("minimum_should_match", minimumShouldMatch);
         }
-
-        if (query == null) {
-            return null;
+        if (quoteFieldSuffix != null) {
+            builder.field("quote_field_suffix", quoteFieldSuffix);
         }
-        query = Queries.fixNegativeQueryIfNeeded(query);
-        if (query instanceof BooleanQuery) {
-            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, this.minimumShouldMatch());
+        if (lenient != null) {
+            builder.field("lenient", lenient);
         }
-        return query;
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //we need to preserve the boost that came out of the parsing phase
-        query.setBoost(query.getBoost() * boost);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        if (locale != null) {
+            builder.field("locale", locale.toString());
+        }
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+        if (escape != null) {
+            builder.field("escape", escape);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
index 6f104a6..3642bc2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
@@ -19,63 +19,67 @@
 
 package org.elasticsearch.index.query;
 
+import com.carrotsearch.hppc.ObjectFloatHashMap;
+import org.apache.lucene.queryparser.classic.MapperQueryParser;
+import org.apache.lucene.queryparser.classic.QueryParserSettings;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
+import org.elasticsearch.common.util.LocaleUtils;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.analysis.NamedAnalyzer;
+import org.elasticsearch.index.query.support.QueryParsers;
+import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.HashMap;
+import java.util.ArrayList;
 import java.util.Locale;
-import java.util.Map;
+
+import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
- * Parser for query_string query
+ *
  */
-public class QueryStringQueryParser extends BaseQueryParser {
+public class QueryStringQueryParser implements QueryParser {
 
+    public static final String NAME = "query_string";
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("fuzzy_min_sim");
 
+    private final boolean defaultAnalyzeWildcard;
+    private final boolean defaultAllowLeadingWildcard;
+
+    @Inject
+    public QueryStringQueryParser(Settings settings) {
+        this.defaultAnalyzeWildcard = settings.getAsBoolean("indices.query.query_string.analyze_wildcard", QueryParserSettings.DEFAULT_ANALYZE_WILDCARD);
+        this.defaultAllowLeadingWildcard = settings.getAsBoolean("indices.query.query_string.allowLeadingWildcard", QueryParserSettings.DEFAULT_ALLOW_LEADING_WILDCARD);
+    }
+
     @Override
     public String[] names() {
-        return new String[]{QueryStringQueryBuilder.NAME, Strings.toCamelCase(QueryStringQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
+
+        String queryName = null;
+        QueryParserSettings qpSettings = new QueryParserSettings();
+        qpSettings.defaultField(parseContext.defaultField());
+        qpSettings.lenient(parseContext.queryStringLenient());
+        qpSettings.analyzeWildcard(defaultAnalyzeWildcard);
+        qpSettings.allowLeadingWildcard(defaultAllowLeadingWildcard);
+        qpSettings.locale(Locale.ROOT);
+
         String currentFieldName = null;
         XContentParser.Token token;
-        String queryString = null;
-        String defaultField = null;
-        String analyzer = null;
-        String quoteAnalyzer = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean autoGeneratePhraseQueries = QueryStringQueryBuilder.DEFAULT_AUTO_GENERATE_PHRASE_QUERIES;
-        int maxDeterminizedStates = QueryStringQueryBuilder.DEFAULT_MAX_DETERMINED_STATES;
-        boolean lowercaseExpandedTerms = QueryStringQueryBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        boolean enablePositionIncrements = QueryStringQueryBuilder.DEFAULT_ENABLE_POSITION_INCREMENTS;
-        boolean escape = QueryStringQueryBuilder.DEFAULT_ESCAPE;
-        boolean useDisMax = QueryStringQueryBuilder.DEFAULT_USE_DIS_MAX;
-        int fuzzyPrefixLength = QueryStringQueryBuilder.DEFAULT_FUZZY_PREFIX_LENGTH;
-        int fuzzyMaxExpansions = QueryStringQueryBuilder.DEFAULT_FUZZY_MAX_EXPANSIONS;
-        int phraseSlop = QueryStringQueryBuilder.DEFAULT_PHRASE_SLOP;
-        float tieBreaker = QueryStringQueryBuilder.DEFAULT_TIE_BREAKER;
-        Boolean analyzeWildcard = null;
-        Boolean allowLeadingWildcard = null;
-        String minimumShouldMatch = null;
-        String quoteFieldSuffix = null;
-        Boolean lenient = null;
-        Operator defaultOperator = QueryStringQueryBuilder.DEFAULT_OPERATOR;
-        String timeZone = null;
-        Locale locale = QueryStringQueryBuilder.DEFAULT_LOCALE;
-        Fuzziness fuzziness = QueryStringQueryBuilder.DEFAULT_FUZZINESS;
-        String fuzzyRewrite = null;
-        String rewrite = null;
-        Map<String, Float> fieldsAndWeights = new HashMap<>();
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -83,7 +87,7 @@ public class QueryStringQueryParser extends BaseQueryParser {
                 if ("fields".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String fField = null;
-                        float fBoost = AbstractQueryBuilder.DEFAULT_BOOST;
+                        float fBoost = -1;
                         char[] text = parser.textCharacters();
                         int end = parser.textOffset() + parser.textLength();
                         for (int i = parser.textOffset(); i < end; i++) {
@@ -97,113 +101,147 @@ public class QueryStringQueryParser extends BaseQueryParser {
                         if (fField == null) {
                             fField = parser.text();
                         }
-                        fieldsAndWeights.put(fField, fBoost);
+                        if (qpSettings.fields() == null) {
+                            qpSettings.fields(new ArrayList<String>());
+                        }
+
+                        if (Regex.isSimpleMatchPattern(fField)) {
+                            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                                qpSettings.fields().add(field);
+                                if (fBoost != -1) {
+                                    if (qpSettings.boosts() == null) {
+                                        qpSettings.boosts(new ObjectFloatHashMap<String>());
+                                    }
+                                    qpSettings.boosts().put(field, fBoost);
+                                }
+                            }
+                        } else {
+                            qpSettings.fields().add(fField);
+                            if (fBoost != -1) {
+                                if (qpSettings.boosts() == null) {
+                                    qpSettings.boosts(new ObjectFloatHashMap<String>());
+                                }
+                                qpSettings.boosts().put(fField, fBoost);
+                            }
+                        }
                     }
                 } else {
-                    throw new ParsingException(parseContext, "[query_string] query does not support [" + currentFieldName + "]");
+                    throw new ParsingException(parseContext, "[query_string] query does not support [" + currentFieldName
+                            + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
-                    queryString = parser.text();
+                    qpSettings.queryString(parser.text());
                 } else if ("default_field".equals(currentFieldName) || "defaultField".equals(currentFieldName)) {
-                    defaultField = parser.text();
+                    qpSettings.defaultField(parser.text());
                 } else if ("default_operator".equals(currentFieldName) || "defaultOperator".equals(currentFieldName)) {
-                    defaultOperator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        qpSettings.defaultOperator(org.apache.lucene.queryparser.classic.QueryParser.Operator.OR);
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        qpSettings.defaultOperator(org.apache.lucene.queryparser.classic.QueryParser.Operator.AND);
+                    } else {
+                        throw new ParsingException(parseContext, "Query default operator [" + op + "] is not allowed");
+                    }
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzer = parser.text();
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new ParsingException(parseContext, "[query_string] analyzer [" + parser.text() + "] not found");
+                    }
+                    qpSettings.forcedAnalyzer(analyzer);
                 } else if ("quote_analyzer".equals(currentFieldName) || "quoteAnalyzer".equals(currentFieldName)) {
-                    quoteAnalyzer = parser.text();
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new ParsingException(parseContext, "[query_string] quote_analyzer [" + parser.text()
+                                + "] not found");
+                    }
+                    qpSettings.forcedQuoteAnalyzer(analyzer);
                 } else if ("allow_leading_wildcard".equals(currentFieldName) || "allowLeadingWildcard".equals(currentFieldName)) {
-                    allowLeadingWildcard = parser.booleanValue();
+                    qpSettings.allowLeadingWildcard(parser.booleanValue());
                 } else if ("auto_generate_phrase_queries".equals(currentFieldName) || "autoGeneratePhraseQueries".equals(currentFieldName)) {
-                    autoGeneratePhraseQueries = parser.booleanValue();
+                    qpSettings.autoGeneratePhraseQueries(parser.booleanValue());
                 } else if ("max_determinized_states".equals(currentFieldName) || "maxDeterminizedStates".equals(currentFieldName)) {
-                    maxDeterminizedStates = parser.intValue();
+                    qpSettings.maxDeterminizedStates(parser.intValue());
                 } else if ("lowercase_expanded_terms".equals(currentFieldName) || "lowercaseExpandedTerms".equals(currentFieldName)) {
-                    lowercaseExpandedTerms = parser.booleanValue();
+                    qpSettings.lowercaseExpandedTerms(parser.booleanValue());
                 } else if ("enable_position_increments".equals(currentFieldName) || "enablePositionIncrements".equals(currentFieldName)) {
-                    enablePositionIncrements = parser.booleanValue();
+                    qpSettings.enablePositionIncrements(parser.booleanValue());
                 } else if ("escape".equals(currentFieldName)) {
-                    escape = parser.booleanValue();
+                    qpSettings.escape(parser.booleanValue());
                 } else if ("use_dis_max".equals(currentFieldName) || "useDisMax".equals(currentFieldName)) {
-                    useDisMax = parser.booleanValue();
+                    qpSettings.useDisMax(parser.booleanValue());
                 } else if ("fuzzy_prefix_length".equals(currentFieldName) || "fuzzyPrefixLength".equals(currentFieldName)) {
-                    fuzzyPrefixLength = parser.intValue();
+                    qpSettings.fuzzyPrefixLength(parser.intValue());
                 } else if ("fuzzy_max_expansions".equals(currentFieldName) || "fuzzyMaxExpansions".equals(currentFieldName)) {
-                    fuzzyMaxExpansions = parser.intValue();
+                    qpSettings.fuzzyMaxExpansions(parser.intValue());
                 } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
-                    fuzzyRewrite = parser.textOrNull();
+                    qpSettings.fuzzyRewriteMethod(QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull()));
                 } else if ("phrase_slop".equals(currentFieldName) || "phraseSlop".equals(currentFieldName)) {
-                    phraseSlop = parser.intValue();
+                    qpSettings.phraseSlop(parser.intValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, FUZZINESS)) {
-                    fuzziness = Fuzziness.parse(parser);
+                    qpSettings.setFuzziness(Fuzziness.parse(parser));
                 } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
+                    qpSettings.boost(parser.floatValue());
                 } else if ("tie_breaker".equals(currentFieldName) || "tieBreaker".equals(currentFieldName)) {
-                    tieBreaker = parser.floatValue();
+                    qpSettings.tieBreaker(parser.floatValue());
                 } else if ("analyze_wildcard".equals(currentFieldName) || "analyzeWildcard".equals(currentFieldName)) {
-                    analyzeWildcard = parser.booleanValue();
+                    qpSettings.analyzeWildcard(parser.booleanValue());
                 } else if ("rewrite".equals(currentFieldName)) {
-                    rewrite = parser.textOrNull();
+                    qpSettings.rewriteMethod(QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull()));
                 } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
-                    minimumShouldMatch = parser.textOrNull();
+                    qpSettings.minimumShouldMatch(parser.textOrNull());
                 } else if ("quote_field_suffix".equals(currentFieldName) || "quoteFieldSuffix".equals(currentFieldName)) {
-                    quoteFieldSuffix = parser.textOrNull();
+                    qpSettings.quoteFieldSuffix(parser.textOrNull());
                 } else if ("lenient".equalsIgnoreCase(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    qpSettings.lenient(parser.booleanValue());
                 } else if ("locale".equals(currentFieldName)) {
                     String localeStr = parser.text();
-                    locale = Locale.forLanguageTag(localeStr);
-                } else if ("time_zone".equals(currentFieldName) || "timeZone".equals(currentFieldName)) {
+                    qpSettings.locale(LocaleUtils.parse(localeStr));
+                } else if ("time_zone".equals(currentFieldName)) {
                     try {
-                        timeZone = parser.text();
+                        qpSettings.timeZone(DateTimeZone.forID(parser.text()));
                     } catch (IllegalArgumentException e) {
-                        throw new ParsingException(parseContext, "[query_string] time_zone [" + parser.text() + "] is unknown");
+                        throw new ParsingException(parseContext,
+                                "[query_string] time_zone [" + parser.text() + "] is unknown");
                     }
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else {
-                    throw new ParsingException(parseContext, "[query_string] query does not support [" + currentFieldName + "]");
+                    throw new ParsingException(parseContext, "[query_string] query does not support [" + currentFieldName
+ + "]");
                 }
             }
         }
-        if (queryString == null) {
+        if (qpSettings.queryString() == null) {
             throw new ParsingException(parseContext, "query_string must be provided with a [query]");
         }
+        qpSettings.defaultAnalyzer(parseContext.mapperService().searchAnalyzer());
+        qpSettings.defaultQuoteAnalyzer(parseContext.mapperService().searchQuoteAnalyzer());
 
-        QueryStringQueryBuilder queryStringQuery = new QueryStringQueryBuilder(queryString);
-        queryStringQuery.fields(fieldsAndWeights);
-        queryStringQuery.defaultField(defaultField);
-        queryStringQuery.defaultOperator(defaultOperator);
-        queryStringQuery.analyzer(analyzer);
-        queryStringQuery.quoteAnalyzer(quoteAnalyzer);
-        queryStringQuery.allowLeadingWildcard(allowLeadingWildcard);
-        queryStringQuery.autoGeneratePhraseQueries(autoGeneratePhraseQueries);
-        queryStringQuery.maxDeterminizedStates(maxDeterminizedStates);
-        queryStringQuery.lowercaseExpandedTerms(lowercaseExpandedTerms);
-        queryStringQuery.enablePositionIncrements(enablePositionIncrements);
-        queryStringQuery.escape(escape);
-        queryStringQuery.useDisMax(useDisMax);
-        queryStringQuery.fuzzyPrefixLength(fuzzyPrefixLength);
-        queryStringQuery.fuzzyMaxExpansions(fuzzyMaxExpansions);
-        queryStringQuery.fuzzyRewrite(fuzzyRewrite);
-        queryStringQuery.phraseSlop(phraseSlop);
-        queryStringQuery.fuzziness(fuzziness);
-        queryStringQuery.tieBreaker(tieBreaker);
-        queryStringQuery.analyzeWildcard(analyzeWildcard);
-        queryStringQuery.rewrite(rewrite);
-        queryStringQuery.minimumShouldMatch(minimumShouldMatch);
-        queryStringQuery.quoteFieldSuffix(quoteFieldSuffix);
-        queryStringQuery.lenient(lenient);
-        queryStringQuery.timeZone(timeZone);
-        queryStringQuery.locale(locale);
-        queryStringQuery.boost(boost);
-        queryStringQuery.queryName(queryName);
-        return queryStringQuery;
-    }
+        if (qpSettings.escape()) {
+            qpSettings.queryString(org.apache.lucene.queryparser.classic.QueryParser.escape(qpSettings.queryString()));
+        }
 
-    @Override
-    public QueryStringQueryBuilder getBuilderPrototype() {
-        return QueryStringQueryBuilder.PROTOTYPE;
+        MapperQueryParser queryParser = parseContext.queryParser(qpSettings);
+
+        try {
+            Query query = queryParser.parse(qpSettings.queryString());
+            if (query == null) {
+                return null;
+            }
+            if (qpSettings.boost() != QueryParserSettings.DEFAULT_BOOST) {
+                query.setBoost(query.getBoost() * qpSettings.boost());
+            }
+            query = fixNegativeQueryIfNeeded(query);
+            if (query instanceof BooleanQuery) {
+                query = Queries.applyMinimumShouldMatch((BooleanQuery) query, qpSettings.minimumShouldMatch());
+            }
+            if (queryName != null) {
+                parseContext.addNamedQuery(queryName, query);
+            }
+            return query;
+        } catch (org.apache.lucene.queryparser.classic.ParseException e) {
+            throw new ParsingException(parseContext, "Failed to parse query [" + qpSettings.queryString() + "]", e);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java b/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
deleted file mode 100644
index 9e0ee2a..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.ValidationException;
-
-import java.util.List;
-
-/**
- * This exception can be used to indicate various reasons why validation of a query has failed.
- */
-public class QueryValidationException extends ValidationException {
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     *
-     * @param queryId the query that caused the error
-     * @param validationError the error message to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationError(String queryId, String validationError, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException();
-        }
-        validationException.addValidationError("[" + queryId + "] " + validationError);
-        return validationException;
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     * @param validationErrors the error messages to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationErrors(List<String> validationErrors, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException();
-        }
-        validationException.addValidationErrors(validationErrors);
-        return validationException;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
deleted file mode 100644
index e905de1..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * QueryBuilder implementation that  holds a lucene query, which can be returned by {@link QueryBuilder#toQuery(QueryShardContext)}.
- * Doesn't support conversion to {@link org.elasticsearch.common.xcontent.XContent} via {@link #doXContent(XContentBuilder, Params)}.
- */
-//norelease to be removed once all queries support separate fromXContent and toQuery methods. Make AbstractQueryBuilder#toQuery final as well then.
-public class QueryWrappingQueryBuilder extends AbstractQueryBuilder<QueryWrappingQueryBuilder> implements SpanQueryBuilder<QueryWrappingQueryBuilder>, MultiTermQueryBuilder<QueryWrappingQueryBuilder>{
-
-    private Query query;
-
-    public QueryWrappingQueryBuilder(Query query) {
-        this.query = query;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return query;
-    }
-
-    @Override
-    public String getWriteableName() {
-        // this should not be called since we overwrite BaseQueryBuilder#toQuery() in this class
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //no-op the wrapper lucene query has already its boost set
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
index 1c8b57c..da23698 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
@@ -19,116 +19,187 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.joda.DateMathParser;
-import org.elasticsearch.common.joda.FormatDateTimeFormatter;
-import org.elasticsearch.common.joda.Joda;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents within an range of terms.
  */
-public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> implements MultiTermQueryBuilder<RangeQueryBuilder> {
+public class RangeQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RangeQueryBuilder> {
 
-    public static final boolean DEFAULT_INCLUDE_UPPER = true;
+    private final String name;
+    private Object from;
+    private Object to;
+    private String timeZone;
+    private boolean includeLower = true;
+    private boolean includeUpper = true;
+    private float boost = -1;
+    private String queryName;
+    private String format;
 
-    public static final boolean DEFAULT_INCLUDE_LOWER = true;
+    /**
+     * A Query that matches documents within an range of terms.
+     *
+     * @param name The field name
+     */
+    public RangeQueryBuilder(String name) {
+        this.name = name;
+    }
 
-    public static final String NAME = "range";
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(Object from) {
+        this.from = from;
+        return this;
+    }
 
-    private final String fieldName;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(String from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object from;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(int from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object to;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(long from) {
+        this.from = from;
+        return this;
+    }
 
-    private DateTimeZone timeZone;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(float from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeLower = DEFAULT_INCLUDE_LOWER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(double from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeUpper = DEFAULT_INCLUDE_UPPER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(String from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    private FormatDateTimeFormatter format;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(Object from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    static final RangeQueryBuilder PROTOTYPE = new RangeQueryBuilder("field");
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(int from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
     /**
-     * A Query that matches documents within an range of terms.
-     *
-     * @param fieldName The field name
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder(String fieldName) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        this.fieldName = fieldName;
+    public RangeQueryBuilder gt(long from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
-     * Get the field name for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public RangeQueryBuilder gt(float from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
-     * In case lower bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
      */
-    public RangeQueryBuilder from(Object from, boolean includeLower) {
-        this.from = convertToBytesRefIfString(from);
-        this.includeLower = includeLower;
+    public RangeQueryBuilder gt(double from) {
+        this.from = from;
+        this.includeLower = false;
         return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder from(Object from) {
-        return from(from, this.includeLower);
+    public RangeQueryBuilder gte(String from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * Gets the lower range value for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public Object from() {
-        return convertToStringIfBytesRef(this.from);
+    public RangeQueryBuilder gte(Object from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gt(Object from) {
-        return from(from, false);
+    public RangeQueryBuilder gte(int from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gte(Object from) {
-        return from(from, true);
+    public RangeQueryBuilder gte(long from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * The to part of the range query. Null indicates unbounded.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder to(Object to, boolean includeUpper) {
-        this.to = convertToBytesRefIfString(to);
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder gte(float from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
+    }
+
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gte(double from) {
+        this.from = from;
+        this.includeLower = true;
         return this;
     }
 
@@ -136,209 +207,229 @@ public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> i
      * The to part of the range query. Null indicates unbounded.
      */
     public RangeQueryBuilder to(Object to) {
-        return to(to, this.includeUpper);
+        this.to = to;
+        return this;
     }
 
     /**
-     * Gets the upper range value for this query.
-     * In case upper bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public Object to() {
-        return convertToStringIfBytesRef(this.to);
+    public RangeQueryBuilder to(String to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lt(Object to) {
-        return to(to, false);
+    public RangeQueryBuilder to(int to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lte(Object to) {
-        return to(to, true);
+    public RangeQueryBuilder to(long to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeLower(boolean includeLower) {
-        this.includeLower = includeLower;
+    public RangeQueryBuilder to(float to) {
+        this.to = to;
         return this;
     }
 
     /**
-     * Gets the includeLower flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeLower() {
-        return this.includeLower;
+    public RangeQueryBuilder to(double to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeUpper(boolean includeUpper) {
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder lt(String to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the includeUpper flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeUpper() {
-        return this.includeUpper;
+    public RangeQueryBuilder lt(Object to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of date field, we can adjust the from/to fields using a timezone
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder timeZone(String timeZone) {
-        if (timeZone == null) {
-            throw new IllegalArgumentException("timezone cannot be null");
-        }
-        this.timeZone = DateTimeZone.forID(timeZone);
+    public RangeQueryBuilder lt(int to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * In case of date field, gets the from/to fields timezone adjustment
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String timeZone() {
-        return this.timeZone == null ? null : this.timeZone.getID();
+    public RangeQueryBuilder lt(long to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of format field, we can parse the from/to fields using this time format
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder format(String format) {
-        if (format == null) {
-            throw new IllegalArgumentException("format cannot be null");
-        }
-        this.format = Joda.forPattern(format);
+    public RangeQueryBuilder lt(float to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the format field to parse the from/to fields
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String format() {
-        return this.format == null ? null : this.format.format();
+    public RangeQueryBuilder lt(double to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("from", convertToStringIfBytesRef(this.from));
-        builder.field("to", convertToStringIfBytesRef(this.to));
-        builder.field("include_lower", includeLower);
-        builder.field("include_upper", includeUpper);
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone.getID());
-        }
-        if (format != null) {
-            builder.field("format", format.format());
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(String to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(Object to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            if (mapper instanceof DateFieldMapper.DateFieldType) {
-                DateMathParser forcedDateParser = null;
-                if (this.format  != null) {
-                    forcedDateParser = new DateMathParser(this.format);
-                }
-                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, timeZone, forcedDateParser);
-            } else  {
-                if (timeZone != null) {
-                    throw new QueryShardException(context, "[range] time_zone can not be applied to non date field ["
-                            + fieldName + "]");
-                }
-                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
-                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
-            }
-        } else {
-            if (timeZone != null) {
-                throw new QueryShardException(context, "[range] time_zone can not be applied to non unmapped field ["
-                        + fieldName + "]");
-            }
-        }
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(int to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
+    }
 
-        if (query == null) {
-            query = new TermRangeQuery(this.fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
-        }
-        return query;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(long to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected RangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder(in.readString());
-        rangeQueryBuilder.from = in.readGenericValue();
-        rangeQueryBuilder.to = in.readGenericValue();
-        rangeQueryBuilder.includeLower = in.readBoolean();
-        rangeQueryBuilder.includeUpper = in.readBoolean();
-        String timeZoneId = in.readOptionalString();
-        if (timeZoneId != null) {
-            rangeQueryBuilder.timeZone = DateTimeZone.forID(timeZoneId);
-        }
-        String formatString = in.readOptionalString();
-        if (formatString != null) {
-            rangeQueryBuilder.format = Joda.forPattern(formatString);
-        }
-        return rangeQueryBuilder;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(float to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.from);
-        out.writeGenericValue(this.to);
-        out.writeBoolean(this.includeLower);
-        out.writeBoolean(this.includeUpper);
-        String timeZoneId = null;
-        if (this.timeZone != null) {
-            timeZoneId = this.timeZone.getID();
-        }
-        out.writeOptionalString(timeZoneId);
-        String formatString = null;
-        if (this.format != null) {
-            formatString = this.format.format();
-        }
-        out.writeOptionalString(formatString);
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(double to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
+    }
+
+    /**
+     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeLower(boolean includeLower) {
+        this.includeLower = includeLower;
+        return this;
+    }
+
+    /**
+     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeUpper(boolean includeUpper) {
+        this.includeUpper = includeUpper;
+        return this;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected int doHashCode() {
-        String timeZoneId = timeZone == null ? null : timeZone.getID();
-        String formatString = format == null ? null : format.format();
-        return Objects.hash(fieldName, from, to, timeZoneId, includeLower, includeUpper, formatString);
+    public RangeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can adjust the from/to fields using a timezone
+     */
+    public RangeQueryBuilder timeZone(String timezone) {
+        this.timeZone = timezone;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can set the format to be used instead of the mapper format
+     */
+    public RangeQueryBuilder format(String format) {
+        this.format = format;
+        return this;
     }
 
     @Override
-    protected boolean doEquals(RangeQueryBuilder other) {
-        String timeZoneId = timeZone == null ? null : timeZone.getID();
-        String formatString = format == null ? null : format.format();
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(from, other.from) &&
-               Objects.equals(to, other.to) &&
-               Objects.equals(timeZoneId, other.timeZone()) &&
-               Objects.equals(includeLower, other.includeLower) &&
-               Objects.equals(includeUpper, other.includeUpper) &&
-               Objects.equals(formatString, other.format());
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(RangeQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("from", from);
+        builder.field("to", to);
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+        if (format != null) {
+            builder.field("format", format);
+        }
+        builder.field("include_lower", includeLower);
+        builder.field("include_upper", includeUpper);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
index c9259ac..ca37f56 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
@@ -19,38 +19,52 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.joda.DateMathParser;
+import org.elasticsearch.common.joda.Joda;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
 
 /**
- * Parser for range query
+ *
  */
-public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
+public class RangeQueryParser implements QueryParser {
 
+    public static final String NAME = "range";
     private static final ParseField FIELDDATA_FIELD = new ParseField("fielddata").withAllDeprecated("[no replacement]");
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of range query");
 
+    @Inject
+    public RangeQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{RangeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RangeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
         Object from = null;
         Object to = null;
-        boolean includeLower = RangeQueryBuilder.DEFAULT_INCLUDE_LOWER;
-        boolean includeUpper = RangeQueryBuilder.DEFAULT_INCLUDE_UPPER;
-        String timeZone = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean includeLower = true;
+        boolean includeUpper = true;
+        DateTimeZone timeZone = null;
+        DateMathParser forcedDateParser = null;
+        float boost = 1.0f;
         String queryName = null;
-        String format = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -88,11 +102,9 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
                             to = parser.objectBytes();
                             includeUpper = true;
                         } else if ("time_zone".equals(currentFieldName) || "timeZone".equals(currentFieldName)) {
-                            timeZone = parser.text();
+                            timeZone = DateTimeZone.forID(parser.text());
                         } else if ("format".equals(currentFieldName)) {
-                            format = parser.text();
-                        } else if ("_name".equals(currentFieldName)) {
-                            queryName = parser.text();
+                            forcedDateParser = new DateMathParser(Joda.forPattern(parser.text()));
                         } else {
                             throw new ParsingException(parseContext, "[range] query does not support [" + currentFieldName + "]");
                         }
@@ -109,24 +121,27 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
             }
         }
 
-        RangeQueryBuilder rangeQuery = new RangeQueryBuilder(fieldName);
-        rangeQuery.from(from);
-        rangeQuery.to(to);
-        rangeQuery.includeLower(includeLower);
-        rangeQuery.includeUpper(includeUpper);
-        if (timeZone != null) {
-            rangeQuery.timeZone(timeZone);
+        Query query = null;
+        MappedFieldType mapper = parseContext.fieldMapper(fieldName);
+        if (mapper != null) {
+            if (mapper instanceof DateFieldMapper.DateFieldType) {
+                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, timeZone, forcedDateParser);
+            } else  {
+                if (timeZone != null) {
+                    throw new ParsingException(parseContext, "[range] time_zone can not be applied to non date field ["
+                            + fieldName + "]");
+                }
+                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
+                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
+            }
         }
-        rangeQuery.boost(boost);
-        rangeQuery.queryName(queryName);
-        if (format != null) {
-            rangeQuery.format(format);
+        if (query == null) {
+            query = new TermRangeQuery(fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
         }
-        return rangeQuery;
-    }
-
-    @Override
-    public RangeQueryBuilder getBuilderPrototype() {
-        return RangeQueryBuilder.PROTOTYPE;
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpFlag.java b/core/src/main/java/org/elasticsearch/index/query/RegexpFlag.java
index 0af6b86..45f58c4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpFlag.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpFlag.java
@@ -89,9 +89,9 @@ public enum RegexpFlag {
     /**
      * Resolves the combined OR'ed value for the given list of regular expression flags. The given flags must follow the
      * following syntax:
-     * <p/>
+     * <p>
      * <tt>flag_name</tt>(|<tt>flag_name</tt>)*
-     * <p/>
+     * <p>
      * Where <tt>flag_name</tt> is one of the following:
      * <ul>
      *     <li>INTERSECTION</li>
@@ -102,10 +102,10 @@ public enum RegexpFlag {
      *     <li>NONE</li>
      *     <li>ALL</li>
      * </ul>
-     * <p/>
+     * <p>
      * Example: <tt>INTERSECTION|COMPLEMENT|EMPTY</tt>
      *
-     * @param flags A string representing a list of regualr expression flags
+     * @param flags A string representing a list of regular expression flags
      * @return The combined OR'ed value for all the flags
      */
     static int resolveValue(String flags) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
index f596bf8..ee143eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
@@ -19,79 +19,48 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
 import org.apache.lucene.util.automaton.Operations;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder> implements MultiTermQueryBuilder<RegexpQueryBuilder> {
+public class RegexpQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RegexpQueryBuilder> {
 
-    public static final String NAME = "regexp";
-
-    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
-
-    public static final int DEFAULT_MAX_DETERMINIZED_STATES = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
-
-    private final String fieldName;
-
-    private final String value;
-
-    private int flagsValue = DEFAULT_FLAGS_VALUE;
-
-    private int maxDeterminizedStates = DEFAULT_MAX_DETERMINIZED_STATES;
+    private final String name;
+    private final String regexp;
 
+    private int flags = RegexpQueryParser.DEFAULT_FLAGS_VALUE;
+    private float boost = -1;
     private String rewrite;
-
-    static final RegexpQueryBuilder PROTOTYPE = new RegexpQueryBuilder("field", "value");
+    private String queryName;
+    private int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
+    private boolean maxDetermizedStatesSet;
 
     /**
-     * Constructs a new regex query.
+     * Constructs a new term query.
      *
-     * @param fieldName  The name of the field
-     * @param value The regular expression
+     * @param name  The name of the field
+     * @param regexp The regular expression
      */
-    public RegexpQueryBuilder(String fieldName, String value) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("value cannot be null.");
-        }
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
+    public RegexpQueryBuilder(String name, String regexp) {
+        this.name = name;
+        this.regexp = regexp;
     }
 
     /**
-     *  Returns the value used in this query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public String value() {
-        return this.value;
+    @Override
+    public RegexpQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public RegexpQueryBuilder flags(RegexpFlag... flags) {
-        if (flags == null) {
-            this.flagsValue = DEFAULT_FLAGS_VALUE;
-            return this;
-        }
         int value = 0;
         if (flags.length == 0) {
             value = RegexpFlag.ALL.value;
@@ -100,108 +69,53 @@ public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder>
                 value |= flag.value;
             }
         }
-        this.flagsValue = value;
-        return this;
-    }
-
-    public RegexpQueryBuilder flags(int flags) {
-        this.flagsValue = flags;
+        this.flags = value;
         return this;
     }
 
-    public int flags() {
-        return this.flagsValue;
-    }
-
     /**
      * Sets the regexp maxDeterminizedStates.
      */
     public RegexpQueryBuilder maxDeterminizedStates(int value) {
         this.maxDeterminizedStates = value;
+        this.maxDetermizedStatesSet = true;
         return this;
     }
 
-    public int maxDeterminizedStates() {
-        return this.maxDeterminizedStates;
-    }
-
     public RegexpQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RegexpQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", this.value);
-        builder.field("flags_value", flagsValue);
-        builder.field("max_determinized_states", maxDeterminizedStates);
+        builder.startObject(RegexpQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", regexp);
+        if (flags != -1) {
+            builder.field("flags_value", flags);
+        }
+        if (maxDetermizedStatesSet) {
+            builder.field("max_determinized_states", maxDeterminizedStates);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (rewrite != null) {
             builder.field("rewrite", rewrite);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, context);
-        }
-        if (query == null) {
-            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
-            if (method != null) {
-                regexpQuery.setRewriteMethod(method);
-            }
-            query = regexpQuery;
-        }
-        return query;
-    }
-
-    @Override
-    public RegexpQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RegexpQueryBuilder regexpQueryBuilder = new RegexpQueryBuilder(in.readString(), in.readString());
-        regexpQueryBuilder.flagsValue = in.readVInt();
-        regexpQueryBuilder.maxDeterminizedStates = in.readVInt();
-        regexpQueryBuilder.rewrite = in.readOptionalString();
-        return regexpQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeVInt(flagsValue);
-        out.writeVInt(maxDeterminizedStates);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, flagsValue, maxDeterminizedStates, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(RegexpQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(flagsValue, other.flagsValue) &&
-                Objects.equals(maxDeterminizedStates, other.maxDeterminizedStates) &&
-                Objects.equals(rewrite, other.rewrite);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
index e20fddf..ab105ba 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
@@ -19,36 +19,52 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RegexpQuery;
+import org.apache.lucene.util.automaton.Operations;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for regexp query
+ *
  */
-public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
+public class RegexpQueryParser implements QueryParser {
+
+    public static final String NAME = "regexp";
+
+    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of regexp query");
 
+    @Inject
+    public RegexpQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{RegexpQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RegexpQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        int flagsValue = RegexpQueryBuilder.DEFAULT_FLAGS_VALUE;
-        int maxDeterminizedStates = RegexpQueryBuilder.DEFAULT_MAX_DETERMINIZED_STATES;
+        float boost = 1.0f;
+        int flagsValue = DEFAULT_FLAGS_VALUE;
+        int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -68,7 +84,7 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else if ("flags".equals(currentFieldName)) {
                             String flags = parser.textOrNull();
                             flagsValue = RegexpFlag.resolveValue(flags);
@@ -96,16 +112,27 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
         if (value == null) {
             throw new ParsingException(parseContext, "No value specified for regexp query");
         }
-        return new RegexpQueryBuilder(fieldName, value)
-                .flags(flagsValue)
-                .maxDeterminizedStates(maxDeterminizedStates)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public RegexpQueryBuilder getBuilderPrototype() {
-        return RegexpQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, parseContext);
+        }
+        if (query == null) {
+            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
+            if (method != null) {
+                regexpQuery.setRewriteMethod(method);
+            }
+            query = regexpQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
index 45ab745..a9a35ac 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
@@ -19,149 +19,40 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RandomAccessWeight;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.script.*;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
-import java.util.Objects;
+import java.util.HashMap;
+import java.util.Map;
 
-public class ScriptQueryBuilder extends AbstractQueryBuilder<ScriptQueryBuilder> {
+public class ScriptQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "script";
+    private Script script;
 
-    static final ScriptQueryBuilder PROTOTYPE = new ScriptQueryBuilder(new Script(""));
-
-    private final Script script;
+    private String queryName;
 
     public ScriptQueryBuilder(Script script) {
-        if (script == null) {
-            throw new IllegalArgumentException("script cannot be null");
-        }
         this.script = script;
     }
 
-    public Script script() {
-        return this.script;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public ScriptQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAME);
-        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return new ScriptQuery(script, context.scriptService(), context.lookup());
-    }
-
-    static class ScriptQuery extends Query {
-
-        private final Script script;
-
-        private final SearchScript searchScript;
-
-        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
-            this.script = script;
-            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
-        }
-
-        @Override
-        public String toString(String field) {
-            StringBuilder buffer = new StringBuilder();
-            buffer.append("ScriptFilter(");
-            buffer.append(script);
-            buffer.append(")");
-            return buffer.toString();
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj)
-                return true;
-            if (!super.equals(obj))
-                return false;
-            ScriptQuery other = (ScriptQuery) obj;
-            return Objects.equals(script, other.script);
-        }
 
-        @Override
-        public int hashCode() {
-            final int prime = 31;
-            int result = super.hashCode();
-            result = prime * result + Objects.hashCode(script);
-            return result;
-        }
-
-        @Override
-        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-            return new RandomAccessWeight(this) {
-                @Override
-                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
-                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
-                    return new Bits() {
-
-                        @Override
-                        public boolean get(int doc) {
-                            leafScript.setDocument(doc);
-                            Object val = leafScript.run();
-                            if (val == null) {
-                                return false;
-                            }
-                            if (val instanceof Boolean) {
-                                return (Boolean) val;
-                            }
-                            if (val instanceof Number) {
-                                return ((Number) val).longValue() != 0;
-                            }
-                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
-                        }
-
-                        @Override
-                        public int length() {
-                            return context.reader().maxDoc();
-                        }
-
-                    };
-                }
-            };
+        builder.startObject(ScriptQueryParser.NAME);
+        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-    }
-
-    @Override
-    protected ScriptQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ScriptQueryBuilder(Script.readScript(in));
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        script.writeTo(out);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    protected boolean doEquals(ScriptQueryBuilder other) {
-        return Objects.equals(script, other.script);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
index e544589..31af574 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
@@ -19,42 +19,60 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RandomAccessWeight;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Objects;
 
 /**
- * Parser for script query
+ *
  */
-public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
+public class ScriptQueryParser implements QueryParser {
+
+    public static final String NAME = "script";
+
+    @Inject
+    public ScriptQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{ScriptQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     @Override
-    public ScriptQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
-        
+
+        XContentParser.Token token;
+
         // also, when caching, since its isCacheable is false, will result in loading all bit set...
         Script script = null;
         Map<String, Object> params = null;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
-
-        XContentParser.Token token;
         String currentFieldName = null;
+
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -71,8 +89,6 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if (!scriptParameterParser.token(currentFieldName, token, parser, parseContext.parseFieldMatcher())) {
                     throw new ParsingException(parseContext, "[script] query does not support [" + currentFieldName + "]");
                 }
@@ -95,13 +111,83 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             throw new ParsingException(parseContext, "script must be provided with a [script] filter");
         }
 
-        return new ScriptQueryBuilder(script)
-                .boost(boost)
-                .queryName(queryName);
+        Query query = new ScriptQuery(script, parseContext.scriptService(), parseContext.lookup());
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public ScriptQueryBuilder getBuilderPrototype() {
-        return ScriptQueryBuilder.PROTOTYPE;
+    static class ScriptQuery extends Query {
+
+        private final Script script;
+
+        private final SearchScript searchScript;
+
+        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
+            this.script = script;
+            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
+        }
+
+        @Override
+        public String toString(String field) {
+            StringBuilder buffer = new StringBuilder();
+            buffer.append("ScriptFilter(");
+            buffer.append(script);
+            buffer.append(")");
+            return buffer.toString();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj)
+                return true;
+            if (!super.equals(obj))
+                return false;
+            ScriptQuery other = (ScriptQuery) obj;
+            return Objects.equals(script, other.script);
+        }
+
+        @Override
+        public int hashCode() {
+            final int prime = 31;
+            int result = super.hashCode();
+            result = prime * result + Objects.hashCode(script);
+            return result;
+        }
+
+        @Override
+        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+            return new RandomAccessWeight(this) {
+                @Override
+                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
+                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
+                    return new Bits() {
+
+                        @Override
+                        public boolean get(int doc) {
+                            leafScript.setDocument(doc);
+                            Object val = leafScript.run();
+                            if (val == null) {
+                                return false;
+                            }
+                            if (val instanceof Boolean) {
+                                return (Boolean) val;
+                            }
+                            if (val instanceof Number) {
+                                return ((Number) val).longValue() != 0;
+                            }
+                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
+                        }
+
+                        @Override
+                        public int length() {
+                            return context.reader().maxDoc();
+                        }
+
+                    };
+                }
+            };
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
index f8b0dea..9ae0703 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
@@ -29,7 +29,6 @@ import org.apache.lucene.util.BytesRef;
 import java.io.IOException;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Wrapper class for Lucene's SimpleQueryParser that allows us to redefine
@@ -202,102 +201,51 @@ public class SimpleQueryParser extends org.apache.lucene.queryparser.simple.Simp
             return new PrefixQuery(new Term(field, termStr));
         }
     }
+
     /**
      * Class encapsulating the settings for the SimpleQueryString query, with
      * their default values
      */
-    static class Settings {
-        /** Locale to use for parsing. */
-        private Locale locale = SimpleQueryStringBuilder.DEFAULT_LOCALE;
-        /** Specifies whether parsed terms should be lowercased. */
-        private boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        /** Specifies whether lenient query parsing should be used. */
-        private boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        /** Specifies whether wildcards should be analyzed. */
-        private boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
+    public static class Settings {
+        private Locale locale = Locale.ROOT;
+        private boolean lowercaseExpandedTerms = true;
+        private boolean lenient = false;
+        private boolean analyzeWildcard = false;
 
-        /**
-         * Generates default {@link Settings} object (uses ROOT locale, does
-         * lowercase terms, no lenient parsing, no wildcard analysis).
-         * */
         public Settings() {
-        }
 
-        public Settings(Locale locale, Boolean lowercaseExpandedTerms, Boolean lenient, Boolean analyzeWildcard) {
-            this.locale = locale;
-            this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-            this.lenient = lenient;
-            this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Specifies the locale to use for parsing, Locale.ROOT by default. */
         public void locale(Locale locale) {
-            this.locale = (locale != null) ? locale : SimpleQueryStringBuilder.DEFAULT_LOCALE;
+            this.locale = locale;
         }
 
-        /** Returns the locale to use for parsing. */
         public Locale locale() {
             return this.locale;
         }
 
-        /**
-         * Specifies whether to lowercase parse terms, defaults to true if
-         * unset.
-         */
         public void lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
             this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         }
 
-        /** Returns whether to lowercase parse terms. */
         public boolean lowercaseExpandedTerms() {
             return this.lowercaseExpandedTerms;
         }
 
-        /** Specifies whether to use lenient parsing, defaults to false. */
         public void lenient(boolean lenient) {
             this.lenient = lenient;
         }
 
-        /** Returns whether to use lenient parsing. */
         public boolean lenient() {
             return this.lenient;
         }
 
-        /** Specifies whether to analyze wildcards. Defaults to false if unset. */
         public void analyzeWildcard(boolean analyzeWildcard) {
             this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Returns whether to analyze wildcards. */
         public boolean analyzeWildcard() {
             return analyzeWildcard;
         }
-
-        @Override
-        public int hashCode() {
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return Objects.hash(locale.toLanguageTag(), lowercaseExpandedTerms, lenient, analyzeWildcard);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj) {
-                return true;
-            }
-            if (obj == null || getClass() != obj.getClass()) {
-                return false;
-            }
-            Settings other = (Settings) obj;
-
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return (Objects.equals(locale.toLanguageTag(), other.locale.toLanguageTag())
-                    && Objects.equals(lowercaseExpandedTerms, other.lowercaseExpandedTerms) 
-                    && Objects.equals(lenient, other.lenient)
-                    && Objects.equals(analyzeWildcard, other.analyzeWildcard));
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
index 3f8cc5d..700ad41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
@@ -19,380 +19,202 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.xcontent.ToXContent.Params;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.SimpleQueryParser.Settings;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
- * SimpleQuery is a query parser that acts similar to a query_string query, but
- * won't throw exceptions for any weird string syntax.
- *
- * For more detailed explanation of the query string syntax see also the <a
- * href=
- * "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html"
- * > online documentation</a>.
+ * SimpleQuery is a query parser that acts similar to a query_string
+ * query, but won't throw exceptions for any weird string syntax.
  */
-public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQueryStringBuilder> {
-    /** Default locale used for parsing.*/
-    public static final Locale DEFAULT_LOCALE = Locale.ROOT;
-    /** Default for lowercasing parsed terms.*/
-    public static final boolean DEFAULT_LOWERCASE_EXPANDED_TERMS = true;
-    /** Default for using lenient query parsing.*/
-    public static final boolean DEFAULT_LENIENT = false;
-    /** Default for wildcard analysis.*/
-    public static final boolean DEFAULT_ANALYZE_WILDCARD = false;
-    /** Default for default operator to use for linking boolean clauses.*/
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    /** Default for search flags to use. */
-    public static final int DEFAULT_FLAGS = SimpleQueryStringFlag.ALL.value;
-    /** Name for (de-)serialization. */
-    public static final String NAME = "simple_query_string";
-
-    static final SimpleQueryStringBuilder PROTOTYPE = new SimpleQueryStringBuilder("");
-
-    /** Query text to parse. */
-    private final String queryText;
-    /**
-     * Fields to query against. If left empty will query default field,
-     * currently _ALL. Uses a TreeMap to hold the fields so boolean clauses are
-     * always sorted in same order for generated Lucene query for easier
-     * testing.
-     *
-     * Can be changed back to HashMap once https://issues.apache.org/jira/browse/LUCENE-6305 is fixed.
-     */
-    private final Map<String, Float> fieldsAndWeights = new TreeMap<>();
-    /** If specified, analyzer to use to parse the query text, defaults to registered default in toQuery. */
+public class SimpleQueryStringBuilder extends QueryBuilder implements BoostableQueryBuilder<SimpleQueryStringBuilder> {
+    private Map<String, Float> fields = new HashMap<>();
     private String analyzer;
-    /** Default operator to use for linking boolean clauses. Defaults to OR according to docs. */
-    private Operator defaultOperator = DEFAULT_OPERATOR;
-    /** If result is a boolean query, minimumShouldMatch parameter to apply. Ignored otherwise. */
+    private Operator operator;
+    private final String queryText;
+    private String queryName;
     private String minimumShouldMatch;
-    /** Any search flags to be used, ALL by default. */
-    private int flags = DEFAULT_FLAGS;
+    private int flags = -1;
+    private float boost = -1.0f;
+    private Boolean lowercaseExpandedTerms;
+    private Boolean lenient;
+    private Boolean analyzeWildcard;
+    private Locale locale;
 
-    /** Further search settings needed by the ES specific query string parser only. */
-    private Settings settings = new Settings();
+    /**
+     * Operators for the default_operator
+     */
+    public static enum Operator {
+        AND,
+        OR
+    }
 
-    /** Construct a new simple query with this query string. */
-    public SimpleQueryStringBuilder(String queryText) {
-        if (queryText == null) {
-            throw new IllegalArgumentException("query text missing");
-        }
-        this.queryText = queryText;
+    /**
+     * Construct a new simple query with the given text
+     */
+    public SimpleQueryStringBuilder(String text) {
+        this.queryText = text;
     }
 
-    /** Returns the text to parse the query from. */
-    public String value() {
-        return this.queryText;
+    /** Set the boost of this query. */
+    @Override
+    public SimpleQueryStringBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+    
+    /** Returns the boost of this query. */
+    public float boost() {
+        return this.boost;
     }
 
-    /** Add a field to run the query against. */
+    /**
+     * Add a field to run the query against
+     */
     public SimpleQueryStringBuilder field(String field) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        this.fields.put(field, null);
         return this;
     }
 
-    /** Add a field to run the query against with a specific boost. */
+    /**
+     * Add a field to run the query against with a specific boost
+     */
     public SimpleQueryStringBuilder field(String field, float boost) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, boost);
+        this.fields.put(field, boost);
         return this;
     }
 
-    /** Add several fields to run the query against with a specific boost. */
-    public SimpleQueryStringBuilder fields(Map<String, Float> fields) {
-        this.fieldsAndWeights.putAll(fields);
+    /**
+     * Specify a name for the query
+     */
+    public SimpleQueryStringBuilder queryName(String name) {
+        this.queryName = name;
         return this;
     }
 
-    /** Returns the fields including their respective boosts to run the query against. */
-    public Map<String, Float> fields() {
-        return this.fieldsAndWeights;
-    }
-
-    /** Specify an analyzer to use for the query. */
+    /**
+     * Specify an analyzer to use for the query
+     */
     public SimpleQueryStringBuilder analyzer(String analyzer) {
         this.analyzer = analyzer;
         return this;
     }
 
-    /** Returns the analyzer to use for the query. */
-    public String analyzer() {
-        return this.analyzer;
-    }
-
     /**
      * Specify the default operator for the query. Defaults to "OR" if no
-     * operator is specified.
+     * operator is specified
      */
     public SimpleQueryStringBuilder defaultOperator(Operator defaultOperator) {
-        this.defaultOperator = (defaultOperator != null) ? defaultOperator : DEFAULT_OPERATOR;
+        this.operator = defaultOperator;
         return this;
     }
 
-    /** Returns the default operator for the query. */
-    public Operator defaultOperator() {
-        return this.defaultOperator;
-    }
-
     /**
-     * Specify the enabled features of the SimpleQueryString. Defaults to ALL if
-     * none are specified.
+     * Specify the enabled features of the SimpleQueryString.
      */
     public SimpleQueryStringBuilder flags(SimpleQueryStringFlag... flags) {
-        if (flags != null && flags.length > 0) {
-            int value = 0;
+        int value = 0;
+        if (flags.length == 0) {
+            value = SimpleQueryStringFlag.ALL.value;
+        } else {
             for (SimpleQueryStringFlag flag : flags) {
                 value |= flag.value;
             }
-            this.flags = value;
-        } else {
-            this.flags = DEFAULT_FLAGS;
         }
-
-        return this;
-    }
-
-    /** For testing and serialisation only. */
-    SimpleQueryStringBuilder flags(int flags) {
-        this.flags = flags;
+        this.flags = value;
         return this;
     }
 
-    /** For testing only: Return the flags set for this query. */
-    int flags() {
-        return this.flags;
-    }
-
-    /**
-     * Specifies whether parsed terms for this query should be lower-cased.
-     * Defaults to true if not set.
-     */
     public SimpleQueryStringBuilder lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-        this.settings.lowercaseExpandedTerms(lowercaseExpandedTerms);
+        this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         return this;
     }
 
-    /** Returns whether parsed terms should be lower cased for this query. */
-    public boolean lowercaseExpandedTerms() {
-        return this.settings.lowercaseExpandedTerms();
-    }
-
-    /** Specifies the locale for parsing terms. Defaults to ROOT if none is set. */
     public SimpleQueryStringBuilder locale(Locale locale) {
-        this.settings.locale(locale);
+        this.locale = locale;
         return this;
     }
 
-    /** Returns the locale for parsing terms for this query. */
-    public Locale locale() {
-        return this.settings.locale();
-    }
-
-    /** Specifies whether query parsing should be lenient. Defaults to false. */
     public SimpleQueryStringBuilder lenient(boolean lenient) {
-        this.settings.lenient(lenient);
+        this.lenient = lenient;
         return this;
     }
 
-    /** Returns whether query parsing should be lenient. */
-    public boolean lenient() {
-        return this.settings.lenient();
-    }
-
-    /** Specifies whether wildcards should be analyzed. Defaults to false. */
     public SimpleQueryStringBuilder analyzeWildcard(boolean analyzeWildcard) {
-        this.settings.analyzeWildcard(analyzeWildcard);
+        this.analyzeWildcard = analyzeWildcard;
         return this;
     }
 
-    /** Returns whether wildcards should by analyzed. */
-    public boolean analyzeWildcard() {
-        return this.settings.analyzeWildcard();
-    }
-
-    /**
-     * Specifies the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
     public SimpleQueryStringBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    /**
-     * Returns the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // field names in builder can have wildcards etc, need to resolve them here
-        Map<String, Float> resolvedFieldsAndWeights = new TreeMap<>();
-        // Use the default field if no fields specified
-        if (fieldsAndWeights.isEmpty()) {
-            resolvedFieldsAndWeights.put(resolveIndexName(context.defaultField(), context), AbstractQueryBuilder.DEFAULT_BOOST);
-        } else {
-            for (Map.Entry<String, Float> fieldEntry : fieldsAndWeights.entrySet()) {
-                if (Regex.isSimpleMatchPattern(fieldEntry.getKey())) {
-                    for (String fieldName : context.mapperService().simpleMatchToIndexNames(fieldEntry.getKey())) {
-                        resolvedFieldsAndWeights.put(fieldName, fieldEntry.getValue());
-                    }
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SimpleQueryStringParser.NAME);
+
+        builder.field("query", queryText);
+
+        if (fields.size() > 0) {
+            builder.startArray("fields");
+            for (Map.Entry<String, Float> entry : fields.entrySet()) {
+                String field = entry.getKey();
+                Float boost = entry.getValue();
+                if (boost != null) {
+                    builder.value(field + "^" + boost);
                 } else {
-                    resolvedFieldsAndWeights.put(resolveIndexName(fieldEntry.getKey(), context), fieldEntry.getValue());
+                    builder.value(field);
                 }
             }
+            builder.endArray();
         }
 
-        // Use standard analyzer by default if none specified
-        Analyzer luceneAnalyzer;
-        if (analyzer == null) {
-            luceneAnalyzer = context.mapperService().searchAnalyzer();
-        } else {
-            luceneAnalyzer = context.analysisService().analyzer(analyzer);
-            if (luceneAnalyzer == null) {
-                throw new QueryShardException(context, "[" + SimpleQueryStringBuilder.NAME + "] analyzer [" + analyzer
-                        + "] not found");
-            }
-
+        if (flags != -1) {
+            builder.field("flags", flags);
         }
 
-        SimpleQueryParser sqp = new SimpleQueryParser(luceneAnalyzer, resolvedFieldsAndWeights, flags, settings);
-        sqp.setDefaultOperator(defaultOperator.toBooleanClauseOccur());
-
-        Query query = sqp.parse(queryText);
-        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
-            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        if (analyzer != null) {
+            builder.field("analyzer", analyzer);
         }
-        return query;
-    }
 
-    private static String resolveIndexName(String fieldName, QueryShardContext context) {
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            return fieldType.names().indexName();
+        if (operator != null) {
+            builder.field("default_operator", operator.name().toLowerCase(Locale.ROOT));
         }
-        return fieldName;
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        query.setBoost(boost * query.getBoost());
-    }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (lowercaseExpandedTerms != null) {
+            builder.field("lowercase_expanded_terms", lowercaseExpandedTerms);
+        }
 
-        builder.field("query", queryText);
+        if (lenient != null) {
+            builder.field("lenient", lenient);
+        }
 
-        if (fieldsAndWeights.size() > 0) {
-            builder.startArray("fields");
-            for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-                builder.value(entry.getKey() + "^" + entry.getValue());
-            }
-            builder.endArray();
+        if (analyzeWildcard != null) {
+            builder.field("analyze_wildcard", analyzeWildcard);
         }
 
-        if (analyzer != null) {
-            builder.field("analyzer", analyzer);
+        if (locale != null) {
+            builder.field("locale", locale.toString());
         }
 
-        builder.field("flags", flags);
-        builder.field("default_operator", defaultOperator.name().toLowerCase(Locale.ROOT));
-        builder.field("lowercase_expanded_terms", settings.lowercaseExpandedTerms());
-        builder.field("lenient", settings.lenient());
-        builder.field("analyze_wildcard", settings.analyzeWildcard());
-        builder.field("locale", (settings.locale().toLanguageTag()));
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected SimpleQueryStringBuilder doReadFrom(StreamInput in) throws IOException {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(in.readString());
-        int size = in.readInt();
-        Map<String, Float> fields = new HashMap<>();
-        for (int i = 0; i < size; i++) {
-            String field = in.readString();
-            Float weight = in.readFloat();
-            fields.put(field, weight);
-        }
-        result.fieldsAndWeights.putAll(fields);
-        result.flags = in.readInt();
-        result.analyzer = in.readOptionalString();
-        result.defaultOperator = Operator.readOperatorFrom(in);
-        result.settings.lowercaseExpandedTerms(in.readBoolean());
-        result.settings.lenient(in.readBoolean());
-        result.settings.analyzeWildcard(in.readBoolean());
-        String localeStr = in.readString();
-        result.settings.locale(Locale.forLanguageTag(localeStr));
-        result.minimumShouldMatch = in.readOptionalString();
-        return result;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(queryText);
-        out.writeInt(fieldsAndWeights.size());
-        for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-            out.writeString(entry.getKey());
-            out.writeFloat(entry.getValue());
+        
+        if (boost != -1.0f) {
+            builder.field("boost", boost);
         }
-        out.writeInt(flags);
-        out.writeOptionalString(analyzer);
-        defaultOperator.writeTo(out);
-        out.writeBoolean(settings.lowercaseExpandedTerms());
-        out.writeBoolean(settings.lenient());
-        out.writeBoolean(settings.analyzeWildcard());
-        out.writeString(settings.locale().toLanguageTag());
-        out.writeOptionalString(minimumShouldMatch);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldsAndWeights, analyzer, defaultOperator, queryText, minimumShouldMatch, settings, flags);
+        builder.endObject();
     }
 
-    @Override
-    protected boolean doEquals(SimpleQueryStringBuilder other) {
-        return Objects.equals(fieldsAndWeights, other.fieldsAndWeights) && Objects.equals(analyzer, other.analyzer)
-                && Objects.equals(defaultOperator, other.defaultOperator) && Objects.equals(queryText, other.queryText)
-                && Objects.equals(minimumShouldMatch, other.minimumShouldMatch)
-                && Objects.equals(settings, other.settings) && (flags == other.flags);
-    }
 }
-
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
index 68d19db..ce0ce88 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
@@ -71,7 +71,7 @@ public enum SimpleQueryStringFlag {
                         magic |= flag.value();
                 }
             } catch (IllegalArgumentException iae) {
-                throw new IllegalArgumentException("Unknown " + SimpleQueryStringBuilder.NAME + " flag [" + s + "]");
+                throw new IllegalArgumentException("Unknown " + SimpleQueryStringParser.NAME + " flag [" + s + "]");
             }
         }
         return magic;
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
index 5d29330..4207f93 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
@@ -19,13 +19,21 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.util.LocaleUtils;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
@@ -34,7 +42,6 @@ import java.util.Map;
  * SimpleQueryStringParser is a query parser that acts similar to a query_string
  * query, but won't throw exceptions for any weird string syntax. It supports
  * the following:
- * <p/>
  * <ul>
  * <li>'{@code +}' specifies {@code AND} operation: <tt>token1+token2</tt>
  * <li>'{@code |}' specifies {@code OR} operation: <tt>token1|token2</tt>
@@ -45,14 +52,14 @@ import java.util.Map;
  * <li>'{@code ~}N' at the end of terms specifies fuzzy query: <tt>term~1</tt>
  * <li>'{@code ~}N' at the end of phrases specifies near/slop query: <tt>"term1 term2"~5</tt>
  * </ul>
- * <p/>
+ * <p>
  * See: {@link SimpleQueryParser} for more information.
- * <p/>
+ * <p>
  * This query supports these options:
- * <p/>
+ * <p>
  * Required:
  * {@code query} - query text to be converted into other queries
- * <p/>
+ * <p>
  * Optional:
  * {@code analyzer} - anaylzer to be used for analyzing tokens to determine
  * which kind of query they should be converted into, defaults to "standard"
@@ -61,30 +68,34 @@ import java.util.Map;
  * {@code fields} - fields to search, defaults to _all if not set, allows
  * boosting a field with ^n
  */
-public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBuilder> {
+public class SimpleQueryStringParser implements QueryParser {
+
+    public static final String NAME = "simple_query_string";
+
+    @Inject
+    public SimpleQueryStringParser() {
+
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SimpleQueryStringBuilder.NAME, Strings.toCamelCase(SimpleQueryStringBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SimpleQueryStringBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String currentFieldName = null;
         String queryBody = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f; 
         String queryName = null;
         String minimumShouldMatch = null;
-        Map<String, Float> fieldsAndWeights = new HashMap<>();
-        Operator defaultOperator = null;
-        String analyzerName = null;
-        int flags = SimpleQueryStringFlag.ALL.value();
-        boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
-        Locale locale = null;
+        Map<String, Float> fieldsAndWeights = null;
+        BooleanClause.Occur defaultOperator = null;
+        Analyzer analyzer = null;
+        int flags = -1;
+        SimpleQueryParser.Settings sqsSettings = new SimpleQueryParser.Settings();
 
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -108,10 +119,26 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                         if (fField == null) {
                             fField = parser.text();
                         }
-                        fieldsAndWeights.put(fField, fBoost);
+
+                        if (fieldsAndWeights == null) {
+                            fieldsAndWeights = new HashMap<>();
+                        }
+
+                        if (Regex.isSimpleMatchPattern(fField)) {
+                            for (String fieldName : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                                fieldsAndWeights.put(fieldName, fBoost);
+                            }
+                        } else {
+                            MappedFieldType fieldType = parseContext.fieldMapper(fField);
+                            if (fieldType != null) {
+                                fieldsAndWeights.put(fieldType.names().indexName(), fBoost);
+                            } else {
+                                fieldsAndWeights.put(fField, fBoost);
+                            }
+                        }
                     }
                 } else {
-                    throw new ParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new ParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -119,9 +146,19 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzerName = parser.text();
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new ParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
+                    }
                 } else if ("default_operator".equals(currentFieldName) || "defaultOperator".equals(currentFieldName)) {
-                    defaultOperator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.SHOULD;
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.MUST;
+                    } else {
+                        throw new ParsingException(parseContext, "[" + NAME + "] default operator [" + op + "] is not allowed");
+                    }
                 } else if ("flags".equals(currentFieldName)) {
                     if (parser.currentToken() != XContentParser.Token.VALUE_NUMBER) {
                         // Possible options are:
@@ -135,37 +172,56 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                     }
                 } else if ("locale".equals(currentFieldName)) {
                     String localeStr = parser.text();
-                    locale = Locale.forLanguageTag(localeStr);
+                    Locale locale = LocaleUtils.parse(localeStr);
+                    sqsSettings.locale(locale);
                 } else if ("lowercase_expanded_terms".equals(currentFieldName)) {
-                    lowercaseExpandedTerms = parser.booleanValue();
+                    sqsSettings.lowercaseExpandedTerms(parser.booleanValue());
                 } else if ("lenient".equals(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    sqsSettings.lenient(parser.booleanValue());
                 } else if ("analyze_wildcard".equals(currentFieldName)) {
-                    analyzeWildcard = parser.booleanValue();
+                    sqsSettings.analyzeWildcard(parser.booleanValue());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else if ("minimum_should_match".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else {
-                    throw new ParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] unsupported field [" + parser.currentName() + "]");
+                    throw new ParsingException(parseContext, "[" + NAME + "] unsupported field [" + parser.currentName() + "]");
                 }
             }
         }
 
         // Query text is required
         if (queryBody == null) {
-            throw new ParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query text missing");
+            throw new ParsingException(parseContext, "[" + NAME + "] query text missing");
         }
 
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder(queryBody);
-        qb.boost(boost).fields(fieldsAndWeights).analyzer(analyzerName).queryName(queryName).minimumShouldMatch(minimumShouldMatch);
-        qb.flags(flags).defaultOperator(defaultOperator).locale(locale).lowercaseExpandedTerms(lowercaseExpandedTerms);
-        qb.lenient(lenient).analyzeWildcard(analyzeWildcard).boost(boost);
-        return qb;
-    }
+        // Use standard analyzer by default
+        if (analyzer == null) {
+            analyzer = parseContext.mapperService().searchAnalyzer();
+        }
 
-    @Override
-    public SimpleQueryStringBuilder getBuilderPrototype() {
-        return SimpleQueryStringBuilder.PROTOTYPE;
+        if (fieldsAndWeights == null) {
+            fieldsAndWeights = Collections.singletonMap(parseContext.defaultField(), 1.0F);
+        }
+        SimpleQueryParser sqp = new SimpleQueryParser(analyzer, fieldsAndWeights, flags, sqsSettings);
+
+        if (defaultOperator != null) {
+            sqp.setDefaultOperator(defaultOperator);
+        }
+
+        Query query = sqp.parse(queryBody);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+
+        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
+            query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        }
+
+        if (query != null) {
+            query.setBoost(boost * query.getBoost());
+        }
+
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
index 485ffff..0b7a3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
@@ -19,101 +19,74 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanContainingQuery}.
  */
-public class SpanContainingQueryBuilder extends AbstractQueryBuilder<SpanContainingQueryBuilder> implements SpanQueryBuilder<SpanContainingQueryBuilder> {
+public class SpanContainingQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanContainingQueryBuilder> {
 
-    public static final String NAME = "span_containing";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanContainingQueryBuilder PROTOTYPE = new SpanContainingQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, SpanTermQueryBuilder.PROTOTYPE);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanContainingQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        if (big == null) {
-            throw new IllegalArgumentException("inner clause [big] cannot be null.");
-        }
-        if (little == null) {
-            throw new IllegalArgumentException("inner clause [little] cannot be null.");
-        }
-        this.little = little;
-        this.big = big;
+    public SpanContainingQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the big clause, it must enclose {@code little} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
+    public SpanContainingQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
+    }
+
+    @Override
+    public SpanContainingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the little clause, it must be contained within {@code big} for a match.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanContainingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (big == null) {
+            throw new IllegalArgumentException("Must specify big clause when building a span_containing query");
+        }
+        if (little == null) {
+            throw new IllegalArgumentException("Must specify little clause when building a span_containing query");
+        }
+        builder.startObject(SpanContainingQueryParser.NAME);
+
         builder.field("big");
         big.toXContent(builder, params);
+
         builder.field("little");
         little.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanContainingQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
-    }
-
-    @Override
-    protected SpanContainingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanContainingQueryBuilder(big, little);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SpanContainingQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
index 4968c22..c172748 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanContainingQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -27,22 +30,29 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_containing query
+ * Parser for {@link SpanContainingQuery}
  */
-public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQueryBuilder> {
+public class SpanContainingQueryParser implements QueryParser {
+
+    public static final String NAME = "span_containing";
+
+    @Inject
+    public SpanContainingQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanContainingQueryBuilder.NAME, Strings.toCamelCase(SpanContainingQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanContainingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder<?> big = null;
-        SpanQueryBuilder<?> little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -51,17 +61,17 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new ParsingException(parseContext, "span_containing [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder<?>) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new ParsingException(parseContext, "span_containing [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder<?>) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new ParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
                 }
@@ -72,15 +82,22 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
             } else {
                 throw new ParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
             }
+        }        
+        
+        if (big == null) {
+            throw new ParsingException(parseContext, "span_containing must include [big]");
+        }
+        if (little == null) {
+            throw new ParsingException(parseContext, "span_containing must include [little]");
         }
 
-        SpanContainingQueryBuilder query = new SpanContainingQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanContainingQuery(big, little);
+        if (boost != 1.0F) {
+            query.setBoost(boost);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanContainingQueryBuilder getBuilderPrototype() {
-        return SpanContainingQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
index b4dcd30..f967a1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
@@ -19,101 +19,51 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanFirstQueryBuilder extends AbstractQueryBuilder<SpanFirstQueryBuilder> implements SpanQueryBuilder<SpanFirstQueryBuilder>{
-
-    public static final String NAME = "span_first";
+public class SpanFirstQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanFirstQueryBuilder> {
 
     private final SpanQueryBuilder matchBuilder;
 
     private final int end;
 
-    static final SpanFirstQueryBuilder PROTOTYPE = new SpanFirstQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, 0);
+    private float boost = -1;
+
+    private String queryName;
 
-    /**
-     * Query that matches spans queries defined in <code>matchBuilder</code>
-     * whose end position is less than or equal to <code>end</code>.
-     * @param matchBuilder inner {@link SpanQueryBuilder}
-     * @param end maximum end position of the match, needs to be positive
-     * @throws IllegalArgumentException for negative <code>end</code> positions
-     */
     public SpanFirstQueryBuilder(SpanQueryBuilder matchBuilder, int end) {
-        if (matchBuilder == null) {
-            throw new IllegalArgumentException("inner span query cannot be null");
-        }
-        if (end < 0) {
-            throw new IllegalArgumentException("parameter [end] needs to be positive.");
-        }
         this.matchBuilder = matchBuilder;
         this.end = end;
     }
 
-    /**
-     * @return the inner {@link SpanQueryBuilder} defined in this query
-     */
-    public SpanQueryBuilder innerQuery() {
-        return this.matchBuilder;
+    @Override
+    public SpanFirstQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return maximum end position of the matching inner span query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public int end() {
-        return this.end;
+    public SpanFirstQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanFirstQueryParser.NAME);
         builder.field("match");
         matchBuilder.toXContent(builder, params);
         builder.field("end", end);
-        printBoostAndQueryName(builder);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerSpanQuery = matchBuilder.toQuery(context);
-        assert innerSpanQuery instanceof SpanQuery;
-        return new SpanFirstQuery((SpanQuery) innerSpanQuery, end);
-    }
-
-    @Override
-    protected SpanFirstQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder matchBuilder = (SpanQueryBuilder)in.readQuery();
-        int end = in.readInt();
-        return new SpanFirstQueryBuilder(matchBuilder, end);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(matchBuilder);
-        out.writeInt(end);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(matchBuilder, end);
-    }
-
-    @Override
-    protected boolean doEquals(SpanFirstQueryBuilder other) {
-        return Objects.equals(matchBuilder, other.matchBuilder) &&
-               Objects.equals(end, other.end);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
index 4773129..5d4693d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -27,23 +30,29 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_first query
+ *
  */
-public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder> {
+public class SpanFirstQueryParser implements QueryParser {
+
+    public static final String NAME = "span_first";
+
+    @Inject
+    public SpanFirstQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanFirstQueryBuilder.NAME, Strings.toCamelCase(SpanFirstQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanFirstQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder match = null;
-        Integer end = null;
+        SpanQuery match = null;
+        int end = -1;
         String queryName = null;
 
         String currentFieldName = null;
@@ -53,11 +62,11 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("match".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new ParsingException(parseContext, "spanFirst [match] must be of type span query");
                     }
-                    match = (SpanQueryBuilder) query;
+                    match = (SpanQuery) query;
                 } else {
                     throw new ParsingException(parseContext, "[span_first] query does not support [" + currentFieldName + "]");
                 }
@@ -76,16 +85,15 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
         if (match == null) {
             throw new ParsingException(parseContext, "spanFirst must have [match] span query clause");
         }
-        if (end == null) {
+        if (end == -1) {
             throw new ParsingException(parseContext, "spanFirst must have [end] set for it");
         }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(match, end);
-        queryBuilder.boost(boost).queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public SpanFirstQueryBuilder getBuilderPrototype() {
-        return SpanFirstQueryBuilder.PROTOTYPE;
+        SpanFirstQuery query = new SpanFirstQuery(match, end);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
index eac2e6a..11b9897 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
@@ -18,80 +18,25 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-/**
- * Query that allows wraping a {@link MultiTermQueryBuilder} (one of wildcard, fuzzy, prefix, term, range or regexp query)
- * as a {@link SpanQueryBuilder} so it can be nested.
- */
-public class SpanMultiTermQueryBuilder extends AbstractQueryBuilder<SpanMultiTermQueryBuilder> implements SpanQueryBuilder<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryBuilder extends SpanQueryBuilder {
 
-    public static final String NAME = "span_multi";
-    private final MultiTermQueryBuilder multiTermQueryBuilder;
-    static final SpanMultiTermQueryBuilder PROTOTYPE = new SpanMultiTermQueryBuilder(RangeQueryBuilder.PROTOTYPE);
+    private MultiTermQueryBuilder multiTermQueryBuilder;
 
     public SpanMultiTermQueryBuilder(MultiTermQueryBuilder multiTermQueryBuilder) {
-        if (multiTermQueryBuilder == null) {
-            throw new IllegalArgumentException("inner multi term query cannot be null");
-        }
         this.multiTermQueryBuilder = multiTermQueryBuilder;
     }
 
-    public MultiTermQueryBuilder innerQuery() {
-        return this.multiTermQueryBuilder;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params)
             throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanMultiTermQueryParser.NAME);
         builder.field(SpanMultiTermQueryParser.MATCH_NAME);
         multiTermQueryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query subQuery = multiTermQueryBuilder.toQuery(context);
-        if (subQuery instanceof MultiTermQuery == false) {
-            throw new UnsupportedOperationException("unsupported inner query, should be " + MultiTermQuery.class.getName() +" but was "
-                    + subQuery.getClass().getName());
-        }
-        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
-    }
-
-    @Override
-    protected SpanMultiTermQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MultiTermQueryBuilder multiTermBuilder = (MultiTermQueryBuilder)in.readQuery();
-        return new SpanMultiTermQueryBuilder(multiTermBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanMultiTermQueryBuilder other) {
-        return Objects.equals(multiTermQueryBuilder, other.multiTermQueryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
index ec097aa..bb043aa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
@@ -18,66 +18,54 @@
  */
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
 
 import java.io.IOException;
 
 /**
- * Parser for span_multi query
+ *
  */
-public class SpanMultiTermQueryParser extends BaseQueryParser<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryParser implements QueryParser {
 
+    public static final String NAME = "span_multi";
     public static final String MATCH_NAME = "match";
 
+    @Inject
+    public SpanMultiTermQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{SpanMultiTermQueryBuilder.NAME, Strings.toCamelCase(SpanMultiTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanMultiTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        String currentFieldName = null;
-        MultiTermQueryBuilder subQuery = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (MATCH_NAME.equals(currentFieldName)) {
-                    QueryBuilder innerQuery = parseContext.parseInnerQueryBuilder();
-                    if (innerQuery instanceof MultiTermQueryBuilder == false) {
-                        throw new ParsingException(parseContext, "[span_multi] [" + MATCH_NAME + "] must be of type multi term query");
-                    }
-                    subQuery = (MultiTermQueryBuilder) innerQuery;
-                } else {
-                    throw new ParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else {
-                    throw new ParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            }
+
+        Token token = parser.nextToken();
+        if (!MATCH_NAME.equals(parser.currentName()) || token != XContentParser.Token.FIELD_NAME) {
+            throw new ParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        if (subQuery == null) {
-            throw new ParsingException(parseContext, "[span_multi] must have [" + MATCH_NAME + "] multi term query clause");
+        token = parser.nextToken();
+        if (token != XContentParser.Token.START_OBJECT) {
+            throw new ParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        return new SpanMultiTermQueryBuilder(subQuery).queryName(queryName).boost(boost);
-    }
+        Query subQuery = parseContext.parseInnerQuery();
+        if (!(subQuery instanceof MultiTermQuery)) {
+            throw new ParsingException(parseContext, "spanMultiTerm [" + MATCH_NAME + "] must be of type multi term query");
+        }
 
-    @Override
-    public SpanMultiTermQueryBuilder getBuilderPrototype() {
-        return SpanMultiTermQueryBuilder.PROTOTYPE;
+        parser.nextToken();
+        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
index b933ced..cb05e08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
@@ -19,171 +19,86 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Matches spans which are near one another. One can specify slop, the maximum number
- * of intervening unmatched positions, as well as whether matches are required to be in-order.
- * The span near query maps to Lucene {@link SpanNearQuery}.
- */
-public class SpanNearQueryBuilder extends AbstractQueryBuilder<SpanNearQueryBuilder> implements SpanQueryBuilder<SpanNearQueryBuilder> {
-
-    public static final String NAME = "span_near";
-
-    /** Default for flag controlling whether matches are required to be in-order */
-    public static boolean DEFAULT_IN_ORDER = true;
-
-    /** Default for flag controlling whether payloads are collected */
-    public static boolean DEFAULT_COLLECT_PAYLOADS = true;
+public class SpanNearQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNearQueryBuilder> {
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final int slop;
+    private Integer slop = null;
 
-    private boolean inOrder = DEFAULT_IN_ORDER;
+    private Boolean inOrder;
 
-    private boolean collectPayloads = DEFAULT_COLLECT_PAYLOADS;
+    private Boolean collectPayloads;
 
-    static final SpanNearQueryBuilder PROTOTYPE = new SpanNearQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, 0);
-
-    /**
-     * @param initialClause an initial span query clause
-     * @param slop controls the maximum number of intervening unmatched positions permitted
-     */
-    public SpanNearQueryBuilder(SpanQueryBuilder initialClause, int slop) {
-        if (initialClause == null) {
-            throw new IllegalArgumentException("query must include at least one clause");
-        }
-        this.clauses.add(initialClause);
-        this.slop = slop;
-    }
+    private float boost = -1;
 
-    /**
-     * @return the maximum number of intervening unmatched positions permitted
-     */
-    public int slop() {
-        return this.slop;
-    }
+    private String queryName;
 
     public SpanNearQueryBuilder clause(SpanQueryBuilder clause) {
-        if (clause == null) {
-            throw new IllegalArgumentException("query clauses cannot be null");
-        }
         clauses.add(clause);
         return this;
     }
 
-    /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
-     */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanNearQueryBuilder slop(int slop) {
+        this.slop = slop;
+        return this;
     }
 
-    /**
-     * When <code>inOrder</code> is true, the spans from each clause
-     * must be in the same order as in <code>clauses</code> and must be non-overlapping.
-     * Defaults to <code>true</code>
-     */
     public SpanNearQueryBuilder inOrder(boolean inOrder) {
         this.inOrder = inOrder;
         return this;
     }
 
-    /**
-     * @see SpanNearQueryBuilder#inOrder(boolean))
-     */
-    public boolean inOrder() {
-        return this.inOrder;
-    }
-
-    /**
-     * @param collectPayloads flag controlling whether payloads are collected
-     */
     public SpanNearQueryBuilder collectPayloads(boolean collectPayloads) {
         this.collectPayloads = collectPayloads;
         return this;
     }
 
+    @Override
+    public SpanNearQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @see SpanNearQueryBuilder#collectPayloads(boolean))
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean collectPayloads() {
-        return this.collectPayloads;
+    public SpanNearQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanNear query");
+        }
+        if (slop == null) {
+            throw new IllegalArgumentException("Must set the slop when building a spanNear query");
+        }
+        builder.startObject(SpanNearQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        builder.field("slop", slop);
-        builder.field("in_order", inOrder);
-        builder.field("collect_payloads", collectPayloads);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        builder.field("slop", slop.intValue());
+        if (inOrder != null) {
+            builder.field("in_order", inOrder);
         }
-        return new SpanNearQuery(spanQueries, slop, inOrder, collectPayloads);
-    }
-
-    @Override
-    protected SpanNearQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        List<QueryBuilder> clauses = readQueries(in);
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder((SpanQueryBuilder)clauses.get(0), in.readVInt());
-        for (int i = 1; i < clauses.size(); i++) {
-            queryBuilder.clauses.add((SpanQueryBuilder)clauses.get(i));
+        if (collectPayloads != null) {
+            builder.field("collect_payloads", collectPayloads);
         }
-        queryBuilder.collectPayloads = in.readBoolean();
-        queryBuilder.inOrder = in.readBoolean();
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, clauses);
-        out.writeVInt(slop);
-        out.writeBoolean(collectPayloads);
-        out.writeBoolean(inOrder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses, slop, collectPayloads, inOrder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanNearQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses) &&
-               Objects.equals(slop, other.slop) &&
-               Objects.equals(collectPayloads, other.collectPayloads) &&
-               Objects.equals(inOrder, other.inOrder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
index 234af5a..86c1e47 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
@@ -19,8 +19,12 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -28,26 +32,32 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for span_near query
+ *
  */
-public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
+public class SpanNearQueryParser implements QueryParser {
+
+    public static final String NAME = "span_near";
+
+    @Inject
+    public SpanNearQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanNearQueryBuilder.NAME, Strings.toCamelCase(SpanNearQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNearQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Integer slop = null;
-        boolean inOrder = SpanNearQueryBuilder.DEFAULT_IN_ORDER;
-        boolean collectPayloads = SpanNearQueryBuilder.DEFAULT_COLLECT_PAYLOADS;
+        boolean inOrder = true;
+        boolean collectPayloads = true;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = new ArrayList<>();
+        List<SpanQuery> clauses = new ArrayList<>();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -57,11 +67,11 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new ParsingException(parseContext, "spanNear [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new ParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
@@ -72,7 +82,7 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 } else if ("collect_payloads".equals(currentFieldName) || "collectPayloads".equals(currentFieldName)) {
                     collectPayloads = parser.booleanValue();
                 } else if ("slop".equals(currentFieldName)) {
-                    slop = parser.intValue();
+                    slop = Integer.valueOf(parser.intValue());
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("_name".equals(currentFieldName)) {
@@ -84,28 +94,18 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 throw new ParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
             }
         }
-
         if (clauses.isEmpty()) {
             throw new ParsingException(parseContext, "span_near must include [clauses]");
         }
-
         if (slop == null) {
             throw new ParsingException(parseContext, "span_near must include [slop]");
         }
 
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(clauses.get(0), slop);
-        for (int i = 1; i < clauses.size(); i++) {
-            queryBuilder.clause(clauses.get(i));
+        SpanNearQuery query = new SpanNearQuery(clauses.toArray(new SpanQuery[clauses.size()]), slop.intValue(), inOrder, collectPayloads);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.inOrder(inOrder);
-        queryBuilder.collectPayloads(collectPayloads);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanNearQueryBuilder getBuilderPrototype() {
-        return SpanNearQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
index ffe3cec..e37cd80 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
@@ -19,166 +19,100 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanNotQueryBuilder extends AbstractQueryBuilder<SpanNotQueryBuilder> implements SpanQueryBuilder<SpanNotQueryBuilder> {
+public class SpanNotQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNotQueryBuilder> {
 
-    public static final String NAME = "span_not";
+    private SpanQueryBuilder include;
 
-    /** the default pre parameter size */
-    public static final int DEFAULT_PRE = 0;
-    /** the default post parameter size */
-    public static final int DEFAULT_POST = 0;
+    private SpanQueryBuilder exclude;
 
-    private final SpanQueryBuilder include;
+    private Integer dist;
 
-    private final SpanQueryBuilder exclude;
+    private Integer pre;
 
-    private int pre = DEFAULT_PRE;
+    private Integer post;
 
-    private int post = DEFAULT_POST;
+    private Float boost;
 
-    static final SpanNotQueryBuilder PROTOTYPE = new SpanNotQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, SpanTermQueryBuilder.PROTOTYPE);
+    private String queryName;
 
-    /**
-     * Construct a span query matching spans from <code>include</code> which
-     * have no overlap with spans from <code>exclude</code>.
-     * @param include the span query whose matches are filtered
-     * @param exclude the span query whose matches must not overlap
-     */
-    public SpanNotQueryBuilder(SpanQueryBuilder include, SpanQueryBuilder exclude) {
-        if (include == null) {
-            throw new IllegalArgumentException("inner clause [include] cannot be null.");
-        }
-        if (exclude == null) {
-            throw new IllegalArgumentException("inner clause [exclude] cannot be null.");
-        }
+    public SpanNotQueryBuilder include(SpanQueryBuilder include) {
         this.include = include;
-        this.exclude = exclude;
-    }
-
-    /**
-     * @return the span query whose matches are filtered
-     */
-    public SpanQueryBuilder includeQuery() {
-        return this.include;
+        return this;
     }
 
-    /**
-     * @return the span query whose matches must not overlap
-     */
-    public SpanQueryBuilder excludeQuery() {
-        return this.exclude;
+    public SpanNotQueryBuilder exclude(SpanQueryBuilder exclude) {
+        this.exclude = exclude;
+        return this;
     }
 
-    /**
-     * @param dist the amount of tokens from within the include span cant have overlap with the exclude span.
-     * Equivalent to setting both pre and post parameter.
-     */
     public SpanNotQueryBuilder dist(int dist) {
-        pre(dist);
-        post(dist);
+        this.dist = dist;
         return this;
     }
 
-    /**
-     * @param pre the amount of tokens before the include span that cant have overlap with the exclude span. Values
-     * smaller than 0 will be ignored and 0 used instead.
-     */
     public SpanNotQueryBuilder pre(int pre) {
-        this.pre = (pre >= 0) ? pre : 0;
+        this.pre = (pre >=0) ? pre : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens before the include span that cant have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#pre(int)
-     */
-    public Integer pre() {
-        return this.pre;
-    }
-
-    /**
-     * @param post the amount of tokens after the include span that cant have overlap with the exclude span.
-     */
     public SpanNotQueryBuilder post(int post) {
         this.post = (post >= 0) ? post : 0;
         return this;
     }
 
+    @Override
+    public SpanNotQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @return the amount of tokens after the include span that cant have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#post(int)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     * @param queryName The query name
+     * @return this
      */
-    public Integer post() {
-        return this.post;
+    public SpanNotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (include == null) {
+            throw new IllegalArgumentException("Must specify include when using spanNot query");
+        }
+        if (exclude == null) {
+            throw new IllegalArgumentException("Must specify exclude when using spanNot query");
+        }
+
+        if (dist != null && (pre != null || post != null)) {
+             throw new IllegalArgumentException("spanNot can either use [dist] or [pre] & [post] (or none)");
+        }
+
+        builder.startObject(SpanNotQueryParser.NAME);
         builder.field("include");
         include.toXContent(builder, params);
         builder.field("exclude");
         exclude.toXContent(builder, params);
-        builder.field("pre", pre);
-        builder.field("post", post);
-        printBoostAndQueryName(builder);
+        if (dist != null) {
+            builder.field("dist", dist);
+        }
+        if (pre != null) {
+            builder.field("pre", pre);
+        }
+        if (post != null) {
+            builder.field("post", post);
+        }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        Query includeQuery = this.include.toQuery(context);
-        assert includeQuery instanceof SpanQuery;
-        Query excludeQuery = this.exclude.toQuery(context);
-        assert excludeQuery instanceof SpanQuery;
-
-        return new SpanNotQuery((SpanQuery) includeQuery, (SpanQuery) excludeQuery, pre, post);
-    }
-
-    @Override
-    protected SpanNotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder include = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder exclude = (SpanQueryBuilder)in.readQuery();
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        queryBuilder.pre(in.readVInt());
-        queryBuilder.post(in.readVInt());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(include);
-        out.writeQuery(exclude);
-        out.writeVInt(pre);
-        out.writeVInt(post);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(include, exclude, pre, post);
-    }
-
-    @Override
-    protected boolean doEquals(SpanNotQueryBuilder other) {
-        return Objects.equals(include, other.include) &&
-               Objects.equals(exclude, other.exclude) &&
-               (pre == other.pre) &&
-               (post == other.post);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
index b435fcb..4b135e1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -27,23 +30,29 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_not query
+ *
  */
-public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
+public class SpanNotQueryParser implements QueryParser {
+
+    public static final String NAME = "span_not";
+
+    @Inject
+    public SpanNotQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanNotQueryBuilder.NAME, Strings.toCamelCase(SpanNotQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder include = null;
-        SpanQueryBuilder exclude = null;
+        SpanQuery include = null;
+        SpanQuery exclude = null;
 
         Integer dist = null;
         Integer pre  = null;
@@ -58,17 +67,17 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("include".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new ParsingException(parseContext, "spanNot [include] must be of type span query");
                     }
-                    include = (SpanQueryBuilder) query;
+                    include = (SpanQuery) query;
                 } else if ("exclude".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new ParsingException(parseContext, "spanNot [exclude] must be of type span query");
                     }
-                    exclude = (SpanQueryBuilder) query;
+                    exclude = (SpanQuery) query;
                 } else {
                     throw new ParsingException(parseContext, "[span_not] query does not support [" + currentFieldName + "]");
                 }
@@ -98,23 +107,26 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
             throw new ParsingException(parseContext, "spanNot can either use [dist] or [pre] & [post] (or none)");
         }
 
-        SpanNotQueryBuilder spanNotQuery = new SpanNotQueryBuilder(include, exclude);
-        if (dist != null) {
-            spanNotQuery.dist(dist);
-        }
-        if (pre != null) {
-            spanNotQuery.pre(pre);
+        // set appropriate defaults
+        if (pre != null && post == null) {
+            post = 0;
+        } else if (pre == null && post != null){
+            pre = 0;
         }
-        if (post != null) {
-            spanNotQuery.post(post);
+
+        SpanNotQuery query;
+        if (pre != null && post != null) {
+            query = new SpanNotQuery(include, exclude, pre, post);
+        } else if (dist != null) {
+            query = new SpanNotQuery(include, exclude, dist);
+        } else {
+            query = new SpanNotQuery(include, exclude);
         }
-        spanNotQuery.boost(boost);
-        spanNotQuery.queryName(queryName);
-        return spanNotQuery;
-    }
 
-    @Override
-    public SpanNotQueryBuilder getBuilderPrototype() {
-        return SpanNotQueryBuilder.PROTOTYPE;
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
index a46bef4..0042aa7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
@@ -19,102 +19,55 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Span query that matches the union of its clauses. Maps to {@link SpanOrQuery}.
- */
-public class SpanOrQueryBuilder extends AbstractQueryBuilder<SpanOrQueryBuilder> implements SpanQueryBuilder<SpanOrQueryBuilder> {
-
-    public static final String NAME = "span_or";
+public class SpanOrQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanOrQueryBuilder> {
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    static final SpanOrQueryBuilder PROTOTYPE = new SpanOrQueryBuilder(SpanTermQueryBuilder.PROTOTYPE);
+    private float boost = -1;
 
-    public SpanOrQueryBuilder(SpanQueryBuilder initialClause) {
-        if (initialClause == null) {
-            throw new IllegalArgumentException("query must include at least one clause");
-        }
-        clauses.add(initialClause);
-    }
+    private String queryName;
 
     public SpanOrQueryBuilder clause(SpanQueryBuilder clause) {
-        if (clause == null) {
-            throw new IllegalArgumentException("inner bool query clause cannot be null");
-        }
         clauses.add(clause);
         return this;
     }
 
+    @Override
+    public SpanOrQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanOrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanOr query");
+        }
+        builder.startObject(SpanOrQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new SpanOrQuery(spanQueries);
-    }
-
-    @Override
-    protected SpanOrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        List<QueryBuilder> clauses = readQueries(in);
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder((SpanQueryBuilder)clauses.get(0));
-        for (int i = 1; i < clauses.size(); i++) {
-            queryBuilder.clauses.add((SpanQueryBuilder)clauses.get(i));
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, clauses);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses);
-    }
-
-    @Override
-    protected boolean doEquals(SpanOrQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
index 4af3077..91f6ad1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
@@ -19,8 +19,12 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -28,23 +32,29 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for span_or query
+ *
  */
-public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
+public class SpanOrQueryParser implements QueryParser {
+
+    public static final String NAME = "span_or";
+
+    @Inject
+    public SpanOrQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanOrQueryBuilder.NAME, Strings.toCamelCase(SpanOrQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanOrQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = new ArrayList<>();
+        List<SpanQuery> clauses = new ArrayList<>();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -54,11 +64,11 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new ParsingException(parseContext, "spanOr [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new ParsingException(parseContext, "[span_or] query does not support [" + currentFieldName + "]");
@@ -73,22 +83,15 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
                 }
             }
         }
-
         if (clauses.isEmpty()) {
             throw new ParsingException(parseContext, "spanOr must include [clauses]");
         }
 
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder(clauses.get(0));
-        for (int i = 1; i < clauses.size(); i++) {
-            queryBuilder.clause(clauses.get(i));
+        SpanOrQuery query = new SpanOrQuery(clauses.toArray(new SpanQuery[clauses.size()]));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanOrQueryBuilder getBuilderPrototype() {
-        return SpanOrQueryBuilder.PROTOTYPE;
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
index d35dcbc..4216f22 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
@@ -19,9 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-/**
- * Marker interface for a specific type of {@link QueryBuilder} that allows to build span queries
- */
-public interface SpanQueryBuilder<QB extends SpanQueryBuilder> extends QueryBuilder<QB> {
+public abstract class SpanQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
index fc41dc4..9d0176e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
@@ -19,76 +19,75 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-/**
- * A Span Query that matches documents containing a term.
- * @see SpanTermQuery
- */
-public class SpanTermQueryBuilder extends BaseTermQueryBuilder<SpanTermQueryBuilder> implements SpanQueryBuilder<SpanTermQueryBuilder> {
+public class SpanTermQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanTermQueryBuilder> {
+
+    private final String name;
+
+    private final Object value;
+
+    private float boost = -1;
 
-    public static final String NAME = "span_term";
-    static final SpanTermQueryBuilder PROTOTYPE = new SpanTermQueryBuilder("name", "value");
+    private String queryName;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
     public SpanTermQueryBuilder(String name, String value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
     public SpanTermQueryBuilder(String name, int value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
     public SpanTermQueryBuilder(String name, long value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
     public SpanTermQueryBuilder(String name, float value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
     public SpanTermQueryBuilder(String name, double value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public SpanTermQueryBuilder(String name, Object value) {
-        super(name, value);
+    private SpanTermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     @Override
-    public SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        BytesRef valueBytes = null;
-        String fieldName = this.fieldName;
-        MappedFieldType mapper = context.fieldMapper(fieldName);
-        if (mapper != null) {
-            fieldName = mapper.names().indexName();
-            valueBytes = mapper.indexedValueForSearch(value);
-        }
-        if (valueBytes == null) {
-            valueBytes = BytesRefs.toBytesRef(this.value);
-        }
-        return new SpanTermQuery(new Term(fieldName, valueBytes));
+    public SpanTermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected SpanTermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanTermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SpanTermQueryParser.NAME);
+        if (boost == -1 && queryName != null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
index 9e0a8cb..d9e28e9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
@@ -19,38 +19,48 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for span_term query
+ *
  */
-public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
+public class SpanTermQueryParser implements QueryParser {
+
+    public static final String NAME = "span_term";
+
+    @Inject
+    public SpanTermQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanTermQueryBuilder.NAME, Strings.toCamelCase(SpanTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, ParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_OBJECT) {
             token = parser.nextToken();
         }
-
         assert token == XContentParser.Token.FIELD_NAME;
         String fieldName = parser.currentName();
 
 
-        Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        String value = null;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -60,9 +70,9 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
                     currentFieldName = parser.currentName();
                 } else {
                     if ("term".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("value".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
@@ -74,7 +84,7 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             }
             parser.nextToken();
         } else {
-            value = parser.objectBytes();
+            value = parser.text();
             // move to the next token
             parser.nextToken();
         }
@@ -83,13 +93,21 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             throw new ParsingException(parseContext, "No value specified for term query");
         }
 
-        SpanTermQueryBuilder result = new SpanTermQueryBuilder(fieldName, value);
-        result.boost(boost).queryName(queryName);
-        return result;
-    }
+        BytesRef valueBytes = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        }
+        if (valueBytes == null) {
+            valueBytes = new BytesRef(value);
+        }
 
-    @Override
-    public SpanTermQueryBuilder getBuilderPrototype() {
-        return SpanTermQueryBuilder.PROTOTYPE;
+        SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
index c3a11c8..d2b2fdc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
@@ -19,59 +19,59 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanWithinQuery}.
  */
-public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQueryBuilder> implements SpanQueryBuilder<SpanWithinQueryBuilder> {
+public class SpanWithinQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanWithinQueryBuilder> {
 
-    public static final String NAME = "span_within";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanWithinQueryBuilder PROTOTYPE = new SpanWithinQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, SpanTermQueryBuilder.PROTOTYPE);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * Query that returns spans from <code>little</code> that are contained in a spans from <code>big</code>.
-     * @param big clause that must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanWithinQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        if (big == null) {
-            throw new IllegalArgumentException("inner clause [big] cannot be null.");
-        }
-        if (little == null) {
-            throw new IllegalArgumentException("inner clause [little] cannot be null.");
-        }
-        this.little = little;
-        this.big = big;
+    public SpanWithinQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the little clause, contained within {@code big} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanWithinQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
+    }
+
+    @Override
+    public SpanWithinQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the big clause that must enclose {@code little} for a match.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
+    public SpanWithinQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (big == null) {
+            throw new IllegalArgumentException("Must specify big clause when building a span_within query");
+        }
+        if (little == null) {
+            throw new IllegalArgumentException("Must specify little clause when building a span_within query");
+        }
+        builder.startObject(SpanWithinQueryParser.NAME);
 
         builder.field("big");
         big.toXContent(builder, params);
@@ -79,46 +79,14 @@ public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQuery
         builder.field("little");
         little.toXContent(builder, params);
 
-        printBoostAndQueryName(builder);
-
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanWithinQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
-    }
-
-    @Override
-    protected SpanWithinQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanWithinQueryBuilder(big, little);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SpanWithinQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
index 1e6e125..8e960d7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
@@ -19,30 +19,40 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanWithinQuery;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for span_within query
+ * Parser for {@link SpanWithinQuery}
  */
-public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilder> {
+public class SpanWithinQueryParser implements QueryParser {
+
+    public static final String NAME = "span_within";
+
+    @Inject
+    public SpanWithinQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanWithinQueryBuilder.NAME, Strings.toCamelCase(SpanWithinQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanWithinQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder big = null;
-        SpanQueryBuilder little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -51,17 +61,17 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new ParsingException(parseContext, "span_within [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new ParsingException(parseContext, "span_within [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new ParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
                 }
@@ -72,8 +82,8 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             } else {
                 throw new ParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
             }
-        }
-
+        }        
+        
         if (big == null) {
             throw new ParsingException(parseContext, "span_within must include [big]");
         }
@@ -81,13 +91,13 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             throw new ParsingException(parseContext, "span_within must include [little]");
         }
 
-        SpanWithinQueryBuilder query = new SpanWithinQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanWithinQuery(big, little);
+        if (boost != 1.0F) {
+            query.setBoost(boost);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanWithinQueryBuilder getBuilderPrototype() {
-        return SpanWithinQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
index d63f160..852977f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
@@ -18,49 +18,35 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Facilitates creating template query requests.
  * */
-public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuilder> {
-
-    /** Name to reference this type of query. */
-    public static final String NAME = "template";
+public class TemplateQueryBuilder extends QueryBuilder {
 
     /** Template to fill. */
-    private final Template template;
+    private Template template;
+    /** Parameters to fill the template with. */
+    private Map<String, Object> vars;
+    /** Template to fill.*/
+    private String templateString;
 
-    static final TemplateQueryBuilder PROTOTYPE = new TemplateQueryBuilder(new Template("proto"));
+    private ScriptService.ScriptType templateType;
 
     /**
      * @param template
      *            the template to use for that query.
      * */
     public TemplateQueryBuilder(Template template) {
-        if (template == null) {
-            throw new IllegalArgumentException("query template cannot be null");
-        }
         this.template = template;
     }
 
-    public Template template() {
-        return template;
-    }
-
     /**
      * @param template
      *            the template to use for that query.
@@ -70,7 +56,7 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
      * */
     @Deprecated
     public TemplateQueryBuilder(String template, Map<String, Object> vars) {
-        this(new Template(template, ScriptService.ScriptType.INLINE, null, null, vars));
+        this(template, ScriptService.ScriptType.INLINE, vars);
     }
 
     /**
@@ -84,55 +70,18 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
      * */
     @Deprecated
     public TemplateQueryBuilder(String template, ScriptService.ScriptType templateType, Map<String, Object> vars) {
-        this(new Template(template, templateType, null, null, vars));
+        this.templateString = template;
+        this.vars = vars;
+        this.templateType = templateType;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.field(TemplateQueryBuilder.NAME);
-        template.toXContent(builder, builderParams);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        BytesReference querySource = context.executeQueryTemplate(template, SearchContext.current());
-        try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            QueryBuilder result = contextCopy.parseContext().parseInnerQueryBuilder();
-            context.combineNamedQueries(contextCopy);
-            return result.toQuery(context);
+        builder.field(TemplateQueryParser.NAME);
+        if (template == null) {
+            new Template(templateString, templateType, null, null, this.vars).toXContent(builder, builderParams);
+        } else {
+            template.toXContent(builder, builderParams);
         }
     }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //no-op this query doesn't support boost
-    }
-
-    @Override
-    protected TemplateQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        TemplateQueryBuilder templateQueryBuilder = new TemplateQueryBuilder(Template.readTemplate(in));
-        return templateQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        template.writeTo(out);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(template);
-    }
-
-    @Override
-    protected boolean doEquals(TemplateQueryBuilder other) {
-        return Objects.equals(template, other.template);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index e08f704..1b5210d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,12 +18,18 @@
  */
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -33,7 +39,14 @@ import java.util.Map;
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
  * */
-public class TemplateQueryParser extends BaseQueryParser<TemplateQueryBuilder> {
+public class TemplateQueryParser implements QueryParser {
+
+    /** Name to reference this type of query. */
+    public static final String NAME = "template";
+    /** Name of query parameter containing the template string. */
+    public static final String QUERY = "query";
+
+    private final ScriptService scriptService;
 
     private final static Map<String, ScriptService.ScriptType> parametersToTypes = new HashMap<>();
     static {
@@ -42,9 +55,14 @@ public class TemplateQueryParser extends BaseQueryParser<TemplateQueryBuilder> {
         parametersToTypes.put("id", ScriptService.ScriptType.INDEXED);
     }
 
+    @Inject
+    public TemplateQueryParser(ScriptService scriptService) {
+        this.scriptService = scriptService;
+    }
+
     @Override
     public String[] names() {
-        return new String[] {TemplateQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     /**
@@ -52,17 +70,27 @@ public class TemplateQueryParser extends BaseQueryParser<TemplateQueryBuilder> {
      * values. Handles both submitting the template as part of the request as
      * well as referencing only the template name.
      *
-     * @param parseContext parse context containing the templated query.
+     * @param parseContext
+     *            parse context containing the templated query.
      */
     @Override
     @Nullable
-    public TemplateQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException {
         XContentParser parser = parseContext.parser();
         Template template = parse(parser, parseContext.parseFieldMatcher());
-        return new TemplateQueryBuilder(template);
+        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, SearchContext.current());
+
+        BytesReference querySource = (BytesReference) executable.run();
+
+        try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            return context.parseInnerQuery();
+        }
     }
 
     public static Template parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String... parameters) throws IOException {
+
         Map<String, ScriptService.ScriptType> parameterMap = new HashMap<>(parametersToTypes);
         for (String parameter : parameters) {
             parameterMap.put(parameter, ScriptService.ScriptType.INLINE);
@@ -86,9 +114,4 @@ public class TemplateQueryParser extends BaseQueryParser<TemplateQueryBuilder> {
     public static Template parse(XContentParser parser, Map<String, ScriptService.ScriptType> parameterMap, ParseFieldMatcher parseFieldMatcher) throws IOException {
         return Template.parse(parser, parameterMap, parseFieldMatcher);
     }
-
-    @Override
-    public TemplateQueryBuilder getBuilderPrototype() {
-        return TemplateQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
index bed373b..5bd911a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
@@ -19,77 +19,128 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 /**
  * A Query that matches documents containing a term.
  */
-public class TermQueryBuilder extends BaseTermQueryBuilder<TermQueryBuilder> {
+public class TermQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermQueryBuilder> {
 
-    public static final String NAME = "term";
-    static final TermQueryBuilder PROTOTYPE = new TermQueryBuilder("name", "value");
+    private final String name;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
-    public TermQueryBuilder(String fieldName, String value) {
-        super(fieldName, (Object) value);
+    private final Object value;
+
+    private float boost = -1;
+
+    private String queryName;
+
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
-    public TermQueryBuilder(String fieldName, int value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
-    public TermQueryBuilder(String fieldName, long value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
-    public TermQueryBuilder(String fieldName, float value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
-    public TermQueryBuilder(String fieldName, double value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, boolean) */
-    public TermQueryBuilder(String fieldName, boolean value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public TermQueryBuilder(String fieldName, Object value) {
-        super(fieldName, value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            query = mapper.termQuery(this.value, context);
-        }
-        if (query == null) {
-            query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value)));
-        }
-        return query;
+    public TermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected TermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermQueryParser.NAME);
+        if (boost == -1 && queryName == null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
index ee52d48..335a22f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
@@ -19,34 +19,45 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for the term query
+ *
  */
-public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
+public class TermQueryParser implements QueryParser {
+
+    public static final String NAME = "term";
 
     private static final ParseField NAME_FIELD = new ParseField("_name").withAllDeprecated("query name is not supported in short version of term query");
     private static final ParseField BOOST_FIELD = new ParseField("boost").withAllDeprecated("boost is not supported in short version of term query");
 
+    @Inject
+    public TermQueryParser() {
+    }
+
     @Override
     public String[] names() {
-        return new String[]{TermQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         String fieldName = null;
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -94,16 +105,22 @@ public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
             }
         }
 
-        TermQueryBuilder termQuery = new TermQueryBuilder(fieldName, value);
-        termQuery.boost(boost);
-        if (queryName != null) {
-            termQuery.queryName(queryName);
+        if (value == null) {
+            throw new ParsingException(parseContext, "No value specified for term query");
         }
-        return termQuery;
-    }
 
-    @Override
-    public TermQueryBuilder getBuilderPrototype() {
-        return TermQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.termQuery(value, parseContext);
+        }
+        if (query == null) {
+            query = new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
index a074e2a..4bdd0da 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
@@ -19,20 +19,93 @@
 
 package org.elasticsearch.index.query;
 
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
- * @deprecated use {@link TermsQueryBuilder} instead.
+ * A filer for a field based on several terms matching on any of them.
  */
-@Deprecated
-public class TermsLookupQueryBuilder extends TermsQueryBuilder {
+public class TermsLookupQueryBuilder extends QueryBuilder {
+
+    private final String name;
+    private String lookupIndex;
+    private String lookupType;
+    private String lookupId;
+    private String lookupRouting;
+    private String lookupPath;
+
+    private String queryName;
 
     public TermsLookupQueryBuilder(String name) {
-        super(name, (Object[]) null);
+        this.name = name;
+    }
+
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermsLookupQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * Sets the index name to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupIndex(String lookupIndex) {
+        this.lookupIndex = lookupIndex;
+        return this;
+    }
+
+    /**
+     * Sets the index type to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupType(String lookupType) {
+        this.lookupType = lookupType;
+        return this;
+    }
+
+    /**
+     * Sets the doc id to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupId(String lookupId) {
+        this.lookupId = lookupId;
+        return this;
+    }
+
+    /**
+     * Sets the path within the document to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupPath(String lookupPath) {
+        this.lookupPath = lookupPath;
+        return this;
+    }
+
+    public TermsLookupQueryBuilder lookupRouting(String lookupRouting) {
+        this.lookupRouting = lookupRouting;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return TermsQueryBuilder.NAME;
-   }
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermsQueryParser.NAME);
+
+        builder.startObject(name);
+        if (lookupIndex != null) {
+            builder.field("index", lookupIndex);
+        }
+        builder.field("type", lookupType);
+        builder.field("id", lookupId);
+        if (lookupRouting != null) {
+            builder.field("routing", lookupRouting);
+        }
+        builder.field("path", lookupPath);
+        builder.endObject();
+
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+        builder.endObject();
+    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
index c913d80..ca54eb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
@@ -19,160 +19,101 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.support.XContentMapValues;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.*;
-import java.util.stream.Collectors;
-import java.util.stream.IntStream;
 
 /**
- * A filter for a field based on several terms matching on any of them.
+ * A filer for a field based on several terms matching on any of them.
  */
-public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
+public class TermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermsQueryBuilder> {
 
-    public static final String NAME = "terms";
+    private final String name;
 
-    static final TermsQueryBuilder PROTOTYPE = new TermsQueryBuilder("field", "value");
+    private final Object values;
 
-    public static final boolean DEFAULT_DISABLE_COORD = false;
-
-    private final String fieldName;
-    private final List<Object> values;
-    @Deprecated
     private String minimumShouldMatch;
-    @Deprecated
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
-    private final TermsLookup termsLookup;
 
-    public TermsQueryBuilder(String fieldName, TermsLookup termsLookup) {
-        this(fieldName, null, null, DEFAULT_DISABLE_COORD, termsLookup);
-    }
+    private Boolean disableCoord;
 
-    /**
-     * constructor used internally for serialization of both value / termslookup variants
-     */
-    TermsQueryBuilder(String fieldName, List<Object> values, String minimumShouldMatch, boolean disableCoord, TermsLookup termsLookup) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name cannot be null.");
-        }
-        if (values == null && termsLookup == null) {
-            throw new IllegalArgumentException("No value or termsLookup specified for terms query");
-        }
-        if (values != null && termsLookup != null) {
-            throw new IllegalArgumentException("Both values and termsLookup specified for terms query");
-        }
-        this.fieldName = fieldName;
-        this.values = values;
-        this.disableCoord = disableCoord;
-        this.minimumShouldMatch = minimumShouldMatch;
-        this.termsLookup = termsLookup;
-    }
+    private String queryName;
+
+    private float boost = -1;
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, String... values) {
-        this(fieldName, values != null ? Arrays.asList(values) : null);
+    public TermsQueryBuilder(String name, String... values) {
+        this(name, (Object[]) values);
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, int... values) {
-        this(fieldName, values != null ? Arrays.stream(values).mapToObj(s -> s).collect(Collectors.toList()) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, int... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, long... values) {
-        this(fieldName, values != null ? Arrays.stream(values).mapToObj(s -> s).collect(Collectors.toList()) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, long... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, float... values) {
-        this(fieldName, values != null ? IntStream.range(0, values.length)
-                           .mapToObj(i -> values[i]).collect(Collectors.toList()) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, float... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, double... values) {
-        this(fieldName, values != null ? Arrays.stream(values).mapToObj(s -> s).collect(Collectors.toList()) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, double... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, Object... values) {
-        this(fieldName, values != null ? Arrays.asList(values) : (Iterable<?>) null);
+    public TermsQueryBuilder(String name, Object... values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
-     * @param fieldName The field name
+     * @param name   The field name
      * @param values The terms
      */
-    public TermsQueryBuilder(String fieldName, Iterable<?> values) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name cannot be null.");
-        }
-        if (values == null) {
-            throw new IllegalArgumentException("No value specified for terms query");
-        }
-        this.fieldName = fieldName;
-        this.values = convertToBytesRefListIfStringList(values);
-        this.termsLookup = null;
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public List<Object> values() {
-        return convertToStringListIfBytesRefList(this.values);
+    public TermsQueryBuilder(String name, Iterable values) {
+        this.name = name;
+        this.values = values;
     }
 
     /**
@@ -185,10 +126,6 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
         return this;
     }
 
-    public String minimumShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Disables <tt>Similarity#coord(int,int)</tt> in scoring. Defaults to <tt>false</tt>.
      * @deprecated use [bool] query instead
@@ -199,174 +136,41 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
         return this;
     }
 
-    boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    public TermsLookup termsLookup() {
-        return this.termsLookup;
-    }
-
     /**
-     * Same as {@link #convertToBytesRefIfString} but on Iterable.
-     * @param objs the Iterable of input object
-     * @return the same input or a list of {@link BytesRef} representation if input was a list of type string
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    private static List<Object> convertToBytesRefListIfStringList(Iterable<?> objs) {
-        if (objs == null) {
-            return null;
-        }
-        List<Object> newObjs = new ArrayList<>();
-        for (Object obj : objs) {
-            newObjs.add(convertToBytesRefIfString(obj));
-        }
-        return newObjs;
+    public TermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
-    /**
-     * Same as {@link #convertToStringIfBytesRef} but on Iterable.
-     * @param objs the Iterable of input object
-     * @return the same input or a list of utf8 string if input was a list of type {@link BytesRef}
-     */
-    private static List<Object> convertToStringListIfBytesRefList(Iterable<?> objs) {
-        if (objs == null) {
-            return null;
-        }
-        List<Object> newObjs = new ArrayList<>();
-        for (Object obj : objs) {
-            newObjs.add(convertToStringIfBytesRef(obj));
-        }
-        return newObjs;
+    @Override
+    public TermsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        if (this.termsLookup != null) {
-            builder.startObject(fieldName);
-            termsLookup.toXContent(builder, params);
-            builder.endObject();
-        } else {
-            builder.field(fieldName, convertToStringListIfBytesRefList(values));
-        }
+        builder.startObject(TermsQueryParser.NAME);
+        builder.field(name, values);
+
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        if (disableCoord != DEFAULT_DISABLE_COORD) {
-            builder.field("disable_coord", disableCoord);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        List<Object> terms;
-        if (this.termsLookup != null) {
-            if (termsLookup.index() == null) {
-                termsLookup.index(context.index().name());
-            }
-            Client client = context.getClient();
-            terms = fetch(termsLookup, client);
-        } else {
-            terms = values;
-        }
-        if (terms == null || terms.isEmpty()) {
-            return Queries.newMatchNoDocsQuery();
-        }
-        return handleTermsQuery(terms, fieldName, context, minimumShouldMatch, disableCoord);
-    }
-
-    private List<Object> fetch(TermsLookup termsLookup, Client client) {
-        List<Object> terms = new ArrayList<>();
-        GetRequest getRequest = new GetRequest(termsLookup.index(), termsLookup.type(), termsLookup.id())
-                .preference("_local").routing(termsLookup.routing());
-        getRequest.copyContextAndHeadersFrom(SearchContext.current());
-        final GetResponse getResponse = client.get(getRequest).actionGet();
-        if (getResponse.isExists()) {
-            List<Object> extractedValues = XContentMapValues.extractRawValues(termsLookup.path(), getResponse.getSourceAsMap());
-            terms.addAll(extractedValues);
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
         }
-        return terms;
-    }
 
-    private static Query handleTermsQuery(List<Object> terms, String fieldName, QueryShardContext context, String minimumShouldMatch, boolean disableCoord) {
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        String indexFieldName;
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-        } else {
-            indexFieldName = fieldName;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
 
-        Query query;
-        if (context.isFilter()) {
-            if (fieldType != null) {
-                query = fieldType.termsQuery(terms, context);
-            } else {
-                BytesRef[] filterValues = new BytesRef[terms.size()];
-                for (int i = 0; i < filterValues.length; i++) {
-                    filterValues[i] = BytesRefs.toBytesRef(terms.get(i));
-                }
-                query = new TermsQuery(indexFieldName, filterValues);
-            }
-        } else {
-            BooleanQuery.Builder bq = new BooleanQuery.Builder();
-            bq.setDisableCoord(disableCoord);
-            for (Object term : terms) {
-                if (fieldType != null) {
-                    bq.add(fieldType.termQuery(term, context), BooleanClause.Occur.SHOULD);
-                } else {
-                    bq.add(new TermQuery(new Term(indexFieldName, BytesRefs.toBytesRef(term))), BooleanClause.Occur.SHOULD);
-                }
-            }
-            query = Queries.applyMinimumShouldMatch(bq.build(), minimumShouldMatch);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return query;
-    }
 
-    @SuppressWarnings("unchecked")
-    @Override
-    protected TermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        String field = in.readString();
-        TermsLookup lookup = null;
-        if (in.readBoolean()) {
-            lookup = TermsLookup.readTermsLookupFrom(in);
-        }
-        List<Object> values = (List<Object>) in.readGenericValue();
-        String minimumShouldMatch = in.readOptionalString();
-        boolean disableCoord = in.readBoolean();
-        return new TermsQueryBuilder(field, values, minimumShouldMatch, disableCoord, lookup);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeBoolean(termsLookup != null);
-        if (termsLookup != null) {
-            termsLookup.writeTo(out);
-        }
-        out.writeGenericValue(values);
-        out.writeOptionalString(minimumShouldMatch);
-        out.writeBoolean(disableCoord);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, values, minimumShouldMatch, disableCoord, termsLookup);
-    }
-
-    @Override
-    protected boolean doEquals(TermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(values, other.values) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(termsLookup, other.termsLookup);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
index e291c1a..898b0ff 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
@@ -19,50 +19,76 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.client.Client;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.support.XContentMapValues;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.indices.cache.query.terms.TermsLookup;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
 /**
- * Parser for terms query and terms lookup.
  *
- * Filters documents that have fields that match any of the provided terms (not analyzed)
- *
- * It also supports a terms lookup mechanism which can be used to fetch the term values from
- * a document in an index.
  */
-public class TermsQueryParser extends BaseQueryParser {
+public class TermsQueryParser implements QueryParser {
 
-    private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match", "minimum_should_match")
-            .withAllDeprecated("Use [bool] query instead");
+    public static final String NAME = "terms";
+    private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match").withAllDeprecated("Use [bool] query instead");
     private static final ParseField DISABLE_COORD_FIELD = new ParseField("disable_coord").withAllDeprecated("Use [bool] query instead");
     private static final ParseField EXECUTION_FIELD = new ParseField("execution").withAllDeprecated("execution is deprecated and has no effect");
+    private Client client;
+
+    @Inject
+    public TermsQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{TermsQueryBuilder.NAME, "in"};
+        return new String[]{NAME, "in"};
+    }
+
+    @Inject(optional = true)
+    public void setClient(Client client) {
+        this.client = client;
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
-        String fieldName = null;
-        List<Object> values = null;
+        String queryName = null;
+        String currentFieldName = null;
+
+        String lookupIndex = parseContext.index().name();
+        String lookupType = null;
+        String lookupId = null;
+        String lookupPath = null;
+        String lookupRouting = null;
         String minShouldMatch = null;
-        boolean disableCoord = TermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        TermsLookup termsLookup = null;
 
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
 
         XContentParser.Token token;
-        String currentFieldName = null;
+        List<Object> terms = new ArrayList<>();
+        String fieldName = null;
+        float boost = 1f;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -73,16 +99,51 @@ public class TermsQueryParser extends BaseQueryParser {
                     throw new ParsingException(parseContext, "[terms] query does not support multiple fields");
                 }
                 fieldName = currentFieldName;
-                values = parseValues(parseContext, parser);
+
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    Object value = parser.objectBytes();
+                    if (value == null) {
+                        throw new ParsingException(parseContext, "No value specified for terms query");
+                    }
+                    terms.add(value);
+                }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 fieldName = currentFieldName;
-                termsLookup = TermsLookup.parseTermsLookup(parseContext, parser);
+                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                    if (token == XContentParser.Token.FIELD_NAME) {
+                        currentFieldName = parser.currentName();
+                    } else if (token.isValue()) {
+                        if ("index".equals(currentFieldName)) {
+                            lookupIndex = parser.text();
+                        } else if ("type".equals(currentFieldName)) {
+                            lookupType = parser.text();
+                        } else if ("id".equals(currentFieldName)) {
+                            lookupId = parser.text();
+                        } else if ("path".equals(currentFieldName)) {
+                            lookupPath = parser.text();
+                        } else if ("routing".equals(currentFieldName)) {
+                            lookupRouting = parser.textOrNull();
+                        } else {
+                            throw new ParsingException(parseContext, "[terms] query does not support [" + currentFieldName
+                                    + "] within lookup element");
+                        }
+                    }
+                }
+                if (lookupType == null) {
+                    throw new ParsingException(parseContext, "[terms] query lookup element requires specifying the type");
+                }
+                if (lookupId == null) {
+                    throw new ParsingException(parseContext, "[terms] query lookup element requires specifying the id");
+                }
+                if (lookupPath == null) {
+                    throw new ParsingException(parseContext, "[terms] query lookup element requires specifying the path");
+                }
             } else if (token.isValue()) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, EXECUTION_FIELD)) {
                     // ignore
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, MIN_SHOULD_MATCH_FIELD)) {
                     if (minShouldMatch != null) {
-                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + TermsQueryBuilder.NAME + "] query");
+                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + NAME + "] query");
                     }
                     minShouldMatch = parser.textOrNull();
                 } else if ("boost".equals(currentFieldName)) {
@@ -98,28 +159,57 @@ public class TermsQueryParser extends BaseQueryParser {
         }
 
         if (fieldName == null) {
-            throw new ParsingException(parseContext, "terms query requires a field name, followed by array of terms or a document lookup specification");
+            throw new ParsingException(parseContext, "terms query requires a field name, followed by array of terms");
         }
-        return new TermsQueryBuilder(fieldName, values, minShouldMatch, disableCoord, termsLookup)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    private static List<Object> parseValues(QueryParseContext parseContext, XContentParser parser) throws IOException {
-        List<Object> values = new ArrayList<>();
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-            Object value = parser.objectBytes();
-            if (value == null) {
-                throw new ParsingException(parseContext, "No value specified for terms query");
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+        }
+
+        if (lookupId != null) {
+            final TermsLookup lookup = new TermsLookup(lookupIndex, lookupType, lookupId, lookupRouting, lookupPath, parseContext);
+            GetRequest getRequest = new GetRequest(lookup.getIndex(), lookup.getType(), lookup.getId()).preference("_local").routing(lookup.getRouting());
+            getRequest.copyContextAndHeadersFrom(SearchContext.current());
+            final GetResponse getResponse = client.get(getRequest).actionGet();
+            if (getResponse.isExists()) {
+                List<Object> values = XContentMapValues.extractRawValues(lookup.getPath(), getResponse.getSourceAsMap());
+                terms.addAll(values);
             }
-            values.add(value);
         }
-        return values;
-    }
 
-    @Override
-    public TermsQueryBuilder getBuilderPrototype() {
-        return TermsQueryBuilder.PROTOTYPE;
+        if (terms.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        Query query;
+        if (parseContext.isFilter()) {
+            if (fieldType != null) {
+                query = fieldType.termsQuery(terms, parseContext);
+            } else {
+                BytesRef[] filterValues = new BytesRef[terms.size()];
+                for (int i = 0; i < filterValues.length; i++) {
+                    filterValues[i] = BytesRefs.toBytesRef(terms.get(i));
+                }
+                query = new TermsQuery(fieldName, filterValues);
+            }
+        } else {
+            BooleanQuery.Builder bq = new BooleanQuery.Builder();
+            bq.setDisableCoord(disableCoord);
+            for (Object term : terms) {
+                if (fieldType != null) {
+                    bq.add(fieldType.termQuery(term, parseContext), Occur.SHOULD);
+                } else {
+                    bq.add(new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(term))), Occur.SHOULD);
+                }
+            }
+            query = Queries.applyMinimumShouldMatch(bq.build(), minShouldMatch);
+        }
+        query.setBoost(boost);
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
index 94cdc24..2a9a6c5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
@@ -19,89 +19,22 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class TypeQueryBuilder extends AbstractQueryBuilder<TypeQueryBuilder> {
+public class TypeQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "type";
-
-    private final BytesRef type;
-
-    static final TypeQueryBuilder PROTOTYPE = new TypeQueryBuilder("type");
+    private final String type;
 
     public TypeQueryBuilder(String type) {
-        if (type == null) {
-            throw new IllegalArgumentException("[type] cannot be null");
-        }
-        this.type = BytesRefs.toBytesRef(type);
-    }
-
-    TypeQueryBuilder(BytesRef type) {
-        if (type == null) {
-            throw new IllegalArgumentException("[type] cannot be null");
-        }
         this.type = type;
     }
 
-    public String type() {
-        return BytesRefs.toString(this.type);
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("value", type.utf8ToString());
-        printBoostAndQueryName(builder);
+        builder.startObject(TypeQueryParser.NAME);
+        builder.field("value", type);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query filter;
-        //LUCENE 4 UPGRADE document mapper should use bytesref as well?
-        DocumentMapper documentMapper = context.mapperService().documentMapper(type.utf8ToString());
-        if (documentMapper == null) {
-            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
-        } else {
-            filter = documentMapper.typeFilter();
-        }
-        return filter;
-    }
-
-    @Override
-    protected TypeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new TypeQueryBuilder(in.readBytesRef());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeBytesRef(type);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(type);
-    }
-
-    @Override
-    protected boolean doEquals(TypeQueryBuilder other) {
-        return Objects.equals(type, other.type);
-    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
index b0a26aa..cf834be 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
@@ -19,59 +19,59 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
 
-/**
- * Parser for type query
- */
-public class TypeQueryParser extends BaseQueryParser<TypeQueryBuilder> {
+public class TypeQueryParser implements QueryParser {
+
+    public static final String NAME = "type";
+
+    @Inject
+    public TypeQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{TypeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TypeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
-        BytesRef type = null;
-
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
-        String currentFieldName = null;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if ("value".equals(currentFieldName)) {
-                    type = parser.utf8Bytes();
-                }
-            } else {
-                throw new ParsingException(parseContext, "[type] filter doesn't support [" + currentFieldName + "]");
-            }
+        XContentParser.Token token = parser.nextToken();
+        if (token != XContentParser.Token.FIELD_NAME) {
+            throw new ParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-
-        if (type == null) {
-            throw new ParsingException(parseContext, "[type] filter needs to be provided with a value for the type");
+        String fieldName = parser.currentName();
+        if (!fieldName.equals("value")) {
+            throw new ParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-        return new TypeQueryBuilder(type)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        token = parser.nextToken();
+        if (token != XContentParser.Token.VALUE_STRING) {
+            throw new ParsingException(parseContext, "[type] filter should have a value field, and the type name");
+        }
+        BytesRef type = parser.utf8Bytes();
+        // move to the next token
+        parser.nextToken();
 
-    @Override
-    public TypeQueryBuilder getBuilderPrototype() {
-        return TypeQueryBuilder.PROTOTYPE;
+        Query filter;
+        //LUCENE 4 UPGRADE document mapper should use bytesref as well? 
+        DocumentMapper documentMapper = parseContext.mapperService().documentMapper(type.utf8ToString());
+        if (documentMapper == null) {
+            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
+        } else {
+            filter = documentMapper.typeFilter();
+        }
+        return filter;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
index 4477592..654f14e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
@@ -19,20 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -42,17 +31,17 @@ import java.util.Objects;
  * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
  * <tt>?</tt>.
  */
-public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuilder> implements MultiTermQueryBuilder<WildcardQueryBuilder> {
+public class WildcardQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<WildcardQueryBuilder> {
 
-    public static final String NAME = "wildcard";
+    private final String name;
 
-    private final String fieldName;
+    private final String wildcard;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final WildcardQueryBuilder PROTOTYPE = new WildcardQueryBuilder("field", "value");
+    private String queryName;
 
     /**
      * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -62,26 +51,12 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
      * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
      * <tt>?</tt>.
      *
-     * @param fieldName The field name
-     * @param value The wildcard query string
+     * @param name     The field name
+     * @param wildcard The wildcard query string
      */
-    public WildcardQueryBuilder(String fieldName, String value) {
-        if (Strings.isEmpty(fieldName)) {
-            throw new IllegalArgumentException("field name is null or empty");
-        }
-        if (value == null) {
-            throw new IllegalArgumentException("value cannot be null.");
-        }
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    public String fieldName() {
-        return fieldName;
-    }
-
-    public String value() {
-        return value;
+    public WildcardQueryBuilder(String name, String wildcard) {
+        this.name = name;
+        this.wildcard = wildcard;
     }
 
     public WildcardQueryBuilder rewrite(String rewrite) {
@@ -89,71 +64,43 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
-    }
-
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public WildcardQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("wildcard", value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public WildcardQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String indexFieldName;
-        BytesRef valueBytes;
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-            valueBytes = fieldType.indexedValueForSearch(value);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(WildcardQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, wildcard);
         } else {
-            indexFieldName = fieldName;
-            valueBytes = new BytesRef(value);
+            builder.startObject(name);
+            builder.field("wildcard", wildcard);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-
-        WildcardQuery query = new WildcardQuery(new Term(indexFieldName, valueBytes));
-        MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-        QueryParsers.setRewriteMethod(query, rewriteMethod);
-        return query;
-    }
-
-    @Override
-    protected WildcardQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(in.readString(), in.readString());
-        wildcardQueryBuilder.rewrite = in.readOptionalString();
-        return wildcardQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(WildcardQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
index 31f7206..ee1a42c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
@@ -19,24 +19,36 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for wildcard query
+ *
  */
-public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
+public class WildcardQueryParser implements QueryParser {
+
+    public static final String NAME = "wildcard";
+
+    @Inject
+    public WildcardQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{WildcardQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public WildcardQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -44,10 +56,10 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
             throw new ParsingException(parseContext, "[wildcard] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -63,7 +75,7 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = parser.textOrNull();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -80,14 +92,22 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
         if (value == null) {
             throw new ParsingException(parseContext, "No value specified for prefix query");
         }
-        return new WildcardQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public WildcardQueryBuilder getBuilderPrototype() {
-        return WildcardQueryBuilder.PROTOTYPE;
+        BytesRef valueBytes;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        } else {
+            valueBytes = new BytesRef(value);
+        }
+
+        WildcardQuery wildcardQuery = new WildcardQuery(new Term(fieldName, valueBytes));
+        QueryParsers.setRewriteMethod(wildcardQuery, parseContext.parseFieldMatcher(), rewriteMethod);
+        wildcardQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, wildcardQuery);
+        }
+        return wildcardQuery;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
index 8fdcd02..e7de5fd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
@@ -19,125 +19,63 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-
 import java.nio.charset.StandardCharsets;
-
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
-import java.util.Arrays;
 
 /**
  * A Query builder which allows building a query given JSON string or binary data provided as input. This is useful when you want
  * to use the Java Builder API but still have JSON query strings at hand that you want to combine with other
  * query builders.
- * <p/>
+ * <p>
  * Example usage in a boolean query :
  * <pre>
- * {@code
+ * <code>
  *      BoolQueryBuilder bool = new BoolQueryBuilder();
  *      bool.must(new WrapperQueryBuilder("{\"term\": {\"field\":\"value\"}}");
  *      bool.must(new TermQueryBuilder("field2","value2");
- * }
+ * </code>
  * </pre>
  */
-public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilder> {
+public class WrapperQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "wrapper";
     private final byte[] source;
-    static final WrapperQueryBuilder PROTOTYPE = new WrapperQueryBuilder((byte[]) new byte[]{0});
+    private final int offset;
+    private final int length;
 
     /**
-     * Creates a query builder given a query provided as a bytes array
+     * Creates a query builder given a query provided as a string
      */
-    public WrapperQueryBuilder(byte[] source) {
-        if (source == null || source.length == 0) {
-            throw new IllegalArgumentException("query source text cannot be null or empty");
-        }
-        this.source = source;
+    public WrapperQueryBuilder(String source) {
+        this.source = source.getBytes(StandardCharsets.UTF_8);
+        this.offset = 0;
+        this.length = this.source.length;
     }
 
     /**
-     * Creates a query builder given a query provided as a string
+     * Creates a query builder given a query provided as a bytes array
      */
-    public WrapperQueryBuilder(String source) {
-        if (Strings.isEmpty(source)) {
-            throw new IllegalArgumentException("query source string cannot be null or empty");
-        }
-        this.source = source.getBytes(StandardCharsets.UTF_8);
+    public WrapperQueryBuilder(byte[] source, int offset, int length) {
+        this.source = source;
+        this.offset = offset;
+        this.length = length;
     }
 
     /**
      * Creates a query builder given a query provided as a {@link BytesReference}
      */
     public WrapperQueryBuilder(BytesReference source) {
-        if (source == null || source.length() == 0) {
-            throw new IllegalArgumentException("query source text cannot be null or empty");
-        }
         this.source = source.array();
-    }
-
-    public byte[] source() {
-        return this.source;
-    }
-
-    @Override
-    public String getName() {
-        return NAME;
+        this.offset = source.arrayOffset();
+        this.length = source.length();
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query", source);
+        builder.startObject(WrapperQueryParser.NAME);
+        builder.field("query", source, offset, length);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        try (XContentParser qSourceParser = XContentFactory.xContent(source).createParser(source)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            QueryBuilder result = contextCopy.parseContext().parseInnerQueryBuilder();
-            context.combineNamedQueries(contextCopy);
-            return result.toQuery(context);
-        }
-    }
-
-    @Override
-    protected void setFinalBoost(Query query) {
-        //no-op this query doesn't support boost
-    }
-
-    @Override
-    protected WrapperQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new WrapperQueryBuilder(in.readByteArray());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeByteArray(this.source);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Arrays.hashCode(source);
-    }
-
-    @Override
-    protected boolean doEquals(WrapperQueryBuilder other) {
-        return Arrays.equals(source, other.source);   // otherwise we compare pointers
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
index d0cadf3..d64d8c1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
@@ -19,8 +19,10 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -28,15 +30,21 @@ import java.io.IOException;
 /**
  * Query parser for JSON Queries.
  */
-public class WrapperQueryParser extends BaseQueryParser {
+public class WrapperQueryParser implements QueryParser {
+
+    public static final String NAME = "wrapper";
+
+    @Inject
+    public WrapperQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{WrapperQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -49,18 +57,14 @@ public class WrapperQueryParser extends BaseQueryParser {
         }
         parser.nextToken();
 
-        byte[] source = parser.binaryValue();
-
-        parser.nextToken();
-
-        if (source == null) {
-            throw new ParsingException(parseContext, "wrapper query has no [query] specified");
+        byte[] querySource = parser.binaryValue();
+        try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            Query result = context.parseInnerQuery();
+            parser.nextToken();
+            parseContext.combineNamedQueries(context);
+            return result;
         }
-        return new WrapperQueryBuilder(source);
-    }
-
-    @Override
-    public WrapperQueryBuilder getBuilderPrototype() {
-        return WrapperQueryBuilder.PROTOTYPE;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
index f210aa1..6e74959 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionParser;
@@ -60,14 +60,14 @@ import java.util.Locale;
  * This parser parses this kind of input
  *
  * <pre>
- * {@code}
+ * <code>
  * {
  *      "fieldname1" : {
  *          "origin" = "someValue",
  *          "scale" = "someValue"
  *      }
  *
- * }
+ * </code>
  * </pre>
  *
  * "origin" here refers to the reference point and "scale" to the level of
@@ -106,7 +106,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
      * Parses bodies of the kind
      *
      * <pre>
-     * {@code}
+     * <code>
      * {
      *      "fieldname1" : {
      *          "origin" = "someValue",
@@ -114,11 +114,12 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
      *      }
      *
      * }
+     * </code>
      * </pre>
      *
      * */
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, ParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException {
         String currentFieldName;
         XContentParser.Token token;
         AbstractDistanceScoreFunction scoreFunction;
@@ -131,7 +132,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             if (token == XContentParser.Token.START_OBJECT) {
                 variableContent.copyCurrentStructure(parser);
                 fieldName = currentFieldName;
-            } else if (context.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
                 multiValueMode = parser.text();
             } else {
                 throw new ElasticsearchParseException("malformed score function score parameters.");
@@ -141,34 +142,34 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             throw new ElasticsearchParseException("malformed score function score parameters.");
         }
         XContentParser variableParser = XContentFactory.xContent(variableContent.string()).createParser(variableContent.string());
-        scoreFunction = parseVariable(fieldName, variableParser, context, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
+        scoreFunction = parseVariable(fieldName, variableParser, parseContext, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
         return scoreFunction;
     }
 
     // parses origin and scale parameter for field "fieldName"
-    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryShardContext context, MultiValueMode mode) throws IOException {
+    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryParseContext parseContext, MultiValueMode mode) throws IOException {
 
         // now, the field must exist, else we cannot read the value for
         // the doc later
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
-            throw new ParsingException(context.parseContext(), "unknown field [{}]", fieldName);
+            throw new ParsingException(parseContext, "unknown field [{}]", fieldName);
         }
 
         // dates and time need special handling
         parser.nextToken();
         if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return parseDateVariable(fieldName, parser, context, (DateFieldMapper.DateFieldType) fieldType, mode);
+            return parseDateVariable(fieldName, parser, parseContext, (DateFieldMapper.DateFieldType) fieldType, mode);
         } else if (fieldType instanceof GeoPointFieldMapper.GeoPointFieldType) {
-            return parseGeoVariable(fieldName, parser, context, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
+            return parseGeoVariable(fieldName, parser, parseContext, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
         } else if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return parseNumberVariable(fieldName, parser, context, (NumberFieldMapper.NumberFieldType) fieldType, mode);
+            return parseNumberVariable(fieldName, parser, parseContext, (NumberFieldMapper.NumberFieldType) fieldType, mode);
         } else {
-            throw new ParsingException(context.parseContext(), "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
+            throw new ParsingException(parseContext, "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
         }
     }
 
-    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             NumberFieldMapper.NumberFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -198,11 +199,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         if (!scaleFound || !refFound) {
             throw new ElasticsearchParseException("both [{}] and [{}] must be set for numeric fields.", DecayFunctionBuilder.SCALE, DecayFunctionBuilder.ORIGIN);
         }
-        IndexNumericFieldData numericFieldData = context.getForField(fieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(fieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
-    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             GeoPointFieldMapper.GeoPointFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -230,12 +231,12 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
         double scale = DistanceUnit.DEFAULT.parse(scaleString, DistanceUnit.DEFAULT);
         double offset = DistanceUnit.DEFAULT.parse(offsetString, DistanceUnit.DEFAULT);
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         return new GeoFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), indexFieldData, mode);
 
     }
 
-    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             DateFieldMapper.DateFieldType dateFieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -270,7 +271,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         double scale = val.getMillis();
         val = TimeValue.parseTimeValue(offsetString, TimeValue.timeValueHours(24), getClass().getSimpleName() + ".offset");
         double offset = val.getMillis();
-        IndexNumericFieldData numericFieldData = context.getForField(dateFieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(dateFieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
index e1726f9..dec90b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
@@ -21,7 +21,7 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
 import java.io.IOException;
@@ -31,10 +31,12 @@ import java.util.ArrayList;
  * A query that uses a filters with a script associated with them to compute the
  * score.
  */
-public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScoreQueryBuilder> {
+public class FunctionScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FunctionScoreQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
+    private Float boost;
+
     private Float maxBoost;
 
     private String scoreMode;
@@ -45,8 +47,6 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
     private ArrayList<ScoreFunctionBuilder> scoreFunctions = new ArrayList<>();
     private Float minScore = null;
 
-    static final FunctionScoreQueryBuilder PROTOTYPE = new FunctionScoreQueryBuilder();
-
     /**
      * Creates a function_score query that executes on documents that match query a query.
      * Query and filter will be wrapped into a filtered_query.
@@ -138,6 +138,17 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         return this;
     }
 
+    /**
+     * Sets the boost for this query. Documents matching this query will (in
+     * addition to the normal weightings) have their score multiplied by the
+     * boost provided.
+     */
+    @Override
+    public FunctionScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(FunctionScoreQueryParser.NAME);
@@ -166,10 +177,13 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         if (maxBoost != null) {
             builder.field("max_boost", maxBoost);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (minScore != null) {
             builder.field("min_score", minScore);
         }
-        printBoostAndQueryName(builder);
+
         builder.endObject();
     }
 
@@ -177,9 +191,4 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         this.minScore = minScore;
         return this;
     }
-
-    @Override
-    public String getWriteableName() {
-        return FunctionScoreQueryParser.NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
index 5c48f2d..d50b81f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
@@ -27,19 +27,20 @@ import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.lucene.search.function.*;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.common.ParsingException;
 
 import java.io.IOException;
 import java.util.ArrayList;
 
 /**
- * Parser for function_score query
+ *
  */
 public class FunctionScoreQueryParser implements QueryParser {
 
@@ -75,14 +76,12 @@ public class FunctionScoreQueryParser implements QueryParser {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
         XContentParser parser = parseContext.parser();
 
         Query query = null;
         Query filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
+        float boost = 1.0f;
 
         FiltersFunctionScoreQuery.ScoreMode scoreMode = FiltersFunctionScoreQuery.ScoreMode.Multiply;
         ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions = new ArrayList<>();
@@ -112,8 +111,6 @@ public class FunctionScoreQueryParser implements QueryParser {
                 maxBoost = parser.floatValue();
             } else if ("boost".equals(currentFieldName)) {
                 boost = parser.floatValue();
-            } else if ("_name".equals(currentFieldName)) {
-                queryName = parser.text();
             } else if ("min_score".equals(currentFieldName) || "minScore".equals(currentFieldName)) {
                 minScore = parser.floatValue();
             } else if ("functions".equals(currentFieldName)) {
@@ -121,7 +118,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     String errorString = "already found [" + singleFunctionName + "], now encountering [functions].";
                     handleMisplacedFunctionsDeclaration(errorString);
                 }
-                currentFieldName = parseFiltersAndFunctions(context, parser, filterFunctions, currentFieldName);
+                currentFieldName = parseFiltersAndFunctions(parseContext, parser, filterFunctions, currentFieldName);
                 functionArrayFound = true;
             } else {
                 ScoreFunction scoreFunction;
@@ -132,7 +129,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     // we try to parse a score function. If there is no score
                     // function for the current field name,
                     // functionParserMapper.get() will throw an Exception.
-                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(context, parser);
+                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(parseContext, parser);
                 }
                 if (functionArrayFound) {
                     String errorString = "already found [functions] array, now encountering [" + currentFieldName + "].";
@@ -163,7 +160,6 @@ public class FunctionScoreQueryParser implements QueryParser {
         if (maxBoost == null) {
             maxBoost = Float.MAX_VALUE;
         }
-        Query result;
         // handle cases where only one score function and no filter was
         // provided. In this case we create a FunctionScoreQuery.
         if (filterFunctions.size() == 0 || filterFunctions.size() == 1 && (filterFunctions.get(0).filter == null || Queries.isConstantMatchAllQuery(filterFunctions.get(0).filter))) {
@@ -172,8 +168,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 theQuery.setCombineFunction(combineFunction);
             }
+            theQuery.setBoost(boost);
             theQuery.setMaxBoost(maxBoost);
-            result = theQuery;
+            return theQuery;
             // in all other cases we create a FiltersFunctionScoreQuery.
         } else {
             FiltersFunctionScoreQuery functionScoreQuery = new FiltersFunctionScoreQuery(query, scoreMode,
@@ -181,22 +178,17 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 functionScoreQuery.setCombineFunction(combineFunction);
             }
-            result = functionScoreQuery;
+            functionScoreQuery.setBoost(boost);
+            return functionScoreQuery;
         }
-        result.setBoost(boost);
-        if (queryName != null) {
-            context.addNamedQuery(queryName, query);
-        }
-        return result;
     }
 
     private void handleMisplacedFunctionsDeclaration(String errorString) {
         throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, MISPLACED_FUNCTION_MESSAGE_PREFIX + errorString);
     }
 
-    private String parseFiltersAndFunctions(QueryShardContext context, XContentParser parser,
+    private String parseFiltersAndFunctions(QueryParseContext parseContext, XContentParser parser,
                                             ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions, String currentFieldName) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
             Query filter = null;
@@ -218,7 +210,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                             // functionParserMapper throws exception if parser
                             // non-existent
                             ScoreFunctionParser functionParser = functionParserMapper.get(parseContext, currentFieldName);
-                            scoreFunction = functionParser.parse(context, parser);
+                            scoreFunction = functionParser.parse(parseContext, parser);
                         }
                     }
                 }
@@ -265,16 +257,4 @@ public class FunctionScoreQueryParser implements QueryParser {
         }
         return cf;
     }
-
-    //norelease to be removed once all queries are moved over to extend BaseQueryParser
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-
-    @Override
-    public FunctionScoreQueryBuilder getBuilderPrototype() {
-        return FunctionScoreQueryBuilder.PROTOTYPE;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
index 5802c59..546dada 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
@@ -21,14 +21,14 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 
 import java.io.IOException;
 
 public interface ScoreFunctionParser {
 
-    ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, ParsingException;
+    ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException;
 
     /**
      * Returns the name of the function, for example "linear", "gauss" etc. This
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
index d16814d..4e81e11 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -52,8 +51,7 @@ public class FieldValueFactorFunctionParser implements ScoreFunctionParser {
     public static String[] NAMES = { "field_value_factor", "fieldValueFactor" };
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, ParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException {
 
         String currentFieldName = null;
         String field = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java
index ea2293c..22285f8 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java
@@ -51,7 +51,7 @@ public class RandomScoreFunctionBuilder extends ScoreFunctionBuilder {
 
     /**
      * seed variant taking a long value.
-     * @see {@link #seed(int)}
+     * @see #seed(int)
      */
     public RandomScoreFunctionBuilder seed(long seed) {
         this.seed = seed;
@@ -60,7 +60,7 @@ public class RandomScoreFunctionBuilder extends ScoreFunctionBuilder {
 
     /**
      * seed variant taking a String value.
-     * @see {@link #seed(int)}
+     * @see #seed(int)
      */
     public RandomScoreFunctionBuilder seed(String seed) {
         this.seed = seed;
@@ -78,4 +78,4 @@ public class RandomScoreFunctionBuilder extends ScoreFunctionBuilder {
         builder.endObject();
     }
 
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
index 01b18e0..621a087 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -50,8 +49,8 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, ParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException {
+
         int seed = -1;
 
         String currentFieldName = null;
@@ -89,7 +88,7 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
         }
 
         if (seed == -1) {
-            seed = hash(context.nowInMillis());
+            seed = hash(parseContext.nowInMillis());
         }
         final ShardId shardId = SearchContext.current().indexShard().shardId();
         final int salt = (shardId.index().name().hashCode() << 10) | shardId.id();
@@ -101,4 +100,4 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
     private static final int hash(long value) {
         return (int) (value ^ (value >>> 32));
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
index 5ae39ca..be77365 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScriptScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -57,8 +56,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, ParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, ParsingException {
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
         Script script = null;
         Map<String, Object> vars = null;
@@ -100,7 +98,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
 
         SearchScript searchScript;
         try {
-            searchScript = context.scriptService().search(context.lookup(), script, ScriptContext.Standard.SEARCH);
+            searchScript = parseContext.scriptService().search(parseContext.lookup(), script, ScriptContext.Standard.SEARCH);
             return new ScriptScoreFunction(script, searchScript);
         } catch (Exception e) {
             throw new ParsingException(parseContext, NAMES[0] + " the script could not be loaded", e);
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java b/core/src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java
new file mode 100644
index 0000000..48a2f59
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java
@@ -0,0 +1,376 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.support;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.highlight.HighlightBuilder;
+import org.elasticsearch.search.sort.SortBuilder;
+import org.elasticsearch.search.sort.SortOrder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ */
+@SuppressWarnings("unchecked")
+public abstract class BaseInnerHitBuilder<T extends BaseInnerHitBuilder> implements ToXContent {
+
+    protected SearchSourceBuilder sourceBuilder;
+
+    /**
+     * The index to start to return hits from. Defaults to <tt>0</tt>.
+     */
+    public T setFrom(int from) {
+        sourceBuilder().from(from);
+        return (T) this;
+    }
+
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public T setSize(int size) {
+        sourceBuilder().size(size);
+        return (T) this;
+    }
+
+    /**
+     * Applies when sorting, and controls if scores will be tracked as well. Defaults to
+     * <tt>false</tt>.
+     */
+    public T setTrackScores(boolean trackScores) {
+        sourceBuilder().trackScores(trackScores);
+        return (T) this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
+     * explanation of the hit (ranking).
+     */
+    public T setExplain(boolean explain) {
+        sourceBuilder().explain(explain);
+        return (T) this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
+     * version.
+     */
+    public T setVersion(boolean version) {
+        sourceBuilder().version(version);
+        return (T) this;
+    }
+
+    /**
+     * Add a stored field to be loaded and returned with the inner hit.
+     */
+    public T field(String name) {
+        sourceBuilder().field(name);
+        return (T) this;
+    }
+
+    /**
+     * Sets no fields to be loaded, resulting in only id and type to be returned per field.
+     */
+    public T setNoFields() {
+        sourceBuilder().noFields();
+        return (T) this;
+    }
+
+    /**
+     * Indicates whether the response should contain the stored _source for every hit
+     */
+    public T setFetchSource(boolean fetch) {
+        sourceBuilder().fetchSource(fetch);
+        return (T) this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
+     * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public T setFetchSource(@Nullable String include, @Nullable String exclude) {
+        sourceBuilder().fetchSource(include, exclude);
+        return (T) this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
+     * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public T setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
+        sourceBuilder().fetchSource(includes, excludes);
+        return (T) this;
+    }
+
+    /**
+     * Adds a field data based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name The field to get from the field data cache
+     */
+    public T addFieldDataField(String name) {
+        sourceBuilder().fieldDataField(name);
+        return (T) this;
+    }
+
+    /**
+     * Adds a script based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name   The name that will represent this value in the return hit
+     * @param script The script to use
+     */
+    public T addScriptField(String name, Script script) {
+        sourceBuilder().scriptField(name, script);
+        return (T) this;
+    }
+
+    /**
+     * Adds a sort against the given field name and the sort ordering.
+     *
+     * @param field The name of the field
+     * @param order The sort ordering
+     */
+    public T addSort(String field, SortOrder order) {
+        sourceBuilder().sort(field, order);
+        return (T) this;
+    }
+
+    /**
+     * Adds a generic sort builder.
+     *
+     * @see org.elasticsearch.search.sort.SortBuilders
+     */
+    public T addSort(SortBuilder sort) {
+        sourceBuilder().sort(sort);
+        return (T) this;
+    }
+
+    public HighlightBuilder highlightBuilder() {
+        return sourceBuilder().highlighter();
+    }
+
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public T addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return (T) this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public T addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return (T) this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public T addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return (T) this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public T addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                        int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return (T) this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public T addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return (T) this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public T setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return (T) this;
+    }
+
+    public T setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return (T) this;
+    }
+
+    public T setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return (T) this;
+    }
+
+    public T setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return (T) this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public T setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return (T) this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public T setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return (T) this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public T setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return (T) this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public T setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return (T) this;
+    }
+
+    public T setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return (T) this;
+    }
+
+    public T setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return (T) this;
+    }
+
+    public T setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return (T) this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public T setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return (T) this;
+    }
+
+    public T setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return (T) this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public T setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
+        return (T) this;
+    }
+
+    /**
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
+     */
+    public T setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
+        return (T) this;
+    }
+
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public T setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return (T) this;
+    }
+
+    public T setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return (T) this;
+    }
+
+    protected SearchSourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        if (sourceBuilder != null) {
+            sourceBuilder.innerToXContent(builder, params);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
index 02b21a1..fd6fc5a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
@@ -21,8 +21,6 @@ package org.elasticsearch.index.query.support;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
@@ -53,12 +51,13 @@ public class InnerHitsQueryParserHelper {
         this.fieldDataFieldsParseElement = fieldDataFieldsParseElement;
     }
 
-    public InnerHitsSubSearchContext parse(XContentParser parser) throws IOException {
+    public InnerHitsSubSearchContext parse(QueryParseContext parserContext) throws IOException, ParsingException {
         String fieldName = null;
         XContentParser.Token token;
         String innerHitName = null;
         SubSearchContext subSearchContext = new SubSearchContext(SearchContext.current());
         try {
+            XContentParser parser = parserContext.parser();
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token == XContentParser.Token.FIELD_NAME) {
                     fieldName = parser.currentName();
@@ -73,7 +72,7 @@ public class InnerHitsQueryParserHelper {
                 }
             }
         } catch (Exception e) {
-            throw new IOException("Failed to parse [_inner_hits]");
+            throw new ParsingException(parserContext, "Failed to parse [_inner_hits]", e);
         }
         return new InnerHitsSubSearchContext(innerHitName, subSearchContext);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
index 717fe3f..761341f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
@@ -28,9 +28,8 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -42,7 +41,6 @@ import java.io.IOException;
  */
 public class NestedInnerQueryParseSupport {
 
-    protected final QueryShardContext shardContext;
     protected final QueryParseContext parseContext;
 
     private BytesReference source;
@@ -62,15 +60,12 @@ public class NestedInnerQueryParseSupport {
     private ObjectMapper parentObjectMapper;
 
     public NestedInnerQueryParseSupport(XContentParser parser, SearchContext searchContext) {
-        parseContext = searchContext.queryParserService().getShardContext().parseContext();
-        shardContext = searchContext.queryParserService().getShardContext();
-        shardContext.reset(parser);
-
+        parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
     }
 
-    public NestedInnerQueryParseSupport(QueryShardContext context) {
-        this.parseContext = context.parseContext();
-        this.shardContext = context;
+    public NestedInnerQueryParseSupport(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void query() throws IOException {
@@ -108,10 +103,10 @@ public class NestedInnerQueryParseSupport {
             return innerQuery;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new ParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!queryFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new ParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             XContentParser old = parseContext.parser();
@@ -137,10 +132,10 @@ public class NestedInnerQueryParseSupport {
             return innerFilter;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new ParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!filterFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new ParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             setPathLevel();
@@ -160,12 +155,12 @@ public class NestedInnerQueryParseSupport {
 
     public void setPath(String path) {
         this.path = path;
-        nestedObjectMapper = shardContext.getObjectMapper(path);
+        nestedObjectMapper = parseContext.getObjectMapper(path);
         if (nestedObjectMapper == null) {
-            throw new QueryShardException(shardContext, "[nested] failed to find nested object under path [" + path + "]");
+            throw new ParsingException(parseContext, "[nested] failed to find nested object under path [" + path + "]");
         }
         if (!nestedObjectMapper.nested().isNested()) {
-            throw new QueryShardException(shardContext, "[nested] nested object under path [" + path + "] is not of nested type");
+            throw new ParsingException(parseContext, "[nested] nested object under path [" + path + "] is not of nested type");
         }
     }
 
@@ -190,18 +185,18 @@ public class NestedInnerQueryParseSupport {
     }
 
     private void setPathLevel() {
-        ObjectMapper objectMapper = shardContext.nestedScope().getObjectMapper();
+        ObjectMapper objectMapper = parseContext.nestedScope().getObjectMapper();
         if (objectMapper == null) {
-            parentFilter = shardContext.bitsetFilter(Queries.newNonNestedFilter());
+            parentFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         } else {
-            parentFilter = shardContext.bitsetFilter(objectMapper.nestedTypeFilter());
+            parentFilter = parseContext.bitsetFilter(objectMapper.nestedTypeFilter());
         }
         childFilter = nestedObjectMapper.nestedTypeFilter();
-        parentObjectMapper = shardContext.nestedScope().nextLevel(nestedObjectMapper);
+        parentObjectMapper = parseContext.nestedScope().nextLevel(nestedObjectMapper);
     }
 
     private void resetPathLevel() {
-        shardContext.nestedScope().previousLevel();
+        parseContext.nestedScope().previousLevel();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHitBuilder.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHitBuilder.java
new file mode 100644
index 0000000..71229ab
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHitBuilder.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.support;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+/**
+ */
+public class QueryInnerHitBuilder extends BaseInnerHitBuilder<QueryInnerHitBuilder> {
+
+    private String name;
+
+    /**
+     * Set the key name to be used in the response.
+     *
+     * Defaults to the path if used in nested query, child type if used in has_child query and parent type if used in has_parent.
+     */
+    public QueryInnerHitBuilder setName(String name) {
+        this.name = name;
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        super.toXContent(builder, params);
+        if (name != null) {
+            builder.field("name", name);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHits.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHits.java
deleted file mode 100644
index fc9b154..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/support/QueryInnerHits.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query.support;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-
-import java.io.IOException;
-
-/**
- */
-public class QueryInnerHits extends ToXContentToBytes implements Writeable<QueryInnerHits> {
-    private final BytesReference queryInnerHitsSearchSource;
-
-    public QueryInnerHits(StreamInput input) throws IOException {
-        queryInnerHitsSearchSource = input.readBytesReference();
-    }
-
-    public QueryInnerHits(XContentParser parser) throws IOException {
-        BytesStreamOutput out = new BytesStreamOutput();
-        try (XContentBuilder builder = XContentFactory.cborBuilder(out)) {
-            builder.copyCurrentStructure(parser);
-            queryInnerHitsSearchSource = builder.bytes();
-        }
-    }
-
-    public QueryInnerHits() {
-        this(null, null);
-    }
-
-    public QueryInnerHits(String name, InnerHitsBuilder.InnerHit innerHit) {
-        BytesStreamOutput out = new BytesStreamOutput();
-        try (XContentBuilder builder = XContentFactory.cborBuilder(out)) {
-            builder.startObject();
-            if (name != null) {
-                builder.field("name", name);
-            }
-            if (innerHit != null) {
-                innerHit.toXContent(builder, ToXContent.EMPTY_PARAMS);
-            }
-            builder.endObject();
-            this.queryInnerHitsSearchSource = builder.bytes();
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to build xcontent", e);
-        }
-    }
-
-    @Override
-    public QueryInnerHits readFrom(StreamInput in) throws IOException {
-        return new QueryInnerHits(in);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field("inner_hits");
-        try (XContentParser parser = XContentType.CBOR.xContent().createParser(queryInnerHitsSearchSource)) {
-            builder.copyCurrentStructure(parser);
-        }
-        return builder;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeBytesReference(queryInnerHitsSearchSource);
-    }
-
-    public XContentParser getXcontentParser() throws IOException {
-        return XContentType.CBOR.xContent().createParser(queryInnerHitsSearchSource);
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) return true;
-        if (o == null || getClass() != o.getClass()) return false;
-
-        QueryInnerHits that = (QueryInnerHits) o;
-
-        return queryInnerHitsSearchSource.equals(that.queryInnerHitsSearchSource);
-
-    }
-
-    @Override
-    public int hashCode() {
-        return queryInnerHitsSearchSource.hashCode();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
index a500393..1a12c74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
@@ -29,12 +29,12 @@ import org.elasticsearch.common.ParseFieldMatcher;
  */
 public final class QueryParsers {
 
-    public static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
-    public static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
-    public static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
-    public static final ParseField TOP_TERMS = new ParseField("top_terms_");
-    public static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
-    public static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
+    private static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
+    private static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
+    private static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
+    private static final ParseField TOP_TERMS = new ParseField("top_terms_");
+    private static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
+    private static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
 
     private QueryParsers() {
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
new file mode 100644
index 0000000..37716d1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
@@ -0,0 +1,136 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.support;
+
+import org.apache.lucene.search.Query;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryParseContext;
+
+import java.io.IOException;
+
+/**
+ * XContentStructure is a class used to capture a subset of query, to be parsed
+ * at a later time when more information (in this case, types) is available.
+ * Note that using this class requires copying the parser's data, which will
+ * result in additional overhead versus parsing the inner query/filter
+ * immediately, however, the extra overhead means that the type not be
+ * extracted prior to query parsing (in the case of unordered JSON).
+ */
+public abstract class XContentStructure {
+
+    private final QueryParseContext parseContext;
+    private BytesReference innerBytes;
+
+    /**
+     * Create a new XContentStructure for the current parsing context.
+     */
+    public XContentStructure(QueryParseContext queryParseContext) {
+        this.parseContext = queryParseContext;
+    }
+
+    /**
+     * "Freeze" the parsing content, which means copying the current parser's
+     * structure into an internal {@link BytesReference} to be parsed later.
+     * @return the original XContentStructure object
+     */
+    public XContentStructure freeze() throws IOException {
+        this.bytes(XContentFactory.smileBuilder().copyCurrentStructure(parseContext.parser()).bytes());
+        return this;
+    }
+
+    /**
+     * Set the bytes to be used for parsing
+     */
+    public void bytes(BytesReference innerBytes) {
+        this.innerBytes = innerBytes;
+    }
+
+    /**
+     * Return the bytes that are going to be used for parsing
+     */
+    public BytesReference bytes() {
+        return this.innerBytes;
+    }
+
+    /**
+     * Use the captured bytes to parse the inner query using the specified
+     * types. The original QueryParseContext's parser is switched during this
+     * parsing, so this method is NOT thread-safe.
+     * @param types types to be used during the inner query parsing
+     * @return {@link Query} parsed from the bytes captured in {@code freeze()}
+     */
+    public Query asQuery(String... types) throws IOException {
+        BytesReference br = this.bytes();
+        assert br != null : "innerBytes must be set with .bytes(bytes) or .freeze() before parsing";
+        XContentParser innerParser = XContentHelper.createParser(br);
+        String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
+        XContentParser old = parseContext.parser();
+        parseContext.parser(innerParser);
+        try {
+            return parseContext.parseInnerQuery();
+        } finally {
+            parseContext.parser(old);
+            QueryParseContext.setTypes(origTypes);
+        }
+    }
+
+    /**
+     * InnerQuery is an extension of {@code XContentStructure} that eagerly
+     * parses the query in a streaming manner if the types are available at
+     * construction time.
+     */
+    public static class InnerQuery extends XContentStructure {
+        private Query query = null;
+        private boolean queryParsed = false;
+        public InnerQuery(QueryParseContext parseContext1, @Nullable String... types) throws IOException {
+            super(parseContext1);
+            if (types != null) {
+                String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
+                try {
+                    query = parseContext1.parseInnerQuery();
+                    queryParsed = true;
+                } finally {
+                    QueryParseContext.setTypes(origTypes);
+                }
+            } else {
+                BytesReference innerBytes = XContentFactory.smileBuilder().copyCurrentStructure(parseContext1.parser()).bytes();
+                super.bytes(innerBytes);
+            }
+        }
+
+        /**
+         * Return the query represented by the XContentStructure object,
+         * returning the cached Query if it has already been parsed.
+         * @param types types to be used during the inner query parsing
+         */
+        @Override
+        public Query asQuery(String... types) throws IOException {
+            if (!queryParsed) { // query can be null
+                this.query = super.asQuery(types);
+            }
+            return this.query;
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
index f0cb1d4..fb5fff8 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
@@ -25,16 +25,12 @@ import org.apache.lucene.queries.ExtendedCommonTermsQuery;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.util.QueryBuilder;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
@@ -42,92 +38,18 @@ import java.util.List;
 
 public class MatchQuery {
 
-    public static enum Type implements Writeable<Type> {
-        /**
-         * The text is analyzed and terms are added to a boolean query.
-         */
-        BOOLEAN(0),
-        /**
-         * The text is analyzed and used as a phrase query.
-         */
-        PHRASE(1),
-        /**
-         * The text is analyzed and used in a phrase query, with the last term acting as a prefix.
-         */
-        PHRASE_PREFIX(2);
-
-        private final int ordinal;
-
-        private static final Type PROTOTYPE = BOOLEAN;
-
-        private Type(int ordinal) {
-            this.ordinal = ordinal;
-        }
-
-        @Override
-        public Type readFrom(StreamInput in) throws IOException {
-            int ord = in.readVInt();
-            for (Type type : Type.values()) {
-                if (type.ordinal == ord) {
-                    return type;
-                }
-            }
-            throw new ElasticsearchException("unknown serialized type [" + ord + "]");
-        }
-
-        public static Type readTypeFrom(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(this.ordinal);
-        }
+    public static enum Type {
+        BOOLEAN,
+        PHRASE,
+        PHRASE_PREFIX
     }
 
-    public static enum ZeroTermsQuery implements Writeable<ZeroTermsQuery> {
-        NONE(0),
-        ALL(1);
-
-        private final int ordinal;
-
-        private static final ZeroTermsQuery PROTOTYPE = NONE;
-
-        private ZeroTermsQuery(int ordinal) {
-            this.ordinal = ordinal;
-        }
-
-        @Override
-        public ZeroTermsQuery readFrom(StreamInput in) throws IOException {
-            int ord = in.readVInt();
-            for (ZeroTermsQuery zeroTermsQuery : ZeroTermsQuery.values()) {
-                if (zeroTermsQuery.ordinal == ord) {
-                    return zeroTermsQuery;
-                }
-            }
-            throw new ElasticsearchException("unknown serialized type [" + ord + "]");
-        }
-
-        public static ZeroTermsQuery readZeroTermsQueryFrom(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(this.ordinal);
-        }
+    public static enum ZeroTermsQuery {
+        NONE,
+        ALL
     }
 
-    /** the default phrase slop */
-    public static final int DEFAULT_PHRASE_SLOP = 0;
-
-    /** the default leniency setting */
-    public static final boolean DEFAULT_LENIENCY = false;
-
-    /** the default zero terms query */
-    public static final ZeroTermsQuery DEFAULT_ZERO_TERMS_QUERY = ZeroTermsQuery.NONE;
-
-    protected final QueryShardContext context;
+    protected final QueryParseContext parseContext;
 
     protected String analyzer;
 
@@ -135,26 +57,26 @@ public class MatchQuery {
 
     protected boolean enablePositionIncrements = true;
 
-    protected int phraseSlop = DEFAULT_PHRASE_SLOP;
+    protected int phraseSlop = 0;
 
     protected Fuzziness fuzziness = null;
-
+    
     protected int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-
+    
     protected int maxExpansions = FuzzyQuery.defaultMaxExpansions;
 
     protected boolean transpositions = FuzzyQuery.defaultTranspositions;
 
     protected MultiTermQuery.RewriteMethod fuzzyRewriteMethod;
 
-    protected boolean lenient = DEFAULT_LENIENCY;
-
-    protected ZeroTermsQuery zeroTermsQuery = DEFAULT_ZERO_TERMS_QUERY;
+    protected boolean lenient;
 
+    protected ZeroTermsQuery zeroTermsQuery = ZeroTermsQuery.NONE;
+    
     protected Float commonTermsCutoff = null;
-
-    public MatchQuery(QueryShardContext context) {
-        this.context = context;
+    
+    public MatchQuery(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void setAnalyzer(String analyzer) {
@@ -164,9 +86,9 @@ public class MatchQuery {
     public void setOccur(BooleanClause.Occur occur) {
         this.occur = occur;
     }
-
-    public void setCommonTermsCutoff(Float cutoff) {
-        this.commonTermsCutoff = cutoff;
+    
+    public void setCommonTermsCutoff(float cutoff) {
+        this.commonTermsCutoff = Float.valueOf(cutoff);
     }
 
     public void setEnablePositionIncrements(boolean enablePositionIncrements) {
@@ -212,11 +134,11 @@ public class MatchQuery {
     protected Analyzer getAnalyzer(MappedFieldType fieldType) {
         if (this.analyzer == null) {
             if (fieldType != null) {
-                return context.getSearchAnalyzer(fieldType);
+                return parseContext.getSearchAnalyzer(fieldType);
             }
-            return context.mapperService().searchAnalyzer();
+            return parseContext.mapperService().searchAnalyzer();
         } else {
-            Analyzer analyzer = context.mapperService().analysisService().analyzer(this.analyzer);
+            Analyzer analyzer = parseContext.mapperService().analysisService().analyzer(this.analyzer);
             if (analyzer == null) {
                 throw new IllegalArgumentException("No analyzer found for [" + this.analyzer + "]");
             }
@@ -226,7 +148,7 @@ public class MatchQuery {
 
     public Query parse(Type type, String fieldName, Object value) throws IOException {
         final String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             field = fieldType.names().indexName();
         } else {
@@ -235,14 +157,14 @@ public class MatchQuery {
 
         if (fieldType != null && fieldType.useTermQueryWithQueryString() && !forceAnalyzeQueryString()) {
             try {
-                return fieldType.termQuery(value, context);
+                return fieldType.termQuery(value, parseContext);
             } catch (RuntimeException e) {
                 if (lenient) {
                     return null;
                 }
                 throw e;
             }
-
+            
         }
         Analyzer analyzer = getAnalyzer(fieldType);
         assert analyzer != null;
@@ -276,7 +198,7 @@ public class MatchQuery {
     }
 
     protected Query zeroTermsQuery() {
-        return zeroTermsQuery == DEFAULT_ZERO_TERMS_QUERY ? Queries.newMatchNoDocsQuery() : Queries.newMatchAllQuery();
+        return zeroTermsQuery == ZeroTermsQuery.NONE ? Queries.newMatchNoDocsQuery() : Queries.newMatchAllQuery();
     }
 
     private class MatchQueryBuilder extends QueryBuilder {
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index 5fb2db0..08cc55f 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -31,7 +31,7 @@ import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -47,10 +47,10 @@ public class MultiMatchQuery extends MatchQuery {
         this.groupTieBreaker = tieBreaker;
     }
 
-    public MultiMatchQuery(QueryShardContext context) {
-        super(context);
+    public MultiMatchQuery(QueryParseContext parseContext) {
+        super(parseContext);
     }
-
+    
     private Query parseAndApply(Type type, String fieldName, Object value, String minimumShouldMatch, Float boostValue) throws IOException {
         Query query = parse(type, fieldName, value);
         if (query instanceof BooleanQuery) {
@@ -162,7 +162,7 @@ public class MultiMatchQuery extends MatchQuery {
             List<Tuple<String, Float>> missing = new ArrayList<>();
             for (Map.Entry<String, Float> entry : fieldNames.entrySet()) {
                 String name = entry.getKey();
-                MappedFieldType fieldType = context.fieldMapper(name);
+                MappedFieldType fieldType = parseContext.fieldMapper(name);
                 if (fieldType != null) {
                     Analyzer actualAnalyzer = getAnalyzer(fieldType);
                     name = fieldType.names().indexName();
diff --git a/core/src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java b/core/src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java
new file mode 100644
index 0000000..49643aa
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java
@@ -0,0 +1,100 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.search.morelikethis;
+
+import org.apache.lucene.index.Fields;
+import org.elasticsearch.action.termvectors.MultiTermVectorsItemResponse;
+import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
+import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
+import org.elasticsearch.action.termvectors.TermVectorsResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+/**
+ *
+ */
+public class MoreLikeThisFetchService extends AbstractComponent {
+
+    private final Client client;
+
+    @Inject
+    public MoreLikeThisFetchService(Client client, Settings settings) {
+        super(settings);
+        this.client = client;
+    }
+
+    public Fields[] fetch(List<Item> items) throws IOException {
+        return getFieldsFor(fetchResponse(items, null, SearchContext.current()), items);
+    }
+
+    public MultiTermVectorsResponse fetchResponse(List<Item> likeItems, @Nullable List<Item> unlikeItems,
+                                                  SearchContext searchContext) throws IOException {
+        MultiTermVectorsRequest request = new MultiTermVectorsRequest();
+        for (Item item : likeItems) {
+            request.add(item.toTermVectorsRequest());
+        }
+        if (unlikeItems != null) {
+            for (Item item : unlikeItems) {
+                request.add(item.toTermVectorsRequest());
+            }
+        }
+        request.copyContextAndHeadersFrom(searchContext);
+        return client.multiTermVectors(request).actionGet();
+    }
+
+    public static Fields[] getFieldsFor(MultiTermVectorsResponse responses, List<Item> items) throws IOException {
+        List<Fields> likeFields = new ArrayList<>();
+
+        Set<Item> selectedItems = new HashSet<>();
+        for (Item request : items) {
+            selectedItems.add(new Item(request.index(), request.type(), request.id()));
+        }
+
+        for (MultiTermVectorsItemResponse response : responses) {
+            if (!hasResponseFromRequest(response, selectedItems)) {
+                continue;
+            }
+            if (response.isFailed()) {
+                continue;
+            }
+            TermVectorsResponse getResponse = response.getResponse();
+            if (!getResponse.isExists()) {
+                continue;
+            }
+            likeFields.add(getResponse.getFields());
+        }
+        return likeFields.toArray(Fields.EMPTY_ARRAY);
+    }
+
+    private static boolean hasResponseFromRequest(MultiTermVectorsItemResponse response, Set<Item> selectedItems) {
+        return selectedItems.contains(new Item(response.getIndex(), response.getType(), response.getId()));
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java b/core/src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java
new file mode 100644
index 0000000..97d0804
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.search.shape;
+
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.geo.builders.ShapeBuilder;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+
+import java.io.IOException;
+
+/**
+ * Service which retrieves pre-indexed Shapes from another index
+ */
+public class ShapeFetchService extends AbstractComponent {
+
+    private final Client client;
+
+    @Inject
+    public ShapeFetchService(Client client, Settings settings) {
+        super(settings);
+        this.client = client;
+    }
+
+    /**
+     * Fetches the Shape with the given ID in the given type and index.
+     *
+     * @param getRequest GetRequest containing index, type and id
+     * @param path      Name or path of the field in the Shape Document where the Shape itself is located
+     * @return Shape with the given ID
+     * @throws IOException Can be thrown while parsing the Shape Document and extracting the Shape
+     */
+    public ShapeBuilder fetch(GetRequest getRequest,String path) throws IOException {
+        getRequest.preference("_local");
+        getRequest.operationThreaded(false);
+        GetResponse response = client.get(getRequest).actionGet();
+        if (!response.isExists()) {
+            throw new IllegalArgumentException("Shape with ID [" + getRequest.id() + "] in type [" + getRequest.type() + "] not found");
+        }
+
+        String[] pathElements = Strings.splitStringToArray(path, '.');
+        int currentPathSlot = 0;
+
+        XContentParser parser = null;
+        try {
+            parser = XContentHelper.createParser(response.getSourceAsBytesRef());
+            XContentParser.Token currentToken;
+            while ((currentToken = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (currentToken == XContentParser.Token.FIELD_NAME) {
+                    if (pathElements[currentPathSlot].equals(parser.currentName())) {
+                        parser.nextToken();
+                        if (++currentPathSlot == pathElements.length) {
+                            return ShapeBuilder.parse(parser);
+                        }
+                    } else {
+                        parser.nextToken();
+                        parser.skipChildren();
+                    }
+                }
+            }
+            throw new IllegalStateException("Shape with name [" + getRequest.id() + "] found but missing " + path + " field");
+        } finally {
+            if (parser != null) {
+                parser.close();
+            }
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/search/shape/ShapeModule.java b/core/src/main/java/org/elasticsearch/index/search/shape/ShapeModule.java
new file mode 100644
index 0000000..510d04f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/search/shape/ShapeModule.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.search.shape;
+
+import org.elasticsearch.common.geo.ShapesAvailability;
+import org.elasticsearch.common.inject.AbstractModule;
+
+public class ShapeModule extends AbstractModule {
+
+    @Override
+    protected void configure() {
+        // TODO: We could wrap this entire module in a JTS_AVAILABILITY check
+        if (ShapesAvailability.JTS_AVAILABLE) {
+            bind(ShapeFetchService.class).asEagerSingleton();
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index a2f5b9d..abf9839 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -65,7 +65,6 @@ import org.elasticsearch.index.deletionpolicy.SnapshotDeletionPolicy;
 import org.elasticsearch.index.deletionpolicy.SnapshotIndexCommit;
 import org.elasticsearch.index.engine.*;
 import org.elasticsearch.index.fielddata.FieldDataStats;
-import org.elasticsearch.index.fielddata.IndexFieldDataCache;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.fielddata.ShardFieldData;
 import org.elasticsearch.index.flush.FlushStats;
diff --git a/core/src/main/java/org/elasticsearch/index/shard/MergePolicyConfig.java b/core/src/main/java/org/elasticsearch/index/shard/MergePolicyConfig.java
index 3895bbe..c664d3a 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/MergePolicyConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/MergePolicyConfig.java
@@ -79,7 +79,7 @@ import org.elasticsearch.index.settings.IndexSettingsService;
  * 
  *     Sets the allowed number of segments per tier. Smaller values mean more
  *     merging but fewer segments. Default is <code>10</code>. Note, this value needs to be
- *     >= than the <code>max_merge_at_once</code> otherwise you'll force too many merges to
+ *     &gt;= than the <code>max_merge_at_once</code> otherwise you'll force too many merges to
  *     occur.
  * 
  * <li><code>index.merge.policy.reclaim_deletes_weight</code>:
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
index ca24985..1983c4e 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/BM25SimilarityProvider.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for the {@link BM25Similarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>k1</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
index 6d30e81..b5a5cdc 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for {@link DFRSimilarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>basic_model</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
index a6e0443..0f9feba 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DefaultSimilarityProvider.java
@@ -26,7 +26,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for {@link DefaultSimilarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>discount_overlaps</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
index 4741247..161ca9c 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for {@link IBSimilarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>distribution</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
index 797ce64..efea285 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMDirichletSimilarityProvider.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for {@link LMDirichletSimilarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>mu</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
index 9be0236..5d30b30 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/LMJelinekMercerSimilarityProvider.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * {@link SimilarityProvider} for {@link LMJelinekMercerSimilarity}.
- * <p/>
+ * <p>
  * Configuration options available:
  * <ul>
  *     <li>lambda</li>
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
index 49e3df9..8b8b688 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/SimilarityLookupService.java
@@ -31,7 +31,7 @@ import java.util.Map;
 
 /**
  * Service for looking up configured {@link SimilarityProvider} implementations by name.
- * <p/>
+ * <p>
  * The service instantiates the Providers through their Factories using configuration
  * values found with the {@link SimilarityModule#SIMILARITY_SETTINGS_PREFIX} prefix.
  */
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java
index 7c77884..8ce487f 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java
@@ -27,7 +27,7 @@ import org.elasticsearch.indices.recovery.RecoveryState;
 
 /**
  * Shard-level snapshot repository
- * <p/>
+ * <p>
  * IndexShardRepository is used on data node to create snapshots of individual shards. See {@link org.elasticsearch.repositories.Repository}
  * for more information.
  */
@@ -35,10 +35,10 @@ public interface IndexShardRepository {
 
     /**
      * Creates a snapshot of the shard based on the index commit point.
-     * <p/>
+     * <p>
      * The index commit point can be obtained by using {@link org.elasticsearch.index.engine.Engine#snapshotIndex} method.
      * IndexShardRepository implementations shouldn't release the snapshot index commit point. It is done by the method caller.
-     * <p/>
+     * <p>
      * As snapshot process progresses, implementation of this method should update {@link IndexShardSnapshotStatus} object and check
      * {@link IndexShardSnapshotStatus#aborted()} to see if the snapshot process should be aborted.
      *
@@ -51,7 +51,7 @@ public interface IndexShardRepository {
 
     /**
      * Restores snapshot of the shard.
-     * <p/>
+     * <p>
      * The index can be renamed on restore, hence different {@code shardId} and {@code snapshotShardId} are supplied.
      *
      * @param snapshotId      snapshot id
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index 947a36a..912be76 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -623,12 +623,11 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
 
         /**
          * Snapshot individual file
-         * <p/>
+         * <p>
          * This is asynchronous method. Upon completion of the operation latch is getting counted down and any failures are
          * added to the {@code failures} list
          *
          * @param fileInfo file to be snapshotted
-         * @throws IOException
          */
         private void snapshotFile(final BlobStoreIndexShardSnapshot.FileInfo fileInfo) throws IOException {
             final String file = fileInfo.physicalName();
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
index 808f13b..56d9882 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java
@@ -228,7 +228,6 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
          * @param file    file info
          * @param builder XContent builder
          * @param params  parameters
-         * @throws IOException
          */
         public static void toXContent(FileInfo file, XContentBuilder builder, ToXContent.Params params) throws IOException {
             builder.startObject();
@@ -257,7 +256,6 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
          *
          * @param parser parser
          * @return file info
-         * @throws IOException
          */
         public static FileInfo fromXContent(XContentParser parser) throws IOException {
             XContentParser.Token token = parser.currentToken();
@@ -446,7 +444,6 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
      *
      * @param builder  XContent builder
      * @param params   parameters
-     * @throws IOException
      */
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
@@ -469,7 +466,6 @@ public class BlobStoreIndexShardSnapshot implements ToXContent, FromXContentBuil
      *
      * @param parser parser
      * @return shard snapshot metadata
-     * @throws IOException
      */
     public BlobStoreIndexShardSnapshot fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
 
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index 268e681..bf8c1a8 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -40,7 +40,7 @@ import java.util.Map;
 
 /**
  * Contains information about all snapshot for the given shard in repository
- * <p/>
+ * <p>
  * This class is used to find files that were already snapshoted and clear out files that no longer referenced by any
  * snapshots
  */
@@ -160,7 +160,7 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
     /**
      * Writes index file for the shard in the following format.
      * <pre>
-     * {@code
+     * <code>
      * {
      *     "files": [{
      *         "name": "__3",
@@ -206,6 +206,7 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
      *     }
      * }
      * }
+     * </code>
      * </pre>
      */
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index 2b5051b..09f8ec2 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -70,7 +70,7 @@ import java.util.zip.Checksum;
  * This class also provides access to metadata information like checksums for committed files. A committed
  * file is a file that belongs to a segment written by a Lucene commit. Files that have not been committed
  * ie. created during a merge or a shard refresh / NRT reopen are not considered in the MetadataSnapshot.
- * <p/>
+ * <p>
  * Note: If you use a store it's reference count should be increased before using it by calling #incRef and a
  * corresponding #decRef must be called in a try/finally block to release the store again ie.:
  * <pre>
@@ -299,7 +299,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
      * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
      * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
      * #decRef} has been called for all outstanding references.
-     * <p/>
+     * <p>
      * Note: Close can safely be called multiple times.
      *
      * @throws AlreadyClosedException iff the reference counter can not be incremented.
@@ -318,7 +318,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
      * corresponding {@link #decRef}, in a finally clause; otherwise the store may never be closed.  Note that
      * {@link #close} simply calls decRef(), which means that the Store will not really be closed until {@link
      * #decRef} has been called for all outstanding references.
-     * <p/>
+     * <p>
      * Note: Close can safely be called multiple times.
      *
      * @see #decRef()
@@ -413,7 +413,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
      * The returned IndexOutput might validate the files checksum if the file has been written with a newer lucene version
      * and the metadata holds the necessary information to detect that it was been written by Lucene 4.8 or newer. If it has only
      * a legacy checksum, returned IndexOutput will not verify the checksum.
-     * <p/>
+     * <p>
      * Note: Checksums are calculated nevertheless since lucene does it by default sicne version 4.8.0. This method only adds the
      * verification against the checksum in the given metadata and does not add any significant overhead.
      */
@@ -723,7 +723,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
      * Only files that are part of the last commit are considered in this datastrucutre.
      * For backwards compatibility the snapshot might include legacy checksums that
      * are derived from a dedicated checksum file written by older elasticsearch version pre 1.3
-     * <p/>
+     * <p>
      * Note: This class will ignore the <tt>segments.gen</tt> file since it's optional and might
      * change concurrently for safety reasons.
      *
@@ -867,13 +867,12 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         /**
          * Reads legacy checksum files found in the directory.
-         * <p/>
+         * <p>
          * Files are expected to start with _checksums- prefix
          * followed by long file version. Only file with the highest version is read, all other files are ignored.
          *
          * @param directory the directory to read checksums from
          * @return a map of file checksums and the checksum file version
-         * @throws IOException
          */
         static Tuple<Map<String, String>, Long> readLegacyChecksums(Directory directory) throws IOException {
             synchronized (directory) {
@@ -902,7 +901,6 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
          *
          * @param directory  the directory to clean
          * @param newVersion the latest checksum file version
-         * @throws IOException
          */
         static void cleanLegacyChecksums(Directory directory, long newVersion) throws IOException {
             synchronized (directory) {
@@ -949,7 +947,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         }
 
         /**
-         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB
+         * Computes a strong hash value for small files. Note that this method should only be used for files &lt; 1MB
          */
         public static BytesRef hashFile(Directory directory, String file) throws IOException {
             final BytesRefBuilder fileHash = new BytesRefBuilder();
@@ -961,7 +959,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
 
         /**
-         * Computes a strong hash value for small files. Note that this method should only be used for files < 1MB
+         * Computes a strong hash value for small files. Note that this method should only be used for files &lt; 1MB
          */
         public static void hashFile(BytesRefBuilder fileHash, InputStream in, long size) throws IOException {
             final int len = (int) Math.min(1024 * 1024, size); // for safety we limit this to 1MB
@@ -1006,10 +1004,10 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
          * <li>all files in this segment have the same length</li>
          * <li>the segments <tt>.si</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>.si</tt> file content as it's hash</li>
          * </ul>
-         * <p/>
+         * <p>
          * The <tt>.si</tt> file contains a lot of diagnostics including a timestamp etc. in the future there might be
          * unique segment identifiers in there hardening this method further.
-         * <p/>
+         * <p>
          * The per-commit files handles very similar. A commit is composed of the <tt>segments_N</tt> files as well as generational files like
          * deletes (<tt>_x_y.del</tt>) or field-info (<tt>_x_y.fnm</tt>) files. On a per-commit level files for a commit are treated
          * as identical iff:
@@ -1018,7 +1016,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
          * <li>all files belonging to this commit have the same length</li>
          * <li>the segments file <tt>segments_N</tt> files hashes are byte-identical Note: This is a using a perfect hash function, The metadata transfers the <tt>segments_N</tt> file content as it's hash</li>
          * </ul>
-         * <p/>
+         * <p>
          * NOTE: this diff will not contain the <tt>segments.gen</tt> file. This file is omitted on recovery.
          */
         public RecoveryDiff recoveryDiff(MetadataSnapshot recoveryTargetSnapshot) {
@@ -1314,7 +1312,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
     /**
      * Index input that calculates checksum as data is read from the input.
-     * <p/>
+     * <p>
      * This class supports random access (it is possible to seek backward and forward) in order to accommodate retry
      * mechanism that is used in some repository plugins (S3 for example). However, the checksum is only calculated on
      * the first read. All consecutive reads of the same data are not used to calculate the checksum.
diff --git a/core/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java b/core/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
index 9aaea73..31ad068 100644
--- a/core/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
+++ b/core/src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java
@@ -87,7 +87,6 @@ public class StoreFileMetaData implements Streamable {
      * Returns a string representation of the files checksum. Since Lucene 4.8 this is a CRC32 checksum written
      * by lucene. Previously we use Adler32 on top of Lucene as the checksum algorithm, if {@link #hasLegacyChecksum()} returns
      * <code>true</code> this is a Adler32 checksum.
-     * @return
      */
     @Nullable
     public String checksum() {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java b/core/src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java
index 7a131f9..d1cd3b1 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/LegacyTranslogReader.java
@@ -32,9 +32,6 @@ public final class LegacyTranslogReader extends LegacyTranslogReaderBase {
     /**
      * Create a snapshot of translog file channel. The length parameter should be consistent with totalOperations and point
      * at the end of the last operation in this snapshot.
-     *
-     * @param generation
-     * @param channelReference
      */
     LegacyTranslogReader(long generation, ChannelReference channelReference, long fileLength) {
         super(generation, channelReference, 0, fileLength);
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 9b1913e..5084895 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -1764,8 +1764,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
 
     /**
      * Returns <code>true</code> iff the given generation is the current gbeneration of this translog
-     * @param generation
-     * @return
      */
     public boolean isCurrent(TranslogGeneration generation) {
         try (ReleasableLock lock = writeLock.acquire()) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
index c7feb8d..590bc31 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
@@ -170,9 +170,6 @@ public abstract class TranslogReader implements Closeable, Comparable<TranslogRe
      * optionally-existing header in the file. If the file does not exist, or
      * has zero length, returns the latest version. If the header does not
      * exist, assumes Version 0 of the translog file format.
-     * <p/>
-     *
-     * @throws IOException
      */
     public static ImmutableTranslogReader open(ChannelReference channelReference, Checkpoint checkpoint, String translogUUID) throws IOException {
         final FileChannel channel = channelReference.getChannel();
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
index ff248fc..32c7bc8 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
@@ -110,7 +110,6 @@ public class IndicesModule extends AbstractModule {
         registerQueryParser(NotQueryParser.class);
         registerQueryParser(ExistsQueryParser.class);
         registerQueryParser(MissingQueryParser.class);
-        registerQueryParser(MatchNoneQueryParser.class);
 
         if (ShapesAvailability.JTS_AVAILABLE) {
             registerQueryParser(GeoShapeQueryParser.class);
diff --git a/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java b/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
index b44a050..1a48c5c 100644
--- a/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
+++ b/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
@@ -41,23 +41,23 @@ import java.util.function.Function;
  * the {@code <path.conf>/hunspell} directory, where each locale has its dedicated sub-directory which holds the dictionary
  * files. For example, the dictionary files for {@code en_US} locale must be placed under {@code <path.conf>/hunspell/en_US}
  * directory.
- * <p/>
+ * <p>
  * The following settings can be set for each dictionary:
  * <ul>
  * <li>{@code ignore_case} - If true, dictionary matching will be case insensitive (defaults to {@code false})</li>
  * <li>{@code strict_affix_parsing} - Determines whether errors while reading a affix rules file will cause exception or simple be ignored (defaults to {@code true})</li>
  * </ul>
- * <p/>
+ * <p>
  * These settings can either be configured as node level configuration, such as:
- * <br/><br/>
+ * <br><br>
  * <pre><code>
  *     indices.analysis.hunspell.dictionary.en_US.ignore_case: true
  *     indices.analysis.hunspell.dictionary.en_US.strict_affix_parsing: false
  * </code></pre>
- * <p/>
+ * <p>
  * or, as dedicated configuration per dictionary, placed in a {@code settings.yml} file under the dictionary directory. For
  * example, the following can be the content of the {@code <path.config>/hunspell/en_US/settings.yml} file:
- * <br/><br/>
+ * <br><br>
  * <pre><code>
  *     ignore_case: true
  *     strict_affix_parsing: false
diff --git a/core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java b/core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java
index b08efaf..d8bf8f1 100644
--- a/core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java
+++ b/core/src/main/java/org/elasticsearch/indices/breaker/CircuitBreakerService.java
@@ -35,8 +35,6 @@ public abstract class CircuitBreakerService extends AbstractLifecycleComponent<C
 
     /**
      * Allows to register of a custom circuit breaker.
-     *
-     * @param breakerSettings
      */
     public abstract void registerBreaker(BreakerSettings breakerSettings);
 
diff --git a/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java b/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
index b642286..12cf865 100644
--- a/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
+++ b/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
@@ -162,7 +162,6 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
 
     /**
      * Validate that child settings are valid
-     * @throws IllegalStateException
      */
     public static void validateSettings(BreakerSettings[] childrenSettings) throws IllegalStateException {
         for (BreakerSettings childSettings : childrenSettings) {
@@ -206,8 +205,6 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
 
     /**
      * Checks whether the parent breaker has been tripped
-     * @param label
-     * @throws CircuitBreakingException
      */
     public void checkParentLimit(String label) throws CircuitBreakingException {
         long totalUsed = 0;
@@ -228,8 +225,6 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
     /**
      * Allows to register a custom circuit breaker.
      * Warning: Will overwrite any existing custom breaker with the same name.
-     *
-     * @param breakerSettings
      */
     @Override
     public void registerBreaker(BreakerSettings breakerSettings) {
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java b/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
index 8e03b1b..28ab04b 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/query/terms/TermsLookup.java
@@ -19,174 +19,58 @@
 
 package org.elasticsearch.indices.cache.query.terms;
 
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.index.query.QueryParseContext;
 
-import java.io.IOException;
-import java.util.Objects;
-
 /**
- * Encapsulates the parameters needed to fetch terms.
  */
-public class TermsLookup implements Writeable<TermsLookup>, ToXContent {
-    static final TermsLookup PROTOTYPE = new TermsLookup("index", "type", "id", "path");
+public class TermsLookup {
 
-    private String index;
+    private final String index;
     private final String type;
     private final String id;
+    private final String routing;
     private final String path;
-    private String routing;
 
-    public TermsLookup(String index, String type, String id, String path) {
-        if (id == null) {
-            throw new IllegalArgumentException("[terms] query lookup element requires specifying the id.");
-        }
-        if (type == null) {
-            throw new IllegalArgumentException("[terms] query lookup element requires specifying the type.");
-        }
-        if (path == null) {
-            throw new IllegalArgumentException("[terms] query lookup element requires specifying the path.");
-        }
+    @Nullable
+    private final QueryParseContext queryParseContext;
+
+    public TermsLookup(String index, String type, String id, String routing, String path, @Nullable QueryParseContext queryParseContext) {
         this.index = index;
         this.type = type;
         this.id = id;
+        this.routing = routing;
         this.path = path;
+        this.queryParseContext = queryParseContext;
     }
 
-    public String index() {
+    public String getIndex() {
         return index;
     }
 
-    public TermsLookup index(String index) {
-        this.index = index;
-        return this;
-    }
-
-    public String type() {
+    public String getType() {
         return type;
     }
 
-    public String id() {
+    public String getId() {
         return id;
     }
 
-    public String path() {
-        return path;
-    }
-
-    public String routing() {
-        return routing;
+    public String getRouting() {
+        return this.routing;
     }
 
-    public TermsLookup routing(String routing) {
-        this.routing = routing;
-        return this;
+    public String getPath() {
+        return path;
     }
 
-    public static TermsLookup parseTermsLookup(QueryParseContext parseContext, XContentParser parser) throws IOException {
-        String index = null;
-        String type = null;
-        String id = null;
-        String path = null;
-        String routing = null;
-        XContentParser.Token token;
-        String currentFieldName = "";
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                switch (currentFieldName) {
-                case "index":
-                    index = parser.textOrNull();
-                    break;
-                case "type":
-                    type = parser.text();
-                    break;
-                case "id":
-                    id = parser.text();
-                    break;
-                case "routing":
-                    routing = parser.textOrNull();
-                    break;
-                case "path":
-                    path = parser.text();
-                    break;
-                default:
-                    throw new ParsingException(parseContext, "[terms] query does not support [" + currentFieldName
-                            + "] within lookup element");
-                }
-            }
-        }
-        return new TermsLookup(index, type, id, path).routing(routing);
+    @Nullable
+    public QueryParseContext getQueryParseContext() {
+        return queryParseContext;
     }
 
     @Override
     public String toString() {
         return index + "/" + type + "/" + id + "/" + path;
     }
-
-    @Override
-    public TermsLookup readFrom(StreamInput in) throws IOException {
-        String type = in.readString();
-        String id = in.readString();
-        String path = in.readString();
-        String index = in.readOptionalString();
-        TermsLookup termsLookup = new TermsLookup(index, type, id, path);
-        termsLookup.routing = in.readOptionalString();
-        return termsLookup;
-    }
-
-    public static TermsLookup readTermsLookupFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(type);
-        out.writeString(id);
-        out.writeString(path);
-        out.writeOptionalString(index);
-        out.writeOptionalString(routing);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (index != null) {
-            builder.field("index", index);
-        }
-        builder.field("type", type);
-        builder.field("id", id);
-        builder.field("path", path);
-        if (routing != null) {
-            builder.field("routing", routing);
-        }
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(index, type, id, path, routing);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        TermsLookup other = (TermsLookup) obj;
-        return Objects.equals(index, other.index) &&
-                Objects.equals(type, other.type) &&
-                Objects.equals(id, other.id) &&
-                Objects.equals(path, other.path) &&
-                Objects.equals(routing, other.routing);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index d621994..4ab4691 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -68,10 +68,10 @@ import static org.elasticsearch.common.Strings.hasLength;
  * with the semantics of NRT (the index reader version is part of the cache key), and relies on size based
  * eviction to evict old reader associated cache entries as well as scheduler reaper to clean readers that
  * are no longer used or closed shards.
- * <p/>
- * Currently, the cache is only enabled for {@link SearchType#COUNT}, and can only be opted in on an index
+ * <p>
+ * Currently, the cache is only enabled for count requests, and can only be opted in on an index
  * level setting that can be dynamically changed and defaults to false.
- * <p/>
+ * <p>
  * There are still several TODOs left in this class, some easily addressable, some more complex, but the support
  * is functional.
  */
diff --git a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java
index 75cfeb9..cfc6357 100644
--- a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java
+++ b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCacheListener.java
@@ -31,7 +31,7 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
 
 /**
- * A {@link IndexFieldDataCache.Listener} implementation that updates indices (node) level statistics / service about
+ * A {@link org.elasticsearch.index.fielddata.IndexFieldDataCache.Listener} implementation that updates indices (node) level statistics / service about
  * field data entries being loaded and unloaded.
  *
  * Currently it only decrements the memory used in the  {@link CircuitBreakerService}.
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index b453503..9a7454b 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -22,10 +22,7 @@ package org.elasticsearch.indices.query;
 import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParser;
 
 import java.util.HashMap;
@@ -34,28 +31,24 @@ import java.util.Set;
 
 public class IndicesQueriesRegistry extends AbstractComponent {
 
-    private ImmutableMap<String, QueryParser<?>> queryParsers;
+    private ImmutableMap<String, QueryParser> queryParsers;
 
     @Inject
-    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
+    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers) {
         super(settings);
-        Map<String, QueryParser<?>> queryParsers = new HashMap<>();
-        for (QueryParser<?> queryParser : injectedQueryParsers) {
+        Map<String, QueryParser> queryParsers = new HashMap<>();
+        for (QueryParser queryParser : injectedQueryParsers) {
             for (String name : queryParser.names()) {
                 queryParsers.put(name, queryParser);
             }
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, queryParser.getBuilderPrototype());
         }
-        // EmptyQueryBuilder is not registered as query parser but used internally.
-        // We need to register it with the NamedWriteableRegistry in order to serialize it
-        namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
         this.queryParsers = ImmutableMap.copyOf(queryParsers);
     }
 
     /**
      * Returns all the registered query parsers
      */
-    public ImmutableMap<String, QueryParser<?>> queryParsers() {
+    public ImmutableMap<String, QueryParser> queryParsers() {
         return queryParsers;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java
index 37c7982..4cd9d7d 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java
@@ -73,7 +73,7 @@ public class RecoveriesCollection {
      * gets the {@link RecoveryStatus } for a given id. The RecoveryStatus returned has it's ref count already incremented
      * to make sure it's safe to use. However, you must call {@link RecoveryStatus#decRef()} when you are done with it, typically
      * by using this method in a try-with-resources clause.
-     * <p/>
+     * <p>
      * Returns null if recovery is not found
      */
     public StatusRef getStatus(long id) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
index b987b4a..102fa98 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
@@ -154,7 +154,7 @@ public class RecoverySourceHandler {
      * Perform phase1 of the recovery operations. Once this {@link SnapshotIndexCommit}
      * snapshot has been performed no commit operations (files being fsync'd)
      * are effectively allowed on this index until all recovery phases are done
-     * <p/>
+     * <p>
      * Phase1 examines the segment files on the target node and copies over the
      * segments that are missing. Only segments that have the same size and
      * checksum can be reused
@@ -482,7 +482,7 @@ public class RecoverySourceHandler {
 
     /**
      * Perform phase2 of the recovery process
-     * <p/>
+     * <p>
      * Phase2 takes a snapshot of the current translog *without* acquiring the
      * write lock (however, the translog snapshot is a point-in-time view of
      * the translog). It then sends each translog operation to the target node
@@ -550,7 +550,7 @@ public class RecoverySourceHandler {
 
     /**
      * Send the given snapshot's operations to this handler's target node.
-     * <p/>
+     * <p>
      * Operations are bulked into a single request depending on an operation
      * count limit or size-in-bytes limit
      *
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
index cc58305..022c326 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java
@@ -528,7 +528,7 @@ public class RecoveryState implements ToXContent, Streamable {
         /**
          * returns the total number of translog operations needed to be recovered at this moment.
          * Note that this can change as the number of operations grows during recovery.
-         * <p/>
+         * <p>
          * A value of -1 ({@link RecoveryState.Translog#UNKNOWN} is return if this is unknown (typically a gateway recovery)
          */
         public synchronized int totalOperations() {
@@ -543,7 +543,7 @@ public class RecoveryState implements ToXContent, Streamable {
         /**
          * returns the total number of translog operations to recovered, on the start of the recovery. Unlike {@link #totalOperations}
          * this does change during recovery.
-         * <p/>
+         * <p>
          * A value of -1 ({@link RecoveryState.Translog#UNKNOWN} is return if this is unknown (typically a gateway recovery)
          */
         public synchronized int totalOperationsOnStart() {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
index cef2454..6e9505f 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
@@ -148,7 +148,7 @@ public class RecoveryStatus extends AbstractRefCounted {
      * cancel the recovery. calling this method will clean temporary files and release the store
      * unless this object is in use (in which case it will be cleaned once all ongoing users call
      * {@link #decRef()}
-     * <p/>
+     * <p>
      * if {@link #CancellableThreads()} was used, the threads will be interrupted.
      */
     public void cancel(String reason) {
@@ -219,7 +219,7 @@ public class RecoveryStatus extends AbstractRefCounted {
     /**
      * Creates an {@link org.apache.lucene.store.IndexOutput} for the given file name. Note that the
      * IndexOutput actually point at a temporary file.
-     * <p/>
+     * <p>
      * Note: You can use {@link #getOpenIndexOutput(String)} with the same filename to retrieve the same IndexOutput
      * at a later stage
      */
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
index 86d7157..10b1b87 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
@@ -63,8 +63,8 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
 
 /**
  * The recovery target handles recoveries of peer shards of the shard+node to recover to.
- * <p/>
- * <p>Note, it can be safely assumed that there will only be a single recovery per shard (index+id) and
+ * <p>
+ * Note, it can be safely assumed that there will only be a single recovery per shard (index+id) and
  * not several of them (since we don't allocate several shard replicas to the same node).
  */
 public class RecoveryTarget extends AbstractComponent {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java b/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
index 31280dc..3a62f4f 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java
@@ -53,11 +53,8 @@ public class StartRecoveryRequest extends TransportRequest {
     /**
      * Start recovery request.
      *
-     * @param shardId
      * @param sourceNode       The node to recover from
      * @param targetNode       The node to recover to
-     * @param markAsRelocated
-     * @param metadataSnapshot
      */
     public StartRecoveryRequest(ShardId shardId, DiscoveryNode sourceNode, DiscoveryNode targetNode, boolean markAsRelocated, Store.MetadataSnapshot metadataSnapshot, RecoveryState.Type recoveryType, long recoveryId) {
         this.recoveryId = recoveryId;
diff --git a/core/src/main/java/org/elasticsearch/monitor/process/ProcessStats.java b/core/src/main/java/org/elasticsearch/monitor/process/ProcessStats.java
index 44ef7c7..de447c9 100644
--- a/core/src/main/java/org/elasticsearch/monitor/process/ProcessStats.java
+++ b/core/src/main/java/org/elasticsearch/monitor/process/ProcessStats.java
@@ -197,7 +197,7 @@ public class ProcessStats implements Streamable, ToXContent {
         /**
          * Get the Process cpu usage.
          * <p>
-         * <p>Supported Platforms: All.
+         * Supported Platforms: All.
          */
         public short getPercent() {
             return percent;
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 2058969..6d5ebc0 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -31,6 +31,7 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.common.StopWatch;
+import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Injector;
@@ -55,6 +56,7 @@ import org.elasticsearch.gateway.GatewayModule;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.http.HttpServer;
 import org.elasticsearch.http.HttpServerModule;
+import org.elasticsearch.index.search.shape.ShapeModule;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.breaker.CircuitBreakerModule;
@@ -103,7 +105,6 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 /**
  * A node represent a node within a cluster (<tt>cluster.name</tt>). The {@link #client()} can be used
  * in order to use a {@link Client} to perform actions/operations against the cluster.
- * <p/>
  * <p>In order to create a node, the {@link NodeBuilder} can be used. When done with it, make sure to
  * call {@link #close()} on it.
  */
@@ -188,6 +189,7 @@ public class Node implements Releasable {
             modules.add(new MonitorModule(settings));
             modules.add(new GatewayModule(settings));
             modules.add(new NodeClientModule());
+            modules.add(new ShapeModule());
             modules.add(new PercolatorModule());
             modules.add(new ResourceWatcherModule());
             modules.add(new RepositoriesModule());
diff --git a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java b/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
index 257d380..377c409 100644
--- a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
+++ b/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
@@ -23,32 +23,30 @@ import org.elasticsearch.common.settings.Settings;
 
 /**
  * A node builder is used to construct a {@link Node} instance.
- * <p/>
- * <p>Settings will be loaded relative to the ES home (with or without <tt>config/</tt> prefix) and if not found,
- * within the classpath (with or without <tt>config/<tt> prefix). The settings file loaded can either be named
+ * <p>
+ * Settings will be loaded relative to the ES home (with or without <tt>config/</tt> prefix) and if not found,
+ * within the classpath (with or without <tt>config/</tt> prefix). The settings file loaded can either be named
  * <tt>elasticsearch.yml</tt> or <tt>elasticsearch.json</tt>).
- * <p/>
- * <p>Explicit settings can be passed by using the {@link #settings(org.elasticsearch.common.settings.Settings)} method.
- * <p/>
- * <p>In any case, settings will be resolved from system properties as well that are either prefixed with <tt>es.</tt>
+ * <p>
+ * Explicit settings can be passed by using the {@link #settings(org.elasticsearch.common.settings.Settings)} method.
+ * <p>
+ * In any case, settings will be resolved from system properties as well that are either prefixed with <tt>es.</tt>
  * or <tt>elasticsearch.</tt>.
- * <p/>
- * <p>An example for creating a simple node with optional settings loaded from the classpath:
- * <p/>
+ * <p>
+ * An example for creating a simple node with optional settings loaded from the classpath:
  * <pre>
  * Node node = NodeBuilder.nodeBuilder().node();
  * </pre>
- * <p/>
- * <p>An example for creating a node with explicit settings (in this case, a node in the cluster that does not hold
+ * <p>
+ * An example for creating a node with explicit settings (in this case, a node in the cluster that does not hold
  * data):
- * <p/>
  * <pre>
  * Node node = NodeBuilder.nodeBuilder()
  *                      .settings(Settings.settingsBuilder().put("node.data", false)
  *                      .node();
  * </pre>
- * <p/>
- * <p>When done with the node, make sure you call {@link Node#close()} on it.
+ * <p>
+ * When done with the node, make sure you call {@link Node#close()} on it.
  *
  *
  */
diff --git a/core/src/main/java/org/elasticsearch/plugins/Plugin.java b/core/src/main/java/org/elasticsearch/plugins/Plugin.java
index 986d397..7207795 100644
--- a/core/src/main/java/org/elasticsearch/plugins/Plugin.java
+++ b/core/src/main/java/org/elasticsearch/plugins/Plugin.java
@@ -29,7 +29,7 @@ import java.util.Collections;
 
 /**
  * An extension point allowing to plug in custom functionality.
- * <p/>
+ * <p>
  * A plugin can be register custom extensions to builtin behavior by implementing <tt>onModule(AnyModule)</tt>,
  * and registering the extension with the given module.
  */
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index df56218..7252190 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -79,16 +79,18 @@ public class PluginManager {
                     "analysis-phonetic",
                     "analysis-smartcn",
                     "analysis-stempel",
-                    "cloud-azure",
                     "cloud-gce",
                     "delete-by-query",
+                    "discovery-azure",
                     "discovery-ec2",
                     "discovery-multicast",
                     "lang-javascript",
                     "lang-python",
                     "mapper-murmur3",
                     "mapper-size",
-                    "repository-s3"
+                    "repository-azure",
+                    "repository-s3",
+                    "store-smb"
             ).build();
 
     private final Environment environment;
diff --git a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
index b11f91f..9cea2e9 100644
--- a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
+++ b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
@@ -76,7 +76,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     /**
      * Registers new repository in the cluster
-     * <p/>
+     * <p>
      * This method can be only called on the master node. It tries to create a new repository on the master
      * and if it was successful it adds new repository to cluster metadata.
      *
@@ -151,7 +151,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
     }
     /**
      * Unregisters repository in the cluster
-     * <p/>
+     * <p>
      * This method can be only called on the master node. It removes repository information from cluster metadata.
      *
      * @param request  unregister repository request
@@ -311,7 +311,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     /**
      * Returns registered repository
-     * <p/>
+     * <p>
      * This method is called only on the master node
      *
      * @param repository repository name
@@ -328,7 +328,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     /**
      * Returns registered index shard repository
-     * <p/>
+     * <p>
      * This method is called only on data nodes
      *
      * @param repository repository name
@@ -345,7 +345,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     /**
      * Creates a new repository and adds it to the list of registered repositories.
-     * <p/>
+     * <p>
      * If a repository with the same name but different types or settings already exists, it will be closed and
      * replaced with the new repository. If a repository with the same name exists but it has the same type and settings
      * the new repository is ignored.
diff --git a/core/src/main/java/org/elasticsearch/repositories/Repository.java b/core/src/main/java/org/elasticsearch/repositories/Repository.java
index adde0ed..a766c3a 100644
--- a/core/src/main/java/org/elasticsearch/repositories/Repository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/Repository.java
@@ -29,11 +29,11 @@ import java.util.List;
 
 /**
  * Snapshot repository interface.
- * <p/>
+ * <p>
  * Responsible for index and cluster level operations. It's called only on master.
  * Shard-level operations are performed using {@link org.elasticsearch.index.snapshots.IndexShardRepository}
  * interface on data nodes.
- * <p/>
+ * <p>
  * Typical snapshot usage pattern:
  * <ul>
  * <li>Master calls {@link #initializeSnapshot(org.elasticsearch.cluster.metadata.SnapshotId, List, org.elasticsearch.cluster.metadata.MetaData)}
@@ -55,7 +55,7 @@ public interface Repository extends LifecycleComponent<Repository> {
 
     /**
      * Returns global metadata associate with the snapshot.
-     * <p/>
+     * <p>
      * The returned meta data contains global metadata as well as metadata for all indices listed in the indices parameter.
      *
      * @param snapshot snapshot
@@ -82,7 +82,7 @@ public interface Repository extends LifecycleComponent<Repository> {
 
     /**
      * Finalizes snapshotting process
-     * <p/>
+     * <p>
      * This method is called on master after all shards are snapshotted.
      *
      * @param snapshotId    snapshot id
@@ -113,7 +113,7 @@ public interface Repository extends LifecycleComponent<Repository> {
 
     /**
      * Verifies repository on the master node and returns the verification token.
-     * <p/>
+     * <p>
      * If the verification token is not null, it's passed to all data nodes for verification. If it's null - no
      * additional verification is required
      *
@@ -123,7 +123,7 @@ public interface Repository extends LifecycleComponent<Repository> {
 
     /**
      * Called at the end of repository verification process.
-     * <p/>
+     * <p>
      * This method should perform all necessary cleanup of the temporary files created in the repository
      *
      * @param verificationToken verification request generated by {@link #startVerification} command
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreFormat.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreFormat.java
index eadba12..2061c23 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreFormat.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreFormat.java
@@ -71,7 +71,6 @@ public abstract class BlobStoreFormat<T extends ToXContent> {
      * @param blobContainer blob container
      * @param blobName blob name
      * @return parsed blob object
-     * @throws IOException
      */
     public abstract T readBlob(BlobContainer blobContainer, String blobName) throws IOException;
 
@@ -81,7 +80,6 @@ public abstract class BlobStoreFormat<T extends ToXContent> {
      * @param blobContainer blob container
      * @param name          name to be translated into
      * @return parsed blob object
-     * @throws IOException
      */
     public T read(BlobContainer blobContainer, String name) throws IOException {
         String blobName = blobName(name);
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
index 4564578..8c5088e 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java
@@ -75,11 +75,10 @@ import java.util.Map;
 
 /**
  * BlobStore - based implementation of Snapshot Repository
- * <p/>
+ * <p>
  * This repository works with any {@link BlobStore} implementation. The blobStore should be initialized in the derived
  * class before {@link #doStart()} is called.
- * <p/>
- * <p/>
+ * <p>
  * BlobStoreRepository maintains the following structure in the blob store
  * <pre>
  * {@code
@@ -229,7 +228,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
 
     /**
      * Returns initialized and ready to use BlobStore
-     * <p/>
+     * <p>
      * This method is first called in the {@link #doStart()} method.
      *
      * @return blob store
@@ -252,7 +251,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
 
     /**
      * Returns data file chunk size.
-     * <p/>
+     * <p>
      * This method should return null if no chunking is needed.
      *
      * @return chunk size
@@ -533,7 +532,6 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
 
     /**
      * In v2.0.0 we changed the matadata file format
-     * @param version
      * @return true if legacy version should be used false otherwise
      */
     public static boolean legacyMetaData(Version version) {
@@ -553,7 +551,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
 
     /**
      * Writes snapshot index file
-     * <p/>
+     * <p>
      * This file can be used by read-only repositories that are unable to list files in the repository
      *
      * @param snapshots list of snapshot ids
@@ -583,7 +581,7 @@ public abstract class BlobStoreRepository extends AbstractLifecycleComponent<Rep
 
     /**
      * Reads snapshot index file
-     * <p/>
+     * <p>
      * This file can be used by read-only repositories that are unable to list files in the repository
      *
      * @return list of snapshots in the repository
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
index 9109dcd..b15da26 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/ChecksumBlobStoreFormat.java
@@ -88,8 +88,6 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
      *
      * @param blobContainer blob container
      * @param blobName blob name
-     * @return
-     * @throws IOException
      */
     public T readBlob(BlobContainer blobContainer, String blobName) throws IOException {
         try (InputStream inputStream = blobContainer.readBlob(blobName)) {
@@ -113,7 +111,7 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
 
     /**
      * Writes blob in atomic manner with resolving the blob name using {@link #blobName} and {@link #tempBlobName} methods.
-     * <p/>
+     * <p>
      * The blob will be compressed and checksum will be written if required.
      *
      * Atomic move might be very inefficient on some repositories. It also cannot override existing files.
@@ -121,7 +119,6 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
      * @param obj           object to be serialized
      * @param blobContainer blob container
      * @param name          blob name
-     * @throws IOException
      */
     public void writeAtomic(T obj, BlobContainer blobContainer, String name) throws IOException {
         String blobName = blobName(name);
@@ -138,13 +135,12 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
 
     /**
      * Writes blob with resolving the blob name using {@link #blobName} method.
-     * <p/>
+     * <p>
      * The blob will be compressed and checksum will be written if required.
      *
      * @param obj           object to be serialized
      * @param blobContainer blob container
      * @param name          blob name
-     * @throws IOException
      */
     public void write(T obj, BlobContainer blobContainer, String name) throws IOException {
         String blobName = blobName(name);
@@ -153,13 +149,12 @@ public class ChecksumBlobStoreFormat<T extends ToXContent> extends BlobStoreForm
 
     /**
      * Writes blob in atomic manner without resolving the blobName using using {@link #blobName} method.
-     * <p/>
+     * <p>
      * The blob will be compressed and checksum will be written if required.
      *
      * @param obj           object to be serialized
      * @param blobContainer blob container
      * @param blobName          blob name
-     * @throws IOException
      */
     protected void writeBlob(T obj, BlobContainer blobContainer, String blobName) throws IOException {
         BytesReference bytes = write(obj);
diff --git a/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java b/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
index a0c956d..2063712 100644
--- a/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
+++ b/core/src/main/java/org/elasticsearch/repositories/blobstore/LegacyBlobStoreFormat.java
@@ -49,7 +49,6 @@ public class LegacyBlobStoreFormat<T extends ToXContent> extends BlobStoreFormat
      * @param blobContainer blob container
      * @param blobName blob name
      * @return parsed blob object
-     * @throws IOException
      */
     public T readBlob(BlobContainer blobContainer, String blobName) throws IOException {
         try (InputStream inputStream = blobContainer.readBlob(blobName)) {
diff --git a/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java b/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
index 0071cc9..4781582 100644
--- a/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
@@ -37,14 +37,14 @@ import java.nio.file.Paths;
 
 /**
  * Shared file system implementation of the BlobStoreRepository
- * <p/>
+ * <p>
  * Shared file system repository supports the following settings
  * <dl>
  * <dt>{@code location}</dt><dd>Path to the root of repository. This is mandatory parameter.</dd>
  * <dt>{@code concurrent_streams}</dt><dd>Number of concurrent read/write stream (per repository on each node). Defaults to 5.</dd>
  * <dt>{@code chunk_size}</dt><dd>Large file can be divided into chunks. This parameter specifies the chunk size. Defaults to not chucked.</dd>
  * <dt>{@code compress}</dt><dd>If set to true metadata files will be stored compressed. Defaults to false.</dd>
- * </ol>
+ * </dl>
  */
 public class FsRepository extends BlobStoreRepository {
 
@@ -64,7 +64,6 @@ public class FsRepository extends BlobStoreRepository {
      * @param name                 repository name
      * @param repositorySettings   repository settings
      * @param indexShardRepository index shard repository
-     * @throws IOException
      */
     @Inject
     public FsRepository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, Environment environment) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
index 922c487..4d36168 100644
--- a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
@@ -40,12 +40,12 @@ import java.util.List;
 
 /**
  * Read-only URL-based implementation of the BlobStoreRepository
- * <p/>
+ * <p>
  * This repository supports the following settings
  * <dl>
  * <dt>{@code url}</dt><dd>URL to the root of repository. This is mandatory parameter.</dd>
  * <dt>{@code concurrent_streams}</dt><dd>Number of concurrent read/write stream (per repository on each node). Defaults to 5.</dd>
- * </ol>
+ * </dl>
  */
 public class URLRepository extends BlobStoreRepository {
 
@@ -75,7 +75,6 @@ public class URLRepository extends BlobStoreRepository {
      * @param name                 repository name
      * @param repositorySettings   repository settings
      * @param indexShardRepository shard repository
-     * @throws IOException
      */
     @Inject
     public URLRepository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, Environment environment) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java b/core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java
index 927c41c..1ae1e57 100644
--- a/core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java
+++ b/core/src/main/java/org/elasticsearch/rest/BaseRestHandler.java
@@ -30,8 +30,8 @@ import java.util.Set;
 
 /**
  * Base handler for REST requests.
- * <p/>
- * This handler makes sure that the headers & context of the handled {@link RestRequest requests} are copied over to
+ * <p>
+ * This handler makes sure that the headers &amp; context of the handled {@link RestRequest requests} are copied over to
  * the transport requests executed by the associated client. While the context is fully copied over, not all the headers
  * are copied, but a selected few. It is possible to control what headers are copied over by registering them using
  * {@link org.elasticsearch.rest.RestController#registerRelevantHeaders(String...)}
@@ -83,4 +83,4 @@ public abstract class BaseRestHandler extends AbstractComponent implements RestH
             super.doExecute(action, request, listener);
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/rest/RestController.java b/core/src/main/java/org/elasticsearch/rest/RestController.java
index 3e33603..cc6c09a 100644
--- a/core/src/main/java/org/elasticsearch/rest/RestController.java
+++ b/core/src/main/java/org/elasticsearch/rest/RestController.java
@@ -179,8 +179,6 @@ public class RestController extends AbstractLifecycleComponent<RestController> {
 
     /**
      * Checks the request parameters against enabled settings for error trace support
-     * @param request
-     * @param channel
      * @return true if the request does not have any parameters that conflict with system settings
      */
     boolean checkRequestParameters(final RestRequest request, final RestChannel channel) {
diff --git a/core/src/main/java/org/elasticsearch/rest/RestStatus.java b/core/src/main/java/org/elasticsearch/rest/RestStatus.java
index ee0e7ef..d78b9c5 100644
--- a/core/src/main/java/org/elasticsearch/rest/RestStatus.java
+++ b/core/src/main/java/org/elasticsearch/rest/RestStatus.java
@@ -59,8 +59,8 @@ public enum RestStatus {
      * entity format is specified by the media type given in the Content-Type header field. The origin server MUST
      * create the resource before returning the 201 status code. If the action cannot be carried out immediately, the
      * server SHOULD respond with 202 (Accepted) response instead.
-     * <p/>
-     * <p>A 201 response MAY contain an ETag response header field indicating the current value of the entity tag
+     * <p>
+     * A 201 response MAY contain an ETag response header field indicating the current value of the entity tag
      * for the requested variant just created, see section 14.19.
      */
     CREATED(201),
@@ -68,8 +68,8 @@ public enum RestStatus {
      * The request has been accepted for processing, but the processing has not been completed.  The request might
      * or might not eventually be acted upon, as it might be disallowed when processing actually takes place. There
      * is no facility for re-sending a status code from an asynchronous operation such as this.
-     * <p/>
-     * <p>The 202 response is intentionally non-committal. Its purpose is to allow a server to accept a request for
+     * <p>
+     * The 202 response is intentionally non-committal. Its purpose is to allow a server to accept a request for
      * some other process (perhaps a batch-oriented process that is only run once per day) without requiring that
      * the user agent's connection to the server persist until the process is completed. The entity returned with
      * this response SHOULD include an indication of the request's current status and either a pointer to a status
@@ -88,13 +88,13 @@ public enum RestStatus {
      * The server has fulfilled the request but does not need to return an entity-body, and might want to return
      * updated meta information. The response MAY include new or updated meta information in the form of
      * entity-headers, which if present SHOULD be associated with the requested variant.
-     * <p/>
-     * <p>If the client is a user agent, it SHOULD NOT change its document view from that which caused the request
+     * <p>
+     * If the client is a user agent, it SHOULD NOT change its document view from that which caused the request
      * to be sent. This response is primarily intended to allow input for actions to take place without causing a
      * change to the user agent's active document view, although any new or updated meta information SHOULD be
      * applied to the document currently in the user agent's active view.
-     * <p/>
-     * <p>The 204 response MUST NOT include a message-body, and thus is always terminated by the first empty
+     * <p>
+     * The 204 response MUST NOT include a message-body, and thus is always terminated by the first empty
      * line after the header fields.
      */
     NO_CONTENT(204),
@@ -109,8 +109,8 @@ public enum RestStatus {
      * The server has fulfilled the partial GET request for the resource. The request MUST have included a Range
      * header field (section 14.35) indicating the desired range, and MAY have included an If-Range header
      * field (section 14.27) to make the request conditional.
-     * <p/>
-     * <p>The response MUST include the following header fields:
+     * <p>
+     * The response MUST include the following header fields:
      * <ul>
      * <li>Either a Content-Range header field (section 14.16) indicating the range included with this response,
      * or a multipart/byteranges Content-Type including Content-Range fields for each part. If a Content-Length
@@ -121,34 +121,34 @@ public enum RestStatus {
      * <li>Expires, Cache-Control, and/or Vary, if the field-value might differ from that sent in any previous
      * response for the same variant</li>
      * </ul>
-     * <p/>
-     * <p>If the 206 response is the result of an If-Range request that used a strong cache validator
+     * <p>
+     * If the 206 response is the result of an If-Range request that used a strong cache validator
      * (see section 13.3.3), the response SHOULD NOT include other entity-headers. If the response is the result
      * of an If-Range request that used a weak validator, the response MUST NOT include other entity-headers;
      * this prevents inconsistencies between cached entity-bodies and updated headers. Otherwise, the response MUST
      * include all of the entity-headers that would have been returned with a 200 (OK) response to the same request.
-     * <p/>
-     * <p>A cache MUST NOT combine a 206 response with other previously cached content if the ETag or Last-Modified
+     * <p>
+     * A cache MUST NOT combine a 206 response with other previously cached content if the ETag or Last-Modified
      * headers do not match exactly, see 13.5.4.
-     * <p/>
-     * <p>A cache that does not support the Range and Content-Range headers MUST NOT cache 206 (Partial) responses.
+     * <p>
+     * A cache that does not support the Range and Content-Range headers MUST NOT cache 206 (Partial) responses.
      */
     PARTIAL_CONTENT(206),
     /**
      * The 207 (Multi-Status) status code provides status for multiple independent operations (see Section 13 for
      * more information).
-     * <p/>
-     * <p>A Multi-Status response conveys information about multiple resources in situations where multiple status
+     * <p>
+     * A Multi-Status response conveys information about multiple resources in situations where multiple status
      * codes might be appropriate. The default Multi-Status response body is a text/xml or application/xml HTTP
      * entity with a 'multistatus' root element. Further elements contain 200, 300, 400, and 500 series status codes
      * generated during the method invocation. 100 series status codes SHOULD NOT be recorded in a 'response'
      * XML element.
-     * <p/>
-     * <p>Although '207' is used as the overall response status code, the recipient needs to consult the contents
+     * <p>
+     * Although '207' is used as the overall response status code, the recipient needs to consult the contents
      * of the multistatus response body for further information about the success or failure of the method execution.
      * The response MAY be used in success, partial success and also in failure situations.
-     * <p/>
-     * <p>The 'multistatus' root element holds zero or more 'response' elements in any order, each with
+     * <p>
+     * The 'multistatus' root element holds zero or more 'response' elements in any order, each with
      * information about an individual resource. Each 'response' element MUST have an 'href' element
      * to identify the resource.
      */
@@ -157,14 +157,14 @@ public enum RestStatus {
      * The requested resource corresponds to any one of a set of representations, each with its own specific
      * location, and agent-driven negotiation information (section 12) is being provided so that the user (or user
      * agent) can select a preferred representation and redirect its request to that location.
-     * <p/>
-     * <p>Unless it was a HEAD request, the response SHOULD include an entity containing a list of resource
+     * <p>
+     * Unless it was a HEAD request, the response SHOULD include an entity containing a list of resource
      * characteristics and location(s) from which the user or user agent can choose the one most appropriate.
      * The entity format is specified by the media type given in the Content-Type header field. Depending upon the
      * format and the capabilities of the user agent, selection of the most appropriate choice MAY be performed
      * automatically. However, this specification does not define any standard for such automatic selection.
-     * <p/>
-     * <p>If the server has a preferred choice of representation, it SHOULD include the specific URI for that
+     * <p>
+     * If the server has a preferred choice of representation, it SHOULD include the specific URI for that
      * representation in the Location field; user agents MAY use the Location field value for automatic redirection.
      * This response is cacheable unless indicated otherwise.
      */
@@ -174,11 +174,11 @@ public enum RestStatus {
      * SHOULD use one of the returned URIs.  Clients with link editing capabilities ought to automatically re-link
      * references to the Request-URI to one or more of the new references returned by the server, where possible.
      * This response is cacheable unless indicated otherwise.
-     * <p/>
-     * <p>The new permanent URI SHOULD be given by the Location field in the response. Unless the request method
+     * <p>
+     * The new permanent URI SHOULD be given by the Location field in the response. Unless the request method
      * was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).
-     * <p/>
-     * <p>If the 301 status code is received in response to a request other than GET or HEAD, the user agent
+     * <p>
+     * If the 301 status code is received in response to a request other than GET or HEAD, the user agent
      * MUST NOT automatically redirect the request unless it can be confirmed by the user, since this might change
      * the conditions under which the request was issued.
      */
@@ -187,11 +187,11 @@ public enum RestStatus {
      * The requested resource resides temporarily under a different URI. Since the redirection might be altered on
      * occasion, the client SHOULD continue to use the Request-URI for future requests.  This response is only
      * cacheable if indicated by a Cache-Control or Expires header field.
-     * <p/>
-     * <p>The temporary URI SHOULD be given by the Location field in the response. Unless the request method was
+     * <p>
+     * The temporary URI SHOULD be given by the Location field in the response. Unless the request method was
      * HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).
-     * <p/>
-     * <p>If the 302 status code is received in response to a request other than GET or HEAD, the user agent
+     * <p>
+     * If the 302 status code is received in response to a request other than GET or HEAD, the user agent
      * MUST NOT automatically redirect the request unless it can be confirmed by the user, since this might change
      * the conditions under which the request was issued.
      */
@@ -202,8 +202,8 @@ public enum RestStatus {
      * user agent to a selected resource. The new URI is not a substitute reference for the originally requested
      * resource. The 303 response MUST NOT be cached, but the response to the second (redirected) request might be
      * cacheable.
-     * <p/>
-     * <p>The different URI SHOULD be given by the Location field in the response. Unless the request method was
+     * <p>
+     * The different URI SHOULD be given by the Location field in the response. Unless the request method was
      * HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).
      */
     SEE_OTHER(303),
@@ -211,8 +211,8 @@ public enum RestStatus {
      * If the client has performed a conditional GET request and access is allowed, but the document has not been
      * modified, the server SHOULD respond with this status code. The 304 response MUST NOT contain a message-body,
      * and thus is always terminated by the first empty line after the header fields.
-     * <p/>
-     * <p>The response MUST include the following header fields:
+     * <p>
+     * The response MUST include the following header fields:
      * <ul>
      * <li>Date, unless its omission is required by section 14.18.1
      * If a clockless origin server obeys these rules, and proxies and clients add their own Date to any
@@ -223,15 +223,15 @@ public enum RestStatus {
      * <li>Expires, Cache-Control, and/or Vary, if the field-value might differ from that sent in any previous
      * response for the same variant</li>
      * </ul>
-     * <p/>
-     * <p>If the conditional GET used a strong cache validator (see section 13.3.3), the response SHOULD NOT include
+     * <p>
+     * If the conditional GET used a strong cache validator (see section 13.3.3), the response SHOULD NOT include
      * other entity-headers. Otherwise (i.e., the conditional GET used a weak validator), the response MUST NOT
      * include other entity-headers; this prevents inconsistencies between cached entity-bodies and updated headers.
-     * <p/>
-     * <p>If a 304 response indicates an entity not currently cached, then the cache MUST disregard the response
+     * <p>
+     * If a 304 response indicates an entity not currently cached, then the cache MUST disregard the response
      * and repeat the request without the conditional.
-     * <p/>
-     * <p>If a cache uses a received 304 response to update a cache entry, the cache MUST update the entry to
+     * <p>
+     * If a cache uses a received 304 response to update a cache entry, the cache MUST update the entry to
      * reflect any new field values given in the response.
      */
     NOT_MODIFIED(304),
@@ -245,13 +245,13 @@ public enum RestStatus {
      * The requested resource resides temporarily under a different URI. Since the redirection MAY be altered on
      * occasion, the client SHOULD continue to use the Request-URI for future requests.  This response is only
      * cacheable if indicated by a Cache-Control or Expires header field.
-     * <p/>
-     * <p>The temporary URI SHOULD be given by the Location field in the response. Unless the request method was
+     * <p>
+     * The temporary URI SHOULD be given by the Location field in the response. Unless the request method was
      * HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s) ,
      * since many pre-HTTP/1.1 user agents do not understand the 307 status. Therefore, the note SHOULD contain
      * the information necessary for a user to repeat the original request on the new URI.
-     * <p/>
-     * <p>If the 307 status code is received in response to a request other than GET or HEAD, the user agent MUST NOT
+     * <p>
+     * If the 307 status code is received in response to a request other than GET or HEAD, the user agent MUST NOT
      * automatically redirect the request unless it can be confirmed by the user, since this might change the
      * conditions under which the request was issued.
      */
@@ -300,18 +300,18 @@ public enum RestStatus {
     /**
      * The resource identified by the request is only capable of generating response entities which have content
      * characteristics not acceptable according to the accept headers sent in the request.
-     * <p/>
-     * <p>Unless it was a HEAD request, the response SHOULD include an entity containing a list of available entity
+     * <p>
+     * Unless it was a HEAD request, the response SHOULD include an entity containing a list of available entity
      * characteristics and location(s) from which the user or user agent can choose the one most appropriate.
      * The entity format is specified by the media type given in the Content-Type header field. Depending upon the
      * format and the capabilities of the user agent, selection of the most appropriate choice MAY be performed
      * automatically. However, this specification does not define any standard for such automatic selection.
-     * <p/>
-     * <p>Note: HTTP/1.1 servers are allowed to return responses which are not acceptable according to the accept
+     * <p>
+     * Note: HTTP/1.1 servers are allowed to return responses which are not acceptable according to the accept
      * headers sent in the request. In some cases, this may even be preferable to sending a 406 response. User
      * agents are encouraged to inspect the headers of an incoming response to determine if it is acceptable.
-     * <p/>
-     * <p>If the response could be unacceptable, a user agent SHOULD temporarily stop receipt of more data and query
+     * <p>
+     * If the response could be unacceptable, a user agent SHOULD temporarily stop receipt of more data and query
      * the user for a decision on further actions.
      */
     NOT_ACCEPTABLE(406),
@@ -334,8 +334,8 @@ public enum RestStatus {
      * resubmit the request. The response body SHOULD include enough information for the user to recognize the
      * source of the conflict. Ideally, the response entity would include enough information for the user or user
      * agent to fix the problem; however, that might not be possible and is not required.
-     * <p/>
-     * <p>Conflicts are most likely to occur in response to a PUT request. For example, if versioning were being
+     * <p>
+     * Conflicts are most likely to occur in response to a PUT request. For example, if versioning were being
      * used and the entity being PUT included changes to a resource which conflict with those made by an earlier
      * (third-party) request, the server might use the 409 response to indicate that it can't complete the request.
      * In this case, the response entity would likely contain a list of the differences between the two versions in
@@ -348,8 +348,8 @@ public enum RestStatus {
      * the Request-URI after user approval. If the server does not know, or has no facility to determine, whether or
      * not the condition is permanent, the status code 404 (Not Found) SHOULD be used instead. This response is
      * cacheable unless indicated otherwise.
-     * <p/>
-     * <p>The 410 response is primarily intended to assist the task of web maintenance by notifying the recipient
+     * <p>
+     * The 410 response is primarily intended to assist the task of web maintenance by notifying the recipient
      * that the resource is intentionally unavailable and that the server owners desire that remote links to that
      * resource be removed. Such an event is common for limited-time, promotional services and for resources belonging
      * to individuals no longer working at the server's site. It is not necessary to mark all permanently unavailable
@@ -372,8 +372,8 @@ public enum RestStatus {
     /**
      * The server is refusing to process a request because the request entity is larger than the server is willing
      * or able to process. The server MAY close the connection to prevent the client from continuing the request.
-     * <p/>
-     * <p>If the condition is temporary, the server SHOULD include a Retry-After header field to indicate that it
+     * <p>
+     * If the condition is temporary, the server SHOULD include a Retry-After header field to indicate that it
      * is temporary and after what time the client MAY try again.
      */
     REQUEST_ENTITY_TOO_LARGE(413),
@@ -397,8 +397,8 @@ public enum RestStatus {
      * selected resource, and the request did not include an If-Range request-header field. (For byte-ranges, this
      * means that the first-byte-pos of all of the byte-range-spec values were greater than the current length of
      * the selected resource.)
-     * <p/>
-     * <p>When this status code is returned for a byte-range request, the response SHOULD include a Content-Range
+     * <p>
+     * When this status code is returned for a byte-range request, the response SHOULD include a Content-Range
      * entity-header field specifying the current length of the selected resource (see section 14.16). This
      * response MUST NOT use the multipart/byteranges content-type.
      */
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
index f29b352..8ccf201 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java
@@ -362,7 +362,7 @@ public class RestNodesAction extends AbstractCatAction {
      * Calculate the percentage of {@code used} from the {@code max} number.
      * @param used The currently used number.
      * @param max The maximum number.
-     * @return 0 if {@code max} is <= 0. Otherwise 100 * {@code used} / {@code max}.
+     * @return 0 if {@code max} is &lt;= 0. Otherwise 100 * {@code used} / {@code max}.
      */
     private short calculatePercentage(long used, long max) {
         return max <= 0 ? 0 : (short)((100d * used) / max);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
index 7c01fdd..ce306c6 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.*;
@@ -75,7 +74,13 @@ public class RestExplainAction extends BaseRestHandler {
             queryStringBuilder.lenient(request.paramAsBoolean("lenient", null));
             String defaultOperator = request.param("default_operator");
             if (defaultOperator != null) {
-                queryStringBuilder.defaultOperator(Operator.fromString(defaultOperator));
+                if ("OR".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+                } else if ("AND".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+                } else {
+                    throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+                }
             }
 
             QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 674aa69..bd17c1d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.RestRequest;
@@ -98,7 +97,13 @@ public class RestActions {
         queryBuilder.lenient(request.paramAsBoolean("lenient", null));
         String defaultOperator = request.param("default_operator");
         if (defaultOperator != null) {
-            queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
+            if ("OR".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+            } else if ("AND".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+            } else {
+                throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+            }
         }
         return new QuerySourceBuilder().setQuery(queryBuilder);
     }
diff --git a/core/src/main/java/org/elasticsearch/rest/support/RestUtils.java b/core/src/main/java/org/elasticsearch/rest/support/RestUtils.java
index cfa2a08..2d8237e 100644
--- a/core/src/main/java/org/elasticsearch/rest/support/RestUtils.java
+++ b/core/src/main/java/org/elasticsearch/rest/support/RestUtils.java
@@ -59,12 +59,14 @@ public class RestUtils {
         if (fromIndex >= s.length()) {
             return;
         }
+        
+        int queryStringLength = s.contains("#") ? s.indexOf("#") : s.length();
 
         String name = null;
         int pos = fromIndex; // Beginning of the unprocessed region
         int i;       // End of the unprocessed region
         char c = 0;  // Current character
-        for (i = fromIndex; i < s.length(); i++) {
+        for (i = fromIndex; i < queryStringLength; i++) {
             c = s.charAt(i);
             if (c == '=' && name == null) {
                 if (pos != i) {
@@ -102,7 +104,7 @@ public class RestUtils {
 
     /**
      * Decodes a bit of an URL encoded by a browser.
-     * <p/>
+     * <p>
      * This is equivalent to calling {@link #decodeComponent(String, Charset)}
      * with the UTF-8 charset (recommended to comply with RFC 3986, Section 2).
      *
@@ -118,13 +120,13 @@ public class RestUtils {
 
     /**
      * Decodes a bit of an URL encoded by a browser.
-     * <p/>
+     * <p>
      * The string is expected to be encoded as per RFC 3986, Section 2.
      * This is the encoding used by JavaScript functions {@code encodeURI}
      * and {@code encodeURIComponent}, but not {@code escape}.  For example
      * in this encoding, &eacute; (in Unicode {@code U+00E9} or in UTF-8
      * {@code 0xC3 0xA9}) is encoded as {@code %C3%A9} or {@code %c3%a9}.
-     * <p/>
+     * <p>
      * This is essentially equivalent to calling
      * <code>{@link java.net.URLDecoder URLDecoder}.{@link
      * java.net.URLDecoder#decode(String, String)}</code>
diff --git a/core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java b/core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java
index 7da1a31..6581312 100644
--- a/core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java
+++ b/core/src/main/java/org/elasticsearch/script/AbstractSearchScript.java
@@ -28,12 +28,12 @@ import java.util.Map;
 
 /**
  * A base class for any script type that is used during the search process (custom score, aggs, and so on).
- * <p/>
- * <p>If the script returns a specific numeric type, consider overriding the type specific base classes
+ * <p>
+ * If the script returns a specific numeric type, consider overriding the type specific base classes
  * such as {@link AbstractDoubleSearchScript}, {@link AbstractFloatSearchScript} and {@link AbstractLongSearchScript}
  * for better performance.
- * <p/>
- * <p>The use is required to implement the {@link #run()} method.
+ * <p>
+ * The use is required to implement the {@link #run()} method.
  */
 public abstract class AbstractSearchScript extends AbstractExecutableScript implements LeafSearchScript {
 
@@ -130,4 +130,4 @@ public abstract class AbstractSearchScript extends AbstractExecutableScript impl
     public double runAsDouble() {
         return ((Number) run()).doubleValue();
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/SearchModule.java b/core/src/main/java/org/elasticsearch/search/SearchModule.java
index fec2a48..419f6ec 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchModule.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchModule.java
@@ -24,6 +24,7 @@ import org.elasticsearch.common.inject.multibindings.Multibinder;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParserMapper;
+import org.elasticsearch.index.search.morelikethis.MoreLikeThisFetchService;
 import org.elasticsearch.search.action.SearchServiceTransportAction;
 import org.elasticsearch.search.aggregations.AggregationParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
@@ -338,6 +339,8 @@ public class SearchModule extends AbstractModule {
         bind(SearchPhaseController.class).asEagerSingleton();
         bind(FetchPhase.class).asEagerSingleton();
         bind(SearchServiceTransportAction.class).asEagerSingleton();
+        bind(MoreLikeThisFetchService.class).asEagerSingleton();
+
         if (searchServiceImpl == SearchService.class) {
             bind(SearchService.class).asEagerSingleton();
         } else {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
index 1cc0f71..13a162d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
@@ -218,7 +218,7 @@ public class AggregationBuilders {
     }
 
     /**
-     * Create a new {@link DateHistogram} aggregation with the given name.
+     * Create a new {@link DateHistogramBuilder} aggregation with the given name.
      */
     public static DateHistogramBuilder dateHistogram(String name) {
         return new DateHistogramBuilder(name);
@@ -232,14 +232,14 @@ public class AggregationBuilders {
     }
 
     /**
-     * Create a new {@link DateRange} aggregation with the given name.
+     * Create a new {@link DateRangeBuilder} aggregation with the given name.
      */
     public static DateRangeBuilder dateRange(String name) {
         return new DateRangeBuilder(name);
     }
 
     /**
-     * Create a new {@link IPv4Range} aggregation with the given name.
+     * Create a new {@link IPv4RangeBuilder} aggregation with the given name.
      */
     public static IPv4RangeBuilder ipRange(String name) {
         return new IPv4RangeBuilder(name);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
index 33244f3..8ee4d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
@@ -40,7 +40,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     /**
      * Parses the aggregation request and creates the appropriate aggregator factory for it.
      *
-     * @see {@link AggregatorFactory}
+     * @see AggregatorFactory
     */
     public interface Parser {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java
index 68ef8aa..22ff6df 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java
@@ -63,7 +63,6 @@ public class BestDocsDeferringCollector extends DeferringBucketCollector impleme
      * 
      * @param shardSize
      *            The number of top-scoring docs to collect for each bucket
-     * @param bigArrays
      */
     public BestDocsDeferringCollector(int shardSize, BigArrays bigArrays) {
         this.shardSize = shardSize;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
index e0b27bb..a7f01e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java
@@ -73,7 +73,7 @@ public abstract class BucketsAggregator extends AggregatorBase {
     }
 
     /**
-     * Same as {@link #collectBucket(int, long)}, but doesn't check if the docCounts needs to be re-sized.
+     * Same as {@link #collectBucket(LeafBucketCollector, int, long)}, but doesn't check if the docCounts needs to be re-sized.
      */
     public final void collectExistingBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {
         docCounts.increment(bucketOrd, 1);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
index c66e3f9..6f61a89 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregationBuilder.java
@@ -50,7 +50,7 @@ public class FiltersAggregationBuilder extends AggregationBuilder<FiltersAggrega
     /**
      * Add a new filter with the given key.
      * NOTE: if a filter was already defined for this key, then this filter will replace it.
-     * NOTE: the same {@link FiltersBuilder} cannot have both keyed and non-keyed filters
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters
      */
     public FiltersAggregationBuilder filter(String key, QueryBuilder filter) {
         if (keyedFilters == null) {
@@ -62,7 +62,7 @@ public class FiltersAggregationBuilder extends AggregationBuilder<FiltersAggrega
 
     /**
      * Add a new filter with no key.
-     * NOTE: the same {@link FiltersBuilder} cannot have both keyed and non-keyed filters.
+     * NOTE: the same {@link FiltersAggregationBuilder} cannot have both keyed and non-keyed filters.
      */
     public FiltersAggregationBuilder filter(QueryBuilder filter) {
         if (nonKeyedFilters == null) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
index 6b7305d..67caf37 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
@@ -28,7 +28,7 @@ import org.joda.time.DateTime;
 import java.io.IOException;
 
 /**
- * Builder for the {@link DateHistogram} aggregation.
+ * Builder for the {@code DateHistogram} aggregation.
  */
 public class DateHistogramBuilder extends ValuesSourceAggregationBuilder<DateHistogramBuilder> {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
index bcd3175..acb55f6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBuilder.java
@@ -23,7 +23,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 
 /**
- * Builder for the {@link Range} aggregation.
+ * Builder for the {@link org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder.Range} aggregation.
  */
 public class RangeBuilder extends AbstractRangeBuilder<RangeBuilder> {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
index aad1f21..35c8a30 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java
@@ -24,7 +24,7 @@ import org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder;
 import java.io.IOException;
 
 /**
- * Builder for the {@link DateRange} aggregation.
+ * Builder for the {@code DateRange} aggregation.
  */
 public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
 
@@ -50,7 +50,7 @@ public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
     }
 
     /**
-     * Same as {@link #addRange(String, double, double)} but the key will be
+     * Same as {@link #addRange(String, Object, Object)} but the key will be
      * automatically generated based on <code>from</code> and <code>to</code>.
      */
     public DateRangeBuilder addRange(Object from, Object to) {
@@ -69,7 +69,7 @@ public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
     }
 
     /**
-     * Same as {@link #addUnboundedTo(String, double)} but the key will be
+     * Same as {@link #addUnboundedTo(String, Object)} but the key will be
      * computed automatically.
      */
     public DateRangeBuilder addUnboundedTo(Object to) {
@@ -88,7 +88,7 @@ public class DateRangeBuilder extends AbstractRangeBuilder<DateRangeBuilder> {
     }
 
     /**
-     * Same as {@link #addUnboundedFrom(String, double)} but the key will be
+     * Same as {@link #addUnboundedFrom(String, Object)} but the key will be
      * computed automatically.
      */
     public DateRangeBuilder addUnboundedFrom(Object from) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
index 6d17bee..218f0dc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java
@@ -25,7 +25,7 @@ import org.elasticsearch.search.builder.SearchSourceBuilderException;
 import java.util.regex.Pattern;
 
 /**
- * Builder for the {@link IPv4Range} aggregation.
+ * Builder for the {@code IPv4Range} aggregation.
  */
 public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
 
@@ -110,7 +110,7 @@ public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
     }
 
     /**
-     * Computes the min & max ip addresses (represented as long values - same way as stored in index) represented by the given CIDR mask
+     * Computes the min &amp; max ip addresses (represented as long values - same way as stored in index) represented by the given CIDR mask
      * expression. The returned array has the length of 2, where the first entry represents the {@code min} address and the second the {@code max}.
      * A {@code -1} value for either the {@code min} or the {@code max}, represents an unbounded end. In other words:
      *
@@ -123,9 +123,6 @@ public class IPv4RangeBuilder extends AbstractRangeBuilder<IPv4RangeBuilder> {
      * <p>
      * {@code max == -1 == "255.255.255.255" }
      * </p>
-     *
-     * @param cidr
-     * @return
      */
     static long[] cidrMaskToMinMax(String cidr) {
         String[] parts = MASK_PATTERN.split(cidr);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
index 046b1c5..b67ce2a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
@@ -31,7 +31,7 @@ import java.io.IOException;
 
 /**
  * Creates an aggregation that finds interesting or unusual occurrences of terms in a result set.
- * <p/>
+ * <p>
  * This feature is marked as experimental, and may be subject to change in the future.  If you
  * use this feature, please let us know your experience with it!
  */
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 99ee7c7..70af2f0 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -117,7 +117,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, ParsingException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
             XContentParser.Token token = parser.nextToken();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index 97264e7..87dce6f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -110,7 +110,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, ParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index c6a6924..e6dcb31 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -140,7 +140,7 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, ParsingException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
             boolean backgroundIsSuperset = true;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index aceae8c..4086d36 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -79,7 +79,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, ParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index 046ca71..14c0554 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -29,10 +29,14 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
-import org.elasticsearch.script.*;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -130,7 +134,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                throws IOException, QueryShardException {
+                throws IOException, ParsingException {
             String heuristicName = parser.currentName();
             Script script = null;
             XContentParser.Token token;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
index 1125abd..9bc1f7a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
@@ -88,7 +88,7 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
      * Define a regular expression that will determine what terms should be aggregated. The regular expression is based
      * on the {@link RegExp} class.
      *
-     * @see {@link RegExp#RegExp(String)}
+     * @see RegExp#RegExp(String)
      */
     public TermsBuilder include(String regex) {
         if (includeTerms != null) {
@@ -152,7 +152,7 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
      * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
      * expression is based on the {@link RegExp} class.
      *
-     * @see {@link RegExp#RegExp(String)}
+     * @see RegExp#RegExp(String)
      */
     public TermsBuilder exclude(String regex) {
         if (excludeTerms != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
index 1eff588..98abe2b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
@@ -94,7 +94,7 @@ public class IncludeExclude {
         }
 
         /**
-         * Returns whether the given value is accepted based on the {@code include} & {@code exclude} patterns.
+         * Returns whether the given value is accepted based on the {@code include} &amp; {@code exclude} patterns.
          */
         @Override
         public boolean accept(BytesRef value) {
@@ -114,7 +114,7 @@ public class IncludeExclude {
 
         /**
          * Returns whether the given value is accepted based on the
-         * {@code include} & {@code exclude} sets.
+         * {@code include} &amp; {@code exclude} sets.
          */
         @Override
         public boolean accept(BytesRef value) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
index 10d0956..881a8e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
@@ -102,9 +102,7 @@ public class BucketHelpers {
         /**
          * Deserialize the GapPolicy from the input stream
          *
-         * @param in
          * @return    GapPolicy Enum
-         * @throws IOException
          */
         public static GapPolicy readFrom(StreamInput in) throws IOException {
             byte id = in.readByte();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
index bd6ad02..b2ee037 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
@@ -39,7 +39,7 @@ public abstract class PipelineAggregator implements Streamable {
      * Parses the pipeline aggregation request and creates the appropriate
      * pipeline aggregator factory for it.
      * 
-     * @see {@link PipelineAggregatorFactory}
+     * @see PipelineAggregatorFactory
      */
     public static interface Parser {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
index 26b38ee..6fc0185 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
@@ -56,10 +56,6 @@ public abstract class PipelineAggregatorFactory {
     /**
      * Validates the state of this factory (makes sure the factory is properly
      * configured)
-     *
-     * @param pipelineAggregatorFactories
-     * @param factories
-     * @param parent
      */
     public final void validate(AggregatorFactory parent, AggregatorFactory[] factories,
             List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
@@ -71,17 +67,6 @@ public abstract class PipelineAggregatorFactory {
     /**
      * Creates the pipeline aggregator
      *
-     * @param context
-     *            The aggregation context
-     * @param parent
-     *            The parent aggregator (if this is a top level factory, the
-     *            parent will be {@code null})
-     * @param collectsFromSingleBucket
-     *            If true then the created aggregator will only be collected
-     *            with <tt>0</tt> as a bucket ordinal. Some factories can take
-     *            advantage of this in order to return more optimized
-     *            implementations.
-     *
      * @return The created aggregator
      */
     public final PipelineAggregator create() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregator.java
index 93ccf2d..89955ef 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsPipelineAggregator.java
@@ -96,7 +96,6 @@ public abstract class BucketMetricsPipelineAggregator extends SiblingPipelineAgg
      *            the pipeline aggregators to add to the resulting aggregation
      * @param metadata
      *            the metadata to add to the resulting aggregation
-     * @return
      */
     protected abstract InternalAggregation buildAggregation(List<PipelineAggregator> pipelineAggregators, Map<String, Object> metadata);
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
index b3aca61..b2dc718 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgBuilder.java
@@ -112,9 +112,6 @@ public class MovAvgBuilder extends PipelineAggregatorBuilder<MovAvgBuilder> {
     /**
      * The hash of settings that should be provided to the model when it is
      * instantiated
-     *
-     * @param settings
-     * @return
      */
     public MovAvgBuilder settings(Map<String, Object> settings) {
         this.settings = settings;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
index 8be63e1..176d4b0 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
@@ -134,7 +134,6 @@ public class HoltWintersModel extends MovAvgModel {
          *
          * @param in  the input stream
          * @return    SeasonalityType Enum
-         * @throws IOException
          */
         public static SeasonalityType readFrom(StreamInput in) throws IOException {
             byte id = in.readByte();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
index 3de4fce..f175513 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
@@ -34,8 +34,6 @@ public abstract class MovAvgModel {
 
     /**
      * Should this model be fit to the data via a cost minimizing algorithm by default?
-     *
-     * @return
      */
     public boolean minimizeByDefault() {
         return false;
@@ -44,16 +42,12 @@ public abstract class MovAvgModel {
     /**
      * Returns if the model can be cost minimized.  Not all models have parameters
      * which can be tuned / optimized.
-     *
-     * @return
      */
     public abstract boolean canBeMinimized();
 
     /**
      * Generates a "neighboring" model, where one of the tunable parameters has been
      * randomly mutated within the allowed range.  Used for minimization
-     *
-     * @return
      */
     public abstract MovAvgModel neighboringModel();
 
@@ -111,7 +105,6 @@ public abstract class MovAvgModel {
     /**
      * Returns an empty set of predictions, filled with NaNs
      * @param numPredictions Number of empty predictions to generate
-     * @return
      */
     protected double[] emptyPredictions(int numPredictions) {
         double[] predictions = new double[numPredictions];
@@ -123,14 +116,11 @@ public abstract class MovAvgModel {
      * Write the model to the output stream
      *
      * @param out   Output stream
-     * @throws IOException
      */
     public abstract void writeTo(StreamOutput out) throws IOException;
 
     /**
      * Clone the model, returning an exact copy
-     *
-     * @return
      */
     public abstract MovAvgModel clone();
 
@@ -165,9 +155,6 @@ public abstract class MovAvgModel {
          * @param settings      Map of settings provided to this model
          * @param name          Name of parameter we are attempting to extract
          * @param defaultValue  Default value to be used if value does not exist in map
-         *
-         * @throws ParseException
-         *
          * @return Double value extracted from settings map
          */
         protected double parseDoubleParam(@Nullable Map<String, Object> settings, String name, double defaultValue) throws ParseException {
@@ -199,9 +186,6 @@ public abstract class MovAvgModel {
          * @param settings      Map of settings provided to this model
          * @param name          Name of parameter we are attempting to extract
          * @param defaultValue  Default value to be used if value does not exist in map
-         *
-         * @throws ParseException
-         *
          * @return Integer value extracted from settings map
          */
         protected int parseIntegerParam(@Nullable Map<String, Object> settings, String name, int defaultValue) throws ParseException {
@@ -227,9 +211,6 @@ public abstract class MovAvgModel {
          * @param settings      Map of settings provided to this model
          * @param name          Name of parameter we are attempting to extract
          * @param defaultValue  Default value to be used if value does not exist in map
-         *
-         * @throws SearchParseException
-         *
          * @return Boolean value extracted from settings map
          */
         protected boolean parseBoolParam(@Nullable Map<String, Object> settings, String name, boolean defaultValue) throws ParseException {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
index 85af60a..84fd26a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationPath.java
@@ -33,18 +33,16 @@ import java.util.ArrayList;
 import java.util.List;
 
 /**
- * A path that can be used to sort/order buckets (in some multi-bucket aggregations, eg terms & histogram) based on
+ * A path that can be used to sort/order buckets (in some multi-bucket aggregations, eg terms &amp; histogram) based on
  * sub-aggregations. The path may point to either a single-bucket aggregation or a metrics aggregation. If the path
  * points to a single-bucket aggregation, the sort will be applied based on the {@code doc_count} of the bucket. If this
  * path points to a metrics aggregation, if it's a single-value metrics (eg. avg, max, min, etc..) the sort will be
  * applied on that single value. If it points to a multi-value metrics, the path should point out what metric should be
  * the sort-by value.
- * <p/>
+ * <p>
  * The path has the following form:
- * <p/>
  * <center>{@code <aggregation_name>['>'<aggregation_name>*]['.'<metric_name>]}</center>
- * <p/>
- * <p/>
+ * <p>
  * Examples:
  *
  * <ul>
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/values/ScriptDoubleValues.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/values/ScriptDoubleValues.java
index 337b298..ee9e127 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/values/ScriptDoubleValues.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/values/ScriptDoubleValues.java
@@ -29,7 +29,7 @@ import java.util.Collection;
 import java.util.Iterator;
 
 /**
- * {@link DoubleValues} implementation which is based on a script
+ * {@link SortingNumericDoubleValues} implementation which is based on a script
  */
 public class ScriptDoubleValues extends SortingNumericDoubleValues implements ScorerAware {
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java
index cd5cb7d..3d1a249 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java
@@ -28,7 +28,6 @@ import org.elasticsearch.search.internal.SearchContext;
 /**
  * Parses field name values from the {@code fielddata_fields} parameter in a
  * search request.
- * <p/>
  * <pre>
  * {
  *   "query": {...},
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
index a14fdfe..125f635 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
@@ -19,15 +19,10 @@
 
 package org.elasticsearch.search.fetch.innerhits;
 
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortOrder;
+import org.elasticsearch.index.query.support.BaseInnerHitBuilder;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -37,12 +32,12 @@ import java.util.Map;
  */
 public class InnerHitsBuilder implements ToXContent {
 
-    private final Map<String, InnerHitsHolder> innerHits = new HashMap<>();
+    private Map<String, InnerHit> innerHits = new HashMap<>();
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("inner_hits");
-        for (Map.Entry<String, InnerHitsHolder> entry : innerHits.entrySet()) {
+        for (Map.Entry<String, InnerHit> entry : innerHits.entrySet()) {
             builder.startObject(entry.getKey());
             entry.getValue().toXContent(builder, params);
             builder.endObject();
@@ -50,425 +45,36 @@ public class InnerHitsBuilder implements ToXContent {
         return builder.endObject();
     }
 
-    /**
-     * For nested inner hits the path to collect child nested docs for.
-     * @param name the name / key of the inner hits in the response
-     * @param path the path into the nested to collect inner hits for
-     * @param innerHit the inner hits definition
-     */
-    public void addNestedInnerHits(String name, String path, InnerHit innerHit) {
-        if (innerHits.containsKey(name)) {
-            throw new IllegalArgumentException("inner hits for name: [" + name +"] is already registered");
-        }
-        innerHits.put(name, new NestedInnerHitsHolder(path, innerHit));
-    }
-
-    /**
-     * For parent/child inner hits the type to collect inner hits for.
-     * @param name the name / key of the inner hits in the response
-     * @param type the document type to collect inner hits for
-     * @param innerHit the inner hits definition
-     */
-    public void addParentChildInnerHits(String name, String type, InnerHit innerHit) {
-        innerHits.put(name, new ParentChildInnerHitsHolder(type, innerHit));
-    }
-
-    private static class InnerHitsHolder implements ToXContent{
-        private final InnerHit hits;
-
-        private InnerHitsHolder(InnerHit hits) {
-            this.hits = hits;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            return hits.toXContent(builder, params);
-        }
-    }
-
-    private static class ParentChildInnerHitsHolder extends InnerHitsHolder {
-
-        private final String type;
-
-        private ParentChildInnerHitsHolder(String type, InnerHit hits) {
-            super(hits);
-            this.type = type;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject("type").startObject(type);
-            super.toXContent(builder, params);
-            return builder.endObject().endObject();
-        }
-    }
-
-    private static class NestedInnerHitsHolder extends InnerHitsHolder {
-
-        private final String path;
-
-        private NestedInnerHitsHolder(String path, InnerHit hits) {
-            super(hits);
-            this.path = path;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject("path").startObject(path);
-            super.toXContent(builder, params);
-            return builder.endObject().endObject();
-        }
+    public void addInnerHit(String name, InnerHit innerHit) {
+        innerHits.put(name, innerHit);
     }
 
-    public static class InnerHit implements ToXContent {
+    public static class InnerHit extends BaseInnerHitBuilder<InnerHit> {
 
-        private SearchSourceBuilder sourceBuilder;
         private String path;
         private String type;
 
         /**
-         * The index to start to return hits from. Defaults to <tt>0</tt>.
-         */
-        public InnerHit setFrom(int from) {
-            sourceBuilder().from(from);
-            return this;
-        }
-
-        /**
-         * The number of search hits to return. Defaults to <tt>10</tt>.
-         */
-        public InnerHit setSize(int size) {
-            sourceBuilder().size(size);
-            return this;
-        }
-
-        /**
-         * Applies when sorting, and controls if scores will be tracked as well. Defaults to
-         * <tt>false</tt>.
-         */
-        public InnerHit setTrackScores(boolean trackScores) {
-            sourceBuilder().trackScores(trackScores);
-            return this;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
-         * explanation of the hit (ranking).
-         */
-        public InnerHit setExplain(boolean explain) {
-            sourceBuilder().explain(explain);
-            return this;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
-         * version.
-         */
-        public InnerHit setVersion(boolean version) {
-            sourceBuilder().version(version);
-            return this;
-        }
-
-        /**
-         * Add a stored field to be loaded and returned with the inner hit.
-         */
-        public InnerHit field(String name) {
-            sourceBuilder().field(name);
-            return this;
-        }
-
-        /**
-         * Sets no fields to be loaded, resulting in only id and type to be returned per field.
-         */
-        public InnerHit setNoFields() {
-            sourceBuilder().noFields();
-            return this;
-        }
-
-        /**
-         * Indicates whether the response should contain the stored _source for every hit
-         */
-        public InnerHit setFetchSource(boolean fetch) {
-            sourceBuilder().fetchSource(fetch);
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
-         * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
-         */
-        public InnerHit setFetchSource(@Nullable String include, @Nullable String exclude) {
-            sourceBuilder().fetchSource(include, exclude);
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
-         * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
-         */
-        public InnerHit setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-            sourceBuilder().fetchSource(includes, excludes);
-            return this;
-        }
-
-        /**
-         * Adds a field data based field to load and return. The field does not have to be stored,
-         * but its recommended to use non analyzed or numeric fields.
-         *
-         * @param name The field to get from the field data cache
-         */
-        public InnerHit addFieldDataField(String name) {
-            sourceBuilder().fieldDataField(name);
-            return this;
-        }
-
-        /**
-         * Adds a script based field to load and return. The field does not have to be stored,
-         * but its recommended to use non analyzed or numeric fields.
-         *
-         * @param name   The name that will represent this value in the return hit
-         * @param script The script to use
-         */
-        public InnerHit addScriptField(String name, Script script) {
-            sourceBuilder().scriptField(name, script);
-            return this;
-        }
-
-        /**
-         * Adds a sort against the given field name and the sort ordering.
-         *
-         * @param field The name of the field
-         * @param order The sort ordering
-         */
-        public InnerHit addSort(String field, SortOrder order) {
-            sourceBuilder().sort(field, order);
-            return this;
-        }
-
-        /**
-         * Adds a generic sort builder.
-         *
-         * @see org.elasticsearch.search.sort.SortBuilders
-         */
-        public InnerHit addSort(SortBuilder sort) {
-            sourceBuilder().sort(sort);
-            return this;
-        }
-
-        public HighlightBuilder highlightBuilder() {
-            return sourceBuilder().highlighter();
-        }
-
-        /**
-         * Adds a field to be highlighted with default fragment size of 100 characters, and
-         * default number of fragments of 5.
-         *
-         * @param name The field to highlight
-         */
-        public InnerHit addHighlightedField(String name) {
-            highlightBuilder().field(name);
-            return this;
-        }
-
-
-        /**
-         * Adds a field to be highlighted with a provided fragment size (in characters), and
-         * default number of fragments of 5.
-         *
-         * @param name         The field to highlight
-         * @param fragmentSize The size of a fragment in characters
-         */
-        public InnerHit addHighlightedField(String name, int fragmentSize) {
-            highlightBuilder().field(name, fragmentSize);
-            return this;
-        }
-
-        /**
-         * Adds a field to be highlighted with a provided fragment size (in characters), and
-         * a provided (maximum) number of fragments.
-         *
-         * @param name              The field to highlight
-         * @param fragmentSize      The size of a fragment in characters
-         * @param numberOfFragments The (maximum) number of fragments
-         */
-        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
-            highlightBuilder().field(name, fragmentSize, numberOfFragments);
-            return this;
-        }
-
-        /**
-         * Adds a field to be highlighted with a provided fragment size (in characters),
-         * a provided (maximum) number of fragments and an offset for the highlight.
-         *
-         * @param name              The field to highlight
-         * @param fragmentSize      The size of a fragment in characters
-         * @param numberOfFragments The (maximum) number of fragments
-         */
-        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments,
-                                            int fragmentOffset) {
-            highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
-            return this;
-        }
-
-        /**
-         * Adds a highlighted field.
-         */
-        public InnerHit addHighlightedField(HighlightBuilder.Field field) {
-            highlightBuilder().field(field);
-            return this;
-        }
-
-        /**
-         * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
-         * are <tt>styled</tt> and <tt>default</tt>.
-         *
-         * @param schemaName The tag scheme name
-         */
-        public InnerHit setHighlighterTagsSchema(String schemaName) {
-            highlightBuilder().tagsSchema(schemaName);
-            return this;
-        }
-
-        public InnerHit setHighlighterFragmentSize(Integer fragmentSize) {
-            highlightBuilder().fragmentSize(fragmentSize);
-            return this;
-        }
-
-        public InnerHit setHighlighterNumOfFragments(Integer numOfFragments) {
-            highlightBuilder().numOfFragments(numOfFragments);
-            return this;
-        }
-
-        public InnerHit setHighlighterFilter(Boolean highlightFilter) {
-            highlightBuilder().highlightFilter(highlightFilter);
-            return this;
-        }
-
-        /**
-         * The encoder to set for highlighting
-         */
-        public InnerHit setHighlighterEncoder(String encoder) {
-            highlightBuilder().encoder(encoder);
-            return this;
-        }
-
-        /**
-         * Explicitly set the pre tags that will be used for highlighting.
-         */
-        public InnerHit setHighlighterPreTags(String... preTags) {
-            highlightBuilder().preTags(preTags);
-            return this;
-        }
-
-        /**
-         * Explicitly set the post tags that will be used for highlighting.
-         */
-        public InnerHit setHighlighterPostTags(String... postTags) {
-            highlightBuilder().postTags(postTags);
-            return this;
-        }
-
-        /**
-         * The order of fragments per field. By default, ordered by the order in the
-         * highlighted text. Can be <tt>score</tt>, which then it will be ordered
-         * by score of the fragments.
-         */
-        public InnerHit setHighlighterOrder(String order) {
-            highlightBuilder().order(order);
-            return this;
-        }
-
-        public InnerHit setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
-            highlightBuilder().requireFieldMatch(requireFieldMatch);
-            return this;
-        }
-
-        public InnerHit setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
-            highlightBuilder().boundaryMaxScan(boundaryMaxScan);
-            return this;
-        }
-
-        public InnerHit setHighlighterBoundaryChars(char[] boundaryChars) {
-            highlightBuilder().boundaryChars(boundaryChars);
-            return this;
-        }
-
-        /**
-         * The highlighter type to use.
-         */
-        public InnerHit setHighlighterType(String type) {
-            highlightBuilder().highlighterType(type);
-            return this;
-        }
-
-        public InnerHit setHighlighterFragmenter(String fragmenter) {
-            highlightBuilder().fragmenter(fragmenter);
-            return this;
-        }
-
-        /**
-         * Sets a query to be used for highlighting all fields instead of the search query.
-         */
-        public InnerHit setHighlighterQuery(QueryBuilder highlightQuery) {
-            highlightBuilder().highlightQuery(highlightQuery);
-            return this;
-        }
-
-        /**
-         * Sets the size of the fragment to return from the beginning of the field if there are no matches to
-         * highlight and the field doesn't also define noMatchSize.
-         *
-         * @param noMatchSize integer to set or null to leave out of request.  default is null.
-         * @return this builder for chaining
+         * Sets the query to run for collecting the inner hits.
          */
-        public InnerHit setHighlighterNoMatchSize(Integer noMatchSize) {
-            highlightBuilder().noMatchSize(noMatchSize);
+        public InnerHit setQuery(QueryBuilder query) {
+            sourceBuilder().query(query);
             return this;
         }
 
         /**
-         * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+         * For parent/child inner hits the type to collect inner hits for.
          */
-        public InnerHit setHighlighterPhraseLimit(Integer phraseLimit) {
-            highlightBuilder().phraseLimit(phraseLimit);
-            return this;
-        }
-
-        public InnerHit setHighlighterOptions(Map<String, Object> options) {
-            highlightBuilder().options(options);
-            return this;
-        }
-
-        protected SearchSourceBuilder sourceBuilder() {
-            if (sourceBuilder == null) {
-                sourceBuilder = new SearchSourceBuilder();
-            }
-            return sourceBuilder;
-        }
-
-        /**
-         * Sets the query to run for collecting the inner hits.
-         */
-        public InnerHit setQuery(QueryBuilder query) {
-            sourceBuilder().query(query);
+        public InnerHit setPath(String path) {
+            this.path = path;
             return this;
         }
 
-
-
-
         /**
-         * Adds a nested inner hit definition that collects inner hits for hits
-         * on this inner hit level.
+         * For nested inner hits the path to collect child nested docs for.
          */
-        public InnerHit addNestedInnerHits(String name, String path, InnerHit innerHit) {
-            sourceBuilder().innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+        public InnerHit setType(String type) {
+            this.type = type;
             return this;
         }
 
@@ -476,17 +82,21 @@ public class InnerHitsBuilder implements ToXContent {
          * Adds a nested inner hit definition that collects inner hits for hits
          * on this inner hit level.
          */
-        public InnerHit addParentChildInnerHits(String name, String type, InnerHit innerHit) {
-            sourceBuilder().innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
+        public InnerHit addInnerHit(String name, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addInnerHit(name, innerHit);
             return this;
         }
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            if (sourceBuilder != null) {
-                sourceBuilder.innerToXContent(builder, params);
+            if (path != null) {
+                builder.startObject("path").startObject(path);
+            } else {
+                builder.startObject("type").startObject(type);
             }
-            return builder;
+            super.toXContent(builder, params);
+            return builder.endObject().endObject();
         }
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
index ac6dc18..c02e2c6 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
@@ -59,15 +59,15 @@ public class InnerHitsParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext searchContext) throws Exception {
-        QueryShardContext context = searchContext.queryParserService().getShardContext();
-        context.reset(parser);
-        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, context, searchContext);
+        QueryParseContext parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
+        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, parseContext, searchContext);
         if (innerHitsMap != null) {
             searchContext.innerHits(new InnerHitsContext(innerHitsMap));
         }
     }
 
-    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryShardContext context, SearchContext searchContext) throws Exception {
+    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext) throws Exception {
         XContentParser.Token token;
         Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -79,7 +79,7 @@ public class InnerHitsParseElement implements SearchParseElement {
             if (token != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Inner hit definition for [" + innerHitName + " starts with a [" + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
             }
-            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, context, searchContext, innerHitName);
+            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, parseContext, searchContext, innerHitName);
             if (innerHitsMap == null) {
                 innerHitsMap = new HashMap<>();
             }
@@ -88,7 +88,7 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHitsMap;
     }
 
-    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryShardContext context, SearchContext searchContext, String innerHitName) throws Exception {
+    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String innerHitName) throws Exception {
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new IllegalArgumentException("Unexpected token " + token + " inside inner hit definition. Either specify [path] or [type] object");
@@ -123,9 +123,9 @@ public class InnerHitsParseElement implements SearchParseElement {
 
         final InnerHitsContext.BaseInnerHits innerHits;
         if (nestedPath != null) {
-            innerHits = parseNested(parser, context, searchContext, fieldName);
+            innerHits = parseNested(parser, parseContext, searchContext, fieldName);
         } else if (type != null) {
-            innerHits = parseParentChild(parser, context, searchContext, fieldName);
+            innerHits = parseParentChild(parser, parseContext, searchContext, fieldName);
         } else {
             throw new IllegalArgumentException("Either [path] or [type] must be defined");
         }
@@ -143,16 +143,16 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHits;
     }
 
-    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryShardContext context, SearchContext searchContext, String type) throws Exception {
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
+    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String type) throws Exception {
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
         DocumentMapper documentMapper = searchContext.mapperService().documentMapper(type);
         if (documentMapper == null) {
             throw new IllegalArgumentException("type [" + type + "] doesn't exist");
         }
-        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), context.mapperService(), documentMapper);
+        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parseContext.mapperService(), documentMapper);
     }
 
-    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryShardContext context, SearchContext searchContext, String nestedPath) throws Exception {
+    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String nestedPath) throws Exception {
         ObjectMapper objectMapper = searchContext.getObjectMapper(nestedPath);
         if (objectMapper == null) {
             throw new IllegalArgumentException("path [" + nestedPath +"] doesn't exist");
@@ -160,14 +160,14 @@ public class InnerHitsParseElement implements SearchParseElement {
         if (objectMapper.nested().isNested() == false) {
             throw new IllegalArgumentException("path [" + nestedPath +"] isn't nested");
         }
-        ObjectMapper parentObjectMapper = context.nestedScope().nextLevel(objectMapper);
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
-        context.nestedScope().previousLevel();
+        ObjectMapper parentObjectMapper = parseContext.nestedScope().nextLevel(objectMapper);
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
+        parseContext.nestedScope().previousLevel();
 
         return new InnerHitsContext.NestedInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parentObjectMapper, objectMapper);
     }
 
-    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryShardContext context, XContentParser parser) throws Exception {
+    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryParseContext parseContext, XContentParser parser) throws Exception {
         ParsedQuery query = null;
         Map<String, InnerHitsContext.BaseInnerHits> childInnerHits = null;
         SubSearchContext subSearchContext = new SubSearchContext(searchContext);
@@ -178,10 +178,10 @@ public class InnerHitsParseElement implements SearchParseElement {
                 fieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(fieldName)) {
-                    Query q = searchContext.queryParserService().parseInnerQuery(context);
-                    query = new ParsedQuery(q, context.copyNamedQueries());
+                    Query q = searchContext.queryParserService().parseInnerQuery(parseContext);
+                    query = new ParsedQuery(q, parseContext.copyNamedQueries());
                 } else if ("inner_hits".equals(fieldName)) {
-                    childInnerHits = parseInnerHits(parser, context, searchContext);
+                    childInnerHits = parseInnerHits(parser, parseContext, searchContext);
                 } else {
                     parseCommonInnerHitOptions(parser, token, fieldName, subSearchContext, sortParseElement, sourceParseElement, highlighterParseElement, scriptFieldsParseElement, fieldDataFieldsParseElement);
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index c082859..695598e 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -603,7 +603,7 @@ public class HighlightBuilder implements ToXContent {
 
         /**
          * Allows to set custom options for custom highlighters.
-         * This overrides global settings set by {@link HighlightBuilder#options(Map<String, Object>)}.
+         * This overrides global settings set by {@link HighlightBuilder#options(Map)}.
          */
         public Field options(Map<String, Object> options) {
             this.options = options;
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 73665a5..6402d5e 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -41,7 +41,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
@@ -70,12 +70,12 @@ public abstract class SearchContext extends DelegatingHasContextAndHeaders imple
 
     public static void setCurrent(SearchContext value) {
         current.set(value);
-        QueryShardContext.setTypes(value.types());
+        QueryParseContext.setTypes(value.types());
     }
 
     public static void removeCurrent() {
         current.remove();
-        QueryShardContext.removeTypes();
+        QueryParseContext.removeTypes();
     }
 
     public static SearchContext current() {
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
index a039250..ca8c074 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
@@ -41,7 +41,6 @@ import static org.elasticsearch.search.Scroll.readScroll;
  * Used by warmers and by api that need to create a search context within their execution.
  *
  * Source structure:
- * <p/>
  * <pre>
  * {
  *  from : 0, size : 20, (optional, can be set on the request)
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
index 3d2f011..0150ef7 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
@@ -25,32 +25,32 @@ import org.apache.lucene.index.LeafReaderContext;
 public class IndexLookup {
 
     /**
-     * Flag to pass to {@link IndexField#get(String, flags)} if you require
+     * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * offsets in the returned {@link IndexFieldTerm}.
      */
     public static final int FLAG_OFFSETS = 2;
 
     /**
-     * Flag to pass to {@link IndexField#get(String, flags)} if you require
+     * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * payloads in the returned {@link IndexFieldTerm}.
      */
     public static final int FLAG_PAYLOADS = 4;
 
     /**
-     * Flag to pass to {@link IndexField#get(String, flags)} if you require
+     * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * frequencies in the returned {@link IndexFieldTerm}. Frequencies might be
      * returned anyway for some lucene codecs even if this flag is no set.
      */
     public static final int FLAG_FREQUENCIES = 8;
 
     /**
-     * Flag to pass to {@link IndexField#get(String, flags)} if you require
+     * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * positions in the returned {@link IndexFieldTerm}.
      */
     public static final int FLAG_POSITIONS = 16;
 
     /**
-     * Flag to pass to {@link IndexField#get(String, flags)} if you require
+     * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * positions in the returned {@link IndexFieldTerm}.
      */
     public static final int FLAG_CACHE = 32;
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
index f83a603..b4c3925 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java
@@ -24,8 +24,9 @@ import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.ObjectParser;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.search.internal.ContextIndexSearcher;
 import org.elasticsearch.search.internal.SearchContext;
@@ -179,44 +180,18 @@ public final class QueryRescorer implements Rescorer {
         }
     }
 
+    private static final ObjectParser<QueryRescoreContext, SearchContext> RESCORE_PARSER = new ObjectParser<>("query", null);
+
+    static {
+        RESCORE_PARSER.declareObject(QueryRescoreContext::setParsedQuery, (p, c) -> c.queryParserService().parse(p), new ParseField("rescore_query"));
+        RESCORE_PARSER.declareFloat(QueryRescoreContext::setQueryWeight, new ParseField("query_weight"));
+        RESCORE_PARSER.declareFloat(QueryRescoreContext::setRescoreQueryWeight, new ParseField("rescore_query_weight"));
+        RESCORE_PARSER.declareString(QueryRescoreContext::setScoreMode, new ParseField("score_mode"));
+    }
+
     @Override
     public RescoreSearchContext parse(XContentParser parser, SearchContext context) throws IOException {
-        Token token;
-        String fieldName = null;
-        QueryRescoreContext rescoreContext = new QueryRescoreContext(this);
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                fieldName = parser.currentName();
-                if ("rescore_query".equals(fieldName)) {
-                    ParsedQuery parsedQuery = context.queryParserService().parse(parser);
-                    rescoreContext.setParsedQuery(parsedQuery);
-                }
-            } else if (token.isValue()) {
-                if ("query_weight".equals(fieldName)) {
-                    rescoreContext.setQueryWeight(parser.floatValue());
-                } else if ("rescore_query_weight".equals(fieldName)) {
-                    rescoreContext.setRescoreQueryWeight(parser.floatValue());
-                } else if ("score_mode".equals(fieldName)) {
-                    String sScoreMode = parser.text();
-                    if ("avg".equals(sScoreMode)) {
-                        rescoreContext.setScoreMode(ScoreMode.Avg);
-                    } else if ("max".equals(sScoreMode)) {
-                        rescoreContext.setScoreMode(ScoreMode.Max);
-                    } else if ("min".equals(sScoreMode)) {
-                        rescoreContext.setScoreMode(ScoreMode.Min);
-                    } else if ("total".equals(sScoreMode)) {
-                        rescoreContext.setScoreMode(ScoreMode.Total);
-                    } else if ("multiply".equals(sScoreMode)) {
-                        rescoreContext.setScoreMode(ScoreMode.Multiply);
-                    } else {
-                        throw new IllegalArgumentException("[rescore] illegal score_mode [" + sScoreMode + "]");
-                    }
-                } else {
-                    throw new IllegalArgumentException("rescore doesn't support [" + fieldName + "]");
-                }
-            }
-        }
-        return rescoreContext;
+        return RESCORE_PARSER.parse(parser, new QueryRescoreContext(this), context);
     }
 
     private final static Comparator<ScoreDoc> SCORE_DOC_COMPARATOR = new Comparator<ScoreDoc>() {
@@ -227,7 +202,7 @@ public final class QueryRescorer implements Rescorer {
         }
     };
 
-    /** Returns a new {@link TopDocs} with the topN from the incoming one, or the same TopDocs if the number of hits is already <=
+    /** Returns a new {@link TopDocs} with the topN from the incoming one, or the same TopDocs if the number of hits is already &lt;=
      *  topN. */
     private TopDocs topN(TopDocs in, int topN) {
         if (in.totalHits < topN) {
@@ -305,6 +280,22 @@ public final class QueryRescorer implements Rescorer {
             this.scoreMode = scoreMode;
         }
 
+        public void setScoreMode(String scoreMode) {
+            if ("avg".equals(scoreMode)) {
+                setScoreMode(ScoreMode.Avg);
+            } else if ("max".equals(scoreMode)) {
+                setScoreMode(ScoreMode.Max);
+            } else if ("min".equals(scoreMode)) {
+                setScoreMode(ScoreMode.Min);
+            } else if ("total".equals(scoreMode)) {
+                setScoreMode(ScoreMode.Total);
+            } else if ("multiply".equals(scoreMode)) {
+                setScoreMode(ScoreMode.Multiply);
+            } else {
+                throw new IllegalArgumentException("illegal score_mode [" + scoreMode + "]");
+            }
+        }
+
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java b/core/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java
index 1a95c96..c415fd5 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java
@@ -102,7 +102,7 @@ public class FieldSortBuilder extends SortBuilder {
     /**
      * Defines what values to pick in the case a document contains multiple values for the targeted sort field.
      * Possible values: min, max, sum and avg
-     * <p/>
+     * <p>
      * The last two values are only applicable for number based fields.
      */
     public FieldSortBuilder sortMode(String sortMode) {
diff --git a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
index 405e2cc..f10db63 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
@@ -170,6 +170,7 @@ public class GeoDistanceSortParser implements SortParser {
 
         final Nested nested;
         if (nestedHelper != null && nestedHelper.getPath() != null) {
+            
             BitSetProducer rootDocumentsFilter = context.bitsetFilterCache().getBitSetProducer(Queries.newNonNestedFilter());
             Filter innerDocumentsFilter;
             if (nestedHelper.filterFound()) {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java b/core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java
index 57d18c3..ea45c10 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/SuggestBuilder.java
@@ -31,7 +31,7 @@ import java.util.List;
 /**
  * Defines how to perform suggesting. This builders allows a number of global options to be specified and
  * an arbitrary number of {@link org.elasticsearch.search.suggest.term.TermSuggestionBuilder} instances.
- * <p/>
+ * <p>
  * Suggesting works by suggesting terms that appear in the suggest text that are similar compared to the terms in
  * provided text. These spelling suggestions are based on several options described in this class.
  */
@@ -53,7 +53,7 @@ public class SuggestBuilder extends ToXContentToBytes {
     /**
      * Sets the text to provide suggestions for. The suggest text is a required option that needs
      * to be set either via this setter or via the {@link org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder#setText(String)} method.
-     * <p/>
+     * <p>
      * The suggest text gets analyzed by the suggest analyzer or the suggest field search analyzer.
      * For each analyzed token, suggested terms are suggested if possible.
      */
@@ -268,7 +268,7 @@ public class SuggestBuilder extends ToXContentToBytes {
          * individual shard. During the reduce phase the only the top N suggestions
          * are returned based on the <code>size</code> option. Defaults to the
          * <code>size</code> option.
-         * <p/>
+         * <p>
          * Setting this to a value higher than the `size` can be useful in order to
          * get a more accurate document frequency for suggested terms. Due to the
          * fact that terms are partitioned amongst shards, the shard level document
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/context/CategoryContextMapping.java b/core/src/main/java/org/elasticsearch/search/suggest/context/CategoryContextMapping.java
index ce807d8..118d95e 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/context/CategoryContextMapping.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/context/CategoryContextMapping.java
@@ -110,7 +110,7 @@ public class CategoryContextMapping extends ContextMapping {
     /**
      * Load the specification of a {@link CategoryContextMapping}
      * 
-     * @param field
+     * @param name
      *            name of the field to use. If <code>null</code> default field
      *            will be used
      * @return new {@link CategoryContextMapping}
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/context/ContextBuilder.java b/core/src/main/java/org/elasticsearch/search/suggest/context/ContextBuilder.java
index 58fb91b..8b554d9 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/context/ContextBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/context/ContextBuilder.java
@@ -57,14 +57,14 @@ public abstract class ContextBuilder<E extends ContextMapping> {
     }
 
     /**
-     * Create a new {@link CategoryMapping}
+     * Create a new {@link CategoryContextMapping.Builder}
      */
     public static CategoryContextMapping.Builder category(String name) {
         return new CategoryContextMapping.Builder(name, null);
     }
 
     /**
-     * Create a new {@link CategoryMapping} with default category
+     * Create a new {@link CategoryContextMapping.Builder} with default category
      * 
      * @param defaultCategory category to use, if it is not provided
      */
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java b/core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java
index 65232ca..bbdb614 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java
@@ -107,9 +107,6 @@ public abstract class ContextMapping implements ToXContent {
      * @param parseContext context of parsing phase 
      * @param parser {@link XContentParser} used to read and setup the configuration
      * @return A {@link ContextConfig} related to <b>this</b> mapping
-     * 
-     * @throws IOException
-     * @throws ElasticsearchParseException
      */
     public abstract ContextConfig parseContext(ParseContext parseContext, XContentParser parser) throws IOException, ElasticsearchParseException;
 
@@ -122,9 +119,6 @@ public abstract class ContextMapping implements ToXContent {
      * @param parser {@link XContentParser} providing the data of the query
      * 
      * @return {@link ContextQuery} according to this mapping
-     * 
-     * @throws IOException
-     * @throws ElasticsearchParseException
      */
     public abstract ContextQuery parseQuery(String name, XContentParser parser) throws IOException, ElasticsearchParseException;
 
@@ -136,8 +130,6 @@ public abstract class ContextMapping implements ToXContent {
      * @param params parameters passed to the builder
      * 
      * @return the builder used
-     * 
-     * @throws IOException
      */
     protected abstract XContentBuilder toInnerXContent(XContentBuilder builder, Params params) throws IOException;
 
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
index d5e942b..1055fbe 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestionBuilder.java
@@ -148,7 +148,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
 
     /**
      * Sets an explicit smoothing model used for this suggester. The default is
-     * {@link PhraseSuggester#StupidBackoff}.
+     * {@link PhraseSuggestionBuilder.StupidBackoff}.
      */
     public PhraseSuggestionBuilder smoothingModel(SmoothingModel model) {
         this.model = model;
@@ -318,8 +318,6 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
         /**
          * Creates a Laplace smoothing model.
          *
-         * @param discount
-         *            the discount given to lower order ngrams if the higher order ngram doesn't exits
          */
         public Laplace(double alpha) {
             super("laplace");
@@ -430,10 +428,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
         private Float minDocFreq;
 
         /**
-         * Sets from what field to fetch the candidate suggestions from. This is
-         * an required option and needs to be set via this setter or
-         * {@link org.elasticsearch.search.suggest.SuggestBuilder.TermSuggestionBuilder#setField(String)}
-         * method
+         * @param field Sets from what field to fetch the candidate suggestions from. 
          */
         public DirectCandidateGenerator(String field) {
             super("direct_generator");
@@ -463,7 +458,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
          * the original suggest text tokens. A value between 0 and 1 can be
          * specified. This value will be compared to the string distance result
          * of each candidate spelling correction.
-         * <p/>
+         * <p>
          * Default is <tt>0.5</tt>
          */
         public DirectCandidateGenerator accuracy(float accuracy) {
@@ -491,7 +486,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
          * <li><code>frequency</code> - Sort should first be based on document
          * frequency, then scotr and then the term itself.
          * </ol>
-         * <p/>
+         * <p>
          * What the score is depends on the suggester being used.
          */
         public DirectCandidateGenerator sort(String sort) {
@@ -548,7 +543,7 @@ public final class PhraseSuggestionBuilder extends SuggestionBuilder<PhraseSugge
          * number (e.g 0.4) or an absolute number to represent document
          * frequencies. If an value higher than 1 is specified then fractional
          * can not be specified. Defaults to <tt>0.01</tt>.
-         * <p/>
+         * <p>
          * This can be used to exclude high frequency terms from being
          * suggested. High frequency terms are usually spelled correctly on top
          * of this this also improves the suggest performance.
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java b/core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java
index 0fd5ae1..03eb388 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestionBuilder.java
@@ -71,7 +71,7 @@ public class TermSuggestionBuilder extends SuggestionBuilder<TermSuggestionBuild
      * original suggest text tokens. A value between 0 and 1 can be specified.
      * This value will be compared to the string distance result of each
      * candidate spelling correction.
-     * <p/>
+     * <p>
      * Default is <tt>0.5</tt>
      */
     public TermSuggestionBuilder setAccuracy(float accuracy) {
@@ -88,7 +88,7 @@ public class TermSuggestionBuilder extends SuggestionBuilder<TermSuggestionBuild
      * <li><code>frequency</code> - Sort should first be based on document
      * frequency, then scotr and then the term itself.
      * </ol>
-     * <p/>
+     * <p>
      * What the score is depends on the suggester being used.
      */
     public TermSuggestionBuilder sort(String sort) {
@@ -145,7 +145,7 @@ public class TermSuggestionBuilder extends SuggestionBuilder<TermSuggestionBuild
      * 0.4) or an absolute number to represent document frequencies. If an value
      * higher than 1 is specified then fractional can not be specified. Defaults
      * to <tt>0.01</tt>.
-     * <p/>
+     * <p>
      * This can be used to exclude high frequency terms from being suggested.
      * High frequency terms are usually spelled correctly on top of this this
      * also improves the suggest performance.
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
index cc9eeb0..866cd38 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreInfo.java
@@ -33,7 +33,7 @@ import java.util.List;
 
 /**
  * Information about successfully completed restore operation.
- * <p/>
+ * <p>
  * Returned as part of {@link org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse}
  */
 public class RestoreInfo implements ToXContent, Streamable {
@@ -176,7 +176,6 @@ public class RestoreInfo implements ToXContent, Streamable {
      *
      * @param in stream input
      * @return restore info
-     * @throws IOException
      */
     public static RestoreInfo readRestoreInfo(StreamInput in) throws IOException {
         RestoreInfo snapshotInfo = new RestoreInfo();
@@ -189,7 +188,6 @@ public class RestoreInfo implements ToXContent, Streamable {
      *
      * @param in stream input
      * @return restore info
-     * @throws IOException
      */
     public static RestoreInfo readOptionalRestoreInfo(StreamInput in) throws IOException {
         return in.readOptionalStreamable(new RestoreInfo());
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index 39c3c5b..5692100 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -104,22 +104,22 @@ import static org.elasticsearch.cluster.metadata.MetaDataIndexStateService.INDEX
 
 /**
  * Service responsible for restoring snapshots
- * <p/>
+ * <p>
  * Restore operation is performed in several stages.
- * <p/>
- * First {@link #restoreSnapshot(RestoreRequest, org.elasticsearch.action.ActionListener))}
+ * <p>
+ * First {@link #restoreSnapshot(RestoreRequest, org.elasticsearch.action.ActionListener)}
  * method reads information about snapshot and metadata from repository. In update cluster state task it checks restore
  * preconditions, restores global state if needed, creates {@link RestoreInProgress} record with list of shards that needs
- * to be restored and adds this shard to the routing table using {@link RoutingTable.Builder#addAsRestore(IndexMetaData, RestoreSource)}
+ * to be restored and adds this shard to the routing table using {@link org.elasticsearch.cluster.routing.RoutingTable.Builder#addAsRestore(IndexMetaData, RestoreSource)}
  * method.
- * <p/>
+ * <p>
  * Individual shards are getting restored as part of normal recovery process in
  * {@link StoreRecoveryService#recover(IndexShard, boolean, StoreRecoveryService.RecoveryListener)}
  * method, which detects that shard should be restored from snapshot rather than recovered from gateway by looking
  * at the {@link org.elasticsearch.cluster.routing.ShardRouting#restoreSource()} property. If this property is not null
  * {@code recover} method uses {@link StoreRecoveryService#restore}
  * method to start shard restore process.
- * <p/>
+ * <p>
  * At the end of the successful restore process {@code IndexShardSnapshotAndRestoreService} calls {@link #indexShardRestoreCompleted(SnapshotId, ShardId)},
  * which updates {@link RestoreInProgress} in cluster state or removes it when all shards are completed. In case of
  * restore failure a normal recovery fail-over process kicks in.
@@ -782,7 +782,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
     /**
      * Adds restore completion listener
-     * <p/>
+     * <p>
      * This listener is called for each snapshot that finishes restore operation in the cluster. It's responsibility of
      * the listener to decide if it's called for the appropriate snapshot or not.
      *
@@ -794,7 +794,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
     /**
      * Removes restore completion listener
-     * <p/>
+     * <p>
      * This listener is called for each snapshot that finishes restore operation in the cluster.
      *
      * @param listener restore completion listener
diff --git a/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java b/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
index 75abc40..d7e0a06 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/Snapshot.java
@@ -164,7 +164,7 @@ public class Snapshot implements Comparable<Snapshot>, ToXContent, FromXContentB
 
     /**
      * Returns time when snapshot ended
-     * <p/>
+     * <p>
      * Can be 0L if snapshot is still running
      *
      * @return snapshot end time
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
index e7b6ce1..57b974c 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java
@@ -131,7 +131,7 @@ public class SnapshotInfo implements ToXContent, Streamable {
 
     /**
      * Returns time when snapshot ended
-     * <p/>
+     * <p>
      * Can be 0L if snapshot is still running
      *
      * @return snapshot end time
@@ -310,7 +310,6 @@ public class SnapshotInfo implements ToXContent, Streamable {
      *
      * @param in stream input
      * @return deserialized snapshot info
-     * @throws IOException
      */
     public static SnapshotInfo readSnapshotInfo(StreamInput in) throws IOException {
         SnapshotInfo snapshotInfo = new SnapshotInfo();
@@ -323,7 +322,6 @@ public class SnapshotInfo implements ToXContent, Streamable {
      *
      * @param in stream input
      * @return deserialized snapshot info or null
-     * @throws IOException
      */
     public static SnapshotInfo readOptionalSnapshotInfo(StreamInput in) throws IOException {
         return in.readOptionalStreamable(new SnapshotInfo());
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java
index 216052e..60bd255 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardFailure.java
@@ -128,7 +128,6 @@ public class SnapshotShardFailure implements ShardOperationFailedException {
      *
      * @param in stream input
      * @return shard failure information
-     * @throws IOException
      */
     public static SnapshotShardFailure readSnapshotShardFailure(StreamInput in) throws IOException {
         SnapshotShardFailure exp = new SnapshotShardFailure();
@@ -165,7 +164,6 @@ public class SnapshotShardFailure implements ShardOperationFailedException {
      * @param snapshotShardFailure snapshot failure information
      * @param builder              XContent builder
      * @param params               additional parameters
-     * @throws IOException
      */
     public static void toXContent(SnapshotShardFailure snapshotShardFailure, XContentBuilder builder, ToXContent.Params params) throws IOException {
         builder.startObject();
@@ -178,7 +176,6 @@ public class SnapshotShardFailure implements ShardOperationFailedException {
      *
      * @param parser JSON parser
      * @return snapshot failure information
-     * @throws IOException
      */
     public static SnapshotShardFailure fromXContent(XContentParser parser) throws IOException {
         SnapshotShardFailure snapshotShardFailure = new SnapshotShardFailure();
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
index 301ceed..58a98c3 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
@@ -358,7 +358,6 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
     /**
      * Checks if any shards were processed that the new master doesn't know about
-     * @param event
      */
     private void syncShardStatsOnNewMaster(ClusterChangedEvent event) {
         SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE);
@@ -583,4 +582,4 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
         }
     }
 
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
index 76485a0..3cb02a4 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
@@ -75,7 +75,7 @@ import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
 
 /**
  * Service responsible for creating snapshots
- * <p/>
+ * <p>
  * A typical snapshot creating process looks like this:
  * <ul>
  * <li>On the master node the {@link #createSnapshot(SnapshotRequest, CreateSnapshotListener)} is called and makes sure that no snapshots is currently running
@@ -172,7 +172,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
 
     /**
      * Initializes the snapshotting process.
-     * <p/>
+     * <p>
      * This method is used by clients to start snapshot. It makes sure that there is no snapshots are currently running and
      * creates a snapshot record in cluster state metadata.
      *
@@ -236,7 +236,6 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      *
      * @param request snapshot request
      * @param state   current cluster state
-     * @throws org.elasticsearch.ElasticsearchException
      */
     private void validate(SnapshotRequest request, ClusterState state) {
         RepositoriesMetaData repositoriesMetaData = state.getMetaData().custom(RepositoriesMetaData.TYPE);
@@ -273,7 +272,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
 
     /**
      * Starts snapshot.
-     * <p/>
+     * <p>
      * Creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.
      *
      * @param clusterState               cluster state
@@ -752,7 +751,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
 
     /**
      * Finalizes the shard in repository and then removes it from cluster state
-     * <p/>
+     * <p>
      * This is non-blocking method that runs on a thread from SNAPSHOT thread pool
      *
      * @param entry snapshot
@@ -764,7 +763,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
 
     /**
      * Finalizes the shard in repository and then removes it from cluster state
-     * <p/>
+     * <p>
      * This is non-blocking method that runs on a thread from SNAPSHOT thread pool
      *
      * @param entry   snapshot
@@ -853,7 +852,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
 
     /**
      * Deletes snapshot from repository.
-     * <p/>
+     * <p>
      * If the snapshot is still running cancels the snapshot first and then deletes it from the repository.
      *
      * @param snapshotId snapshot id
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java b/core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java
index e396b03..382a419 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java
@@ -33,21 +33,21 @@ public interface TransportServiceAdapter {
     /** called by the {@link Transport} implementation once a request has been sent */
     void onRequestSent(DiscoveryNode node, long requestId, String action, TransportRequest request, TransportRequestOptions options);
 
-    /** called by the {@link Transport) implementation once a response was sent to calling node */
+    /** called by the {@link Transport} implementation once a response was sent to calling node */
     void onResponseSent(long requestId, String action, TransportResponse response, TransportResponseOptions options);
 
-    /** called by the {@link Transport) implementation after an exception was sent as a response to an incoming request */
+    /** called by the {@link Transport} implementation after an exception was sent as a response to an incoming request */
     void onResponseSent(long requestId, String action, Throwable t);
 
     /**
-     * called by the {@link Transport) implementation when a response or an exception has been recieved for a previously
+     * called by the {@link Transport} implementation when a response or an exception has been received for a previously
      * sent request (before any processing or deserialization was done). Returns the appropriate response handler or null if not
      * found.
      */
     TransportResponseHandler onResponseReceived(long requestId);
 
     /**
-     * called by the {@link Transport) implementation when an incoming request arrives but before
+     * called by the {@link Transport} implementation when an incoming request arrives but before
      * any parsing of it has happened (with the exception of the requestId and action)
      */
     void onRequestReceived(long requestId, String action);
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index b4d5015..35617dd 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -60,15 +60,15 @@ import java.util.concurrent.CopyOnWriteArrayList;
 /**
  * The tribe service holds a list of node clients connected to a list of tribe members, and uses their
  * cluster state events to update this local node cluster state with the merged view of it.
- * <p/>
+ * <p>
  * The {@link #processSettings(org.elasticsearch.common.settings.Settings)} method should be called before
  * starting the node, so it will make sure to configure this current node properly with the relevant tribe node
  * settings.
- * <p/>
+ * <p>
  * The tribe node settings make sure the discovery used is "local", but with no master elected. This means no
  * write level master node operations will work ({@link org.elasticsearch.discovery.MasterNotDiscoveredException}
  * will be thrown), and state level metadata operations with automatically use the local flag.
- * <p/>
+ * <p>
  * The state merged from different clusters include the list of nodes, metadata, and routing table. Each node merged
  * will have in its tribe which tribe member it came from. Each index merged will have in its settings which tribe
  * member it came from. In case an index has already been merged from one cluster, and the same name index is discovered
@@ -304,7 +304,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
                         }
                     }
 
-                    return ClusterState.builder(currentState).blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
+                    return ClusterState.builder(currentState).incrementVersion().blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
                 }
 
                 private void removeIndex(ClusterBlocks.Builder blocks, MetaData.Builder metaData, RoutingTable.Builder routingTable, IndexMetaData index) {
diff --git a/core/src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java b/core/src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java
index 24ab69a..5e36761 100644
--- a/core/src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java
+++ b/core/src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java
@@ -72,7 +72,7 @@ public abstract class AbstractResourceWatcher<Listener> implements ResourceWatch
 
     /**
      * Will be called periodically
-     * <p/>
+     * <p>
      * Implementing watcher should check resource and notify all {@link #listeners()}.
      */
     protected abstract void doCheckAndNotify() throws IOException;
diff --git a/core/src/main/java/org/elasticsearch/watcher/ResourceWatcher.java b/core/src/main/java/org/elasticsearch/watcher/ResourceWatcher.java
index 0d4bef1..4356c87 100644
--- a/core/src/main/java/org/elasticsearch/watcher/ResourceWatcher.java
+++ b/core/src/main/java/org/elasticsearch/watcher/ResourceWatcher.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 
 /**
  * Abstract resource watcher interface.
- * <p/>
+ * <p>
  * Different resource watchers can be registered with {@link ResourceWatcherService} to be called
  * periodically in order to check for changes in different external resources.
  */
diff --git a/core/src/main/java/org/joda/time/base/BaseDateTime.java b/core/src/main/java/org/joda/time/base/BaseDateTime.java
index c2aa9ec..4cf6ff6 100644
--- a/core/src/main/java/org/joda/time/base/BaseDateTime.java
+++ b/core/src/main/java/org/joda/time/base/BaseDateTime.java
@@ -29,11 +29,11 @@ import java.io.Serializable;
 /**
  * BaseDateTime is an abstract implementation of ReadableDateTime that stores
  * data in <code>long</code> and <code>Chronology</code> fields.
- * <p/>
+ * <p>
  * This class should generally not be used directly by API users.
  * The {@link ReadableDateTime} interface should be used when different
  * kinds of date/time objects are to be referenced.
- * <p/>
+ * <p>
  * BaseDateTime subclasses may be mutable and not thread-safe.
  *
  * @author Stephen Colebourne
@@ -73,7 +73,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance set to the current system millisecond time
      * using <code>ISOChronology</code> in the specified time zone.
-     * <p/>
+     * <p>
      * If the specified time zone is null, the default zone is used.
      *
      * @param zone the time zone, null means default zone
@@ -85,7 +85,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance set to the current system millisecond time
      * using the specified chronology.
-     * <p/>
+     * <p>
      * If the chronology is null, <code>ISOChronology</code>
      * in the default time zone is used.
      *
@@ -110,7 +110,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance set to the milliseconds from 1970-01-01T00:00:00Z
      * using <code>ISOChronology</code> in the specified time zone.
-     * <p/>
+     * <p>
      * If the specified time zone is null, the default zone is used.
      *
      * @param instant the milliseconds from 1970-01-01T00:00:00Z
@@ -123,7 +123,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance set to the milliseconds from 1970-01-01T00:00:00Z
      * using the specified chronology.
-     * <p/>
+     * <p>
      * If the chronology is null, <code>ISOChronology</code>
      * in the default time zone is used.
      *
@@ -145,10 +145,10 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance from an Object that represents a datetime,
      * forcing the time zone to that specified.
-     * <p/>
+     * <p>
      * If the object contains no chronology, <code>ISOChronology</code> is used.
      * If the specified time zone is null, the default zone is used.
-     * <p/>
+     * <p>
      * The recognised object types are defined in
      * {@link org.joda.time.convert.ConverterManager ConverterManager} and
      * include ReadableInstant, String, Calendar and Date.
@@ -168,9 +168,9 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance from an Object that represents a datetime,
      * using the specified chronology.
-     * <p/>
+     * <p>
      * If the chronology is null, ISO in the default time zone is used.
-     * <p/>
+     * <p>
      * The recognised object types are defined in
      * {@link org.joda.time.convert.ConverterManager ConverterManager} and
      * include ReadableInstant, String, Calendar and Date.
@@ -215,7 +215,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance from datetime field values
      * using <code>ISOChronology</code> in the specified time zone.
-     * <p/>
+     * <p>
      * If the specified time zone is null, the default zone is used.
      *
      * @param year           the year
@@ -243,7 +243,7 @@ public abstract class BaseDateTime
     /**
      * Constructs an instance from datetime field values
      * using the specified chronology.
-     * <p/>
+     * <p>
      * If the chronology is null, <code>ISOChronology</code>
      * in the default time zone is used.
      *
@@ -277,7 +277,7 @@ public abstract class BaseDateTime
     /**
      * Checks the specified chronology before storing it, potentially altering it.
      * This method must not access any instance variables.
-     * <p/>
+     * <p>
      * This implementation converts nulls to ISOChronology in the default zone.
      *
      * @param chronology the chronology to use, may be null
@@ -290,7 +290,7 @@ public abstract class BaseDateTime
     /**
      * Checks the specified instant before storing it, potentially altering it.
      * This method must not access any instance variables.
-     * <p/>
+     * <p>
      * This implementation simply returns the instant.
      *
      * @param instant    the milliseconds from 1970-01-01T00:00:00Z to round
@@ -328,7 +328,7 @@ public abstract class BaseDateTime
 
     /**
      * Sets the milliseconds of the datetime.
-     * <p/>
+     * <p>
      * All changes to the millisecond field occurs via this method.
      * Override and block this method to make a subclass immutable.
      *
@@ -340,7 +340,7 @@ public abstract class BaseDateTime
 
     /**
      * Sets the chronology of the datetime.
-     * <p/>
+     * <p>
      * All changes to the chronology field occurs via this method.
      * Override and block this method to make a subclass immutable.
      *
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index c21e58d..47444df 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -57,13 +57,26 @@ grant codeBase "${es.security.plugin.cloud-gce}" {
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
+grant codeBase "${es.security.plugin.lang-javascript}" {
+  // needed to generate runtime classes
+  permission java.lang.RuntimePermission "createClassLoader";
+};
+
+grant codeBase "${es.security.plugin.lang-python}" {
+  // needed to generate runtime classes
+  permission java.lang.RuntimePermission "createClassLoader";
+};
+
 //// test framework permissions.
 //// These are mock objects and test management that we allow test framework libs
 //// to provide on our behalf. But tests themselves cannot do this stuff!
 
 grant codeBase "${es.security.jar.elasticsearch.securemock}" {
+  // needed to access ReflectionFactory (see below)
+  permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
   // needed to support creation of mocks
   permission java.lang.RuntimePermission "reflectionFactoryAccess";
+  // needed for spy interception, etc
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 758d055..5ad6323 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -38,16 +38,18 @@ OFFICIAL PLUGINS
     - analysis-phonetic
     - analysis-smartcn
     - analysis-stempel
-    - cloud-azure
     - cloud-gce
     - delete-by-query
+    - discovery-azure
     - discovery-ec2
     - discovery-multicast
     - lang-javascript
     - lang-python
     - mapper-murmur3
     - mapper-size
+    - repository-azure
     - repository-s3
+    - store-smb
 
 
 OPTIONS
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index 03c0172..c689599 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -37,7 +37,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.TestParsingException;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.SearchShardTarget;
@@ -274,8 +274,8 @@ public class ESExceptionTests extends ESTestCase {
 
     public void testSerializeUnknownException() throws IOException {
         BytesStreamOutput out = new BytesStreamOutput();
-        ParsingException ParsingException = new ParsingException(new Index("foo"), 1, 2, "foobar", null);
-        Throwable ex = new Throwable("wtf", ParsingException);
+        ParsingException parsingException = new ParsingException(new Index("foo"), 1, 2, "foobar", null);
+        Throwable ex = new Throwable("wtf", parsingException);
         out.writeThrowable(ex);
 
         StreamInput in = StreamInput.wrap(out.bytes());
@@ -283,10 +283,10 @@ public class ESExceptionTests extends ESTestCase {
         assertEquals("wtf", throwable.getMessage());
         assertTrue(throwable instanceof ElasticsearchException);
         ParsingException e = (ParsingException)throwable.getCause();
-                assertEquals(ParsingException.getIndex(), e.getIndex());
-        assertEquals(ParsingException.getMessage(), e.getMessage());
-        assertEquals(ParsingException.getLineNumber(), e.getLineNumber());
-        assertEquals(ParsingException.getColumnNumber(), e.getColumnNumber());
+                assertEquals(parsingException.getIndex(), e.getIndex());
+        assertEquals(parsingException.getMessage(), e.getMessage());
+        assertEquals(parsingException.getLineNumber(), e.getLineNumber());
+        assertEquals(parsingException.getColumnNumber(), e.getColumnNumber());
     }
 
     public void testWriteThrowable() throws IOException {
@@ -309,7 +309,7 @@ public class ESExceptionTests extends ESTestCase {
                 new OutOfMemoryError("no memory left"),
                 new AlreadyClosedException("closed!!", new NullPointerException()),
                 new LockObtainFailedException("can't lock directory", new NullPointerException()),
-                new Throwable("this exception is unknown", new QueryShardException(new Index("foo"), "foobar", null) ), // somethin unknown
+                new Throwable("this exception is unknown", new ParsingException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
         };
         for (Throwable t : causes) {
             BytesStreamOutput out = new BytesStreamOutput();
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index da68cf2..ccdd52b 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -47,12 +47,12 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.util.CancellableThreadsTests;
+import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.common.util.CancellableThreadsTests;
-import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
@@ -61,7 +61,7 @@ import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.TestParsingException;
 import org.elasticsearch.index.shard.IllegalIndexShardStateException;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
@@ -111,7 +111,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         final Path startPath = PathUtils.get(ElasticsearchException.class.getProtectionDomain().getCodeSource().getLocation().toURI()).resolve("org").resolve("elasticsearch");
         final Set<? extends Class> ignore = Sets.newHashSet(
                 org.elasticsearch.test.rest.parser.RestTestParseException.class,
-                org.elasticsearch.index.query.TestParsingException.class,
+                TestParsingException.class,
                 org.elasticsearch.test.rest.client.RestException.class,
                 CancellableThreadsTests.CustomException.class,
                 org.elasticsearch.rest.BytesRestResponseTests.WithHeadersException.class,
@@ -240,16 +240,6 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.getColumnNumber(), 2);
     }
 
-    public void testQueryShardException() throws IOException {
-        QueryShardException ex = serialize(new QueryShardException(new Index("foo"), "fobar", null));
-        assertEquals(ex.getIndex(), "foo");
-        assertEquals(ex.getMessage(), "fobar");
-
-        ex = serialize(new QueryShardException((Index)null, null, null));
-        assertNull(ex.getIndex());
-        assertNull(ex.getMessage());
-    }
-
     public void testSearchException() throws IOException {
         SearchShardTarget target = new SearchShardTarget("foo", "bar", 1);
         SearchException ex = serialize(new SearchException(target, "hello world"));
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 6bbec12..ccde0fa 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -149,7 +149,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
         logger.info("--> making sure that filter was stored with alias [alias1] and filter [user:kimchy]");
         ClusterState clusterState = admin().cluster().prepareState().get().getState();
         IndexMetaData indexMd = clusterState.metaData().index("test");
-        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":{\"value\":\"kimchy\",\"boost\":1.0}}}"));
+        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":\"kimchy\"}}"));
 
     }
 
@@ -411,8 +411,8 @@ public class IndexAliasesIT extends ESIntegTestCase {
         assertThat(client().prepareCount("bars").setQuery(QueryBuilders.matchAllQuery()).get().getCount(), equalTo(1L));
     }
 
-
-
+    
+    
     @Test
     public void testDeleteAliases() throws Exception {
         logger.info("--> creating index [test1] and [test2]");
@@ -432,17 +432,17 @@ public class IndexAliasesIT extends ESIntegTestCase {
                 .addAlias("test2", "aliasToTests")
                 .addAlias("test2", "foos", termQuery("name", "foo"))
                 .addAlias("test2", "tests", termQuery("name", "test")));
-
-        String[] indices = {"test1", "test2"};
+        
+        String[] indices = {"test1", "test2"}; 
         String[] aliases = {"aliasToTest1", "foos", "bars", "tests", "aliasToTest2", "aliasToTests"};
-
+        
         admin().indices().prepareAliases().removeAlias(indices, aliases).get();
-
+        
         AliasesExistResponse response = admin().indices().prepareAliasesExist(aliases).get();
         assertThat(response.exists(), equalTo(false));
     }
 
-
+    
     @Test
     public void testWaitForAliasCreationMultipleShards() throws Exception {
         logger.info("--> creating index [test]");
@@ -530,16 +530,16 @@ public class IndexAliasesIT extends ESIntegTestCase {
 
         logger.info("--> verify that filter was updated");
         AliasMetaData aliasMetaData = ((AliasOrIndex.Alias) internalCluster().clusterService().state().metaData().getAliasAndIndexLookup().get("alias1")).getFirstAliasMetaData();
-        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":{\"value\":\"bar\",\"boost\":1.0}}}"));
+        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":\"bar\"}}"));
 
         logger.info("--> deleting alias1");
         stopWatch.start();
         assertAcked((admin().indices().prepareAliases().removeAlias("test", "alias1").setTimeout(timeout)));
         assertThat(stopWatch.stop().lastTaskTime().millis(), lessThan(timeout.millis()));
 
-
+        
     }
-
+    
     @Test(expected = AliasesNotFoundException.class)
     public void testIndicesRemoveNonExistingAliasResponds404() throws Exception {
         logger.info("--> creating index [test]");
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
index 3d22f07..bc0d188 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.benchmark.search.child;
 
-import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
@@ -282,12 +281,12 @@ public class ChildSearchBenchmark {
         System.out.println("--> Running has_child query with score type");
         // run parent child score query
         for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
+            client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode("max")).execute().actionGet();
         }
 
         totalQueryTime = 0;
         for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
+            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode("max")).execute().actionGet();
             if (j % 10 == 0) {
                 System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
             }
@@ -297,7 +296,7 @@ public class ChildSearchBenchmark {
         
         totalQueryTime = 0;
         for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", matchAllQuery()).scoreMode(ScoreMode.Max)).execute().actionGet();
+            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", matchAllQuery()).scoreMode("max")).execute().actionGet();
             if (j % 10 == 0) {
                 System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
             }
@@ -308,12 +307,12 @@ public class ChildSearchBenchmark {
         System.out.println("--> Running has_parent query with score type");
         // run parent child score query
         for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
+            client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).scoreMode("score")).execute().actionGet();
         }
 
         totalQueryTime = 0;
         for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
+            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).scoreMode("score")).execute().actionGet();
             if (j % 10 == 0) {
                 System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
             }
@@ -323,7 +322,7 @@ public class ChildSearchBenchmark {
 
         totalQueryTime = 0;
         for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", matchAllQuery()).score(true)).execute().actionGet();
+            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", matchAllQuery()).scoreMode("score")).execute().actionGet();
             if (j % 10 == 0) {
                 System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
             }
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
index 388bf95..f7eaa74 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.benchmark.search.child;
 
-import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
@@ -179,7 +178,7 @@ public class ChildSearchShortCircuitBenchmark {
         for (int i = 1; i < PARENT_COUNT; i *= 2) {
             for (int j = 0; j < QUERY_COUNT; j++) {
                 SearchResponse searchResponse = client.prepareSearch(indexName)
-                        .setQuery(hasChildQuery("child", matchQuery("field2", i)).scoreMode(ScoreMode.Max))
+                        .setQuery(hasChildQuery("child", matchQuery("field2", i)).scoreMode("max"))
                         .execute().actionGet();
                 if (searchResponse.getHits().totalHits() != i) {
                     System.err.println("--> mismatch on hits");
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
index 76a31bc..5d65bf4 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
@@ -77,7 +77,7 @@ import static org.hamcrest.Matchers.*;
 public class BasicBackwardsCompatibilityIT extends ESBackcompatTestCase {
 
     /**
-     * Basic test using Index & Realtime Get with external versioning. This test ensures routing works correctly across versions.
+     * Basic test using Index &amp; Realtime Get with external versioning. This test ensures routing works correctly across versions.
      */
     @Test
     public void testExternalVersion() throws Exception {
@@ -101,7 +101,7 @@ public class BasicBackwardsCompatibilityIT extends ESBackcompatTestCase {
     }
 
     /**
-     * Basic test using Index & Realtime Get with internal versioning. This test ensures routing works correctly across versions.
+     * Basic test using Index &amp; Realtime Get with internal versioning. This test ensures routing works correctly across versions.
      */
     @Test
     public void testInternalVersion() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/common/geo/GeoDistanceTests.java b/core/src/test/java/org/elasticsearch/common/geo/GeoDistanceTests.java
deleted file mode 100644
index 924926b..0000000
--- a/core/src/test/java/org/elasticsearch/common/geo/GeoDistanceTests.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.geo;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- * Basic Tests for {@link GeoDistance}
- */
-public class GeoDistanceTests extends ESTestCase {
-
-    @Test
-    public void testGeoDistanceSerialization() throws IOException  {
-        // make sure that ordinals don't change, because we rely on then in serialization
-        assertThat(GeoDistance.PLANE.ordinal(), equalTo(0));
-        assertThat(GeoDistance.FACTOR.ordinal(), equalTo(1));
-        assertThat(GeoDistance.ARC.ordinal(), equalTo(2));
-        assertThat(GeoDistance.SLOPPY_ARC.ordinal(), equalTo(3));
-        assertThat(GeoDistance.values().length, equalTo(4));
-
-        GeoDistance geoDistance = randomFrom(GeoDistance.PLANE, GeoDistance.FACTOR, GeoDistance.ARC, GeoDistance.SLOPPY_ARC);
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            geoDistance.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {;
-                GeoDistance copy = GeoDistance.readGeoDistanceFrom(in);
-                assertEquals(copy.toString() + " vs. " + geoDistance.toString(), copy, geoDistance);
-            }
-        }
-    }
-
-    @Test(expected = IOException.class)
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            if (randomBoolean()) {
-                out.writeVInt(randomIntBetween(GeoDistance.values().length, Integer.MAX_VALUE));
-            } else {
-                out.writeVInt(randomIntBetween(Integer.MIN_VALUE, -1));
-            }
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                GeoDistance.readGeoDistanceFrom(in);
-            }
-        }
-    }
-
-    @Test
-    public void testDistanceCheck() {
-        // Note, is within is an approximation, so, even though 0.52 is outside 50mi, we still get "true"
-        GeoDistance.DistanceBoundingCheck check = GeoDistance.distanceBoundingCheck(0, 0, 50, DistanceUnit.MILES);
-        assertThat(check.isWithin(0.5, 0.5), equalTo(true));
-        assertThat(check.isWithin(0.52, 0.52), equalTo(true));
-        assertThat(check.isWithin(1, 1), equalTo(false));
-
-        check = GeoDistance.distanceBoundingCheck(0, 179, 200, DistanceUnit.MILES);
-        assertThat(check.isWithin(0, -179), equalTo(true));
-        assertThat(check.isWithin(0, -178), equalTo(false));
-    }
-
-    @Test
-    public void testArcDistanceVsPlaneInEllipsis() {
-        GeoPoint centre = new GeoPoint(48.8534100, 2.3488000);
-        GeoPoint northernPoint = new GeoPoint(48.8801108681, 2.35152032666);
-        GeoPoint westernPoint = new GeoPoint(48.85265, 2.308896);
-
-        // With GeoDistance.ARC both the northern and western points are within the 4km range
-        assertThat(GeoDistance.ARC.calculate(centre.lat(), centre.lon(), northernPoint.lat(),
-                northernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
-        assertThat(GeoDistance.ARC.calculate(centre.lat(), centre.lon(), westernPoint.lat(),
-                westernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
-
-        // With GeoDistance.PLANE, only the northern point is within the 4km range,
-        // the western point is outside of the range due to the simple math it employs,
-        // meaning results will appear elliptical
-        assertThat(GeoDistance.PLANE.calculate(centre.lat(), centre.lon(), northernPoint.lat(),
-                northernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
-        assertThat(GeoDistance.PLANE.calculate(centre.lat(), centre.lon(), westernPoint.lat(),
-                westernPoint.lon(), DistanceUnit.KILOMETERS), greaterThan(4D));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java b/core/src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java
index 53463e5..94666d8 100644
--- a/core/src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java
+++ b/core/src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java
@@ -44,7 +44,7 @@ import static org.elasticsearch.common.geo.builders.ShapeBuilder.SPATIAL_CONTEXT
 
 
 /**
- * Tests for {@link GeoJSONShapeParser}
+ * Tests for {@code GeoJSONShapeParser}
  */
 public class GeoJSONShapeParserTests extends ESTestCase {
 
diff --git a/core/src/test/java/org/elasticsearch/common/geo/ShapeRelationTests.java b/core/src/test/java/org/elasticsearch/common/geo/ShapeRelationTests.java
deleted file mode 100644
index 83b6671..0000000
--- a/core/src/test/java/org/elasticsearch/common/geo/ShapeRelationTests.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.geo;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class ShapeRelationTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(ShapeRelation.INTERSECTS.ordinal(), equalTo(0));
-        assertThat(ShapeRelation.DISJOINT.ordinal(), equalTo(1));
-        assertThat(ShapeRelation.WITHIN.ordinal(), equalTo(2));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ShapeRelation.INTERSECTS.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ShapeRelation.DISJOINT.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ShapeRelation.WITHIN.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(2));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ShapeRelation.DISJOINT.readFrom(in), equalTo(ShapeRelation.INTERSECTS));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ShapeRelation.DISJOINT.readFrom(in), equalTo(ShapeRelation.DISJOINT));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(2);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ShapeRelation.DISJOINT.readFrom(in), equalTo(ShapeRelation.WITHIN));
-            }
-        }
-    }
-
-    @Test(expected = IOException.class)
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(3, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                ShapeRelation.DISJOINT.readFrom(in);
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/geo/SpatialStrategyTests.java b/core/src/test/java/org/elasticsearch/common/geo/SpatialStrategyTests.java
deleted file mode 100644
index c53a3fb..0000000
--- a/core/src/test/java/org/elasticsearch/common/geo/SpatialStrategyTests.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.geo;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class SpatialStrategyTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(SpatialStrategy.TERM.ordinal(), equalTo(0));
-        assertThat(SpatialStrategy.RECURSIVE.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SpatialStrategy.TERM.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SpatialStrategy.RECURSIVE.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SpatialStrategy.TERM.readFrom(in), equalTo(SpatialStrategy.TERM));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SpatialStrategy.TERM.readFrom(in), equalTo(SpatialStrategy.RECURSIVE));
-            }
-        }
-    }
-
-    @Test(expected = IOException.class)
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                SpatialStrategy.TERM.readFrom(in);
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
index 3d4eaf9..eeac546 100644
--- a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
@@ -80,7 +80,7 @@ public abstract class ModuleTestCase extends ESTestCase {
     }
 
     /**
-     * Configures the module and checks a Map<String, Class> of the "to" class
+     * Configures the module and checks a Map&lt;String, Class&gt; of the "to" class
      * is bound to "theClass".
      */
     public void assertMapMultiBinding(Module module, Class to, Class theClass) {
diff --git a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
index afc17ce..d313dd7 100644
--- a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
@@ -26,7 +26,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-
 import java.util.Objects;
 
 import static org.hamcrest.Matchers.closeTo;
diff --git a/core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java b/core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java
index b3a2cea..53998c5 100644
--- a/core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java
+++ b/core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java
@@ -62,8 +62,8 @@ public class RoundingTests extends ESTestCase {
 
     /**
      * Simple test case to illustrate how Rounding.Offset works on readable input.
-     * offset shifts input value back before rounding (so here 6 - 7 -> -1)
-     * then shifts rounded Value back  (here -10 -> -3)
+     * offset shifts input value back before rounding (so here 6 - 7 -&gt; -1)
+     * then shifts rounded Value back  (here -10 -&gt; -3)
      */
     @Test
     public void testOffsetRounding() {
diff --git a/core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java b/core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
index 4e42c68..cc6f9cb 100644
--- a/core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
+++ b/core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
@@ -72,7 +72,7 @@ public class TimeZoneRoundingTests extends ESTestCase {
     }
 
     /**
-     * test TimeIntervalTimeZoneRounding, (interval < 12h) with time zone shift
+     * test TimeIntervalTimeZoneRounding, (interval &lt; 12h) with time zone shift
      */
     @Test
     public void testTimeIntervalTimeZoneRounding() {
@@ -88,7 +88,7 @@ public class TimeZoneRoundingTests extends ESTestCase {
     }
 
     /**
-     * test DayIntervalTimeZoneRounding, (interval >= 12h) with time zone shift
+     * test DayIntervalTimeZoneRounding, (interval &gt;= 12h) with time zone shift
      */
     @Test
     public void testDayIntervalTimeZoneRounding() {
diff --git a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
index 807b4a7..234e341 100644
--- a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
+++ b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.common.unit;
 
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.xcontent.XContent;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
@@ -164,29 +162,4 @@ public class FuzzinessTests extends ESTestCase {
         }
     }
 
-    @Test
-    public void testSerialization() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-
-        fuzziness = Fuzziness.fromEdits(randomIntBetween(0, 2));
-        deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-    }
-
-    @Test
-    public void testSerializationAuto() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-        assertEquals(fuzziness.asInt(), deserializedFuzziness.asInt());
-    }
-
-    private static Fuzziness doSerializeRoundtrip(Fuzziness in) throws IOException {
-        BytesStreamOutput output = new BytesStreamOutput();
-        in.writeTo(output);
-        StreamInput streamInput = StreamInput.wrap(output.bytes());
-        return Fuzziness.readFuzzinessFrom(streamInput);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/ObjectParserTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/ObjectParserTests.java
new file mode 100644
index 0000000..0437292
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/ObjectParserTests.java
@@ -0,0 +1,281 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.common.xcontent;
+
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+public class ObjectParserTests extends ESTestCase {
+
+    public void testBasics() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{\"test\" : \"foo\", \"test_number\" : 2, \"testArray\":  [1,2,3,4]}");
+        class TestStruct {
+            public String test;
+            int testNumber;
+            List<Integer> ints = new ArrayList<>();
+            public void setTestNumber(int testNumber) {
+                this.testNumber = testNumber;
+            }
+
+            public void setInts(List<Integer> ints) {
+                this.ints = ints;
+            }
+        }
+        ObjectParser<TestStruct, Void> objectParser = new ObjectParser("foo");
+        TestStruct s = new TestStruct();
+
+        objectParser.declareField((i, c, x) -> c.test = i.text(), new ParseField("test"), ObjectParser.ValueType.STRING);
+        objectParser.declareInt(TestStruct::setTestNumber, new ParseField("test_number"));
+        objectParser.declareIntArray(TestStruct::setInts, new ParseField("test_array"));
+        parser.setParseFieldMatcher(ParseFieldMatcher.STRICT);
+        objectParser.parse(parser, s);
+        assertEquals(s.test, "foo");
+        assertEquals(s.testNumber, 2);
+        assertEquals(s.ints, Arrays.asList(1, 2, 3, 4));
+        assertEquals(objectParser.toString(), "ObjectParser{name='foo', fields=[FieldParser{preferred_name=test, supportedTokens=[VALUE_STRING], type=STRING}, FieldParser{preferred_name=test_number, supportedTokens=[VALUE_STRING, VALUE_NUMBER], type=INT}, FieldParser{preferred_name=test_array, supportedTokens=[START_ARRAY], type=INT_ARRAY}, FieldParser{preferred_name=test_array, supportedTokens=[START_ARRAY], type=INT_ARRAY}, FieldParser{preferred_name=test_number, supportedTokens=[VALUE_STRING, VALUE_NUMBER], type=INT}]}");
+    }
+
+    public void testExceptions() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{\"test\" : \"foo\"}");
+        class TestStruct {
+            public int test;
+
+            public void setTest(int test) {
+                this.test = test;
+            }
+        }
+        ObjectParser<TestStruct, TestStruct> objectParser = new ObjectParser("the_parser");
+        TestStruct s = new TestStruct();
+        objectParser.declareInt(TestStruct::setTest, new ParseField("test"));
+
+        try {
+            objectParser.parse(parser, s);
+            fail("numeric value expected");
+        } catch (ParsingException ex) {
+            assertEquals(ex.getMessage(), "[the_parser] failed to parse field [test]");
+            assertTrue(ex.getCause() instanceof NumberFormatException);
+        }
+
+        parser = XContentType.JSON.xContent().createParser("{\"not_supported_field\" : \"foo\"}");
+        try {
+            objectParser.parse(parser, s);
+            fail("field not supported");
+        } catch (IllegalArgumentException ex) {
+            assertEquals(ex.getMessage(), "[the_parser] unknown field [not_supported_field], parser not found");
+        }
+    }
+
+    public void testDeprecationFail() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{\"old_test\" : \"foo\"}");
+        class TestStruct {
+            public String test;
+        }
+        ObjectParser<TestStruct, Void> objectParser = new ObjectParser("foo");
+        TestStruct s = new TestStruct();
+
+        objectParser.declareField((i, v, c) -> v.test = i.text(), new ParseField("test", "old_test"), ObjectParser.ValueType.STRING);
+        parser.setParseFieldMatcher(ParseFieldMatcher.STRICT);
+
+        try {
+            objectParser.parse(parser, s);
+            fail("deprecated value");
+        } catch (IllegalArgumentException ex) {
+            assertEquals(ex.getMessage(), "Deprecated field [old_test] used, expected [test] instead");
+
+        }
+        assertNull(s.test);
+        parser = XContentType.JSON.xContent().createParser("{\"old_test\" : \"foo\"}");
+        parser.setParseFieldMatcher(ParseFieldMatcher.EMPTY);
+        objectParser.parse(parser, s);
+        assertEquals("foo", s.test);
+    }
+
+    public void testFailOnValueType() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{\"numeric_value\" : false}");
+        class TestStruct {
+            public String test;
+        }
+        ObjectParser<TestStruct, Void> objectParser = new ObjectParser("foo");
+        TestStruct s = new TestStruct();
+
+        objectParser.declareField((i, c, x) -> c.test = i.text(), new ParseField("numeric_value"), ObjectParser.ValueType.FLOAT);
+        parser.setParseFieldMatcher(ParseFieldMatcher.STRICT);
+        try {
+            objectParser.parse(parser, s);
+            fail("wrong type - must be number");
+        } catch (IllegalArgumentException ex) {
+            assertEquals(ex.getMessage(), "[foo] numeric_value doesn't support values of type: VALUE_BOOLEAN");
+        }
+    }
+
+    public void testParseNested() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{ \"test\" : 1, \"object\" : { \"test\": 2}}");
+        class TestStruct {
+            public int test;
+            TestStruct object;
+        }
+        ObjectParser<TestStruct, Void> objectParser = new ObjectParser("foo");
+        TestStruct s = new TestStruct();
+        s.object = new TestStruct();
+        objectParser.declareField((i, c, x) -> c.test = i.intValue(), new ParseField("test"), ObjectParser.ValueType.INT);
+        objectParser.declareField((i, c, x) -> objectParser.parse(parser, c.object), new ParseField("object"), ObjectParser.ValueType.OBJECT);
+        objectParser.parse(parser, s);
+        assertEquals(s.test, 1);
+        assertEquals(s.object.test, 2);
+    }
+
+    public void testParseNestedShortcut() throws IOException {
+        XContentParser parser = XContentType.JSON.xContent().createParser("{ \"test\" : 1, \"object\" : { \"test\": 2}}");
+        ObjectParser<StaticTestStruct, Void> objectParser = new ObjectParser("foo", StaticTestStruct::new);
+        objectParser.declareInt(StaticTestStruct::setTest, new ParseField("test"));
+        objectParser.declareObject(StaticTestStruct::setObject, objectParser, new ParseField("object"));
+        StaticTestStruct s = objectParser.parse(parser);
+        assertEquals(s.test, 1);
+        assertEquals(s.object.test, 2);
+    }
+
+    static class StaticTestStruct {
+        public int test;
+        StaticTestStruct object;
+
+        public void setTest(int test) {
+            this.test = test;
+        }
+
+        public void setObject(StaticTestStruct object) {
+            this.object = object;
+        }
+    }
+
+    public void testAllVariants() throws IOException {
+        XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent());
+        builder.startObject();
+        builder.field("int_field", randomBoolean() ? "1" : 1);
+        builder.array("int_array_field", randomBoolean() ? "1" : 1);
+        builder.field("double_field", randomBoolean() ? "2.1" : 2.1d);
+        builder.array("double_array_field", randomBoolean() ? "2.1" : 2.1d);
+        builder.field("float_field", randomBoolean() ? "3.1" : 3.1f);
+        builder.array("float_array_field", randomBoolean() ? "3.1" : 3.1);
+        builder.field("long_field", randomBoolean() ? "4" : 4);
+        builder.array("long_array_field", randomBoolean() ? "4" : 4);
+        builder.field("string_field", "5");
+        builder.array("string_array_field", "5");
+        boolean nullValue = randomBoolean();
+        builder.field("boolean_field", nullValue);
+        builder.field("string_or_null", nullValue ? null : "5");
+        builder.endObject();
+        XContentParser parser = XContentType.JSON.xContent().createParser(builder.string());
+        class TestStruct {
+            int int_field;
+            long long_field;
+            float float_field;
+            double double_field;
+            String string_field;
+            List<Integer> int_array_field;
+            List<Long> long_array_field;
+            List<Float> float_array_field;
+            List<Double> double_array_field;
+            List<String> string_array_field;
+            boolean null_value;
+            String string_or_null = "adsfsa";
+            public void setInt_field(int int_field) {
+                this.int_field = int_field;
+            }
+            public void setLong_field(long long_field) {
+                this.long_field = long_field;
+            }
+            public void setFloat_field(float float_field) {
+                this.float_field = float_field;
+            }
+            public void setDouble_field(double double_field) {
+                this.double_field = double_field;
+            }
+            public void setString_field(String string_field) {
+                this.string_field = string_field;
+            }
+            public void setInt_array_field(List<Integer> int_array_field) {
+                this.int_array_field = int_array_field;
+            }
+            public void setLong_array_field(List<Long> long_array_field) {
+                this.long_array_field = long_array_field;
+            }
+            public void setFloat_array_field(List<Float> float_array_field) {
+                this.float_array_field = float_array_field;
+            }
+            public void setDouble_array_field(List<Double> double_array_field) {
+                this.double_array_field = double_array_field;
+            }
+            public void setString_array_field(List<String> string_array_field) {
+                this.string_array_field = string_array_field;
+            }
+
+            public void setNull_value(boolean null_value) {
+                this.null_value = null_value;
+            }
+
+            public void setString_or_null(String string_or_null) {
+                this.string_or_null = string_or_null;
+            }
+        }
+        ObjectParser<TestStruct, Void> objectParser = new ObjectParser("foo");
+        objectParser.declareInt(TestStruct::setInt_field, new ParseField("int_field"));
+        objectParser.declareIntArray(TestStruct::setInt_array_field, new ParseField("int_array_field"));
+        objectParser.declareLong(TestStruct::setLong_field, new ParseField("long_field"));
+        objectParser.declareLongArray(TestStruct::setLong_array_field, new ParseField("long_array_field"));
+        objectParser.declareDouble(TestStruct::setDouble_field, new ParseField("double_field"));
+        objectParser.declareDoubleArray(TestStruct::setDouble_array_field, new ParseField("double_array_field"));
+        objectParser.declareFloat(TestStruct::setFloat_field, new ParseField("float_field"));
+        objectParser.declareFloatArray(TestStruct::setFloat_array_field, new ParseField("float_array_field"));
+        objectParser.declareString(TestStruct::setString_field, new ParseField("string_field"));
+        objectParser.declareStringArray(TestStruct::setString_array_field, new ParseField("string_array_field"));
+
+        objectParser.declareStringOrNull(TestStruct::setString_or_null, new ParseField("string_or_null"));
+        objectParser.declareBoolean(TestStruct::setNull_value, new ParseField("boolean_field"));
+        TestStruct parse = objectParser.parse(parser, new TestStruct());
+        assertArrayEquals(parse.double_array_field.toArray(), Arrays.asList(2.1d).toArray());
+        assertEquals(parse.double_field, 2.1d, 0.0d);
+
+        assertArrayEquals(parse.long_array_field.toArray(), Arrays.asList(4l).toArray());
+        assertEquals(parse.long_field, 4l);
+
+        assertArrayEquals(parse.string_array_field.toArray(), Arrays.asList("5").toArray());
+        assertEquals(parse.string_field, "5");
+
+        assertArrayEquals(parse.int_array_field.toArray(), Arrays.asList(1).toArray());
+        assertEquals(parse.int_field, 1);
+
+        assertArrayEquals(parse.float_array_field.toArray(), Arrays.asList(3.1f).toArray());
+        assertEquals(parse.float_field, 3.1f, 0.0f);
+
+        assertEquals(nullValue, parse.null_value);
+        if (nullValue) {
+            assertNull(parse.string_or_null);
+        } else {
+            assertEquals(parse.string_field, "5");
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index e5de3be..2ac6900 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -21,30 +21,22 @@ package org.elasticsearch.discovery;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;
-import org.elasticsearch.action.admin.indices.flush.FlushResponse;
-import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;
-import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.*;
-import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.DjbHashFunction;
-import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
@@ -55,9 +47,7 @@ import org.elasticsearch.discovery.zen.ping.ZenPing;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
 import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
-import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.indices.recovery.RecoverySource;
 import org.elasticsearch.indices.store.IndicesStoreIntegrationIT;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -179,8 +169,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
     /**
      * Test that no split brain occurs under partial network partition. See https://github.com/elasticsearch/elasticsearch/issues/2488
-     *
-     * @throws Exception
      */
     @Test
     public void failWithMinimumMasterNodesConfigured() throws Exception {
@@ -402,8 +390,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
     /**
      * Test that we do not loose document whose indexing request was successful, under a randomly selected disruption scheme
-     * We also collect & report the type of indexing failures that occur.
-     * <p/>
+     * We also collect &amp; report the type of indexing failures that occur.
+     * <p>
      * This test is a superset of tests run in the Jepsen test suite, with the exception of versioned updates
      */
     @Test
@@ -693,8 +681,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     /**
      * Test that a document which is indexed on the majority side of a partition, is available from the minority side,
      * once the partition is healed
-     *
-     * @throws Exception
      */
     @Test
     @TestLogging(value = "cluster.service:TRACE")
@@ -942,234 +928,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         ensureStableCluster(3);
     }
 
-    /*
-     * Tests a visibility issue if a shard is in POST_RECOVERY
-     *
-     * When a user indexes a document, then refreshes and then a executes a search and all are successful and no timeouts etc then
-     * the document must be visible for the search.
-     *
-     * When a primary is relocating from node_1 to node_2, there can be a short time where both old and new primary
-     * are started and accept indexing and read requests. However, the new primary might not be visible to nodes
-     * that lag behind one cluster state. If such a node then sends a refresh to the index, this refresh request
-     * must reach the new primary on node_2 too. Otherwise a different node that searches on the new primary might not
-     * find the indexed document although a refresh was executed before.
-     *
-     * In detail:
-     * Cluster state 0:
-     * node_1: [index][0] STARTED   (ShardRoutingState)
-     * node_2: no shard
-     *
-     * 0. primary ([index][0]) relocates from node_1 to node_2
-     * Cluster state 1:
-     * node_1: [index][0] RELOCATING   (ShardRoutingState), (STARTED from IndexShardState perspective on node_1)
-     * node_2: [index][0] INITIALIZING (ShardRoutingState), (IndexShardState on node_2 is RECOVERING)
-     *
-     * 1. node_2 is done recovering, moves its shard to IndexShardState.POST_RECOVERY and sends a message to master that the shard is ShardRoutingState.STARTED
-     * Cluster state is still the same but the IndexShardState on node_2 has changed and it now accepts writes and reads:
-     * node_1: [index][0] RELOCATING   (ShardRoutingState), (STARTED from IndexShardState perspective on node_1)
-     * node_2: [index][0] INITIALIZING (ShardRoutingState), (IndexShardState on node_2 is POST_RECOVERY)
-     *
-     * 2. any node receives an index request which is then executed on node_1 and node_2
-     *
-     * 3. node_3 sends a refresh but it is a little behind with cluster state processing and still on cluster state 0.
-     * If refresh was a broadcast operation it send it to node_1 only because it does not know node_2 has a shard too
-     *
-     * 4. node_3 catches up with the cluster state and acks it to master which now can process the shard started message
-     *  from node_2 before and updates cluster state to:
-     * Cluster state 2:
-     * node_1: [index][0] no shard
-     * node_2: [index][0] STARTED (ShardRoutingState), (IndexShardState on node_2 is still POST_RECOVERY)
-     *
-     * master sends this to all nodes.
-     *
-     * 5. node_4 and node_3 process cluster state 2, but node_1 and node_2 have not yet
-     *
-     * If now node_4 searches for document that was indexed before, it will search at node_2 because it is on
-     * cluster state 2. It should be able to retrieve it with a search because the refresh from before was
-     * successful.
-     */
-    @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/13316")
-    public void testReadOnPostRecoveryShards() throws Exception {
-        List<BlockClusterStateProcessing> clusterStateBlocks = new ArrayList<>();
-        try {
-            configureUnicastCluster(5, null, 1);
-            // we could probably write a test without a dedicated master node but it is easier if we use one
-            InternalTestCluster.Async<String> masterNodeFuture = internalCluster().startMasterOnlyNodeAsync();
-            // node_1 will have the shard in the beginning
-            InternalTestCluster.Async<String> node1Future = internalCluster().startDataOnlyNodeAsync();
-            final String masterNode = masterNodeFuture.get();
-            final String node_1 = node1Future.get();
-            logger.info("--> creating index [test] with one shard and zero replica");
-            assertAcked(prepareCreate("test").setSettings(
-                            Settings.builder().put(indexSettings())
-                                    .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
-                                    .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
-                                    .put(IndexShard.INDEX_REFRESH_INTERVAL, -1))
-                            .addMapping("doc", jsonBuilder().startObject().startObject("doc")
-                                    .startObject("properties").startObject("text").field("type", "string").endObject().endObject()
-                                    .endObject().endObject())
-            );
-            ensureGreen("test");
-            logger.info("--> starting three more data nodes");
-            List<String> nodeNamesFuture = internalCluster().startDataOnlyNodesAsync(3).get();
-            final String node_2 = nodeNamesFuture.get(0);
-            final String node_3 = nodeNamesFuture.get(1);
-            final String node_4 = nodeNamesFuture.get(2);
-            logger.info("--> running cluster_health");
-            ClusterHealthResponse clusterHealth = client().admin().cluster().prepareHealth()
-                    .setWaitForNodes("5")
-                    .setWaitForRelocatingShards(0)
-                    .get();
-            assertThat(clusterHealth.isTimedOut(), equalTo(false));
-
-            logger.info("--> move shard from node_1 to node_2, and wait for relocation to finish");
-
-            // block cluster state updates on node_3 so that it only sees the shard on node_1
-            BlockClusterStateProcessing disruptionNode3 = new BlockClusterStateProcessing(node_3, getRandom());
-            clusterStateBlocks.add(disruptionNode3);
-            internalCluster().setDisruptionScheme(disruptionNode3);
-            disruptionNode3.startDisrupting();
-            // register a Tracer that notifies begin and end of a relocation
-            MockTransportService transportServiceNode2 = (MockTransportService) internalCluster().getInstance(TransportService.class, node_2);
-            CountDownLatch beginRelocationLatchNode2 = new CountDownLatch(1);
-            CountDownLatch endRelocationLatchNode2 = new CountDownLatch(1);
-            transportServiceNode2.addTracer(new StartRecoveryToShardStaredTracer(logger, beginRelocationLatchNode2, endRelocationLatchNode2));
-
-            // block cluster state updates on node_1 and node_2 so that we end up with two primaries
-            BlockClusterStateProcessing disruptionNode2 = new BlockClusterStateProcessing(node_2, getRandom());
-            clusterStateBlocks.add(disruptionNode2);
-            disruptionNode2.applyToCluster(internalCluster());
-            BlockClusterStateProcessing disruptionNode1 = new BlockClusterStateProcessing(node_1, getRandom());
-            clusterStateBlocks.add(disruptionNode1);
-            disruptionNode1.applyToCluster(internalCluster());
-
-            logger.info("--> move shard from node_1 to node_2");
-            // don't block on the relocation. cluster state updates are blocked on node_3 and the relocation would timeout
-            Future<ClusterRerouteResponse> rerouteFuture = internalCluster().client().admin().cluster().prepareReroute().add(new MoveAllocationCommand(new ShardId("test", 0), node_1, node_2)).setTimeout(new TimeValue(1000, TimeUnit.MILLISECONDS)).execute();
-
-            logger.info("--> wait for relocation to start");
-            // wait for relocation to start
-            beginRelocationLatchNode2.await();
-            // start to block cluster state updates on node_1 and node_2 so that we end up with two primaries
-            // one STARTED on node_1 and one in POST_RECOVERY on node_2
-            disruptionNode1.startDisrupting();
-            disruptionNode2.startDisrupting();
-            endRelocationLatchNode2.await();
-            final Client node3Client = internalCluster().client(node_3);
-            final Client node2Client = internalCluster().client(node_2);
-            final Client node1Client = internalCluster().client(node_1);
-            final Client node4Client = internalCluster().client(node_4);
-            logger.info("--> index doc");
-            logLocalClusterStates(node1Client, node2Client, node3Client, node4Client);
-            assertTrue(node3Client.prepareIndex("test", "doc").setSource("{\"text\":\"a\"}").get().isCreated());
-            //sometimes refresh and sometimes flush
-            int refreshOrFlushType = randomIntBetween(1, 2);
-            switch (refreshOrFlushType) {
-                case 1: {
-                    logger.info("--> refresh from node_3");
-                    RefreshResponse refreshResponse = node3Client.admin().indices().prepareRefresh().get();
-                    assertThat(refreshResponse.getFailedShards(), equalTo(0));
-                    // the total shards is num replicas + 1 so that can be lower here because one shard
-                    // is relocating and counts twice as successful
-                    assertThat(refreshResponse.getTotalShards(), equalTo(2));
-                    assertThat(refreshResponse.getSuccessfulShards(), equalTo(2));
-                    break;
-                }
-                case 2: {
-                    logger.info("--> flush from node_3");
-                    FlushResponse flushResponse = node3Client.admin().indices().prepareFlush().get();
-                    assertThat(flushResponse.getFailedShards(), equalTo(0));
-                    // the total shards is num replicas + 1 so that can be lower here because one shard
-                    // is relocating and counts twice as successful
-                    assertThat(flushResponse.getTotalShards(), equalTo(2));
-                    assertThat(flushResponse.getSuccessfulShards(), equalTo(2));
-                    break;
-                }
-                default:
-                    fail("this is  test bug, number should be between 1 and 2");
-            }
-            // now stop disrupting so that node_3 can ack last cluster state to master and master can continue
-            // to publish the next cluster state
-            logger.info("--> stop disrupting node_3");
-            disruptionNode3.stopDisrupting();
-            rerouteFuture.get();
-            logger.info("--> wait for node_4 to get new cluster state");
-            // wait until node_4 actually has the new cluster state in which node_1 has no shard
-            assertBusy(new Runnable() {
-                @Override
-                public void run() {
-                    ClusterState clusterState = node4Client.admin().cluster().prepareState().setLocal(true).get().getState();
-                    // get the node id from the name. TODO: Is there a better way to do this?
-                    String nodeId = null;
-                    for (RoutingNode node : clusterState.getRoutingNodes()) {
-                        if (node.node().name().equals(node_1)) {
-                            nodeId = node.nodeId();
-                        }
-                    }
-                    assertNotNull(nodeId);
-                    // check that node_1 does not have the shard in local cluster state
-                    assertFalse(clusterState.getRoutingNodes().routingNodeIter(nodeId).hasNext());
-                }
-            });
-
-            logger.info("--> run count from node_4");
-            logLocalClusterStates(node1Client, node2Client, node3Client, node4Client);
-            CountResponse countResponse = node4Client.prepareCount("test").setPreference("local").get();
-            assertThat(countResponse.getCount(), equalTo(1l));
-            logger.info("--> stop disrupting node_1 and node_2");
-            disruptionNode2.stopDisrupting();
-            disruptionNode1.stopDisrupting();
-            // wait for relocation to finish
-            logger.info("--> wait for relocation to finish");
-            clusterHealth = client().admin().cluster().prepareHealth()
-                    .setWaitForRelocatingShards(0)
-                    .get();
-            assertThat(clusterHealth.isTimedOut(), equalTo(false));
-        } catch (AssertionError e) {
-            for (BlockClusterStateProcessing blockClusterStateProcessing : clusterStateBlocks) {
-                blockClusterStateProcessing.stopDisrupting();
-            }
-            throw e;
-        }
-    }
-
-    /**
-     * This Tracer can be used to signal start of a recovery and shard started event after translog was copied
-     */
-    public static class StartRecoveryToShardStaredTracer extends MockTransportService.Tracer {
-        private final ESLogger logger;
-        private final CountDownLatch beginRelocationLatch;
-        private final CountDownLatch sentShardStartedLatch;
-
-        public StartRecoveryToShardStaredTracer(ESLogger logger, CountDownLatch beginRelocationLatch, CountDownLatch sentShardStartedLatch) {
-            this.logger = logger;
-            this.beginRelocationLatch = beginRelocationLatch;
-            this.sentShardStartedLatch = sentShardStartedLatch;
-        }
-
-        @Override
-        public void requestSent(DiscoveryNode node, long requestId, String action, TransportRequestOptions options) {
-            if (action.equals(RecoverySource.Actions.START_RECOVERY)) {
-                logger.info("sent: {}, relocation starts", action);
-                beginRelocationLatch.countDown();
-            }
-            if (action.equals(ShardStateAction.SHARD_STARTED_ACTION_NAME)) {
-                logger.info("sent: {}, shard started", action);
-                sentShardStartedLatch.countDown();
-            }
-        }
-    }
-
-    private void logLocalClusterStates(Client... clients) {
-        int counter = 1;
-        for (Client client : clients) {
-            ClusterState clusterState = client.admin().cluster().prepareState().setLocal(true).get().getState();
-            logger.info("--> cluster state on node_{} {}", counter, clusterState.prettyPrint());
-            counter++;
-        }
-    }
-
     /**
      * This test creates a scenario where a primary shard (0 replicas) relocates and is in POST_RECOVERY on the target
      * node but already deleted on the source node. Search request should still work.
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index 159576d..0cbcaf9 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -556,7 +556,6 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
     /**
      * Tests that shadow replicas can be "naturally" rebalanced and relocated
      * around the cluster. By "naturally" I mean without using the reroute API
-     * @throws Exception
      */
     @Test
     public void testShadowReplicaNaturalRelocation() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
index 6d28f2d..42a9df6 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationIT.java
@@ -22,12 +22,14 @@ package org.elasticsearch.index.mapper.externalvalues;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.geo.ShapeRelation;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.util.Arrays;
 import java.util.Collection;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -66,13 +68,13 @@ public class ExternalValuesMapperIntegrationIT extends ESIntegTestCase {
         assertThat(response.getHits().totalHits(), equalTo((long) 1));
 
         response = client().prepareSearch("test-idx")
-                .setPostFilter(QueryBuilders.geoDistanceRangeQuery("field.point", 42.0, 51.0).to("1km"))
+                .setPostFilter(QueryBuilders.geoDistanceRangeQuery("field.point").point(42.0, 51.0).to("1km"))
                 .execute().actionGet();
 
         assertThat(response.getHits().totalHits(), equalTo((long) 1));
 
         response = client().prepareSearch("test-idx")
-                .setPostFilter(QueryBuilders.geoShapeQuery("field.shape", ShapeBuilder.newPoint(-100, 45)).relation(ShapeRelation.WITHIN))
+                .setPostFilter(QueryBuilders.geoShapeQuery("field.shape", ShapeBuilder.newPoint(-100, 45), ShapeRelation.WITHIN))
                         .execute().actionGet();
 
         assertThat(response.getHits().totalHits(), equalTo((long) 1));
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
index 2fe5978..26f7129 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java
@@ -65,7 +65,6 @@ public class GeoShapeFieldMapperTests extends ESSingleNodeTestCase {
 
     /**
      * Test that orientation parameter correctly parses
-     * @throws IOException
      */
     public void testOrientationParsing() throws IOException {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
@@ -104,7 +103,6 @@ public class GeoShapeFieldMapperTests extends ESSingleNodeTestCase {
 
     /**
      * Test that orientation parameter correctly parses
-     * @throws IOException
      */
     public void testCoerceParsing() throws IOException {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
deleted file mode 100644
index 2d45f58..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ /dev/null
@@ -1,668 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.CodepointSetGenerator;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.support.PlainActionFuture;
-import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
-import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayIndexFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.test.cluster.TestClusterService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.*;
-
-import java.io.IOException;
-import java.lang.reflect.InvocationHandler;
-import java.lang.reflect.Method;
-import java.lang.reflect.Proxy;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ExecutionException;
-
-import static org.hamcrest.Matchers.*;
-
-public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>> extends ESTestCase {
-
-    private static final GeohashGenerator geohashGenerator = new GeohashGenerator();
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String STRING_FIELD_NAME_2 = "mapped_string_2";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String GEO_POINT_FIELD_NAME = "mapped_geo_point";
-    protected static final String GEO_SHAPE_FIELD_NAME = "mapped_geo_shape";
-    protected static final String[] MAPPED_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME, DOUBLE_FIELD_NAME,
-            BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME, GEO_POINT_FIELD_NAME, GEO_SHAPE_FIELD_NAME };
-    protected static final String[] MAPPED_LEAF_FIELD_NAMES = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME, DOUBLE_FIELD_NAME,
-            BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, GEO_POINT_FIELD_NAME };
-
-    private static Injector injector;
-    private static IndexQueryParserService queryParserService;
-
-    protected static IndexQueryParserService queryParserService() {
-        return queryParserService;
-    }
-
-    private static Index index;
-
-    protected static Index getIndex() {
-        return index;
-    }
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-    
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static String[] randomTypes;
-    private static ClientInvocationHandler clientInvocationHandler = new ClientInvocationHandler();
-
-    /**
-     * Setup for the whole base test class.
-     * @throws IOException
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
-        Version version = randomBoolean() ? Version.CURRENT : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
-        Settings settings = Settings.settingsBuilder()
-                .put("name", AbstractQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-        Settings indexSettings = Settings.settingsBuilder()
-                .put(IndexMetaData.SETTING_VERSION_CREATED, version).build();
-        index = new Index(randomAsciiOfLengthBetween(1, 10));
-        final TestClusterService clusterService = new TestClusterService();
-        clusterService.setState(new ClusterState.Builder(clusterService.state()).metaData(new MetaData.Builder().put(
-                new IndexMetaData.Builder(index.name()).settings(indexSettings).numberOfShards(1).numberOfReplicas(0))));
-        final Client proxy = (Client) Proxy.newProxyInstance(
-                Client.class.getClassLoader(),
-                new Class[]{Client.class},
-                clientInvocationHandler);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, indexSettings),
-                new IndexCacheModule(indexSettings),
-                new AnalysisModule(indexSettings, new IndicesAnalysisService(indexSettings)),
-                new SimilarityModule(indexSettings),
-                new IndexNameModule(index),
-        new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(Client.class).toInstance(proxy);
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of(clusterService));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        queryParserService = injector.getInstance(IndexQueryParserService.class);
-
-        MapperService mapperService = queryParserService.mapperService;
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
-                    STRING_FIELD_NAME, "type=string",
-                    STRING_FIELD_NAME_2, "type=string",
-                    INT_FIELD_NAME, "type=integer",
-                    DOUBLE_FIELD_NAME, "type=double",
-                    BOOLEAN_FIELD_NAME, "type=boolean",
-                    DATE_FIELD_NAME, "type=date",
-                    OBJECT_FIELD_NAME, "type=object",
-                    GEO_POINT_FIELD_NAME, "type=geo_point,lat_lon=true,geohash=true,geohash_prefix=true",
-                    GEO_SHAPE_FIELD_NAME, "type=geo_shape"
-            ).string()), false, false);
-            // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
-            currentTypes[i] = type;
-        }
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        queryParserService = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-        randomTypes = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        clientInvocationHandler.delegate = this;
-        //set some random types to be queried as part the search request, before each test
-        randomTypes = getRandomTypes();
-    }
-
-    protected void setSearchContext(String[] types) {
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        clientInvocationHandler.delegate = null;
-        QueryShardContext.removeTypes();
-        SearchContext.removeCurrent();
-    }
-
-    protected final QB createTestQueryBuilder() {
-        QB query = doCreateTestQueryBuilder();
-        //we should not set boost and query name for queries that don't parse it
-        if (supportsBoostAndQueryName()) {
-            if (randomBoolean()) {
-                query.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                query.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-        }
-        return query;
-    }
-
-    /**
-     * Create the query that is being tested
-     */
-    protected abstract QB doCreateTestQueryBuilder();
-
-    /**
-     * Generic test that creates new query from the test query and checks both for equality
-     * and asserts equality on the two queries.
-     */
-    @Test
-    public void testFromXContent() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        assertParsedQuery(testQuery.toString(), testQuery);
-        for (Map.Entry<String, QB> alternateVersion : getAlternateVersions().entrySet()) {
-            assertParsedQuery(alternateVersion.getKey(), alternateVersion.getValue());
-        }
-    }
-
-    /**
-     * Returns alternate string representation of the query that need to be tested as they are never used as output
-     * of {@link QueryBuilder#toXContent(XContentBuilder, ToXContent.Params)}. By default there are no alternate versions.
-     */
-    protected Map<String, QB> getAlternateVersions() {
-        return Collections.emptyMap();
-    }
-
-    /**
-     * Parses the query provided as string argument and compares it with the expected result provided as argument as a {@link QueryBuilder}
-     */
-    protected void assertParsedQuery(String queryAsString, QueryBuilder<?> expectedQuery) throws IOException {
-        assertParsedQuery(queryAsString, expectedQuery, ParseFieldMatcher.STRICT);
-    }
-
-    protected void assertParsedQuery(String queryAsString, QueryBuilder<?> expectedQuery, ParseFieldMatcher matcher) throws IOException {
-        QueryBuilder<?> newQuery = parseQuery(queryAsString, matcher);
-        assertNotSame(newQuery, expectedQuery);
-        assertEquals(expectedQuery, newQuery);
-        assertEquals(expectedQuery.hashCode(), newQuery.hashCode());
-    }
-
-    protected QueryBuilder<?> parseQuery(String queryAsString) throws IOException {
-        return parseQuery(queryAsString, ParseFieldMatcher.STRICT);
-    }
-
-    protected QueryBuilder<?> parseQuery(String queryAsString, ParseFieldMatcher matcher) throws IOException {
-        XContentParser parser = XContentFactory.xContent(queryAsString).createParser(queryAsString);
-        QueryParseContext context = createParseContext();
-        context.reset(parser);
-        context.parseFieldMatcher(matcher);
-        return context.parseInnerQueryBuilder();
-    }
-
-    /**
-     * Test creates the {@link Query} from the {@link QueryBuilder} under test and delegates the
-     * assertions being made on the result to the implementing subclass.
-     */
-    @Test
-    public void testToQuery() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        QB firstQuery = createTestQueryBuilder();
-        setSearchContext(randomTypes); // only set search context for toQuery to be more realistic
-        Query firstLuceneQuery = firstQuery.toQuery(context);
-        assertLuceneQuery(firstQuery, firstLuceneQuery, context);
-        SearchContext.removeCurrent(); // remove after assertLuceneQuery since the assertLuceneQuery impl might access the context as well
-
-
-        QB secondQuery = copyQuery(firstQuery);
-        //query _name never should affect the result of toQuery, we randomly set it to make sure
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        }
-        setSearchContext(randomTypes); // only set search context for toQuery to be more realistic
-        Query secondLuceneQuery = secondQuery.toQuery(context);
-        assertLuceneQuery(secondQuery, secondLuceneQuery, context);
-        SearchContext.removeCurrent(); // remove after assertLuceneQuery since the assertLuceneQuery impl might access the context as well
-
-        assertThat("two equivalent query builders lead to different lucene queries", secondLuceneQuery, equalTo(firstLuceneQuery));
-
-        //if the initial lucene query is null, changing its boost won't have any effect, we shouldn't test that
-        if (firstLuceneQuery != null && supportsBoostAndQueryName()) {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-            setSearchContext(randomTypes); // only set search context for toQuery to be more realistic
-            Query thirdLuceneQuery = secondQuery.toQuery(context);
-            SearchContext.removeCurrent();
-            assertThat("modifying the boost doesn't affect the corresponding lucene query", firstLuceneQuery, not(equalTo(thirdLuceneQuery)));
-        }
-    }
-
-    /**
-     * Few queries allow you to set the boost and queryName on the java api, although the corresponding parser doesn't parse them as they are not supported.
-     * This method allows to disable boost and queryName related tests for those queries. Those queries are easy to identify: their parsers
-     * don't parse `boost` and `_name` as they don't apply to the specific query: filter query, wrapper query and match_none
-     */
-    protected boolean supportsBoostAndQueryName() {
-        return true;
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Verifies that named queries and boost are properly handled and delegates to {@link #doAssertLuceneQuery(AbstractQueryBuilder, Query, QueryShardContext)}
-     * for query specific checks.
-     */
-    protected final void assertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.queryName() != null) {
-            Query namedQuery = context.copyNamedQueries().get(queryBuilder.queryName());
-            assertThat(namedQuery, equalTo(query));
-        }
-        if (query != null) {
-            assertBoost(queryBuilder, query);
-        }
-        doAssertLuceneQuery(queryBuilder, query, context);
-    }
-
-    /**
-     * Allows to override boost assertions for queries that don't have the default behaviour
-     */
-    protected void assertBoost(QB queryBuilder, Query query) throws IOException {
-        assertThat(query.getBoost(), equalTo(queryBuilder.boost()));
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Contains the query specific checks to be implemented by subclasses.
-     */
-    protected abstract void doAssertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException;
-
-    /**
-     * Test serialization and deserialization of the test query.
-     */
-    @Test
-    public void testSerialization() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        assertSerialization(testQuery);
-    }
-
-    /**
-     * Serialize the given query builder and asserts that both are equal
-     */
-    protected QB assertSerialization(QB testQuery) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testQuery.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParser(testQuery.getName()).getBuilderPrototype();
-                QueryBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testQuery);
-                assertEquals(deserializedQuery.hashCode(), testQuery.hashCode());
-                assertNotSame(deserializedQuery, testQuery);
-                return (QB) deserializedQuery;
-            }
-        }
-    }
-
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        QB firstQuery = createTestQueryBuilder();
-        assertFalse("query is equal to null", firstQuery.equals(null));
-        assertFalse("query is equal to incompatible type", firstQuery.equals(""));
-        assertTrue("query is not equal to self", firstQuery.equals(firstQuery));
-        assertThat("same query's hashcode returns different values if called multiple times", firstQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB secondQuery = copyQuery(firstQuery);
-        assertTrue("query is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("query is not equal to its copy", firstQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB thirdQuery = copyQuery(secondQuery);
-        assertTrue("query is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("query is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", firstQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstQuery));
-
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        } else {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-        }
-        assertThat("different queries should not be equal", secondQuery, not(equalTo(firstQuery)));
-        assertThat("different queries should have different hashcode", secondQuery.hashCode(), not(equalTo(firstQuery.hashCode())));
-    }
-
-    private QueryParser<?> queryParser(String queryId) {
-        return queryParserService.indicesQueriesRegistry().queryParsers().get(queryId);
-    }
-
-    //we use the streaming infra to create a copy of the query provided as argument
-    protected QB copyQuery(QB query) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            query.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParser(query.getName()).getBuilderPrototype();
-                @SuppressWarnings("unchecked")
-                QB secondQuery = (QB)prototype.readFrom(in);
-                return secondQuery;
-            }
-        }
-    }
-
-    /**
-     * @return a new {@link QueryShardContext} based on the base test index and queryParserService
-     */
-    protected static QueryShardContext createShardContext() {
-        QueryShardContext queryCreationContext = new QueryShardContext(index, queryParserService);
-        queryCreationContext.reset();
-        queryCreationContext.parseFieldMatcher(ParseFieldMatcher.EMPTY);
-
-        return queryCreationContext;
-    }
-
-    /**
-     * @return a new {@link QueryParseContext} based on the base test index and queryParserService
-     */
-    protected static QueryParseContext createParseContext() {
-        return createShardContext().parseContext();
-    }
-
-    /**
-     * create a random value for either {@link AbstractQueryTestCase#BOOLEAN_FIELD_NAME}, {@link AbstractQueryTestCase#INT_FIELD_NAME},
-     * {@link AbstractQueryTestCase#DOUBLE_FIELD_NAME}, {@link AbstractQueryTestCase#STRING_FIELD_NAME} or
-     * {@link AbstractQueryTestCase#DATE_FIELD_NAME}, or a String value by default
-     */
-    protected static Object getRandomValueForFieldName(String fieldName) {
-        Object value;
-        switch (fieldName) {
-            case STRING_FIELD_NAME:
-                value = rarely() ? randomUnicodeOfLength(10) : randomAsciiOfLengthBetween(1, 10); // unicode in 10% cases
-                break;
-            case INT_FIELD_NAME:
-                value = randomIntBetween(0, 10);
-                break;
-            case DOUBLE_FIELD_NAME:
-                value = randomDouble() * 10;
-                break;
-            case BOOLEAN_FIELD_NAME:
-                value = randomBoolean();
-                break;
-            case DATE_FIELD_NAME:
-                value = new DateTime(System.currentTimeMillis(), DateTimeZone.UTC).toString();
-                break;
-            default:
-                value = randomAsciiOfLengthBetween(1, 10);
-        }
-        return value;
-    }
-
-    protected static String getRandomQueryText() {
-        int terms = randomIntBetween(0, 3);
-        StringBuilder builder = new StringBuilder();
-        for (int i = 0; i < terms; i++) {
-            builder.append(randomAsciiOfLengthBetween(1, 10) + " ");
-        }
-        return builder.toString().trim();
-    }
-
-    /**
-     * Helper method to return a mapped or a random field
-     */
-    protected String getRandomFieldName() {
-        // if no type is set then return a random field name
-        if (currentTypes == null || currentTypes.length == 0 || randomBoolean()) {
-            return randomAsciiOfLengthBetween(1, 10);
-        }
-        return randomFrom(MAPPED_LEAF_FIELD_NAMES);
-    }
-
-    /**
-     * Helper method to return a random field (mapped or unmapped) and a value
-     */
-    protected Tuple<String, Object> getRandomFieldNameAndValue() {
-        String fieldName = getRandomFieldName();
-        return new Tuple<>(fieldName, getRandomValueForFieldName(fieldName));
-    }
-
-    /**
-     * Helper method to return a random rewrite method
-     */
-    protected static String getRandomRewriteMethod() {
-        String rewrite;
-        if (randomBoolean()) {
-            rewrite = randomFrom(QueryParsers.CONSTANT_SCORE,
-                    QueryParsers.SCORING_BOOLEAN,
-                    QueryParsers.CONSTANT_SCORE_BOOLEAN).getPreferredName();
-        } else {
-            rewrite = randomFrom(QueryParsers.TOP_TERMS,
-                    QueryParsers.TOP_TERMS_BOOST,
-                    QueryParsers.TOP_TERMS_BLENDED_FREQS).getPreferredName() + "1";
-        }
-        return rewrite;
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    protected String getRandomType() {
-        return (currentTypes.length == 0) ? MetaData.ALL : randomFrom(currentTypes);
-    }
-
-    public static String randomGeohash(int minPrecision, int maxPrecision) {
-        return geohashGenerator.ofStringLength(getRandom(), minPrecision, maxPrecision);
-    }
-
-    public static class GeohashGenerator extends CodepointSetGenerator {
-        private final static char[] ASCII_SET = "0123456789bcdefghjkmnpqrstuvwxyz".toCharArray();
-
-        public GeohashGenerator() {
-            super(ASCII_SET);
-        }
-    }
-
-    protected static Fuzziness randomFuzziness(String fieldName) {
-        if (randomBoolean()) {
-            return Fuzziness.fromEdits(randomIntBetween(0, 2));
-        }
-        if (randomBoolean()) {
-            return Fuzziness.AUTO;
-        }
-        switch (fieldName) {
-            case INT_FIELD_NAME:
-                return Fuzziness.build(randomIntBetween(3, 100));
-            case DOUBLE_FIELD_NAME:
-                return Fuzziness.build(1 + randomFloat() * 10);
-            case DATE_FIELD_NAME:
-                return Fuzziness.build(randomTimeValue());
-            default:
-                return Fuzziness.AUTO;
-        }
-    }
-
-    protected static boolean isNumericFieldName(String fieldName) {
-        return INT_FIELD_NAME.equals(fieldName) || DOUBLE_FIELD_NAME.equals(fieldName);
-    }
-
-    protected static String randomAnalyzer() {
-        return randomFrom("simple", "standard", "keyword", "whitespace");
-    }
-
-    protected static String randomMinimumShouldMatch() {
-        return randomFrom("1", "-1", "75%", "-25%", "2<75%", "2<-25%");
-    }
-
-    protected static String randomTimeZone() {
-        return randomFrom(TIMEZONE_IDS);
-    }
-
-    private static final List<String> TIMEZONE_IDS = new ArrayList<>(DateTimeZone.getAvailableIDs());
-
-    private static class ClientInvocationHandler implements InvocationHandler {
-        AbstractQueryTestCase delegate;
-        @Override
-        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
-            if (method.equals(Client.class.getDeclaredMethod("get", GetRequest.class))) {
-                return new PlainActionFuture<GetResponse>() {
-                    @Override
-                    public GetResponse get() throws InterruptedException, ExecutionException {
-                        return delegate.executeGet((GetRequest) args[0]);
-                    }
-                };
-            } else if (method.equals(Client.class.getDeclaredMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
-                    return new PlainActionFuture<MultiTermVectorsResponse>() {
-                        @Override
-                        public MultiTermVectorsResponse get() throws InterruptedException, ExecutionException {
-                            return delegate.executeMultiTermVectors((MultiTermVectorsRequest) args[0]);
-                        }
-                    };
-            } else if (method.equals(Object.class.getDeclaredMethod("toString"))) {
-                return "MockClient";
-            }
-            throw new UnsupportedOperationException("this test can't handle calls to: " + method);
-        }
-
-    }
-
-    /**
-     * Override this to handle {@link Client#get(GetRequest)} calls from parsers / builders
-     */
-    protected GetResponse executeGet(GetRequest getRequest) {
-        throw new UnsupportedOperationException("this test can't handle GET requests");
-    }
-
-    /**
-     * Override this to handle {@link Client#get(GetRequest)} calls from parsers / builders
-     */
-    protected MultiTermVectorsResponse executeMultiTermVectors(MultiTermVectorsRequest mtvRequest) {
-        throw new UnsupportedOperationException("this test can't handle MultiTermVector requests");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractTermQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractTermQueryTestCase.java
deleted file mode 100644
index ccfe619..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractTermQueryTestCase.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public abstract class AbstractTermQueryTestCase<QB extends BaseTermQueryBuilder<QB>> extends AbstractQueryTestCase<QB> {
-
-    @Override
-    protected final QB doCreateTestQueryBuilder() {
-        String fieldName = null;
-        Object value;
-        switch (randomIntBetween(0, 3)) {
-            case 0:
-                if (randomBoolean()) {
-                    fieldName = BOOLEAN_FIELD_NAME;
-                }
-                value = randomBoolean();
-                break;
-            case 1:
-                if (randomBoolean()) {
-                    fieldName = STRING_FIELD_NAME;
-                }
-                if (frequently()) {
-                    value = randomAsciiOfLengthBetween(1, 10);
-                } else {
-                    // generate unicode string in 10% of cases
-                    value = randomUnicodeOfLength(10);
-                }
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    fieldName = INT_FIELD_NAME;
-                }
-                value = randomInt(10000);
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    fieldName = DOUBLE_FIELD_NAME;
-                }
-                value = randomDouble();
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-
-        if (fieldName == null) {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        return createQueryBuilder(fieldName, value);
-    }
-
-    protected abstract QB createQueryBuilder(String fieldName, Object value);
-
-    @Test
-    public void testIllegalArguments() throws QueryShardException {
-        try {
-            if (randomBoolean()) {
-                createQueryBuilder(null, randomAsciiOfLengthBetween(1, 30));
-            } else {
-                createQueryBuilder("", randomAsciiOfLengthBetween(1, 30));
-            }
-            fail("fieldname cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            createQueryBuilder("field", null);
-            fail("value cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Override
-    protected Map<String, QB> getAlternateVersions() {
-        HashMap<String, QB> alternateVersions = new HashMap<>();
-        QB tempQuery = createTestQueryBuilder();
-        QB testQuery = createQueryBuilder(tempQuery.fieldName(), tempQuery.value());
-        boolean isString = testQuery.value() instanceof String;
-        String value = (isString ? "\"" : "") + testQuery.value() + (isString ? "\"" : "");
-        String contentString = "{\n" +
-                "    \"" + testQuery.getName() + "\" : {\n" +
-                "        \"" + testQuery.fieldName() + "\" : " + value + "\n" +
-                "    }\n" +
-                "}";
-        alternateVersions.put(contentString, testQuery);
-        return alternateVersions;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java
deleted file mode 100644
index 3f7e57e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java
+++ /dev/null
@@ -1,176 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class BoolQueryBuilderTests extends AbstractQueryTestCase<BoolQueryBuilder> {
-
-    @Override
-    protected BoolQueryBuilder doCreateTestQueryBuilder() {
-        BoolQueryBuilder query = new BoolQueryBuilder();
-        if (randomBoolean()) {
-            query.adjustPureNegative(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.minimumNumberShouldMatch(randomMinimumShouldMatch());
-        }
-        int mustClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustClauses; i++) {
-            query.must(RandomQueryBuilder.createQuery(random()));
-        }
-        int mustNotClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustNotClauses; i++) {
-            query.mustNot(RandomQueryBuilder.createQuery(random()));
-        }
-        int shouldClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < shouldClauses; i++) {
-            query.should(RandomQueryBuilder.createQuery(random()));
-        }
-        int filterClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < filterClauses; i++) {
-            query.filter(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoolQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (!queryBuilder.hasClauses()) {
-            assertThat(query, instanceOf(MatchAllDocsQuery.class));
-        } else {
-            List<BooleanClause> clauses = new ArrayList<>();
-            clauses.addAll(getBooleanClauses(queryBuilder.must(), BooleanClause.Occur.MUST, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.mustNot(), BooleanClause.Occur.MUST_NOT, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.should(), BooleanClause.Occur.SHOULD, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.filter(), BooleanClause.Occur.FILTER, context));
-
-            if (clauses.isEmpty()) {
-                assertThat(query, instanceOf(MatchAllDocsQuery.class));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                if (queryBuilder.adjustPureNegative()) {
-                    boolean isNegative = true;
-                    for (BooleanClause clause : clauses) {
-                        if (clause.isProhibited() == false) {
-                            isNegative = false;
-                            break;
-                        }
-                    }
-                    if (isNegative) {
-                        clauses.add(new BooleanClause(new MatchAllDocsQuery(), BooleanClause.Occur.MUST));
-                    }
-                }
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<BooleanClause> clauseIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery.getClauses()) {
-                    assertThat(booleanClause, equalTo(clauseIterator.next()));
-                }
-            }
-        }
-    }
-
-    private static List<BooleanClause> getBooleanClauses(List<QueryBuilder> queryBuilders, BooleanClause.Occur occur, QueryShardContext context) throws IOException {
-        List<BooleanClause> clauses = new ArrayList<>();
-        for (QueryBuilder query : queryBuilders) {
-            Query innerQuery = query.toQuery(context);
-            if (innerQuery != null) {
-                clauses.add(new BooleanClause(innerQuery, occur));
-            }
-        }
-        return clauses;
-    }
-
-    @Override
-    protected Map<String, BoolQueryBuilder> getAlternateVersions() {
-        Map<String, BoolQueryBuilder> alternateVersions = new HashMap<>();
-        BoolQueryBuilder tempQueryBuilder = createTestQueryBuilder();
-        BoolQueryBuilder expectedQuery = new BoolQueryBuilder();
-        String contentString = "{\n" +
-                "    \"bool\" : {\n";
-        if (tempQueryBuilder.must().size() > 0) {
-            QueryBuilder must = tempQueryBuilder.must().get(0);
-            contentString += "must: " + must.toString() + ",";
-            expectedQuery.must(must);
-        }
-        if (tempQueryBuilder.mustNot().size() > 0) {
-            QueryBuilder mustNot = tempQueryBuilder.mustNot().get(0);
-            contentString += (randomBoolean() ? "must_not: " : "mustNot: ") + mustNot.toString() + ",";
-            expectedQuery.mustNot(mustNot);
-        }
-        if (tempQueryBuilder.should().size() > 0) {
-            QueryBuilder should = tempQueryBuilder.should().get(0);
-            contentString += "should: " + should.toString() + ",";
-            expectedQuery.should(should);
-        }
-        if (tempQueryBuilder.filter().size() > 0) {
-            QueryBuilder filter = tempQueryBuilder.filter().get(0);
-            contentString += "filter: " + filter.toString() + ",";
-            expectedQuery.filter(filter);
-        }
-        contentString = contentString.substring(0, contentString.length() - 1);
-        contentString += "    }    \n" + "}";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        BoolQueryBuilder booleanQuery = new BoolQueryBuilder();
-
-        try {
-            booleanQuery.must(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-        }
-
-        try {
-            booleanQuery.mustNot(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-        }
-
-        try {
-            booleanQuery.filter(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-        }
-
-        try {
-            booleanQuery.should(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTests.java
deleted file mode 100644
index 57fab99..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTests.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.BoostingQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class BoostingQueryBuilderTests extends AbstractQueryTestCase<BoostingQueryBuilder> {
-
-    @Override
-    protected BoostingQueryBuilder doCreateTestQueryBuilder() {
-        BoostingQueryBuilder query = new BoostingQueryBuilder(RandomQueryBuilder.createQuery(random()), RandomQueryBuilder.createQuery(random()));
-        query.negativeBoost(2.0f / randomIntBetween(1, 20));
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoostingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query positive = queryBuilder.positiveQuery().toQuery(context);
-        Query negative = queryBuilder.negativeQuery().toQuery(context);
-        if (positive == null || negative == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BoostingQuery.class));
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new BoostingQueryBuilder(null, new MatchAllQueryBuilder());
-            fail("must not be null");
-        } catch (IllegalArgumentException e) {
-            //
-        }
-
-        try {
-            new BoostingQueryBuilder(new MatchAllQueryBuilder(), null);
-            fail("must not be null");
-        } catch (IllegalArgumentException e) {
-            //
-        }
-
-        try {
-            new BoostingQueryBuilder(new MatchAllQueryBuilder(), new MatchAllQueryBuilder()).negativeBoost(-1.0f);
-            fail("must not be negative");
-        } catch (IllegalArgumentException e) {
-            //
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTests.java
deleted file mode 100644
index 8a7bc8f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTests.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class CommonTermsQueryBuilderTests extends AbstractQueryTestCase<CommonTermsQueryBuilder> {
-
-    @Override
-    protected CommonTermsQueryBuilder doCreateTestQueryBuilder() {
-        CommonTermsQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new CommonTermsQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new CommonTermsQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-
-        if (randomBoolean()) {
-            query.cutoffFrequency((float) randomIntBetween(1, 10));
-        }
-
-        if (randomBoolean()) {
-            query.lowFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of low frequency terms that must match
-        if (randomBoolean()) {
-            query.lowFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.highFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of high frequency terms that must match
-        if (randomBoolean()) {
-            query.highFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.analyzer(randomAnalyzer());
-        }
-
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(CommonTermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ExtendedCommonTermsQuery.class));
-        ExtendedCommonTermsQuery extendedCommonTermsQuery = (ExtendedCommonTermsQuery) query;
-        assertThat(extendedCommonTermsQuery.getHighFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.highFreqMinimumShouldMatch()));
-        assertThat(extendedCommonTermsQuery.getLowFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.lowFreqMinimumShouldMatch()));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new CommonTermsQueryBuilder(null, "text");
-            } else {
-                new CommonTermsQueryBuilder("", "text");
-            }
-            fail("must be non null");
-        } catch (IllegalArgumentException e) {
-            // okay
-        }
-
-        try {
-            new CommonTermsQueryBuilder("fieldName", null);
-            fail("must be non null");
-        } catch (IllegalArgumentException e) {
-            // okay
-        }
-    }
-
-    @Test
-    public void testNoTermsFromQueryString() throws IOException {
-        CommonTermsQueryBuilder builder = new CommonTermsQueryBuilder(STRING_FIELD_NAME, "");
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        assertNull(builder.toQuery(context));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java
deleted file mode 100644
index bb6d22e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTests.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.ParsingException;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class ConstantScoreQueryBuilderTests extends AbstractQueryTestCase<ConstantScoreQueryBuilder> {
-
-    /**
-     * @return a {@link ConstantScoreQueryBuilder} with random boost between 0.1f and 2.0f
-     */
-    @Override
-    protected ConstantScoreQueryBuilder doCreateTestQueryBuilder() {
-        return new ConstantScoreQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ConstantScoreQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test that missing "filter" element causes {@link ParsingException}
-     */
-    @Test(expected=ParsingException.class)
-    public void testFilterElement() throws IOException {
-        String queryString = "{ \"" + ConstantScoreQueryBuilder.NAME + "\" : {}";
-        parseQuery(queryString);
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new ConstantScoreQueryBuilder(null);
-            fail("must not be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTests.java
deleted file mode 100644
index 329a162..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTests.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class DisMaxQueryBuilderTests extends AbstractQueryTestCase<DisMaxQueryBuilder> {
-
-    /**
-     * @return a {@link DisMaxQueryBuilder} with random inner queries
-     */
-    @Override
-    protected DisMaxQueryBuilder doCreateTestQueryBuilder() {
-        DisMaxQueryBuilder dismax = new DisMaxQueryBuilder();
-        int clauses = randomIntBetween(1, 5);
-        for (int i = 0; i < clauses; i++) {
-            dismax.add(RandomQueryBuilder.createQuery(random()));
-        }
-        if (randomBoolean()) {
-            dismax.tieBreaker(2.0f / randomIntBetween(1, 20));
-        }
-        return dismax;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(DisMaxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Collection<Query> queries = AbstractQueryBuilder.toQueries(queryBuilder.innerQueries(), context);
-        if (queries.isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-            DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query;
-            assertThat(disjunctionMaxQuery.getTieBreakerMultiplier(), equalTo(queryBuilder.tieBreaker()));
-            assertThat(disjunctionMaxQuery.getDisjuncts().size(), equalTo(queries.size()));
-            Iterator<Query> queryIterator = queries.iterator();
-            for (int i = 0; i < disjunctionMaxQuery.getDisjuncts().size(); i++) {
-                assertThat(disjunctionMaxQuery.getDisjuncts().get(i), equalTo(queryIterator.next()));
-            }
-        }
-    }
-
-    @Override
-    protected Map<String, DisMaxQueryBuilder> getAlternateVersions() {
-        Map<String, DisMaxQueryBuilder> alternateVersions = new HashMap<>();
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQueries().get(0);
-        DisMaxQueryBuilder expectedQuery = new DisMaxQueryBuilder();
-        expectedQuery.add(innerQuery);
-        String contentString = "{\n" +
-                "    \"dis_max\" : {\n" +
-                "        \"queries\" : " + innerQuery.toString() +
-                "    }\n" +
-                "}";
-        alternateVersions.put(contentString, expectedQuery);
-        return alternateVersions;
-    }
-
-    /**
-     * test `null`return value for missing inner queries
-     * @throws IOException
-     */
-    @Test
-    public void testNoInnerQueries() throws IOException {
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder();
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-    }
-
-    /**
-     * Test inner query parsing to null. Current DSL allows inner filter element to parse to <tt>null</tt>.
-     * Those should be ignored upstream. To test this, we use inner {@link ConstantScoreQueryBuilder}
-     * with empty inner filter.
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        String queryString = "{ \"" + ConstantScoreQueryBuilder.NAME + "\" : { \"filter\" : { } } }";
-        QueryBuilder<?> innerQueryBuilder = parseQuery(queryString);
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder().add(innerQueryBuilder);
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        try {
-            disMaxQuery.add(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTests.java
deleted file mode 100644
index 92523bb..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTests.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class ExistsQueryBuilderTests extends AbstractQueryTestCase<ExistsQueryBuilder> {
-
-    @Override
-    protected ExistsQueryBuilder doCreateTestQueryBuilder() {
-        String fieldPattern;
-        if (randomBoolean()) {
-            fieldPattern = randomFrom(MAPPED_FIELD_NAMES);
-        } else {
-            fieldPattern = randomAsciiOfLengthBetween(1, 10);
-        }
-        // also sometimes test wildcard patterns
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                fieldPattern = fieldPattern + "*";
-            } else {
-                fieldPattern = MetaData.ALL;
-            }
-        }
-        return new ExistsQueryBuilder(fieldPattern);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ExistsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldPattern = queryBuilder.fieldName();
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (getCurrentTypes().length == 0 || fields.size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery();
-            assertThat(booleanQuery.clauses().size(), equalTo(fields.size()));
-            for (int i = 0; i < fields.size(); i++) {
-                BooleanClause booleanClause = booleanQuery.clauses().get(i);
-                assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-            }
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new ExistsQueryBuilder(null);
-            } else {
-                new ExistsQueryBuilder("");
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTests.java
deleted file mode 100644
index 64724d2..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTests.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class FieldMaskingSpanQueryBuilderTests extends AbstractQueryTestCase<FieldMaskingSpanQueryBuilder> {
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName;
-        if (randomBoolean()) {
-            fieldName = randomFrom(MAPPED_FIELD_NAMES);
-        } else {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        SpanTermQueryBuilder innerQuery = new SpanTermQueryBuilderTests().createTestQueryBuilder();
-        return new FieldMaskingSpanQueryBuilder(innerQuery, fieldName);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FieldMaskingSpanQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldInQuery = queryBuilder.fieldName();
-        MappedFieldType fieldType = context.fieldMapper(fieldInQuery);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        assertThat(query, instanceOf(FieldMaskingSpanQuery.class));
-        FieldMaskingSpanQuery fieldMaskingSpanQuery = (FieldMaskingSpanQuery) query;
-        assertThat(fieldMaskingSpanQuery.getField(), equalTo(fieldInQuery));
-        assertThat(fieldMaskingSpanQuery.getMaskedQuery(), equalTo(queryBuilder.innerQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new FieldMaskingSpanQueryBuilder(null, "maskedField");
-            fail("must be non null");
-        } catch (IllegalArgumentException e) {
-            // okay
-        }
-
-        try {
-            SpanQueryBuilder span = new SpanTermQueryBuilder("name", "value");
-            if (randomBoolean()) {
-                new FieldMaskingSpanQueryBuilder(span, null);
-            } else {
-                new FieldMaskingSpanQueryBuilder(span, "");
-            }
-            fail("must be non null");
-        } catch (IllegalArgumentException e) {
-            // okay
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTests.java
deleted file mode 100644
index f213ba8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTests.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-
-public class FuzzyQueryBuilderTests extends AbstractQueryTestCase<FuzzyQueryBuilder> {
-
-    @Override
-    protected FuzzyQueryBuilder doCreateTestQueryBuilder() {
-        Tuple<String, Object> fieldAndValue = getRandomFieldNameAndValue();
-        FuzzyQueryBuilder query = new FuzzyQueryBuilder(fieldAndValue.v1(), fieldAndValue.v2());
-        if (randomBoolean()) {
-            query.fuzziness(randomFuzziness(query.fieldName()));
-        }
-        if (randomBoolean()) {
-            query.prefixLength(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            query.maxExpansions(randomIntBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            query.transpositions(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FuzzyQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (isNumericFieldName(queryBuilder.fieldName()) || queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            assertThat(query, instanceOf(FuzzyQuery.class));
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new FuzzyQueryBuilder(null, "text");
-            fail("must not be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new FuzzyQueryBuilder("", "text");
-            fail("must not be empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new FuzzyQueryBuilder("field", null);
-            fail("must not be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testUnsupportedFuzzinessForStringType() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(STRING_FIELD_NAME, "text");
-        fuzzyQueryBuilder.fuzziness(Fuzziness.build(randomFrom("a string which is not auto", "3h", "200s")));
-
-        try {
-            fuzzyQueryBuilder.toQuery(context);
-            fail("should have failed with NumberFormatException");
-        } catch (NumberFormatException e) {
-            assertThat(e.getMessage(), Matchers.containsString("For input string"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
deleted file mode 100644
index 8b8ebae..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilderTests.java
+++ /dev/null
@@ -1,322 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.spatial4j.core.io.GeohashUtils;
-import com.spatial4j.core.shape.Rectangle;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class GeoBoundingBoxQueryBuilderTests extends AbstractQueryTestCase<GeoBoundingBoxQueryBuilder> {
-    /** Randomly generate either NaN or one of the two infinity values. */
-    private static Double[] brokenDoubles = {Double.NaN, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY};
-    
-    @Override
-    protected GeoBoundingBoxQueryBuilder doCreateTestQueryBuilder() {
-        GeoBoundingBoxQueryBuilder builder = new GeoBoundingBoxQueryBuilder(GEO_POINT_FIELD_NAME);
-        Rectangle box = RandomShapeGenerator.xRandomRectangle(getRandom(), RandomShapeGenerator.xRandomPoint(getRandom()));
-
-        if (randomBoolean()) {
-            // check the top-left/bottom-right combination of setters
-            int path = randomIntBetween(0, 2);
-            switch (path) {
-            case 0:
-                builder.setCorners(
-                        new GeoPoint(box.getMaxY(), box.getMinX()), 
-                        new GeoPoint(box.getMinY(), box.getMaxX()));
-                break;
-            case 1:
-                builder.setCorners(
-                        GeohashUtils.encodeLatLon(box.getMaxY(), box.getMinX()),
-                        GeohashUtils.encodeLatLon(box.getMinY(), box.getMaxX()));
-                break;
-            default:
-                builder.setCorners(box.getMaxY(), box.getMinX(), box.getMinY(), box.getMaxX());
-            }
-        } else {
-            // check the bottom-left/ top-right combination of setters
-            if (randomBoolean()) {
-                builder.setCornersOGC(
-                        new GeoPoint(box.getMinY(), box.getMinX()),
-                        new GeoPoint(box.getMaxY(), box.getMaxX()));
-            } else {
-                builder.setCornersOGC(
-                        GeohashUtils.encodeLatLon(box.getMinY(), box.getMinX()),
-                        GeohashUtils.encodeLatLon(box.getMaxY(), box.getMaxX()));
-            }
-        }
-
-        if (randomBoolean()) {
-            builder.coerce(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.ignoreMalformed(randomBoolean());
-        }
-
-        builder.type(randomFrom(GeoExecType.values()));
-        return builder;
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testValidationNullFieldname() {
-        new GeoBoundingBoxQueryBuilder(null);
-    }
-
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testValidationNullType() {
-        GeoBoundingBoxQueryBuilder qb = new GeoBoundingBoxQueryBuilder("teststring");
-        qb.type((GeoExecType) null);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testValidationNullTypeString() {
-        GeoBoundingBoxQueryBuilder qb = new GeoBoundingBoxQueryBuilder("teststring");
-        qb.type((String) null);
-    }
-
-    @Test
-    @Override
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-    
-    @Test(expected = QueryShardException.class)
-    public void testExceptionOnMissingTypes() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length == 0);
-        super.testToQuery();
-    }
-
-    @Test
-    public void testBrokenCoordinateCannotBeSet() {
-        PointTester[] testers = { new TopTester(), new LeftTester(), new BottomTester(), new RightTester() };
-
-        GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        builder.coerce(false).ignoreMalformed(false);
-
-        for (PointTester tester : testers) {
-            try {
-                tester.invalidateCoordinate(builder, true);
-                fail("expected exception for broken " + tester.getClass().getName() + " coordinate");
-            } catch (IllegalArgumentException e) {
-                // exptected
-            }
-        }
-    }
-
-    @Test
-    public void testBrokenCoordinateCanBeSetWithIgnoreMalformed() {
-        PointTester[] testers = { new TopTester(), new LeftTester(), new BottomTester(), new RightTester() };
-
-        GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        builder.ignoreMalformed(true);
-
-        for (PointTester tester : testers) {
-            tester.invalidateCoordinate(builder, true);
-        }
-    }
-
-
-    @Test
-    public void testValidation() {
-        PointTester[] testers = { new TopTester(), new LeftTester(), new BottomTester(), new RightTester() };
-
-        for (PointTester tester : testers) {
-            QueryValidationException except = null;
-
-            GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-            tester.invalidateCoordinate(builder.coerce(true), false);
-            except = builder.checkLatLon(true);
-            assertNull("Inner post 2.0 validation w/ coerce should ignore invalid "
-                    + tester.getClass().getName()
-                    + " coordinate: "
-                    + tester.invalidCoordinate + " ",
-                    except);
-
-            tester.invalidateCoordinate(builder.coerce(true), false);
-            except = builder.checkLatLon(false);
-            assertNull("Inner pre 2.0 validation w/ coerce should ignore invalid coordinate: "
-                    + tester.getClass().getName()
-                    + " coordinate: "
-                    + tester.invalidCoordinate + " ",
-                    except);
-
-            tester.invalidateCoordinate(builder.coerce(false).ignoreMalformed(false), false);
-            except = builder.checkLatLon(true);
-            assertNull("Inner pre 2.0 validation w/o coerce should ignore invalid coordinate for old indexes: "
-                    + tester.getClass().getName()
-                    + " coordinate: "
-                    + tester.invalidCoordinate,
-                    except);
-
-            tester.invalidateCoordinate(builder.coerce(false).ignoreMalformed(false), false);
-            except = builder.checkLatLon(false);
-            assertNotNull("Inner post 2.0 validation w/o coerce should detect invalid coordinate: "
-                    + tester.getClass().getName()
-                    + " coordinate: "
-                    + tester.invalidCoordinate,
-                    except);
-        }
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testTopBottomCannotBeFlipped() {
-        GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        double top = builder.topLeft().getLat();
-        double left = builder.topLeft().getLon();
-        double bottom = builder.bottomRight().getLat();
-        double right = builder.bottomRight().getLon();
-
-        assumeTrue("top should not be equal to bottom for flip check", top != bottom);
-        System.out.println("top: " + top + " bottom: " + bottom);
-        builder.coerce(false).ignoreMalformed(false).setCorners(bottom, left, top, right);
-    }
-
-    @Test
-    public void testTopBottomCanBeFlippedOnIgnoreMalformed() {
-        GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        double top = builder.topLeft().getLat();
-        double left = builder.topLeft().getLon();
-        double bottom = builder.bottomRight().getLat();
-        double right = builder.bottomRight().getLon();
-
-        assumeTrue("top should not be equal to bottom for flip check", top != bottom);
-        builder.coerce(false).ignoreMalformed(true).setCorners(bottom, left, top, right);
-    }
-
-    @Test
-    public void testLeftRightCanBeFlipped() {
-        GeoBoundingBoxQueryBuilder builder = createTestQueryBuilder();
-        double top = builder.topLeft().getLat();
-        double left = builder.topLeft().getLon();
-        double bottom = builder.bottomRight().getLat();
-        double right = builder.bottomRight().getLon();
-        
-        builder.ignoreMalformed(true).setCorners(top, right, bottom, left);
-        builder.ignoreMalformed(false).setCorners(top, right, bottom, left);
-    }
-
-    @Test
-    public void testNormalization() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        GeoBoundingBoxQueryBuilder qb = createTestQueryBuilder();
-        if (getCurrentTypes().length != 0 && "mapped_geo".equals(qb.fieldName())) {
-            // only execute this test if we are running on a valid geo field
-            qb.setCorners(200, 200, qb.bottomRight().getLat(), qb.bottomRight().getLon());
-            qb.coerce(true);
-            Query query = qb.toQuery(createShardContext());
-            if (query instanceof ConstantScoreQuery) {
-                ConstantScoreQuery result = (ConstantScoreQuery) query;
-                BooleanQuery bboxFilter = (BooleanQuery) result.getQuery();
-                for (BooleanClause clause : bboxFilter.clauses()) {
-                    NumericRangeQuery boundary = (NumericRangeQuery) clause.getQuery();
-                    if (boundary.getMax() != null) {
-                        assertTrue("If defined, non of the maximum range values should be larger than 180", boundary.getMax().intValue() <= 180);
-                    }
-                }
-            } else {
-                assertTrue("memory queries should result in InMemoryGeoBoundingBoxQuery", query instanceof InMemoryGeoBoundingBoxQuery);
-            }
-        }
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(GeoBoundingBoxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.type() == GeoExecType.INDEXED) {
-            assertTrue("Found no indexed geo query.", query instanceof ConstantScoreQuery);
-        } else {
-            assertTrue("Found no indexed geo query.", query instanceof InMemoryGeoBoundingBoxQuery);
-        }
-    }
-
-    // Java really could do with function pointers - is there any Java8 feature that would help me here which I don't know of?
-    public abstract class PointTester {
-        private double brokenCoordinate = randomFrom(brokenDoubles);
-        private double invalidCoordinate;
-
-        public PointTester(double invalidCoodinate) {
-            this.invalidCoordinate = invalidCoodinate;
-        }
-        public void invalidateCoordinate(GeoBoundingBoxQueryBuilder qb, boolean useBrokenDouble) {
-            if (useBrokenDouble) {
-                fillIn(brokenCoordinate, qb);
-            } else {
-                fillIn(invalidCoordinate, qb);
-            }
-        }
-        protected abstract void fillIn(double fillIn, GeoBoundingBoxQueryBuilder qb);
-    }
-
-    public class TopTester extends PointTester {
-        public TopTester() {
-            super(randomDoubleBetween(GeoUtils.MAX_LAT, Double.MAX_VALUE, false));
-        }
-
-        @Override
-        public void fillIn(double coordinate, GeoBoundingBoxQueryBuilder qb) {
-            qb.setCorners(coordinate, qb.topLeft().getLon(), qb.bottomRight().getLat(), qb.bottomRight().getLon());
-        }
-    }
-
-    public class LeftTester extends PointTester {
-        public LeftTester() {
-            super(randomDoubleBetween(-Double.MAX_VALUE, GeoUtils.MIN_LON, true));
-        }
-
-        @Override
-        public void fillIn(double coordinate, GeoBoundingBoxQueryBuilder qb) {
-            qb.setCorners(qb.topLeft().getLat(), coordinate, qb.bottomRight().getLat(), qb.bottomRight().getLon());
-        }
-    }
-
-    public class BottomTester extends PointTester {
-        public BottomTester() {
-            super(randomDoubleBetween(-Double.MAX_VALUE, GeoUtils.MIN_LAT, false));
-        }
-
-        @Override
-        public void fillIn(double coordinate, GeoBoundingBoxQueryBuilder qb) {
-            qb.setCorners(qb.topLeft().getLat(), qb.topLeft().getLon(), coordinate, qb.bottomRight().getLon());
-        }
-    }
-
-    public class RightTester extends PointTester {
-        public RightTester() {
-            super(randomDoubleBetween(GeoUtils.MAX_LON, Double.MAX_VALUE, true));
-        } 
-
-        @Override
-        public void fillIn(double coordinate, GeoBoundingBoxQueryBuilder qb) {
-            qb.setCorners(qb.topLeft().getLat(), qb.topLeft().getLon(), qb.topLeft().getLat(), coordinate);
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java
deleted file mode 100644
index 4feded1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceQueryBuilderTests.java
+++ /dev/null
@@ -1,183 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.spatial4j.core.shape.Point;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class GeoDistanceQueryBuilderTests extends AbstractQueryTestCase<GeoDistanceQueryBuilder> {
-
-    @Override
-    protected GeoDistanceQueryBuilder doCreateTestQueryBuilder() {
-        GeoDistanceQueryBuilder qb = new GeoDistanceQueryBuilder(GEO_POINT_FIELD_NAME);
-        String distance = "" + randomDouble();
-        if (randomBoolean()) {
-            DistanceUnit unit = randomFrom(DistanceUnit.values());
-            distance = distance + unit.toString();
-        }
-        int selector = randomIntBetween(0, 2);
-        switch (selector) {
-            case 0:
-                qb.distance(randomDouble(), randomFrom(DistanceUnit.values()));
-                break;
-            case 1:
-                qb.distance(distance, randomFrom(DistanceUnit.values()));
-                break;
-            case 2:
-                qb.distance(distance);
-                break;
-        }
-
-        Point p = RandomShapeGenerator.xRandomPoint(random());
-        qb.point(new GeoPoint(p.getY(), p.getX()));
-
-        if (randomBoolean()) {
-            qb.coerce(randomBoolean());
-        }
-
-        if (randomBoolean()) {
-            qb.ignoreMalformed(randomBoolean());
-        }
-
-        if (randomBoolean()) {
-            qb.optimizeBbox(randomFrom("none", "memory", "indexed"));
-        }
-
-        if (randomBoolean()) {
-            qb.geoDistance(randomFrom(GeoDistance.values()));
-        }
-        return qb;
-    }
-
-    public void testIllegalValues() {
-        try {
-            if (randomBoolean()) {
-                new GeoDistanceQueryBuilder("");
-            } else {
-                new GeoDistanceQueryBuilder(null);
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        GeoDistanceQueryBuilder query = new GeoDistanceQueryBuilder("fieldName");
-        try {
-            if (randomBoolean()) {
-                query.distance("");
-            } else {
-                query.distance(null);
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            if (randomBoolean()) {
-                query.distance("", DistanceUnit.DEFAULT);
-            } else {
-                query.distance(null, DistanceUnit.DEFAULT);
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            query.distance("1", null);
-            fail("unit must not be null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            query.distance(1, null);
-            fail("unit must not be null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            query.geohash(null);
-            fail("geohash must not be null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            query.geoDistance(null);
-            fail("geodistance must not be null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            query.optimizeBbox(null);
-            fail("optimizeBbox must not be null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-    }
-
-    /**
-     * Overridden here to ensure the test is only run if at least one type is
-     * present in the mappings. Geo queries do not execute if the field is not
-     * explicitly mapped
-     */
-    @Override
-    @Test
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(GeoDistanceQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(GeoDistanceRangeQuery.class));
-        GeoDistanceRangeQuery geoQuery = (GeoDistanceRangeQuery) query;
-        assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
-        if (queryBuilder.point() != null) {
-            assertThat(geoQuery.lat(), equalTo(queryBuilder.point().lat()));
-            assertThat(geoQuery.lon(), equalTo(queryBuilder.point().lon()));
-        }
-        assertThat(geoQuery.geoDistance(), equalTo(queryBuilder.geoDistance()));
-        assertThat(geoQuery.minInclusiveDistance(), equalTo(Double.NEGATIVE_INFINITY));
-        double distance = queryBuilder.distance();
-        if (queryBuilder.geoDistance() != null) {
-                distance = queryBuilder.geoDistance().normalize(distance, DistanceUnit.DEFAULT);
-        }
-        assertThat(geoQuery.maxInclusiveDistance(), closeTo(distance, Math.abs(distance) / 1000));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
deleted file mode 100644
index fa827a0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
+++ /dev/null
@@ -1,211 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class GeoDistanceRangeQueryTests extends AbstractQueryTestCase<GeoDistanceRangeQueryBuilder> {
-
-    @Override
-    protected GeoDistanceRangeQueryBuilder doCreateTestQueryBuilder() {
-        GeoDistanceRangeQueryBuilder builder;
-        if (randomBoolean()) {
-            builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, randomGeohash(1, 12));
-        } else {
-            double lat = randomDouble() * 180 - 90;
-            double lon = randomDouble() * 360 - 180;
-            if (randomBoolean()) {
-                builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint(lat, lon));
-            } else {
-                builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, lat, lon);
-            }
-        }
-        int fromValue = randomInt(1000000);
-        int toValue = randomIntBetween(fromValue, 1000000);
-        String fromToUnits = randomFrom(DistanceUnit.values()).toString();
-        if (randomBoolean()) {
-            int branch = randomInt(2);
-            switch (branch) {
-            case 0:
-                builder.from(fromValue);
-                break;
-            case 1:
-                builder.to(toValue);
-                break;
-            case 2:
-                builder.from(fromValue);
-                builder.to(toValue);
-                break;
-            }
-        } else {
-            int branch = randomInt(2);
-            switch (branch) {
-            case 0:
-                builder.from(fromValue + fromToUnits);
-                break;
-            case 1:
-                builder.to(toValue + fromToUnits);
-                break;
-            case 2:
-                builder.from(fromValue + fromToUnits);
-                builder.to(toValue + fromToUnits);
-                break;
-            }
-        }
-        if (randomBoolean()) {
-            builder.includeLower(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.includeUpper(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.geoDistance(randomFrom(GeoDistance.values()));
-        }
-        if (randomBoolean()) {
-            builder.unit(randomFrom(DistanceUnit.values()));
-        }
-        if (randomBoolean()) {
-            builder.optimizeBbox(randomFrom("none", "memory", "indexed"));
-        }
-        if (randomBoolean()) {
-            builder.coerce(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.ignoreMalformed(randomBoolean());
-        }
-        return builder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(GeoDistanceRangeQueryBuilder queryBuilder, Query query, QueryShardContext context)
-            throws IOException {
-        assertThat(query, instanceOf(GeoDistanceRangeQuery.class));
-        GeoDistanceRangeQuery geoQuery = (GeoDistanceRangeQuery) query;
-        assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
-        if (queryBuilder.point() != null) {
-            assertThat(geoQuery.lat(), equalTo(queryBuilder.point().lat()));
-            assertThat(geoQuery.lon(), equalTo(queryBuilder.point().lon()));
-        }
-        assertThat(geoQuery.geoDistance(), equalTo(queryBuilder.geoDistance()));
-        if (queryBuilder.from() != null && queryBuilder.from() instanceof Number) {
-            double fromValue = ((Number) queryBuilder.from()).doubleValue();
-            if (queryBuilder.unit() != null) {
-                fromValue = queryBuilder.unit().toMeters(fromValue);
-            }
-            if (queryBuilder.geoDistance() != null) {
-                fromValue = queryBuilder.geoDistance().normalize(fromValue, DistanceUnit.DEFAULT);
-            }
-            assertThat(geoQuery.minInclusiveDistance(), closeTo(fromValue, Math.abs(fromValue) / 1000));
-        }
-        if (queryBuilder.to() != null && queryBuilder.to() instanceof Number) {
-            double toValue = ((Number) queryBuilder.to()).doubleValue();
-            if (queryBuilder.unit() != null) {
-                toValue = queryBuilder.unit().toMeters(toValue);
-            }
-            if (queryBuilder.geoDistance() != null) {
-                toValue = queryBuilder.geoDistance().normalize(toValue, DistanceUnit.DEFAULT);
-            }
-            assertThat(geoQuery.maxInclusiveDistance(), closeTo(toValue, Math.abs(toValue) / 1000));
-        }
-    }
-
-    /**
-     * Overridden here to ensure the test is only run if at least one type is
-     * present in the mappings. Geo queries do not execute if the field is not
-     * explicitly mapped
-     */
-    @Override
-    @Test
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testNullFieldName() {
-        if (randomBoolean()) {
-            new GeoDistanceRangeQueryBuilder(null, new GeoPoint());
-        } else {
-            new GeoDistanceRangeQueryBuilder("", new GeoPoint());
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testNoPoint() {
-        if (randomBoolean()) {
-            new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, (GeoPoint) null);
-        } else {
-            new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, (String) null);
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidFrom() {
-        GeoDistanceRangeQueryBuilder builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        if (randomBoolean()) {
-            builder.from((String) null);
-        } else {
-            builder.from((Number) null);
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidTo() {
-        GeoDistanceRangeQueryBuilder builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        if (randomBoolean()) {
-            builder.to((String) null);
-        } else {
-            builder.to((Number) null);
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidOptimizeBBox() {
-        GeoDistanceRangeQueryBuilder builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        if (randomBoolean()) {
-            builder.optimizeBbox(null);
-        } else {
-            builder.optimizeBbox("foo");
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidGeoDistance() {
-        GeoDistanceRangeQueryBuilder builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        builder.geoDistance(null);
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidDistanceUnit() {
-        GeoDistanceRangeQueryBuilder builder = new GeoDistanceRangeQueryBuilder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        builder.unit(null);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java
deleted file mode 100644
index 096d309..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/GeoPolygonQueryBuilderTests.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.spatial4j.core.shape.jts.JtsGeometry;
-import com.vividsolutions.jts.geom.Coordinate;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.geo.GeoUtils;
-import org.elasticsearch.common.geo.builders.ShapeBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.search.geo.GeoPolygonQuery;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-import org.elasticsearch.test.geo.RandomShapeGenerator.ShapeType;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class GeoPolygonQueryBuilderTests extends AbstractQueryTestCase<GeoPolygonQueryBuilder> {
-
-    @Override
-    protected GeoPolygonQueryBuilder doCreateTestQueryBuilder() {
-        List<GeoPoint> polygon = randomPolygon(randomIntBetween(4, 50));
-        GeoPolygonQueryBuilder builder = new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, polygon);
-        builder.coerce(randomBoolean());
-        builder.ignoreMalformed(randomBoolean());
-        return builder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(GeoPolygonQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(GeoPolygonQuery.class));
-        GeoPolygonQuery geoQuery = (GeoPolygonQuery) query;
-        assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
-        List<GeoPoint> queryBuilderPoints = queryBuilder.points();
-        GeoPoint[] queryPoints = geoQuery.points();
-        assertThat(queryPoints.length, equalTo(queryBuilderPoints.size()));
-        if (queryBuilder.coerce()) {
-            for (int i = 0; i < queryBuilderPoints.size(); i++) {
-                GeoPoint queryBuilderPoint = queryBuilderPoints.get(i);
-                GeoUtils.normalizePoint(queryBuilderPoint, true, true);
-                assertThat(queryPoints[i], equalTo(queryBuilderPoint));
-            }
-        } else {
-            for (int i = 0; i < queryBuilderPoints.size(); i++) {
-                assertThat(queryPoints[i], equalTo(queryBuilderPoints.get(i)));
-            }
-        }
-
-    }
-
-    /**
-     * Overridden here to ensure the test is only run if at least one type is
-     * present in the mappings. Geo queries do not execute if the field is not
-     * explicitly mapped
-     */
-    @Override
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-
-    public List<GeoPoint> randomPolygon(int numPoints) {
-        ShapeBuilder shapeBuilder = null;
-        // This is a temporary fix because sometimes the RandomShapeGenerator
-        // returns null. This is if there is an error generating the polygon. So
-        // in this case keep trying until we successfully generate one
-        while (shapeBuilder == null) {
-            shapeBuilder = RandomShapeGenerator.createShapeWithin(getRandom(), null, ShapeType.POLYGON);
-        }
-        JtsGeometry shape = (JtsGeometry) shapeBuilder.build();
-        Coordinate[] coordinates = shape.getGeom().getCoordinates();
-        ArrayList<GeoPoint> polygonPoints = new ArrayList<>();
-        for (Coordinate coord : coordinates) {
-            polygonPoints.add(new GeoPoint(coord.y, coord.x));
-        }
-        return polygonPoints;
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testNullFieldName() {
-        new GeoPolygonQueryBuilder(null, randomPolygon(5));
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testEmptyPolygon() {
-        if (randomBoolean()) {
-            new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, new ArrayList<GeoPoint>());
-        } else {
-            new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, null);
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidClosedPolygon() {
-        List<GeoPoint> points = new ArrayList<>();
-        points.add(new GeoPoint(0, 90));
-        points.add(new GeoPoint(90, 90));
-        points.add(new GeoPoint(0, 90));
-        new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, points);
-
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidOpenPolygon() {
-        List<GeoPoint> points = new ArrayList<>();
-        points.add(new GeoPoint(0, 90));
-        points.add(new GeoPoint(90, 90));
-        new GeoPolygonQueryBuilder(GEO_POINT_FIELD_NAME, points);
-    }
-
-    public void testDeprecatedXContent() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-        builder.startObject();
-        builder.startObject("geo_polygon");
-        builder.startObject(GEO_POINT_FIELD_NAME);
-        builder.startArray("points");
-        builder.value("0,0");
-        builder.value("0,90");
-        builder.value("90,90");
-        builder.value("90,0");
-        builder.endArray();
-        builder.endObject();
-        builder.field("normalize", true); // deprecated
-        builder.endObject();
-        builder.endObject();
-        try {
-            parseQuery(builder.string());
-            fail("normalize is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [normalize] used, expected [coerce] instead", ex.getMessage());
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
index 7f3763c..7a4d1d9 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
@@ -19,185 +19,13 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.geo.ShapeRelation;
-import org.elasticsearch.common.geo.SpatialStrategy;
 import org.elasticsearch.common.geo.builders.EnvelopeBuilder;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-import org.junit.After;
+import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQueryBuilder> {
-
-    private static String indexedShapeId;
-    private static String indexedShapeType;
-    private static String indexedShapePath;
-    private static String indexedShapeIndex;
-    private static ShapeBuilder indexedShapeToReturn;
-
-    @Override
-    protected GeoShapeQueryBuilder doCreateTestQueryBuilder() {
-        ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
-        GeoShapeQueryBuilder builder;
-        if (randomBoolean()) {
-            try {
-                builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        } else {
-            indexedShapeToReturn = shape;
-            indexedShapeId = randomAsciiOfLengthBetween(3, 20);
-            indexedShapeType = randomAsciiOfLengthBetween(3, 20);
-            builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, indexedShapeId, indexedShapeType);
-            if (randomBoolean()) {
-                indexedShapeIndex = randomAsciiOfLengthBetween(3, 20);
-                builder.indexedShapeIndex(indexedShapeIndex);
-            }
-            if (randomBoolean()) {
-                indexedShapePath = randomAsciiOfLengthBetween(3, 20);
-                builder.indexedShapePath(indexedShapePath);
-            }
-        }
-        SpatialStrategy strategy = randomFrom(SpatialStrategy.values());
-        builder.strategy(strategy);
-        if (strategy != SpatialStrategy.TERM) {
-            builder.relation(randomFrom(ShapeRelation.values()));
-        }
-        return builder;
-    }
-
-    @Override
-    protected GetResponse executeGet(GetRequest getRequest) {
-        assertThat(indexedShapeToReturn, notNullValue());
-        assertThat(indexedShapeId, notNullValue());
-        assertThat(indexedShapeType, notNullValue());
-        assertThat(getRequest.id(), equalTo(indexedShapeId));
-        assertThat(getRequest.type(), equalTo(indexedShapeType));
-        String expectedShapeIndex = indexedShapeIndex == null ? GeoShapeQueryBuilder.DEFAULT_SHAPE_INDEX_NAME : indexedShapeIndex;
-        assertThat(getRequest.index(), equalTo(expectedShapeIndex));
-        String expectedShapePath = indexedShapePath == null ? GeoShapeQueryBuilder.DEFAULT_SHAPE_FIELD_NAME : indexedShapePath;
-        String json;
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-            builder.startObject();
-            builder.field(expectedShapePath, indexedShapeToReturn);
-            builder.endObject();
-            json = builder.string();
-        } catch (IOException ex) {
-            throw new ElasticsearchException("boom", ex);
-        }
-        GetResponse response = new GetResponse(new GetResult(indexedShapeIndex, indexedShapeType, indexedShapeId, 0, true, new BytesArray(
-                json), null));
-        return response;
-    }
-
-    @After
-    public void clearShapeFields() {
-        indexedShapeToReturn = null;
-        indexedShapeId = null;
-        indexedShapeType = null;
-        indexedShapePath = null;
-        indexedShapeIndex = null;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(GeoShapeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        // Logic for doToQuery is complex and is hard to test here. Need to rely
-        // on Integration tests to determine if created query is correct
-        // TODO improve GeoShapeQueryBuilder.doToQuery() method to make it
-        // easier to test here
-        assertThat(query, anyOf(instanceOf(BooleanQuery.class), instanceOf(ConstantScoreQuery.class)));
-    }
-
-    /**
-     * Overridden here to ensure the test is only run if at least one type is
-     * present in the mappings. Geo queries do not execute if the field is not
-     * explicitly mapped
-     */
-    @Override
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testNoFieldName() throws Exception {
-        ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
-        new GeoShapeQueryBuilder(null, shape);
-    }
-
-    @Test
-    public void testNoShape() throws IOException {
-        try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, (ShapeBuilder) null);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testNoIndexedShape() throws IOException {
-        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, (String) null, "type");
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testNoIndexedShapeType() throws IOException {
-        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, "id", (String) null);
-    }
-
-    @Test
-    public void testNoRelation() throws IOException {
-        ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
-        try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
-            builder.relation(null);
-            fail("relation cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testInvalidRelation() {
-        ShapeBuilder shape = RandomShapeGenerator.createShapeWithin(getRandom(), null);
-        try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, shape);
-            builder.strategy(SpatialStrategy.TERM);
-            ShapeRelation relation = randomFrom(ShapeRelation.DISJOINT, ShapeRelation.WITHIN);
-            builder.relation(relation);
-            QueryValidationException exception = builder.validate();
-            assertThat(exception, notNullValue());
-            assertThat(exception.validationErrors(), notNullValue());
-            assertThat(exception.validationErrors().size(), equalTo(1));
-            assertThat(
-                    exception.validationErrors().get(0),
-                    equalTo("[" + GeoShapeQueryBuilder.NAME + "] strategy [" + SpatialStrategy.TERM.getStrategyName()
-                            + "] only supports relation [" + ShapeRelation.INTERSECTS.getRelationName() + "] found relation ["
-                            + relation.getRelationName() + "]"));
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
+public class GeoShapeQueryBuilderTests extends ESTestCase {
 
     @Test // see #3878
     public void testThatXContentSerializationInsideOfArrayWorks() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
deleted file mode 100644
index 0db757f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.query.GeohashCellQuery.Builder;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class GeohashCellQueryBuilderTests extends AbstractQueryTestCase<Builder> {
-
-    @Override
-    protected Builder doCreateTestQueryBuilder() {
-        GeohashCellQuery.Builder builder = new Builder(GEO_POINT_FIELD_NAME, randomGeohash(1, 12));
-        if (randomBoolean()) {
-            builder.neighbors(randomBoolean());
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                builder.precision(randomIntBetween(1, 12));
-            } else {
-                builder.precision(randomIntBetween(1, 1000000) + randomFrom(DistanceUnit.values()).toString());
-            }
-        }
-        return builder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(Builder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.neighbors()) {
-            assertThat(query, instanceOf(TermsQuery.class));
-        } else {
-            assertThat(query, instanceOf(TermQuery.class));
-            TermQuery termQuery = (TermQuery) query;
-            Term term = termQuery.getTerm();
-            assertThat(term.field(), equalTo(queryBuilder.fieldName() + GeoPointFieldMapper.Names.GEOHASH_SUFFIX));
-            String geohash = queryBuilder.geohash();
-            if (queryBuilder.precision() != null) {
-                int len = Math.min(queryBuilder.precision(), geohash.length());
-                geohash = geohash.substring(0, len);
-            }
-            assertThat(term.text(), equalTo(geohash));
-        }
-    }
-
-    /**
-     * Overridden here to ensure the test is only run if at least one type is
-     * present in the mappings. Geo queries do not execute if the field is not
-     * explicitly mapped
-     */
-    @Override
-    public void testToQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        super.testToQuery();
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testNullField() {
-        if (randomBoolean()) {
-            new Builder(null, new GeoPoint());
-        } else {
-            new Builder("", new GeoPoint());
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testNullGeoPoint() {
-        if (randomBoolean()) {
-            new Builder(GEO_POINT_FIELD_NAME, (GeoPoint) null);
-        } else {
-            new Builder(GEO_POINT_FIELD_NAME, "");
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testInvalidPrecision() {
-        GeohashCellQuery.Builder builder = new Builder(GEO_POINT_FIELD_NAME, new GeoPoint());
-        builder.precision(-1);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
deleted file mode 100644
index a307cf1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java
+++ /dev/null
@@ -1,202 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.test.TestSearchContext;
-
-import java.io.IOException;
-
-import static org.elasticsearch.test.StreamsUtils.copyToStringFromClasspath;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class HasChildQueryBuilderTests extends AbstractQueryTestCase<HasChildQueryBuilder> {
-    protected static final String PARENT_TYPE = "parent";
-    protected static final String CHILD_TYPE = "child";
-
-    public void setUp() throws Exception {
-        super.setUp();
-        MapperService mapperService = queryParserService().mapperService;
-        mapperService.merge(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
-                STRING_FIELD_NAME, "type=string",
-                INT_FIELD_NAME, "type=integer",
-                DOUBLE_FIELD_NAME, "type=double",
-                BOOLEAN_FIELD_NAME, "type=boolean",
-                DATE_FIELD_NAME, "type=date",
-                OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-        mapperService.merge(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
-                "_parent", "type=" + PARENT_TYPE,
-                STRING_FIELD_NAME, "type=string",
-                INT_FIELD_NAME, "type=integer",
-                DOUBLE_FIELD_NAME, "type=double",
-                BOOLEAN_FIELD_NAME, "type=boolean",
-                DATE_FIELD_NAME, "type=date",
-                OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-    }
-
-    protected void setSearchContext(String[] types) {
-        final MapperService mapperService = queryParserService().mapperService;
-        final IndexFieldDataService fieldData = queryParserService().fieldDataService;
-        TestSearchContext testSearchContext = new TestSearchContext() {
-            private InnerHitsContext context;
-
-
-            @Override
-            public void innerHits(InnerHitsContext innerHitsContext) {
-                context = innerHitsContext;
-            }
-
-            @Override
-            public InnerHitsContext innerHits() {
-                return context;
-            }
-
-            @Override
-            public MapperService mapperService() {
-                return mapperService; // need to build / parse inner hits sort fields
-            }
-
-            @Override
-            public IndexFieldDataService fieldData() {
-                return fieldData; // need to build / parse inner hits sort fields
-            }
-        };
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    /**
-     * @return a {@link HasChildQueryBuilder} with random values all over the place
-     */
-    @Override
-    protected HasChildQueryBuilder doCreateTestQueryBuilder() {
-        int min = randomIntBetween(0, Integer.MAX_VALUE / 2);
-        int max = randomIntBetween(min, Integer.MAX_VALUE);
-        InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit().setSize(100).addSort(STRING_FIELD_NAME, SortOrder.ASC);
-        return new HasChildQueryBuilder(CHILD_TYPE,
-                RandomQueryBuilder.createQuery(random()), max, min,
-                RandomPicks.randomFrom(random(), ScoreMode.values()),
-                randomBoolean()  ? null : new QueryInnerHits("inner_hits_name", innerHit));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(HasChildQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        QueryBuilder innerQueryBuilder = queryBuilder.query();
-        if (innerQueryBuilder instanceof EmptyQueryBuilder) {
-            assertNull(query);
-        } else {
-            assertThat(query, instanceOf(HasChildQueryBuilder.LateParsingQuery.class));
-            HasChildQueryBuilder.LateParsingQuery lpq = (HasChildQueryBuilder.LateParsingQuery) query;
-            assertEquals(queryBuilder.minChildren(), lpq.getMinChildren());
-            assertEquals(queryBuilder.maxChildren(), lpq.getMaxChildren());
-            assertEquals(queryBuilder.scoreMode(), lpq.getScoreMode()); // WTF is this why do we have two?
-        }
-        if (queryBuilder.innerHit() != null) {
-            assertNotNull(SearchContext.current());
-            if (query != null) {
-                assertNotNull(SearchContext.current().innerHits());
-                assertEquals(1, SearchContext.current().innerHits().getInnerHits().size());
-                assertTrue(SearchContext.current().innerHits().getInnerHits().containsKey("inner_hits_name"));
-                InnerHitsContext.BaseInnerHits innerHits = SearchContext.current().innerHits().getInnerHits().get("inner_hits_name");
-                assertEquals(innerHits.size(), 100);
-                assertEquals(innerHits.sort().getSort().length, 1);
-                assertEquals(innerHits.sort().getSort()[0].getField(), STRING_FIELD_NAME);
-            } else {
-                assertNull(SearchContext.current().innerHits());
-            }
-        }
-    }
-
-    public void testIllegalValues() {
-        QueryBuilder query = RandomQueryBuilder.createQuery(random());
-        try {
-            new HasChildQueryBuilder(null, query);
-            fail("must not be null");
-        } catch (IllegalArgumentException ex) {
-
-        }
-
-        try {
-            new HasChildQueryBuilder("foo", null);
-            fail("must not be null");
-        } catch (IllegalArgumentException ex) {
-
-        }
-        HasChildQueryBuilder foo = new HasChildQueryBuilder("foo", query);// all good
-        try {
-            foo.scoreMode(null);
-            fail("must not be null");
-        } catch (IllegalArgumentException ex) {
-
-        }
-        final int positiveValue = randomIntBetween(0, Integer.MAX_VALUE);
-        try {
-            foo.minChildren(randomIntBetween(Integer.MIN_VALUE, -1));
-            fail("must not be negative");
-        } catch (IllegalArgumentException ex) {
-
-        }
-        foo.minChildren(positiveValue);
-        assertEquals(positiveValue, foo.minChildren());
-        try {
-            foo.maxChildren(randomIntBetween(Integer.MIN_VALUE, -1));
-            fail("must not be negative");
-        } catch (IllegalArgumentException ex) {
-
-        }
-
-        foo.maxChildren(positiveValue);
-        assertEquals(positiveValue, foo.maxChildren());
-    }
-
-    public void testParseFromJSON() throws IOException {
-        String query = copyToStringFromClasspath("/org/elasticsearch/index/query/has-child-with-inner-hits.json").trim();
-        HasChildQueryBuilder queryBuilder = (HasChildQueryBuilder) parseQuery(query);
-        assertEquals(query, queryBuilder.maxChildren(), 1217235442);
-        assertEquals(query, queryBuilder.minChildren(), 883170873);
-        assertEquals(query, queryBuilder.boost(), 2.0f, 0.0f);
-        assertEquals(query, queryBuilder.queryName(), "WNzYMJKRwePuRBh");
-        assertEquals(query, queryBuilder.childType(), "child");
-        assertEquals(query, queryBuilder.scoreMode(), ScoreMode.Avg);
-        assertNotNull(query, queryBuilder.innerHit());
-        assertEquals(query, queryBuilder.innerHit(), new QueryInnerHits("inner_hits_name", new InnerHitsBuilder.InnerHit().setSize(100).addSort("mapped_string", SortOrder.ASC)));
-        // now assert that we actually generate the same JSON
-        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-        queryBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        assertEquals(query, builder.string());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
deleted file mode 100644
index 9366c08..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/HasParentQueryBuilderTests.java
+++ /dev/null
@@ -1,194 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.test.TestSearchContext;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class HasParentQueryBuilderTests extends AbstractQueryTestCase<HasParentQueryBuilder> {
-    protected static final String PARENT_TYPE = "parent";
-    protected static final String CHILD_TYPE = "child";
-
-    public void setUp() throws Exception {
-        super.setUp();
-        MapperService mapperService = queryParserService().mapperService;
-        mapperService.merge(PARENT_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(PARENT_TYPE,
-                STRING_FIELD_NAME, "type=string",
-                INT_FIELD_NAME, "type=integer",
-                DOUBLE_FIELD_NAME, "type=double",
-                BOOLEAN_FIELD_NAME, "type=boolean",
-                DATE_FIELD_NAME, "type=date",
-                OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-        mapperService.merge(CHILD_TYPE, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(CHILD_TYPE,
-                "_parent", "type=" + PARENT_TYPE,
-                STRING_FIELD_NAME, "type=string",
-                INT_FIELD_NAME, "type=integer",
-                DOUBLE_FIELD_NAME, "type=double",
-                BOOLEAN_FIELD_NAME, "type=boolean",
-                DATE_FIELD_NAME, "type=date",
-                OBJECT_FIELD_NAME, "type=object"
-        ).string()), false, false);
-    }
-
-    protected void setSearchContext(String[] types) {
-        final MapperService mapperService = queryParserService().mapperService;
-        final IndexFieldDataService fieldData = queryParserService().fieldDataService;
-        TestSearchContext testSearchContext = new TestSearchContext() {
-            private InnerHitsContext context;
-
-
-            @Override
-            public void innerHits(InnerHitsContext innerHitsContext) {
-                context = innerHitsContext;
-            }
-
-            @Override
-            public InnerHitsContext innerHits() {
-                return context;
-            }
-
-            @Override
-            public MapperService mapperService() {
-                return mapperService; // need to build / parse inner hits sort fields
-            }
-
-            @Override
-            public IndexFieldDataService fieldData() {
-                return fieldData; // need to build / parse inner hits sort fields
-            }
-        };
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    /**
-     * @return a {@link HasChildQueryBuilder} with random values all over the place
-     */
-    @Override
-    protected HasParentQueryBuilder doCreateTestQueryBuilder() {
-        InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit().setSize(100).addSort(STRING_FIELD_NAME, SortOrder.ASC);
-        return new HasParentQueryBuilder(PARENT_TYPE,
-                RandomQueryBuilder.createQuery(random()),randomBoolean(),
-                randomBoolean() ? null : new QueryInnerHits("inner_hits_name", innerHit));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(HasParentQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        QueryBuilder innerQueryBuilder = queryBuilder.query();
-        if (innerQueryBuilder instanceof EmptyQueryBuilder) {
-            assertNull(query);
-        } else {
-            assertThat(query, instanceOf(HasChildQueryBuilder.LateParsingQuery.class));
-            HasChildQueryBuilder.LateParsingQuery lpq = (HasChildQueryBuilder.LateParsingQuery) query;
-            assertEquals(queryBuilder.score() ? ScoreMode.Max : ScoreMode.None, lpq.getScoreMode());
-        }
-        if (queryBuilder.innerHit() != null) {
-            assertNotNull(SearchContext.current());
-            if (query != null) {
-                assertNotNull(SearchContext.current().innerHits());
-                assertEquals(1, SearchContext.current().innerHits().getInnerHits().size());
-                assertTrue(SearchContext.current().innerHits().getInnerHits().containsKey("inner_hits_name"));
-                InnerHitsContext.BaseInnerHits innerHits = SearchContext.current().innerHits().getInnerHits().get("inner_hits_name");
-                assertEquals(innerHits.size(), 100);
-                assertEquals(innerHits.sort().getSort().length, 1);
-                assertEquals(innerHits.sort().getSort()[0].getField(), STRING_FIELD_NAME);
-            } else {
-                assertNull(SearchContext.current().innerHits());
-            }
-        }
-    }
-
-    public void testIllegalValues() {
-        QueryBuilder query = RandomQueryBuilder.createQuery(random());
-        try {
-            new HasParentQueryBuilder(null, query);
-            fail("must not be null");
-        } catch (IllegalArgumentException ex) {
-
-        }
-
-        try {
-            new HasParentQueryBuilder("foo", null);
-            fail("must not be null");
-        } catch (IllegalArgumentException ex) {
-
-        }
-    }
-
-    public void testDeprecatedXContent() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-        builder.startObject();
-        builder.startObject("has_parent");
-        builder.field("query");
-        EmptyQueryBuilder.PROTOTYPE.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        builder.field("type", "foo"); // deprecated
-        builder.endObject();
-        builder.endObject();
-        try {
-            parseQuery(builder.string());
-            fail("type is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [type] used, expected [parent_type] instead", ex.getMessage());
-        }
-
-        HasParentQueryBuilder queryBuilder = (HasParentQueryBuilder) parseQuery(builder.string(), ParseFieldMatcher.EMPTY);
-        assertEquals("foo", queryBuilder.type());
-
-        boolean score = randomBoolean();
-        String key = RandomPicks.randomFrom(random(), Arrays.asList("score_mode", "scoreMode"));
-        builder = XContentFactory.jsonBuilder().prettyPrint();
-        builder.startObject();
-        builder.startObject("has_parent");
-        builder.field("query");
-        EmptyQueryBuilder.PROTOTYPE.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        builder.field(key, score ? "score": "none");
-        builder.field("parent_type", "foo");
-        builder.endObject();
-        builder.endObject();
-        try {
-            parseQuery(builder.string());
-            fail(key + " is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [" + key + "] used, replaced by [score]", ex.getMessage());
-        }
-
-        queryBuilder = (HasParentQueryBuilder) parseQuery(builder.string(), ParseFieldMatcher.EMPTY);
-        assertEquals(score, queryBuilder.score());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
deleted file mode 100644
index 4b14824..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParsingException;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IdsQueryBuilderTests extends AbstractQueryTestCase<IdsQueryBuilder> {
-
-    /**
-     * check that parser throws exception on missing values field
-     * @throws IOException
-     */
-    @Test(expected=ParsingException.class)
-    public void testIdsNotProvided() throws IOException {
-        String noIdsFieldQuery = "{\"ids\" : { \"type\" : \"my_type\"  }";
-        parseQuery(noIdsFieldQuery);
-    }
-
-    @Override
-    protected IdsQueryBuilder doCreateTestQueryBuilder() {
-        String[] types;
-        if (getCurrentTypes().length > 0 && randomBoolean()) {
-            int numberOfTypes = randomIntBetween(1, getCurrentTypes().length);
-            types = new String[numberOfTypes];
-            for (int i = 0; i < numberOfTypes; i++) {
-                if (frequently()) {
-                    types[i] = randomFrom(getCurrentTypes());
-                } else {
-                    types[i] = randomAsciiOfLengthBetween(1, 10);
-                }
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[]{MetaData.ALL};
-            } else {
-                types = new String[0];
-            }
-        }
-        int numberOfIds = randomIntBetween(0, 10);
-        String[] ids = new String[numberOfIds];
-        for (int i = 0; i < numberOfIds; i++) {
-            ids[i] = randomAsciiOfLengthBetween(1, 10);
-        }
-        IdsQueryBuilder query;
-        if (types.length > 0 || randomBoolean()) {
-            query = new IdsQueryBuilder(types);
-            query.addIds(ids);
-        } else {
-            query = new IdsQueryBuilder();
-            query.addIds(ids);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IdsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.ids().size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            assertThat(((BooleanQuery)query).clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(TermsQuery.class));
-        }
-    }
-
-    @Override
-    protected Map<String, IdsQueryBuilder> getAlternateVersions() {
-        Map<String, IdsQueryBuilder> alternateVersions = new HashMap<>();
-
-        IdsQueryBuilder tempQuery = createTestQueryBuilder();
-        if (tempQuery.types() != null && tempQuery.types().length > 0) {
-            String type = tempQuery.types()[0];
-            IdsQueryBuilder testQuery = new IdsQueryBuilder(type);
-
-            //single value type can also be called _type
-            String contentString1 = "{\n" +
-                        "    \"ids\" : {\n" +
-                        "        \"_type\" : \"" + type + "\",\n" +
-                        "        \"values\" : []\n" +
-                        "    }\n" +
-                        "}";
-            alternateVersions.put(contentString1, testQuery);
-
-            //array of types can also be called type rather than types
-            String contentString2 = "{\n" +
-                        "    \"ids\" : {\n" +
-                        "        \"type\" : [\"" + type + "\"],\n" +
-                        "        \"values\" : []\n" +
-                        "    }\n" +
-                        "}";
-            alternateVersions.put(contentString2, testQuery);
-        }
-
-        return alternateVersions;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java
index 89b74db..8b4c405 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java
@@ -83,7 +83,7 @@ public class IndexQueryParserFilterDateRangeFormatTests extends ESSingleNodeTest
             SearchContext.setCurrent(new TestSearchContext());
             // We need to rewrite, because range on date field initially returns LateParsingQuery
             queryParser.parse(query).query().rewrite(null);
-            fail("A Range Filter with a specific format but with an unexpected date should raise a ParsingException");
+            fail("A Range Filter with a specific format but with an unexpected date should raise a QueryParsingException");
         } catch (ElasticsearchParseException e) {
             // We expect it
         } finally {
@@ -119,7 +119,7 @@ public class IndexQueryParserFilterDateRangeFormatTests extends ESSingleNodeTest
         try {
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query().rewrite(null);
-            fail("A Range Query with a specific format but with an unexpected date should raise a ParsingException");
+            fail("A Range Query with a specific format but with an unexpected date should raise a QueryParsingException");
         } catch (ElasticsearchParseException e) {
             // We expect it
         } finally {
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
index 2175225..8bf70de 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.query;
 
 import org.apache.lucene.search.NumericRangeQuery;
 import org.apache.lucene.search.Query;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Injector;
@@ -82,8 +83,8 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
         try {
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
-            fail("A Range Filter on a numeric field with a TimeZone should raise a ParsingException");
-        } catch (QueryShardException e) {
+            fail("A Range Filter on a numeric field with a TimeZone should raise a QueryParsingException");
+        } catch (ParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
@@ -119,8 +120,8 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
         try {
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
-            fail("A Range Query on a numeric field with a TimeZone should raise a ParsingException");
-        } catch (QueryShardException e) {
+            fail("A Range Query on a numeric field with a TimeZone should raise a QueryParsingException");
+        } catch (ParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTests.java
deleted file mode 100644
index 8db4317..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IndicesQueryBuilderTests.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class IndicesQueryBuilderTests extends AbstractQueryTestCase<IndicesQueryBuilder> {
-
-    @Override
-    protected IndicesQueryBuilder doCreateTestQueryBuilder() {
-        String[] indices;
-        if (randomBoolean()) {
-            indices = new String[]{getIndex().getName()};
-        } else {
-            indices = generateRandomStringArray(5, 10, false, false);
-        }
-        IndicesQueryBuilder query = new IndicesQueryBuilder(RandomQueryBuilder.createQuery(random()), indices);
-
-        switch (randomInt(2)) {
-            case 0:
-                query.noMatchQuery(RandomQueryBuilder.createQuery(random()));
-                break;
-            case 1:
-                query.noMatchQuery(randomFrom(QueryBuilders.matchAllQuery(), new MatchNoneQueryBuilder()));
-                break;
-            default:
-                // do not set noMatchQuery
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IndicesQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query expected;
-        if (queryBuilder.indices().length == 1 && getIndex().getName().equals(queryBuilder.indices()[0])) {
-            expected = queryBuilder.innerQuery().toQuery(context);
-        } else {
-            expected = queryBuilder.noMatchQuery().toQuery(context);
-        }
-        if (expected != null && queryBuilder.boost() != AbstractQueryBuilder.DEFAULT_BOOST) {
-            expected.setBoost(queryBuilder.boost());
-        }
-        assertEquals(query, expected);
-    }
-
-    @Override
-    protected void assertBoost(IndicesQueryBuilder queryBuilder, Query query) throws IOException {
-        //nothing to do here, boost check is already included in equality check done as part of doAssertLuceneQuery above
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new IndicesQueryBuilder(null, "index");
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new IndicesQueryBuilder(EmptyQueryBuilder.PROTOTYPE, (String[]) null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new IndicesQueryBuilder(EmptyQueryBuilder.PROTOTYPE, new String[0]);
-            fail("cannot be empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        IndicesQueryBuilder indicesQueryBuilder = new IndicesQueryBuilder(EmptyQueryBuilder.PROTOTYPE, "index");
-        try {
-            indicesQueryBuilder.noMatchQuery((QueryBuilder) null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            indicesQueryBuilder.noMatchQuery((String) null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTests.java
deleted file mode 100644
index 1603855..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTests.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class MatchAllQueryBuilderTests extends AbstractQueryTestCase<MatchAllQueryBuilder> {
-
-    @Override
-    protected MatchAllQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MatchAllQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTests.java
deleted file mode 100644
index cb80f31..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchNoneQueryBuilderTests.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class MatchNoneQueryBuilderTests extends AbstractQueryTestCase {
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    @Override
-    protected AbstractQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchNoneQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(AbstractQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery booleanQuery = (BooleanQuery) query;
-        assertThat(booleanQuery.clauses().size(), equalTo(0));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
deleted file mode 100644
index ef3477d..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MatchQuery.ZeroTermsQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Locale;
-
-import static org.hamcrest.CoreMatchers.either;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class MatchQueryBuilderTests extends AbstractQueryTestCase<MatchQueryBuilder> {
-
-    @Override
-    protected MatchQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomFrom(STRING_FIELD_NAME, BOOLEAN_FIELD_NAME, INT_FIELD_NAME,
-                DOUBLE_FIELD_NAME, DATE_FIELD_NAME);
-        if (fieldName.equals(DATE_FIELD_NAME)) {
-            assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        }
-        Object value = "";
-        if (fieldName.equals(STRING_FIELD_NAME)) {
-            int terms = randomIntBetween(0, 3);
-            StringBuilder builder = new StringBuilder();
-            for (int i = 0; i < terms; i++) {
-                builder.append(randomAsciiOfLengthBetween(1, 10) + " ");
-            }
-            value = builder.toString().trim();
-        } else {
-            value = getRandomValueForFieldName(fieldName);
-        }
-
-        MatchQueryBuilder matchQuery = new MatchQueryBuilder(fieldName, value);
-        matchQuery.type(randomFrom(MatchQuery.Type.values()));
-        matchQuery.operator(randomFrom(Operator.values()));
-
-        if (randomBoolean()) {
-            matchQuery.analyzer(randomFrom("simple", "keyword", "whitespace"));
-        }
-
-        if (randomBoolean()) {
-            matchQuery.slop(randomIntBetween(0, 10));
-        }
-
-        if (randomBoolean()) {
-            matchQuery.fuzziness(randomFuzziness(fieldName));
-        }
-
-        if (randomBoolean()) {
-            matchQuery.prefixLength(randomIntBetween(0, 10));
-        }
-
-        if (randomBoolean()) {
-            matchQuery.minimumShouldMatch(randomMinimumShouldMatch());
-        }
-
-        if (randomBoolean()) {
-            matchQuery.fuzzyRewrite(getRandomRewriteMethod());
-        }
-
-        if (randomBoolean()) {
-            matchQuery.fuzzyTranspositions(randomBoolean());
-        }
-
-        if (randomBoolean()) {
-            matchQuery.lenient(randomBoolean());
-        }
-
-        if (randomBoolean()) {
-            matchQuery.zeroTermsQuery(randomFrom(MatchQuery.ZeroTermsQuery.values()));
-        }
-
-        if (randomBoolean()) {
-            matchQuery.cutoffFrequency((float) 10 / randomIntBetween(1, 100));
-        }
-        return matchQuery;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MatchQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, notNullValue());
-
-        if (query instanceof MatchAllDocsQuery) {
-            assertThat(queryBuilder.zeroTermsQuery(), equalTo(ZeroTermsQuery.ALL));
-            return;
-        }
-
-        switch (queryBuilder.type()) {
-        case BOOLEAN:
-            assertThat(query, either(instanceOf(BooleanQuery.class)).or(instanceOf(ExtendedCommonTermsQuery.class))
-                    .or(instanceOf(TermQuery.class)).or(instanceOf(FuzzyQuery.class)));
-            break;
-        case PHRASE:
-            assertThat(query, either(instanceOf(BooleanQuery.class)).or(instanceOf(PhraseQuery.class))
-                    .or(instanceOf(TermQuery.class)).or(instanceOf(FuzzyQuery.class)));
-            break;
-        case PHRASE_PREFIX:
-            assertThat(query, either(instanceOf(BooleanQuery.class)).or(instanceOf(MultiPhrasePrefixQuery.class))
-                    .or(instanceOf(TermQuery.class)).or(instanceOf(FuzzyQuery.class)));
-            break;
-        }
-
-        MappedFieldType fieldType = context.fieldMapper(queryBuilder.fieldName());
-        if (query instanceof TermQuery && fieldType != null) {
-            String queryValue = queryBuilder.value().toString();
-            if (queryBuilder.analyzer() == null || queryBuilder.analyzer().equals("simple")) {
-                queryValue = queryValue.toLowerCase(Locale.ROOT);
-            }
-            Query expectedTermQuery = fieldType.termQuery(queryValue, context);
-            // the real query will have boost applied, so we set it to our expeced as well
-            expectedTermQuery.setBoost(queryBuilder.boost());
-            assertEquals(expectedTermQuery, query);
-        }
-
-        if (query instanceof BooleanQuery) {
-            BooleanQuery bq = (BooleanQuery) query;
-            if (queryBuilder.analyzer() == null && queryBuilder.value().toString().length() > 0) {
-                assertEquals(bq.clauses().size(), queryBuilder.value().toString().split(" ").length);
-            }
-        }
-
-        if (query instanceof ExtendedCommonTermsQuery) {
-            assertTrue(queryBuilder.cutoffFrequency() != null);
-            ExtendedCommonTermsQuery ectq = (ExtendedCommonTermsQuery) query;
-            assertEquals((float) queryBuilder.cutoffFrequency(), ectq.getMaxTermFrequency(), Float.MIN_VALUE);
-        }
-
-        if (query instanceof FuzzyQuery) {
-            assertTrue(queryBuilder.fuzziness() != null);
-            FuzzyQuery fuzzyQuery = (FuzzyQuery) query;
-            fuzzyQuery.getTerm().equals(new Term(STRING_FIELD_NAME, BytesRefs.toBytesRef(queryBuilder.value())));
-            assertThat(queryBuilder.prefixLength(), equalTo(fuzzyQuery.getPrefixLength()));
-            assertThat(queryBuilder.fuzzyTranspositions(), equalTo(fuzzyQuery.getTranspositions()));
-        }
-    }
-
-    public void testIllegalValues() {
-        try {
-            new MatchQueryBuilder(null, "value");
-            fail("value must not be non-null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            new MatchQueryBuilder("fieldName", null);
-            fail("value must not be non-null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        MatchQueryBuilder matchQuery = new MatchQueryBuilder("fieldName", "text");
-        try {
-            matchQuery.prefixLength(-1);
-            fail("must not be positive");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            matchQuery.maxExpansions(-1);
-            fail("must not be positive");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            matchQuery.operator(null);
-            fail("must not be non-null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            matchQuery.type(null);
-            fail("must not be non-null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-
-        try {
-            matchQuery.zeroTermsQuery(null);
-            fail("must not be non-null");
-        } catch (IllegalArgumentException ex) {
-            // expected
-        }
-    }
-
-    @Test(expected = QueryShardException.class)
-    public void testBadAnalyzer() throws IOException {
-        MatchQueryBuilder matchQuery = new MatchQueryBuilder("fieldName", "text");
-        matchQuery.analyzer("bogusAnalyzer");
-        matchQuery.doToQuery(createShardContext());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java
deleted file mode 100644
index 0314ca9..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class MissingQueryBuilderTests extends AbstractQueryTestCase<MissingQueryBuilder> {
-
-    @Override
-    protected MissingQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? randomFrom(MAPPED_FIELD_NAMES) : randomAsciiOfLengthBetween(1, 10);
-        Boolean existence = randomBoolean();
-        Boolean nullValue = randomBoolean();
-        if (existence == false && nullValue == false) {
-            if (randomBoolean()) {
-                existence = true;
-            } else {
-                nullValue = true;
-            }
-        }
-        return new MissingQueryBuilder(fieldName, nullValue, existence);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MissingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        //too many mapping dependent cases to test, we don't want to end up duplication the toQuery method
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new MissingQueryBuilder("", true, true);
-            } else {
-                new MissingQueryBuilder(null, true, true);
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MissingQueryBuilder("fieldname", false, false);
-            fail("existence and nullValue cannot both be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MissingQueryBuilder("fieldname", MissingQueryBuilder.DEFAULT_NULL_VALUE, false);
-            fail("existence and nullValue cannot both be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test(expected = QueryShardException.class)
-    public void testBothNullValueAndExistenceFalse() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        MissingQueryBuilder.newFilter(context, "field", false, false);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilderTests.java
deleted file mode 100644
index 6eb53ac..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilderTests.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.memory.MemoryIndex;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.termvectors.*;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.lucene.search.MoreLikeThisQuery;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
-import org.hamcrest.Matchers;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.stream.Stream;
-
-import static org.hamcrest.Matchers.is;
-
-public class MoreLikeThisQueryBuilderTests extends AbstractQueryTestCase<MoreLikeThisQueryBuilder> {
-
-    private static String[] randomFields;
-    private static Item[] randomLikeItems;
-    private static Item[] randomUnlikeItems;
-
-    @Before
-    public void setup() {
-        // MLT only supports string fields, unsupported fields are tested below
-        randomFields = randomStringFields();
-        // we also preset the item requests
-        randomLikeItems = new Item[randomIntBetween(1, 3)];
-        for (int i = 0; i < randomLikeItems.length; i++) {
-            randomLikeItems[i] = generateRandomItem();
-        }
-        // and for the unlike items too
-        randomUnlikeItems = new Item[randomIntBetween(1, 3)];
-        for (int i = 0; i < randomUnlikeItems.length; i++) {
-            randomUnlikeItems[i] = generateRandomItem();
-        }
-    }
-
-    private static String[] randomStringFields() {
-        String[] mappedStringFields = new String[]{STRING_FIELD_NAME, STRING_FIELD_NAME_2};
-        String[] unmappedStringFields = generateRandomStringArray(2, 5, false, false);
-        return Stream.concat(Arrays.stream(mappedStringFields), Arrays.stream(unmappedStringFields)).toArray(String[]::new);
-    }
-
-    private Item generateRandomItem() {
-        String index = randomBoolean() ? getIndex().getName() : null;
-        String type = getRandomType();  // set to one type to avoid ambiguous types
-        // indexed item or artificial document
-        Item item;
-        if (randomBoolean()) {
-            item = new Item(index, type, randomAsciiOfLength(10));
-        } else {
-            item = new Item(index, type, randomArtificialDoc());
-        }
-        // if no field is specified MLT uses all mapped fields for this item
-        if (randomBoolean()) {
-            item.fields(randomFrom(randomFields));
-        }
-        // per field analyzer
-        if (randomBoolean()) {
-            item.perFieldAnalyzer(randomPerFieldAnalyzer());
-        }
-        if (randomBoolean()) {
-            item.routing(randomAsciiOfLength(10));
-        }
-        if (randomBoolean()) {
-            item.version(randomInt(5));
-        }
-        if (randomBoolean()) {
-            item.versionType(randomFrom(VersionType.values()));
-        }
-        return item;
-    }
-
-    private XContentBuilder randomArtificialDoc() {
-        XContentBuilder doc;
-        try {
-            doc = XContentFactory.jsonBuilder().startObject();
-            for (String field : randomFields) {
-                doc.field(field, randomAsciiOfLength(10));
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("Unable to generate random artificial doc!");
-        }
-        return doc;
-    }
-
-    private Map<String, String> randomPerFieldAnalyzer() {
-        Map<String, String> perFieldAnalyzer = new HashMap<>();
-        for (String field : randomFields) {
-            perFieldAnalyzer.put(field, randomAnalyzer());
-        }
-        return perFieldAnalyzer;
-    }
-
-    @Override
-    protected MoreLikeThisQueryBuilder doCreateTestQueryBuilder() {
-        MoreLikeThisQueryBuilder queryBuilder;
-        if (randomBoolean()) { // for the default field
-            queryBuilder = new MoreLikeThisQueryBuilder();
-        } else {
-            queryBuilder = new MoreLikeThisQueryBuilder(randomFields);
-        }
-        // like field is required
-        if (randomBoolean()) {
-            queryBuilder.like(generateRandomStringArray(5, 5, false, false));
-        } else {
-            queryBuilder.like(randomLikeItems);
-        }
-        if (randomBoolean()) {
-            queryBuilder.unlike(generateRandomStringArray(5, 5, false, false));
-        }
-        if (randomBoolean()) {
-            queryBuilder.unlike(randomUnlikeItems);
-        }
-        if (randomBoolean()) {
-            queryBuilder.maxQueryTerms(randomInt(25));
-        }
-        if (randomBoolean()) {
-            queryBuilder.minTermFreq(randomInt(5));
-        }
-        if (randomBoolean()) {
-            queryBuilder.minDocFreq(randomInt(5));
-        }
-        if (randomBoolean()) {
-            queryBuilder.maxDocFreq(randomInt(100));
-        }
-        if (randomBoolean()) {
-            queryBuilder.minWordLength(randomInt(5));
-        }
-        if (randomBoolean()) {
-            queryBuilder.maxWordLength(randomInt(25));
-        }
-        if (randomBoolean()) {
-            queryBuilder.stopWords(generateRandomStringArray(5, 5, false, false));
-        }
-        if (randomBoolean()) {
-            queryBuilder.analyzer(randomAnalyzer());  // fix the analyzer?
-        }
-        if (randomBoolean()) {
-            queryBuilder.minimumShouldMatch(randomMinimumShouldMatch());
-        }
-        if (randomBoolean()) {
-            queryBuilder.boostTerms(randomFloat() * 10);
-        }
-        if (randomBoolean()) {
-            queryBuilder.include(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryBuilder.failOnUnsupportedField(randomBoolean());
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected MultiTermVectorsResponse executeMultiTermVectors(MultiTermVectorsRequest mtvRequest) {
-        try {
-            MultiTermVectorsItemResponse[] responses = new MultiTermVectorsItemResponse[mtvRequest.size()];
-            int i = 0;
-            for (TermVectorsRequest request : mtvRequest) {
-                TermVectorsResponse response = new TermVectorsResponse(request.index(), request.type(), request.id());
-                response.setExists(true);
-                Fields generatedFields;
-                if (request.doc() != null) {
-                    generatedFields = generateFields(randomFields, request.doc().toUtf8());
-                } else {
-                    generatedFields = generateFields(request.selectedFields().toArray(new String[0]), request.id());
-                }
-                EnumSet<TermVectorsRequest.Flag> flags = EnumSet.of(TermVectorsRequest.Flag.Positions, TermVectorsRequest.Flag.Offsets);
-                response.setFields(generatedFields, request.selectedFields(), flags, generatedFields);
-                responses[i++] = new MultiTermVectorsItemResponse(response, null);
-            }
-            return new MultiTermVectorsResponse(responses);
-        } catch (IOException ex) {
-            throw new ElasticsearchException("boom", ex);
-        }
-    }
-
-    /**
-     * Here we could go overboard and use a pre-generated indexed random document for a given Item,
-     * but for now we'd prefer to simply return the id as the content of the document and that for
-     * every field.
-     */
-    private static Fields generateFields(String[] fieldNames, String text) throws IOException {
-        MemoryIndex index = new MemoryIndex();
-        for (String fieldName : fieldNames) {
-            index.addField(fieldName, text, new WhitespaceAnalyzer());
-        }
-        return MultiFields.getFields(index.createSearcher().getIndexReader());
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MoreLikeThisQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (!queryBuilder.likeItems().isEmpty()) {
-            assertThat(query, Matchers.instanceOf(BooleanQuery.class));
-        } else {
-            // we rely on integration tests for a deeper check here
-            assertThat(query, Matchers.instanceOf(MoreLikeThisQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        MoreLikeThisQueryBuilder queryBuilder = new MoreLikeThisQueryBuilder(Strings.EMPTY_ARRAY);
-        assertThat(queryBuilder.validate().validationErrors().size(), is(2));
-
-        queryBuilder = new MoreLikeThisQueryBuilder(Strings.EMPTY_ARRAY).like("some text");
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = new MoreLikeThisQueryBuilder("field").like(Strings.EMPTY_ARRAY);
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = new MoreLikeThisQueryBuilder("field").like(Item.EMPTY_ARRAY);
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = new MoreLikeThisQueryBuilder("field").like("some text");
-        assertNull(queryBuilder.validate());
-    }
-
-    @Test
-    public void testUnsupportedFields() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        String unsupportedField = randomFrom(INT_FIELD_NAME, DOUBLE_FIELD_NAME, DATE_FIELD_NAME);
-        MoreLikeThisQueryBuilder queryBuilder = new MoreLikeThisQueryBuilder(unsupportedField)
-                .like("some text")
-                .failOnUnsupportedField(true);
-        try {
-            queryBuilder.toQuery(createShardContext());
-            fail("should have failed with IllegalArgumentException for field: " + unsupportedField);
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.containsString("more_like_this doesn't support binary/numeric fields"));
-        }
-    }
-
-    @Test
-    public void testItemSerialization() throws IOException {
-        Item expectedItem = generateRandomItem();
-        BytesStreamOutput output = new BytesStreamOutput();
-        expectedItem.writeTo(output);
-        Item newItem = Item.readItemFrom(StreamInput.wrap(output.bytes()));
-        assertEquals(expectedItem, newItem);
-    }
-
-    @Test
-    public void testItemFromXContent() throws IOException {
-        Item expectedItem = generateRandomItem();
-        String json = expectedItem.toXContent(XContentFactory.jsonBuilder(), ToXContent.EMPTY_PARAMS).string();
-        XContentParser parser = XContentFactory.xContent(json).createParser(json);
-        Item newItem = Item.parse(parser, ParseFieldMatcher.STRICT, new Item());
-        assertEquals(expectedItem, newItem);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MultiMatchQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MultiMatchQueryBuilderTests.java
deleted file mode 100644
index df4860f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MultiMatchQueryBuilderTests.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.lucene.all.AllTermQuery;
-import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
-import org.elasticsearch.index.search.MatchQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBooleanSubQuery;
-import static org.hamcrest.CoreMatchers.*;
-
-public class MultiMatchQueryBuilderTests extends AbstractQueryTestCase<MultiMatchQueryBuilder> {
-
-    @Override
-    protected MultiMatchQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomFrom(STRING_FIELD_NAME, INT_FIELD_NAME, DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME);
-        if (fieldName.equals(DATE_FIELD_NAME)) {
-            assumeTrue("test with date fields runs only when at least a type is registered", getCurrentTypes().length > 0);
-        }
-        // creates the query with random value and field name
-        Object value;
-        if (fieldName.equals(STRING_FIELD_NAME)) {
-            value = getRandomQueryText();
-        } else {
-            value = getRandomValueForFieldName(fieldName);
-        }
-        MultiMatchQueryBuilder query = new MultiMatchQueryBuilder(value, fieldName);
-        // field with random boost
-        if (randomBoolean()) {
-            query.field(fieldName, randomFloat() * 10);
-        }
-        // sets other parameters of the multi match query
-        if (randomBoolean()) {
-            query.type(randomFrom(MultiMatchQueryBuilder.Type.values()));
-        }
-        if (randomBoolean()) {
-            query.operator(randomFrom(Operator.values()));
-        }
-        if (randomBoolean()) {
-            query.analyzer(randomAnalyzer());
-        }
-        if (randomBoolean()) {
-            query.slop(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            query.fuzziness(randomFuzziness(fieldName));
-        }
-        if (randomBoolean()) {
-            query.prefixLength(randomIntBetween(0, 5));
-        }
-        if (randomBoolean()) {
-            query.maxExpansions(randomIntBetween(1, 5));
-        }
-        if (randomBoolean()) {
-            query.minimumShouldMatch(randomMinimumShouldMatch());
-        }
-        if (randomBoolean()) {
-            query.fuzzyRewrite(getRandomRewriteMethod());
-        }
-        if (randomBoolean()) {
-            query.useDisMax(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.tieBreaker(randomFloat());
-        }
-        if (randomBoolean()) {
-            query.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.cutoffFrequency((float) 10 / randomIntBetween(1, 100));
-        }
-        if (randomBoolean()) {
-            query.zeroTermsQuery(randomFrom(MatchQuery.ZeroTermsQuery.values()));
-        }
-        // test with fields with boost and patterns delegated to the tests further below
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MultiMatchQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        // we rely on integration tests for deeper checks here
-        assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(AllTermQuery.class))
-                .or(instanceOf(BooleanQuery.class)).or(instanceOf(DisjunctionMaxQuery.class))
-                .or(instanceOf(FuzzyQuery.class)).or(instanceOf(MultiPhrasePrefixQuery.class))
-                .or(instanceOf(MatchAllDocsQuery.class)).or(instanceOf(ExtendedCommonTermsQuery.class))
-                .or(instanceOf(MatchNoDocsQuery.class)).or(instanceOf(PhraseQuery.class)));
-    }
-
-    @Test
-    public void testIllegaArguments() {
-        try {
-            new MultiMatchQueryBuilder(null, "field");
-            fail("value must not be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MultiMatchQueryBuilder("value", (String[]) null);
-            fail("initial fields must be supplied at construction time must not be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MultiMatchQueryBuilder("value", new String[]{""});
-            fail("field names cannot be empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Override
-    protected void assertBoost(MultiMatchQueryBuilder queryBuilder, Query query) throws IOException {
-        //we delegate boost checks to specific boost tests below
-    }
-
-    @Test
-    public void testToQueryBoost() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        QueryShardContext shardContext = createShardContext();
-        MultiMatchQueryBuilder multiMatchQueryBuilder = new MultiMatchQueryBuilder("test");
-        multiMatchQueryBuilder.field(STRING_FIELD_NAME, 5);
-        Query query = multiMatchQueryBuilder.toQuery(shardContext);
-        assertThat(query, instanceOf(TermQuery.class));
-        assertThat(query.getBoost(), equalTo(5f));
-
-        multiMatchQueryBuilder = new MultiMatchQueryBuilder("test");
-        multiMatchQueryBuilder.field(STRING_FIELD_NAME, 5);
-        multiMatchQueryBuilder.boost(2);
-        query = multiMatchQueryBuilder.toQuery(shardContext);
-        assertThat(query, instanceOf(TermQuery.class));
-        assertThat(query.getBoost(), equalTo(10f));
-    }
-
-    @Test
-    public void testToQueryMultipleTermsBooleanQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = multiMatchQuery("test1 test2").field(STRING_FIELD_NAME).useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test1")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test2")));
-    }
-
-    @Test
-    public void testToQueryMultipleFieldsBooleanQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = multiMatchQuery("test").field(STRING_FIELD_NAME).field(STRING_FIELD_NAME_2).useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-
-    @Test
-    public void testToQueryMultipleFieldsDisMaxQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = multiMatchQuery("test").field(STRING_FIELD_NAME).field(STRING_FIELD_NAME_2).useDisMax(true).toQuery(createShardContext());
-        assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-        DisjunctionMaxQuery disMaxQuery = (DisjunctionMaxQuery) query;
-        List<Query> disjuncts = disMaxQuery.getDisjuncts();
-        assertThat(((TermQuery) disjuncts.get(0)).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(((TermQuery) disjuncts.get(1)).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-
-    @Test
-    public void testToQueryFieldsWildcard() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = multiMatchQuery("test").field("mapped_str*").useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
deleted file mode 100644
index 0299068..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/NestedQueryBuilderTests.java
+++ /dev/null
@@ -1,188 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.ScoreMode;
-import org.apache.lucene.search.join.ToParentBlockJoinQuery;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.support.QueryInnerHits;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.test.TestSearchContext;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class NestedQueryBuilderTests extends AbstractQueryTestCase<NestedQueryBuilder> {
-
-    @Override
-    public void setUp() throws Exception {
-        super.setUp();
-        MapperService mapperService = queryParserService().mapperService;
-        mapperService.merge("nested_doc", new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef("nested_doc",
-                STRING_FIELD_NAME, "type=string",
-                INT_FIELD_NAME, "type=integer",
-                DOUBLE_FIELD_NAME, "type=double",
-                BOOLEAN_FIELD_NAME, "type=boolean",
-                DATE_FIELD_NAME, "type=date",
-                OBJECT_FIELD_NAME, "type=object",
-                "nested1", "type=nested"
-        ).string()), false, false);
-    }
-
-    @Override
-    protected void setSearchContext(String[] types) {
-        final MapperService mapperService = queryParserService().mapperService;
-        final IndexFieldDataService fieldData = queryParserService().fieldDataService;
-        TestSearchContext testSearchContext = new TestSearchContext() {
-            private InnerHitsContext context;
-
-
-            @Override
-            public void innerHits(InnerHitsContext innerHitsContext) {
-                context = innerHitsContext;
-            }
-
-            @Override
-            public InnerHitsContext innerHits() {
-                return context;
-            }
-
-            @Override
-            public MapperService mapperService() {
-                return mapperService; // need to build / parse inner hits sort fields
-            }
-
-            @Override
-            public IndexFieldDataService fieldData() {
-                return fieldData; // need to build / parse inner hits sort fields
-            }
-        };
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    /**
-     * @return a {@link HasChildQueryBuilder} with random values all over the place
-     */
-    @Override
-    protected NestedQueryBuilder doCreateTestQueryBuilder() {
-        InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit().setSize(100).addSort(STRING_FIELD_NAME, SortOrder.ASC);
-        return new NestedQueryBuilder("nested1", RandomQueryBuilder.createQuery(random()),
-                RandomPicks.randomFrom(random(), ScoreMode.values()),
-                SearchContext.current() == null ? null : new QueryInnerHits("inner_hits_name", innerHit));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(NestedQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        QueryBuilder innerQueryBuilder = queryBuilder.query();
-        if (innerQueryBuilder instanceof EmptyQueryBuilder) {
-            assertNull(query);
-        } else {
-            assertThat(query, instanceOf(ToParentBlockJoinQuery.class));
-            ToParentBlockJoinQuery parentBlockJoinQuery = (ToParentBlockJoinQuery) query;
-            //TODO how to assert this?
-        }
-        if (queryBuilder.innerHit() != null) {
-            assertNotNull(SearchContext.current());
-            if (query != null) {
-                assertNotNull(SearchContext.current().innerHits());
-                assertEquals(1, SearchContext.current().innerHits().getInnerHits().size());
-                assertTrue(SearchContext.current().innerHits().getInnerHits().containsKey("inner_hits_name"));
-                InnerHitsContext.BaseInnerHits innerHits = SearchContext.current().innerHits().getInnerHits().get("inner_hits_name");
-                assertEquals(innerHits.size(), 100);
-                assertEquals(innerHits.sort().getSort().length, 1);
-                assertEquals(innerHits.sort().getSort()[0].getField(), STRING_FIELD_NAME);
-            } else {
-                assertNull(SearchContext.current().innerHits());
-            }
-        }
-    }
-
-    public void testParseDeprecatedFilter() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-        builder.startObject();
-            builder.startObject("nested");
-                builder.startObject("filter");
-                    builder.startObject("terms").array(STRING_FIELD_NAME, "a", "b").endObject();// deprecated
-                builder.endObject();
-                builder.field("path", "foo.bar");
-            builder.endObject();
-        builder.endObject();
-
-        QueryShardContext shardContext = createShardContext();
-        QueryParseContext context = shardContext.parseContext();
-        XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(builder.string());
-        context.reset(parser);
-        context.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        try {
-            context.parseInnerQueryBuilder();
-            fail("filter is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [filter] used, replaced by [query]", ex.getMessage());
-        }
-
-        parser = XContentFactory.xContent(XContentType.JSON).createParser(builder.string());
-        context.reset(parser);
-        NestedQueryBuilder queryBuilder = (NestedQueryBuilder) context.parseInnerQueryBuilder();
-        QueryBuilder query = queryBuilder.query();
-        assertTrue(query instanceof TermsQueryBuilder);
-        TermsQueryBuilder tqb = (TermsQueryBuilder) query;
-        assertEquals(tqb.values(), Arrays.asList("a", "b"));
-    }
-
-    @Test
-    public void testValidate() {
-        try {
-            new NestedQueryBuilder(null, EmptyQueryBuilder.PROTOTYPE);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new NestedQueryBuilder("path", null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        NestedQueryBuilder nestedQueryBuilder = new NestedQueryBuilder("path", EmptyQueryBuilder.PROTOTYPE);
-        try {
-            nestedQueryBuilder.scoreMode(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTests.java
deleted file mode 100644
index b5baf4b..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTests.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.*;
-
-public class NotQueryBuilderTests extends AbstractQueryTestCase<NotQueryBuilder> {
-
-    /**
-     * @return a NotQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected NotQueryBuilder doCreateTestQueryBuilder() {
-        return new NotQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(NotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query filter = queryBuilder.innerQuery().toQuery(context);
-        if (filter == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(2));
-            assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-            assertThat(booleanQuery.clauses().get(0).getQuery(), instanceOf(MatchAllDocsQuery.class));
-            assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.MUST_NOT));
-            assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(filter));
-        }
-    }
-
-    /**
-     * @throws IOException
-     */
-    @Test(expected=ParsingException.class)
-    public void testMissingFilterSection() throws IOException {
-        String queryString = "{ \"not\" : {}";
-        parseQuery(queryString);
-    }
-
-    @Override
-    protected Map<String, NotQueryBuilder> getAlternateVersions() {
-        Map<String, NotQueryBuilder> alternateVersions = new HashMap<>();
-        QueryBuilder innerQuery = createTestQueryBuilder().innerQuery();
-        //not doesn't support empty query when query/filter element is not specified
-        if (innerQuery != EmptyQueryBuilder.PROTOTYPE) {
-            NotQueryBuilder testQuery2 = new NotQueryBuilder(innerQuery);
-            String contentString2 = "{\n" +
-                    "    \"not\" : " + testQuery2.innerQuery().toString() +  "\n}";
-            alternateVersions.put(contentString2, testQuery2);
-        }
-
-        return alternateVersions;
-    }
-
-
-    public void testDeprecatedXContent() throws IOException {
-        String deprecatedJson = "{\n" +
-                "    \"not\" : {\n" +
-                "        \"filter\" : " + EmptyQueryBuilder.PROTOTYPE.toString() + "\n" +
-                "    }\n" +
-                "}";
-        try {
-            parseQuery(deprecatedJson);
-            fail("filter is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [filter] used, expected [query] instead", ex.getMessage());
-        }
-
-        NotQueryBuilder queryBuilder = (NotQueryBuilder) parseQuery(deprecatedJson, ParseFieldMatcher.EMPTY);
-        assertEquals(EmptyQueryBuilder.PROTOTYPE, queryBuilder.innerQuery());
-    }
-
-    @Test
-    public void testValidate() {
-        try {
-            new NotQueryBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTests.java
deleted file mode 100644
index 4dd3758..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTests.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class PrefixQueryBuilderTests extends AbstractQueryTestCase<PrefixQueryBuilder> {
-
-    @Override
-    protected PrefixQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        PrefixQueryBuilder query = new PrefixQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(PrefixQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(PrefixQuery.class));
-        PrefixQuery prefixQuery = (PrefixQuery) query;
-        assertThat(prefixQuery.getPrefix().field(), equalTo(queryBuilder.fieldName()));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new PrefixQueryBuilder(null, "text");
-            } else {
-                new PrefixQueryBuilder("", "text");
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new PrefixQueryBuilder("field", null);
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTests.java
deleted file mode 100644
index 8c9a815..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTests.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.*;
-
-@SuppressWarnings("deprecation")
-public class QueryFilterBuilderTests extends AbstractQueryTestCase<QueryFilterBuilder> {
-
-    @Override
-    protected QueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new QueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(QueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} } }";
-        QueryBuilder<?> innerQuery = parseQuery(queryString);
-        // check that when wrapping this filter, toQuery() returns null
-        QueryFilterBuilder queryFilterQuery = new QueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        try {
-            new QueryFilterBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java
deleted file mode 100644
index 43fc329..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/QueryStringQueryBuilderTests.java
+++ /dev/null
@@ -1,302 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
-import org.apache.lucene.util.automaton.TooComplexToDeterminizeException;
-import org.elasticsearch.common.lucene.all.AllTermQuery;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBooleanSubQuery;
-import static org.hamcrest.CoreMatchers.either;
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.Matchers.*;
-
-public class QueryStringQueryBuilderTests extends AbstractQueryTestCase<QueryStringQueryBuilder> {
-
-    @Override
-    protected QueryStringQueryBuilder doCreateTestQueryBuilder() {
-        int numTerms = randomIntBetween(0, 5);
-        String query = "";
-        for (int i = 0; i < numTerms; i++) {
-            //min length 4 makes sure that the text is not an operator (AND/OR) so toQuery won't break
-            query += (randomBoolean() ? STRING_FIELD_NAME + ":" : "") + randomAsciiOfLengthBetween(4, 10) + " ";
-        }
-        QueryStringQueryBuilder queryStringQueryBuilder = new QueryStringQueryBuilder(query);
-        if (randomBoolean()) {
-            queryStringQueryBuilder.defaultField(randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            int numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-                if (randomBoolean()) {
-                    queryStringQueryBuilder.field(fieldName);
-                } else {
-                    queryStringQueryBuilder.field(fieldName, randomFloat());
-                }
-            }
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.defaultOperator(randomFrom(Operator.values()));
-        }
-        if (randomBoolean()) {
-            //we only use string fields (either mapped or unmapped)
-            queryStringQueryBuilder.fuzziness(randomFuzziness(STRING_FIELD_NAME));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.analyzer(randomAnalyzer());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.quoteAnalyzer(randomAnalyzer());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.allowLeadingWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.analyzeWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.maxDeterminizedStates(randomIntBetween(1, 100));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.lowercaseExpandedTerms(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.autoGeneratePhraseQueries(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.enablePositionIncrements(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.escape(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.phraseSlop(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.fuzzyMaxExpansions(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.fuzzyPrefixLength(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.fuzzyRewrite(getRandomRewriteMethod());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.rewrite(getRandomRewriteMethod());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.quoteFieldSuffix(randomAsciiOfLengthBetween(1, 3));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.tieBreaker(randomFloat());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.minimumShouldMatch(randomMinimumShouldMatch());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.useDisMax(randomBoolean());
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.locale(randomLocale(getRandom()));
-        }
-        if (randomBoolean()) {
-            queryStringQueryBuilder.timeZone(randomTimeZone());
-        }
-        return queryStringQueryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(QueryStringQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if ("".equals(queryBuilder.queryString())) {
-            assertThat(query, instanceOf(MatchNoDocsQuery.class));
-        } else {
-            assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(AllTermQuery.class))
-                    .or(instanceOf(BooleanQuery.class)).or(instanceOf(DisjunctionMaxQuery.class)));
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new QueryStringQueryBuilder(null);
-            fail("null is not allowed");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testToQueryMatchAllQuery() throws Exception {
-        Query query = queryStringQuery("*:*").toQuery(createShardContext());
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-
-    @Test
-    public void testToQueryTermQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test").defaultField(STRING_FIELD_NAME).toQuery(createShardContext());
-        assertThat(query, instanceOf(TermQuery.class));
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-    }
-
-    @Test
-    public void testToQueryPhraseQuery() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("\"term1 term2\"").defaultField(STRING_FIELD_NAME).phraseSlop(3).toQuery(createShardContext());
-        assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-        DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query;
-        assertThat(disjunctionMaxQuery.getDisjuncts().size(), equalTo(1));
-        assertThat(disjunctionMaxQuery.getDisjuncts().get(0), instanceOf(PhraseQuery.class));
-        PhraseQuery phraseQuery = (PhraseQuery)disjunctionMaxQuery.getDisjuncts().get(0);
-        assertThat(phraseQuery.getTerms().length, equalTo(2));
-        assertThat(phraseQuery.getTerms()[0], equalTo(new Term(STRING_FIELD_NAME, "term1")));
-        assertThat(phraseQuery.getTerms()[1], equalTo(new Term(STRING_FIELD_NAME, "term2")));
-        assertThat(phraseQuery.getSlop(), equalTo(3));
-    }
-
-    @Test
-    public void testToQueryBoosts() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        QueryShardContext shardContext = createShardContext();
-        QueryStringQueryBuilder queryStringQuery = queryStringQuery(STRING_FIELD_NAME + ":boosted^2");
-        Query query = queryStringQuery.toQuery(shardContext);
-        assertThat(query, instanceOf(BoostQuery.class));
-        BoostQuery boostQuery = (BoostQuery) query;
-        assertThat(boostQuery.getBoost(), Matchers.equalTo(2.0f));
-        assertThat(boostQuery.getQuery(), instanceOf(TermQuery.class));
-        assertThat(((TermQuery) boostQuery.getQuery()).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "boosted")));
-        queryStringQuery.boost(2.0f);
-        query = queryStringQuery.toQuery(shardContext);
-        assertThat(query, instanceOf(BoostQuery.class));
-        assertThat(((BoostQuery) query).getBoost(), Matchers.equalTo(4.0f));
-
-        queryStringQuery = queryStringQuery("((" + STRING_FIELD_NAME + ":boosted^2) AND (" + STRING_FIELD_NAME + ":foo^1.5))^3");
-        query = queryStringQuery.toQuery(shardContext);
-        assertThat(query, instanceOf(BoostQuery.class));
-        boostQuery = (BoostQuery) query;
-        assertThat(boostQuery.getBoost(), equalTo(3.0f));
-        BoostQuery boostQuery1 = assertBooleanSubQuery(boostQuery.getQuery(), BoostQuery.class, 0);
-        assertThat(boostQuery1.getBoost(), equalTo(2.0f));
-        assertThat(boostQuery1.getQuery(), instanceOf(TermQuery.class));
-        assertThat(((TermQuery)boostQuery1.getQuery()).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "boosted")));
-        BoostQuery boostQuery2 = assertBooleanSubQuery(boostQuery.getQuery(), BoostQuery.class, 1);
-        assertThat(boostQuery2.getBoost(), equalTo(1.5f));
-        assertThat(boostQuery2.getQuery(), instanceOf(TermQuery.class));
-        assertThat(((TermQuery)boostQuery2.getQuery()).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "foo")));
-        queryStringQuery.boost(2.0f);
-        query = queryStringQuery.toQuery(shardContext);
-        assertThat(query, instanceOf(BoostQuery.class));
-        boostQuery = (BoostQuery) query;
-        assertThat(boostQuery.getBoost(), equalTo(6.0f));
-    }
-
-    @Test
-    public void testToQueryMultipleTermsBooleanQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test1 test2").field(STRING_FIELD_NAME).useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test1")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test2")));
-    }
-
-    @Test
-    public void testToQueryMultipleFieldsBooleanQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test").field(STRING_FIELD_NAME).field(STRING_FIELD_NAME_2).useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-
-    @Test
-    public void testToQueryMultipleFieldsDisMaxQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test").field(STRING_FIELD_NAME).field(STRING_FIELD_NAME_2).useDisMax(true).toQuery(createShardContext());
-        assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-        DisjunctionMaxQuery disMaxQuery = (DisjunctionMaxQuery) query;
-        List<Query> disjuncts = disMaxQuery.getDisjuncts();
-        assertThat(((TermQuery) disjuncts.get(0)).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(((TermQuery) disjuncts.get(1)).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-
-    @Test
-    public void testToQueryFieldsWildcard() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test").field("mapped_str*").useDisMax(false).toQuery(createShardContext());
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery bQuery = (BooleanQuery) query;
-        assertThat(bQuery.clauses().size(), equalTo(2));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 0).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat(assertBooleanSubQuery(query, TermQuery.class, 1).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-    }
-
-    @Test
-    public void testToQueryDisMaxQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("test").field(STRING_FIELD_NAME, 2.2f).field(STRING_FIELD_NAME_2).useDisMax(true).toQuery(createShardContext());
-        assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-        DisjunctionMaxQuery disMaxQuery = (DisjunctionMaxQuery) query;
-        List<Query> disjuncts = disMaxQuery.getDisjuncts();
-        assertThat(((TermQuery) disjuncts.get(0)).getTerm(), equalTo(new Term(STRING_FIELD_NAME, "test")));
-        assertThat((double) disjuncts.get(0).getBoost(), closeTo(2.2, 0.01));
-        assertThat(((TermQuery) disjuncts.get(1)).getTerm(), equalTo(new Term(STRING_FIELD_NAME_2, "test")));
-        assertThat((double) disjuncts.get(1).getBoost(), closeTo(1, 0.01));
-    }
-
-    @Test
-    public void testToQueryRegExpQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("/foo*bar/").defaultField(STRING_FIELD_NAME).maxDeterminizedStates(5000).toQuery(createShardContext());
-        assertThat(query, instanceOf(RegexpQuery.class));
-        RegexpQuery regexpQuery = (RegexpQuery) query;
-        assertTrue(regexpQuery.toString().contains("/foo*bar/"));
-    }
-
-    @Test(expected = TooComplexToDeterminizeException.class)
-    public void testToQueryRegExpQueryTooComplex() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        queryStringQuery("/[ac]*a[ac]{50,200}/").defaultField(STRING_FIELD_NAME).toQuery(createShardContext());
-    }
-
-    @Test
-    public void testToQueryNumericRangeQuery() throws Exception {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        Query query = queryStringQuery("12~0.2").defaultField(INT_FIELD_NAME).toQuery(createShardContext());
-        assertThat(query, instanceOf(NumericRangeQuery.class));
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java b/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
deleted file mode 100644
index 0bd69f8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import java.util.Random;
-
-/**
- * Utility class for creating random QueryBuilders.
- * So far only leaf queries like {@link MatchAllQueryBuilder}, {@link TermQueryBuilder} or
- * {@link IdsQueryBuilder} are returned.
- */
-public class RandomQueryBuilder {
-
-    /**
-     * Create a new query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder}
-     */
-    public static QueryBuilder createQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 4)) {
-            case 0:
-                return new MatchAllQueryBuilderTests().createTestQueryBuilder();
-            case 1:
-                return new TermQueryBuilderTests().createTestQueryBuilder();
-            case 2:
-                return new IdsQueryBuilderTests().createTestQueryBuilder();
-            case 3:
-                return createMultiTermQuery(r);
-            case 4:
-                return EmptyQueryBuilder.PROTOTYPE;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    /**
-     * Create a new multi term query of a random type
-     * @param r random seed
-     * @return a random {@link MultiTermQueryBuilder}
-     */
-    public static MultiTermQueryBuilder createMultiTermQuery(Random r) {
-        // for now, only use String Rangequeries for MultiTerm test, numeric and date makes little sense
-        // see issue #12123 for discussion
-        // Prefix / Fuzzy / RegEx / Wildcard can go here later once refactored and they have random query generators
-        RangeQueryBuilder query = new RangeQueryBuilder(AbstractQueryTestCase.STRING_FIELD_NAME);
-        query.from("a" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        query.to("z" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        return query;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
deleted file mode 100644
index 5c81998..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
+++ /dev/null
@@ -1,155 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-
-public class RangeQueryBuilderTests extends AbstractQueryTestCase<RangeQueryBuilder> {
-
-    @Override
-    protected RangeQueryBuilder doCreateTestQueryBuilder() {
-        RangeQueryBuilder query;
-        // switch between numeric and date ranges
-        switch (randomIntBetween(0, 2)) {
-            case 0:
-                if (randomBoolean()) {
-                    // use mapped integer field for numeric range queries
-                    query = new RangeQueryBuilder(INT_FIELD_NAME);
-                    query.from(randomIntBetween(1, 100));
-                    query.to(randomIntBetween(101, 200));
-                } else {
-                    // use unmapped field for numeric range queries
-                    query = new RangeQueryBuilder(randomAsciiOfLengthBetween(1, 10));
-                    query.from(0.0 - randomDouble());
-                    query.to(randomDouble());
-                }
-                break;
-            case 1:
-                // use mapped date field, using date string representation
-                query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                query.from(new DateTime(System.currentTimeMillis() - randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                query.to(new DateTime(System.currentTimeMillis() + randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                // Create timestamp option only then we have a date mapper,
-                // otherwise we could trigger exception.
-                if (createShardContext().mapperService().smartNameFieldType(DATE_FIELD_NAME) != null) {
-                    if (randomBoolean()) {
-                        query.timeZone(randomTimeZone());
-                    }
-                    if (randomBoolean()) {
-                        query.format("yyyy-MM-dd'T'HH:mm:ss.SSSZZ");
-                    }
-                }
-                break;
-            case 2:
-            default:
-                query = new RangeQueryBuilder(STRING_FIELD_NAME);
-                query.from("a" + randomAsciiOfLengthBetween(1, 10));
-                query.to("z" + randomAsciiOfLengthBetween(1, 10));
-                break;
-        }
-        query.includeLower(randomBoolean()).includeUpper(randomBoolean());
-        if (randomBoolean()) {
-            query.from(null);
-        }
-        if (randomBoolean()) {
-            query.to(null);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RangeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (getCurrentTypes().length == 0 || (queryBuilder.fieldName().equals(DATE_FIELD_NAME) == false && queryBuilder.fieldName().equals(INT_FIELD_NAME) == false)) {
-            assertThat(query, instanceOf(TermRangeQuery.class));
-        } else if (queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            //we can't properly test unmapped dates because LateParsingQuery is package private
-        } else if (queryBuilder.fieldName().equals(INT_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            throw new UnsupportedOperationException();
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new RangeQueryBuilder(null);
-            } else {
-                new RangeQueryBuilder("");
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder("test");
-        try {
-            if (randomBoolean()) {
-                rangeQueryBuilder.timeZone(null);
-            } else {
-                rangeQueryBuilder.timeZone("badID");
-            }
-            fail("cannot be null or unknown id");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            if (randomBoolean()) {
-                rangeQueryBuilder.format(null);
-            } else {
-                rangeQueryBuilder.format("badFormat");
-            }
-            fail("cannot be null or bad format");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    /**
-     * Specifying a timezone together with a numeric range query should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryNonDateWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder(INT_FIELD_NAME);
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-
-    /**
-     * Specifying a timezone together with an unmapped field should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryUnmappedWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder("bogus_field");
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTests.java
deleted file mode 100644
index 014c795..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-
-public class RegexpQueryBuilderTests extends AbstractQueryTestCase<RegexpQueryBuilder> {
-
-    @Override
-    protected RegexpQueryBuilder doCreateTestQueryBuilder() {
-        // mapped or unmapped fields
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        RegexpQueryBuilder query = new RegexpQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            List<RegexpFlag> flags = new ArrayList<>();
-            int iter = randomInt(5);
-            for (int i = 0; i < iter; i++) {
-                flags.add(randomFrom(RegexpFlag.values()));
-            }
-            query.flags(flags.toArray(new RegexpFlag[flags.size()]));
-        }
-        if (randomBoolean()) {
-            query.maxDeterminizedStates(randomInt(50000));
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RegexpQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(RegexpQuery.class));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new RegexpQueryBuilder(null, "text");
-            } else {
-                new RegexpQueryBuilder("", "text");
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new RegexpQueryBuilder("field", null);
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java
deleted file mode 100644
index 9a76c7a..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.instanceOf;
-
-public class ScriptQueryBuilderTests extends AbstractQueryTestCase<ScriptQueryBuilder> {
-
-    @Override
-    protected ScriptQueryBuilder doCreateTestQueryBuilder() {
-        String script;
-        Map<String, Object> params = null;
-        if (randomBoolean()) {
-            script = "5 * 2 > param";
-            params = new HashMap<>();
-            params.put("param", 1);
-        } else {
-            script = "5 * 2 > 2";
-        }
-        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, "expression", params));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ScriptQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ScriptQueryBuilder.ScriptQuery.class));
-    }
-
-    @Test
-    public void testIllegalConstructorArg() {
-        try {
-            new ScriptQueryBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
index 66abfbf..478f2c2 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
@@ -61,9 +61,11 @@ import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.automaton.TooComplexToDeterminizeException;
-import org.elasticsearch.action.support.PlainActionFuture;
-import org.elasticsearch.action.termvectors.*;
-import org.elasticsearch.client.Client;
+import org.elasticsearch.action.termvectors.MultiTermVectorsItemResponse;
+import org.elasticsearch.action.termvectors.MultiTermVectorsResponse;
+import org.elasticsearch.action.termvectors.TermVectorsRequest;
+import org.elasticsearch.action.termvectors.TermVectorsResponse;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -81,10 +83,12 @@ import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 import org.elasticsearch.index.search.geo.GeoPolygonQuery;
 import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
+import org.elasticsearch.index.search.morelikethis.MoreLikeThisFetchService;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.hamcrest.Matchers;
@@ -92,11 +96,11 @@ import org.junit.Before;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.lang.reflect.Proxy;
 import java.util.Arrays;
 import java.util.EnumSet;
+import java.util.HashSet;
 import java.util.List;
-import java.util.concurrent.ExecutionException;
+import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
@@ -316,7 +320,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
         try {
             queryParser.parse(copyToStringFromClasspath("/org/elasticsearch/index/query/query-timezone-incorrect.json"));
-            fail("we expect a ParsingException as we are providing an unknown time_zome");
+            fail("we expect a QueryParsingException as we are providing an unknown time_zome");
         } catch (ParsingException e) {
             // We expect this one
         }
@@ -751,7 +755,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/not-filter.json");
         Query parsedQuery = queryParser.parse(query).query();
-        Query expected =
+        Query expected = 
                 Queries.not(new TermQuery(new Term("name.first", "shay1")));
         assertEquals(expected, parsedQuery);
     }
@@ -777,7 +781,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testBoostingQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(boostingQuery(termQuery("field1", "value1"), termQuery("field1", "value2")).negativeBoost(0.2f)).query();
+        Query parsedQuery = queryParser.parse(boostingQuery().positive(termQuery("field1", "value1")).negative(termQuery("field1", "value2")).negativeBoost(0.2f)).query();
         assertThat(parsedQuery, instanceOf(BoostingQuery.class));
     }
 
@@ -1060,7 +1064,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNotQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNotQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query parsedQuery = queryParser.parse(spanNotQuery().include(spanTermQuery("age", 34)).exclude(spanTermQuery("age", 35))).query();
         assertThat(parsedQuery, instanceOf(SpanNotQuery.class));
         SpanNotQuery spanNotQuery = (SpanNotQuery) parsedQuery;
         // since age is automatically registered in data, we encode it as numeric
@@ -1089,7 +1093,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         little.setBoost(3);
         Query expectedQuery = new SpanWithinQuery(big, little);
 
-        SpanWithinQueryBuilder spanWithinQueryBuilder = spanWithinQuery(spanTermQuery("age", 34).boost(2), spanTermQuery("age", 35).boost(3));
+        SpanWithinQueryBuilder spanWithinQueryBuilder = spanWithinQuery()
+                .big(spanTermQuery("age", 34).boost(2))
+                .little(spanTermQuery("age", 35).boost(3));
         Query actualQuery = queryParser.parse(spanWithinQueryBuilder).query();
         assertEquals(expectedQuery, actualQuery);
 
@@ -1119,7 +1125,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         little.setBoost(3);
         Query expectedQuery = new SpanContainingQuery(big, little);
 
-        SpanContainingQueryBuilder spanContainingQueryBuilder = spanContainingQuery(spanTermQuery("age", 34).boost(2), spanTermQuery("age", 35).boost(3));
+        SpanContainingQueryBuilder spanContainingQueryBuilder = spanContainingQuery()
+                .big(spanTermQuery("age", 34).boost(2))
+                .little(spanTermQuery("age", 35).boost(3));
         Query actualQuery = queryParser.parse(spanContainingQueryBuilder).query();
         assertEquals(expectedQuery, actualQuery);
 
@@ -1166,7 +1174,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNearQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNearQuery(spanTermQuery("age", 34), 12).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).inOrder(false).collectPayloads(false)).query();
+        Query parsedQuery = queryParser.parse(spanNearQuery().clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).slop(12).inOrder(false).collectPayloads(false)).query();
         assertThat(parsedQuery, instanceOf(SpanNearQuery.class));
         SpanNearQuery spanNearQuery = (SpanNearQuery) parsedQuery;
         assertThat(spanNearQuery.getClauses().length, equalTo(3));
@@ -1208,7 +1216,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanOrQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanOrQuery(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36))).query();
+        Query parsedQuery = queryParser.parse(spanOrQuery().clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36))).query();
         assertThat(parsedQuery, instanceOf(SpanOrQuery.class));
         SpanOrQuery spanOrQuery = (SpanOrQuery) parsedQuery;
         assertThat(spanOrQuery.getClauses().length, equalTo(3));
@@ -1286,7 +1294,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         NumericRangeQuery<Long> expectedWrapped = NumericRangeQuery.newLongRange("age", NumberFieldMapper.Defaults.PRECISION_STEP_64_BIT, 7l, 17l, true, true);
         expectedWrapped.setBoost(2.0f);
         SpanMultiTermQueryWrapper<MultiTermQuery> wrapper = (SpanMultiTermQueryWrapper<MultiTermQuery>) parsedQuery;
-        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<>(expectedWrapped)));
+        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<MultiTermQuery>(expectedWrapped)));
     }
 
     @Test
@@ -1298,7 +1306,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         NumericRangeQuery<Long> expectedWrapped = NumericRangeQuery.newLongRange("age", NumberFieldMapper.Defaults.PRECISION_STEP_64_BIT, 10l, 20l, true, false);
         expectedWrapped.setBoost(2.0f);
         SpanMultiTermQueryWrapper<MultiTermQuery> wrapper = (SpanMultiTermQueryWrapper<MultiTermQuery>) parsedQuery;
-        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<>(expectedWrapped)));
+        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<MultiTermQuery>(expectedWrapped)));
     }
 
     @Test
@@ -1310,7 +1318,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         TermRangeQuery expectedWrapped = TermRangeQuery.newStringRange("user", "alice", "bob", true, false);
         expectedWrapped.setBoost(2.0f);
         SpanMultiTermQueryWrapper<MultiTermQuery> wrapper = (SpanMultiTermQueryWrapper<MultiTermQuery>) parsedQuery;
-        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<>(expectedWrapped)));
+        assertThat(wrapper, equalTo(new SpanMultiTermQueryWrapper<MultiTermQuery>(expectedWrapped)));
     }
 
     @Test
@@ -1341,16 +1349,12 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
     @Test
     public void testMoreLikeThisIds() throws Exception {
+        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.queryParser("more_like_this");
+        parser.setFetchService(new MockMoreLikeThisFetchService());
+
         IndexQueryParserService queryParser = queryParser();
-        final Client proxy = getMLTClientProxy();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/mlt-items.json");
-        QueryShardContext ctx = new QueryShardContext(queryParser.index(), queryParser) {
-            @Override
-            public Client getClient() {
-                return proxy;
-            }
-        };
-        Query parsedQuery = queryParser.parse(ctx, new BytesArray(query)).query();
+        Query parsedQuery = queryParser.parse(query).query();
         assertThat(parsedQuery, instanceOf(BooleanQuery.class));
         BooleanQuery booleanQuery = (BooleanQuery) parsedQuery;
         assertThat(booleanQuery.getClauses().length, is(1));
@@ -1368,51 +1372,16 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         }
     }
 
-    private Client getMLTClientProxy() {
-        return (Client) Proxy.newProxyInstance(
-                Client.class.getClassLoader(),
-                new Class[]{Client.class},
-                (proxy1, method, args) -> {
-                    if (method.equals(Client.class.getDeclaredMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
-                        return new PlainActionFuture<MultiTermVectorsResponse>() {
-                            @Override
-                            public MultiTermVectorsResponse get() throws InterruptedException, ExecutionException {
-                                try {
-                                    MultiTermVectorsRequest request = (MultiTermVectorsRequest) args[0];
-                                    MultiTermVectorsItemResponse[] responses = new MultiTermVectorsItemResponse[request.size()];
-                                    int i = 0;
-                                    for (TermVectorsRequest item : request) {
-                                        TermVectorsResponse response = new TermVectorsResponse(item.index(), item.type(), item.id());
-                                        response.setExists(true);
-                                        Fields generatedFields = generateFields(item.selectedFields().toArray(new String[0]), item.id());
-                                        EnumSet<TermVectorsRequest.Flag> flags = EnumSet.of(TermVectorsRequest.Flag.Positions, TermVectorsRequest.Flag.Offsets);
-                                        response.setFields(generatedFields, item.selectedFields(), flags, generatedFields);
-                                        responses[i++] = new MultiTermVectorsItemResponse(response, null);
-                                    }
-                                    return new MultiTermVectorsResponse(responses);
-                                } catch (IOException ex) {
-                                    throw new ExecutionException(ex);
-                                }
-                            }
-                        };
-                    }
-                    throw new UnsupportedOperationException("not supported");
-                });
-    }
-
     @Test
     public void testMLTMinimumShouldMatch() throws Exception {
-        final Client proxy = getMLTClientProxy();
+        // setup for mocking fetching items
+        MoreLikeThisQueryParser parser = (MoreLikeThisQueryParser) queryParser.queryParser("more_like_this");
+        parser.setFetchService(new MockMoreLikeThisFetchService());
+
         // parsing the ES query
         IndexQueryParserService queryParser = queryParser();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/mlt-items.json");
-        QueryShardContext ctx = new QueryShardContext(queryParser.index(), queryParser) {
-            @Override
-            public Client getClient() {
-                return proxy;
-            }
-        };
-        BooleanQuery parsedQuery = (BooleanQuery) queryParser.parse(ctx, new BytesArray(query)).query();
+        BooleanQuery parsedQuery = (BooleanQuery) queryParser.parse(query).query();
 
         // get MLT query, other clause is for include/exclude items
         MoreLikeThisQuery mltQuery = (MoreLikeThisQuery) parsedQuery.getClauses()[0].getQuery();
@@ -1440,6 +1409,27 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertThat(minNumberShouldMatch, is(2));
     }
 
+    private static class MockMoreLikeThisFetchService extends MoreLikeThisFetchService {
+
+        public MockMoreLikeThisFetchService() {
+            super(null, Settings.Builder.EMPTY_SETTINGS);
+        }
+
+        @Override
+        public MultiTermVectorsResponse fetchResponse(List<Item> items, List<Item> unlikeItems, SearchContext searchContext) throws IOException {
+            MultiTermVectorsItemResponse[] responses = new MultiTermVectorsItemResponse[items.size()];
+            int i = 0;
+            for (Item item : items) {
+                TermVectorsResponse response = new TermVectorsResponse(item.index(), item.type(), item.id());
+                response.setExists(true);
+                Fields generatedFields = generateFields(item.fields(), item.id());
+                EnumSet<TermVectorsRequest.Flag> flags = EnumSet.of(TermVectorsRequest.Flag.Positions, TermVectorsRequest.Flag.Offsets);
+                response.setFields(generatedFields, new HashSet<String>(Arrays.asList(item.fields())), flags, generatedFields);
+                responses[i++] = new MultiTermVectorsItemResponse(response, null);
+            }
+            return new MultiTermVectorsResponse(responses);
+        }
+    }
 
     private static Fields generateFields(String[] fieldNames, String text) throws IOException {
         MemoryIndex index = new MemoryIndex();
@@ -1957,7 +1947,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
 
     public void testCrossFieldMultiMatchQuery() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(multiMatchQuery("banon").field("name.first", 2).field("name.last", 3).field("foobar").type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)).query();
+        Query parsedQuery = queryParser.parse(multiMatchQuery("banon", "name.first^2", "name.last^3", "foobar").type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)).query();
         try (Engine.Searcher searcher = indexService.shardSafe(0).acquireSearcher("test")) {
             Query rewrittenQuery = searcher.searcher().rewrite(parsedQuery);
 
@@ -1971,6 +1961,31 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     }
 
     @Test
+    public void testSimpleQueryString() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String query = copyToStringFromClasspath("/org/elasticsearch/index/query/simple-query-string.json");
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));
+    }
+
+    @Test
+    public void testSimpleQueryStringBoost() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        SimpleQueryStringBuilder simpleQueryStringBuilder = new SimpleQueryStringBuilder("test");
+        simpleQueryStringBuilder.field("body", 5);
+        Query parsedQuery = queryParser.parse(simpleQueryStringBuilder.toString()).query();
+        assertThat(parsedQuery, instanceOf(TermQuery.class));
+        assertThat(parsedQuery.getBoost(), equalTo(5f));
+
+        simpleQueryStringBuilder = new SimpleQueryStringBuilder("test");
+        simpleQueryStringBuilder.field("body", 5);
+        simpleQueryStringBuilder.boost(2);
+        parsedQuery = queryParser.parse(simpleQueryStringBuilder.toString()).query();
+        assertThat(parsedQuery, instanceOf(TermQuery.class));
+        assertThat(parsedQuery.getBoost(), equalTo(10f));
+    }
+
+    @Test
     public void testMatchWithFuzzyTranspositions() throws Exception {
         IndexQueryParserService queryParser = queryParser();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/match-with-fuzzy-transpositions.json");
@@ -2065,10 +2080,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
             assertThat(e.getDetailedMessage(), containsString("you can either define [functions] array or a single function, not both. already found [weight], now encountering [functions]."));
         }
     }
-
-    /**
-     * helper to extract term from TermQuery.
-     */
+    
+    /** 
+     * helper to extract term from TermQuery. */
     private Term getTerm(Query query) {
         while (query instanceof QueryWrapperFilter) {
             query = ((QueryWrapperFilter) query).getQuery();
@@ -2119,4 +2133,19 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
             assertThat(prefixQuery.getRewriteMethod(), instanceOf(MultiTermQuery.TopTermsBlendedFreqScoringRewrite.class));
         }
     }
+
+    @Test
+    public void testSimpleQueryStringNoFields() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String queryText = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
+        String query = "{\n" +
+                "    \"simple_query_string\" : {\n" +
+                "        \"query\" : \"" + queryText + "\"\n" +
+                "    }\n" +
+                "}";
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(TermQuery.class));
+        TermQuery termQuery = (TermQuery) parsedQuery;
+        assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, queryText)));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
deleted file mode 100644
index 5ae54d4..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
+++ /dev/null
@@ -1,330 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class SimpleQueryStringBuilderTests extends AbstractQueryTestCase<SimpleQueryStringBuilder> {
-
-    @Override
-    protected SimpleQueryStringBuilder doCreateTestQueryBuilder() {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            result.analyzeWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lowercaseExpandedTerms(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.locale(randomLocale(getRandom()));
-        }
-        if (randomBoolean()) {
-            result.minimumShouldMatch(randomMinimumShouldMatch());
-        }
-        if (randomBoolean()) {
-            result.analyzer(randomAnalyzer());
-        }
-        if (randomBoolean()) {
-            result.defaultOperator(randomFrom(Operator.values()));
-        }
-        if (randomBoolean()) {
-            Set<SimpleQueryStringFlag> flagSet = new HashSet<>();
-            int size = randomIntBetween(0, SimpleQueryStringFlag.values().length);
-            for (int i = 0; i < size; i++) {
-                flagSet.add(randomFrom(SimpleQueryStringFlag.values()));
-            }
-            if (flagSet.size() > 0) {
-                result.flags(flagSet.toArray(new SimpleQueryStringFlag[flagSet.size()]));
-            }
-        }
-
-        int fieldCount = randomIntBetween(0, 10);
-        Map<String, Float> fields = new HashMap<>();
-        for (int i = 0; i < fieldCount; i++) {
-            if (randomBoolean()) {
-                fields.put(randomAsciiOfLengthBetween(1, 10), AbstractQueryBuilder.DEFAULT_BOOST);
-            } else {
-                fields.put(randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10), 2.0f / randomIntBetween(1, 20));
-            }
-        }
-        result.fields(fields);
-
-        return result;
-    }
-
-    @Test
-    public void testDefaults() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-
-        assertEquals("Wrong default default boost.", AbstractQueryBuilder.DEFAULT_BOOST, qb.boost(), 0.001);
-        assertEquals("Wrong default default boost field.", AbstractQueryBuilder.DEFAULT_BOOST, SimpleQueryStringBuilder.DEFAULT_BOOST,
-                0.001);
-
-        assertEquals("Wrong default flags.", SimpleQueryStringFlag.ALL.value, qb.flags());
-        assertEquals("Wrong default flags field.", SimpleQueryStringFlag.ALL.value(), SimpleQueryStringBuilder.DEFAULT_FLAGS);
-
-        assertEquals("Wrong default default operator.", Operator.OR, qb.defaultOperator());
-        assertEquals("Wrong default default operator field.", Operator.OR, SimpleQueryStringBuilder.DEFAULT_OPERATOR);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-
-        assertEquals("Wrong default default analyze_wildcard.", false, qb.analyzeWildcard());
-        assertEquals("Wrong default default analyze_wildcard field.", false, SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD);
-
-        assertEquals("Wrong default default lowercase_expanded_terms.", true, qb.lowercaseExpandedTerms());
-        assertEquals("Wrong default default lowercase_expanded_terms field.", true,
-                SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS);
-
-        assertEquals("Wrong default default lenient.", false, qb.lenient());
-        assertEquals("Wrong default default lenient field.", false, SimpleQueryStringBuilder.DEFAULT_LENIENT);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-    }
-
-    @Test
-    public void testDefaultNullLocale() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.locale(null);
-        assertEquals("Setting locale to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_LOCALE,
-                qb.locale());
-    }
-
-    @Test
-    public void testDefaultNullComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags((SimpleQueryStringFlag[]) null);
-        assertEquals("Setting flags to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_FLAGS,
-                qb.flags());
-    }
-
-    @Test
-    public void testDefaultEmptyComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags(new SimpleQueryStringFlag[]{});
-        assertEquals("Setting flags to empty should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_FLAGS,
-                qb.flags());
-    }
-
-    @Test
-    public void testDefaultNullComplainOp() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.defaultOperator(null);
-        assertEquals("Setting operator to null should result in returning to default value.", SimpleQueryStringBuilder.DEFAULT_OPERATOR,
-                qb.defaultOperator());
-    }
-
-    // Check operator handling, and default field handling.
-    @Test
-    public void testDefaultOperatorHandling() throws IOException {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.").field(STRING_FIELD_NAME);
-        QueryShardContext shardContext = createShardContext();
-        shardContext.setAllowUnmappedFields(true); // to avoid occasional cases
-                                                   // in setup where we didn't
-                                                   // add types but strict field
-                                                   // resolution
-        BooleanQuery boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-
-        qb.defaultOperator(Operator.AND);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(0));
-
-        qb.defaultOperator(Operator.OR);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-    }
-
-    @Test
-    public void testIllegalConstructorArg() {
-        try {
-            new SimpleQueryStringBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNullAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null, AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmpty() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("");
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmptyAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("", AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    /**
-     * The following should fail fast - never silently set the map containing
-     * fields and weights to null but refuse to accept null instead.
-     * */
-    @Test(expected = NullPointerException.class)
-    public void testFieldsCannotBeSetToNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.fields(null);
-    }
-
-    @Test
-    public void testDefaultFieldParsing() throws IOException {
-        QueryParseContext context = createParseContext();
-        String query = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
-        String contentString = "{\n" +
-                "    \"simple_query_string\" : {\n" +
-                "      \"query\" : \"" + query + "\"" +
-                "    }\n" +
-                "}";
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        context.reset(parser);
-        SimpleQueryStringBuilder queryBuilder = new SimpleQueryStringParser().fromXContent(context);
-        assertThat(queryBuilder.value(), equalTo(query));
-        assertThat(queryBuilder.fields(), notNullValue());
-        assertThat(queryBuilder.fields().size(), equalTo(0));
-        QueryShardContext shardContext = createShardContext();
-
-        // the remaining tests requires either a mapping that we register with types in base test setup
-        // no strict field resolution (version before V_1_4_0_Beta1)
-        if (getCurrentTypes().length > 0 || shardContext.indexQueryParserService().getIndexCreatedVersion().before(Version.V_1_4_0_Beta1)) {
-            Query luceneQuery = queryBuilder.toQuery(shardContext);
-            assertThat(luceneQuery, instanceOf(TermQuery.class));
-            TermQuery termQuery = (TermQuery) luceneQuery;
-            assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, query)));
-        }
-    }
-
-    /*
-     * This assumes that Lucene query parsing is being checked already, adding
-     * checks only for our parsing extensions.
-     *
-     * Also this relies on {@link SimpleQueryStringTests} to test most of the
-     * actual functionality of query parsing.
-     */
-    @Override
-    protected void doAssertLuceneQuery(SimpleQueryStringBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, notNullValue());
-
-        if ("".equals(queryBuilder.value())) {
-            assertTrue("Query should have been MatchNoDocsQuery but was " + query.getClass().getName(), query instanceof MatchNoDocsQuery);
-        } else if (queryBuilder.fields().size() > 1) {
-            assertTrue("Query should have been BooleanQuery but was " + query.getClass().getName(), query instanceof BooleanQuery);
-
-            BooleanQuery boolQuery = (BooleanQuery) query;
-            if (queryBuilder.lowercaseExpandedTerms()) {
-                for (BooleanClause clause : boolQuery.clauses()) {
-                    if (clause.getQuery() instanceof TermQuery) {
-                        TermQuery inner = (TermQuery) clause.getQuery();
-                        assertThat(inner.getTerm().bytes().toString(), is(inner.getTerm().bytes().toString().toLowerCase(Locale.ROOT)));
-                    }
-                }
-            }
-
-            assertThat(boolQuery.clauses().size(), equalTo(queryBuilder.fields().size()));
-            Iterator<String> fields = queryBuilder.fields().keySet().iterator();
-            for (BooleanClause booleanClause : boolQuery) {
-                assertThat(booleanClause.getQuery(), instanceOf(TermQuery.class));
-                TermQuery termQuery = (TermQuery) booleanClause.getQuery();
-                assertThat(termQuery.getTerm().field(), equalTo(fields.next()));
-                assertThat(termQuery.getTerm().text().toLowerCase(Locale.ROOT), equalTo(queryBuilder.value().toLowerCase(Locale.ROOT)));
-            }
-
-            if (queryBuilder.minimumShouldMatch() != null) {
-                assertThat(boolQuery.getMinimumNumberShouldMatch(), greaterThan(0));
-            }
-        } else if (queryBuilder.fields().size() <= 1) {
-            assertTrue("Query should have been TermQuery but was " + query.getClass().getName(), query instanceof TermQuery);
-
-            TermQuery termQuery = (TermQuery) query;
-            String field;
-            if (queryBuilder.fields().size() == 0) {
-                field = MetaData.ALL;
-            } else {
-                field = queryBuilder.fields().keySet().iterator().next();
-            }
-            assertThat(termQuery.getTerm().field(), equalTo(field));
-            assertThat(termQuery.getTerm().text().toLowerCase(Locale.ROOT), equalTo(queryBuilder.value().toLowerCase(Locale.ROOT)));
-        } else {
-            fail("Encountered lucene query type we do not have a validation implementation for in our " + SimpleQueryStringBuilderTests.class.getSimpleName());
-        }
-    }
-
-    @Override
-    protected void assertBoost(SimpleQueryStringBuilder queryBuilder, Query query) throws IOException {
-        //boost may get parsed from the random query, we then combine the main boost with that one coming from lucene
-        //instead of trying to reparse the query and guess what the boost should be, we delegate boost checks to specific boost tests below
-    }
-
-
-    private int shouldClauses(BooleanQuery query) {
-        int result = 0;
-        for (BooleanClause c : query.clauses()) {
-            if (c.getOccur() == BooleanClause.Occur.SHOULD) {
-                result++;
-            }
-        }
-        return result;
-    }
-
-    @Test
-    public void testToQueryBoost() throws IOException {
-        assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length > 0);
-        QueryShardContext shardContext = createShardContext();
-        SimpleQueryStringBuilder simpleQueryStringBuilder = new SimpleQueryStringBuilder("test");
-        simpleQueryStringBuilder.field(STRING_FIELD_NAME, 5);
-        Query query = simpleQueryStringBuilder.toQuery(shardContext);
-        assertThat(query, instanceOf(TermQuery.class));
-        assertThat(query.getBoost(), equalTo(5f));
-
-        simpleQueryStringBuilder = new SimpleQueryStringBuilder("test");
-        simpleQueryStringBuilder.field(STRING_FIELD_NAME, 5);
-        simpleQueryStringBuilder.boost(2);
-        query = simpleQueryStringBuilder.toQuery(shardContext);
-        assertThat(query, instanceOf(TermQuery.class));
-        assertThat(query.getBoost(), equalTo(10f));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTests.java
deleted file mode 100644
index ff5882a..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanContainingQueryBuilderTests extends AbstractQueryTestCase<SpanContainingQueryBuilder> {
-
-    @Override
-    protected SpanContainingQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(2);
-        return new SpanContainingQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanContainingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanContainingQuery.class));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new SpanContainingQueryBuilder(null, SpanTermQueryBuilder.PROTOTYPE);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new SpanContainingQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTests.java
deleted file mode 100644
index 325db41..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTests.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanFirstQueryBuilderTests extends AbstractQueryTestCase<SpanFirstQueryBuilder> {
-
-    @Override
-    protected SpanFirstQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(1);
-        return new SpanFirstQueryBuilder(spanTermQueries[0], randomIntBetween(0, 1000));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanFirstQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanFirstQuery.class));
-    }
-
-    /**
-     * test exception on missing `end` and `match` parameter in parser
-     */
-    @Test
-    public void testParseEnd() throws IOException {
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanFirstQueryBuilder.NAME);
-            builder.field("match");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("missing [end] parameter should raise exception");
-            } catch (ParsingException e) {
-                assertTrue(e.getMessage().contains("spanFirst must have [end] set"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanFirstQueryBuilder.NAME);
-            builder.field("end", 10);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("missing [match] parameter should raise exception");
-            } catch (ParsingException e) {
-                assertTrue(e.getMessage().contains("spanFirst must have [match] span query clause"));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTests.java
deleted file mode 100644
index 7c9e50a..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTests.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanMultiTermQueryBuilderTests extends AbstractQueryTestCase<SpanMultiTermQueryBuilder> {
-
-    @Override
-    protected SpanMultiTermQueryBuilder doCreateTestQueryBuilder() {
-        MultiTermQueryBuilder multiTermQueryBuilder = RandomQueryBuilder.createMultiTermQuery(random());
-        return new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanMultiTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanMultiTermQueryWrapper.class));
-        SpanMultiTermQueryWrapper spanMultiTermQueryWrapper = (SpanMultiTermQueryWrapper) query;
-        Query multiTermQuery = queryBuilder.innerQuery().toQuery(context);
-        assertThat(multiTermQuery, instanceOf(MultiTermQuery.class));
-        assertThat(spanMultiTermQueryWrapper.getWrappedQuery(), equalTo(new SpanMultiTermQueryWrapper<>((MultiTermQuery)multiTermQuery).getWrappedQuery()));
-    }
-
-    @Test
-    public void testIllegalArgument() {
-        try {
-            new SpanMultiTermQueryBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    /**
-     * test checks that we throw an {@link UnsupportedOperationException} if the query wrapped
-     * by {@link SpanMultiTermQueryBuilder} does not generate a lucene {@link MultiTermQuery}.
-     * This is currently the case for {@link RangeQueryBuilder} when the target field is mapped
-     * to a date.
-     */
-    @Test
-    public void testUnsupportedInnerQueryType() throws IOException {
-        QueryShardContext context = createShardContext();
-        // test makes only sense if we have at least one type registered with date field mapping
-        if (getCurrentTypes().length > 0 && context.fieldMapper(DATE_FIELD_NAME) != null) {
-            try {
-                RangeQueryBuilder query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                new SpanMultiTermQueryBuilder(query).toQuery(createShardContext());
-                fail("Exception expected, range query on date fields should not generate a lucene " + MultiTermQuery.class.getName());
-            } catch (UnsupportedOperationException e) {
-                assert(e.getMessage().contains("unsupported inner query, should be " + MultiTermQuery.class.getName()));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTests.java
deleted file mode 100644
index 02dcddb..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTests.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanNearQueryBuilderTests extends AbstractQueryTestCase<SpanNearQueryBuilder> {
-
-    @Override
-    protected SpanNearQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(spanTermQueries[0], randomIntBetween(-10, 10));
-        for (int i = 1; i < spanTermQueries.length; i++) {
-            queryBuilder.clause(spanTermQueries[i]);
-        }
-        queryBuilder.inOrder(randomBoolean());
-        queryBuilder.collectPayloads(randomBoolean());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNearQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNearQuery.class));
-        SpanNearQuery spanNearQuery = (SpanNearQuery) query;
-        assertThat(spanNearQuery.getSlop(), equalTo(queryBuilder.slop()));
-        assertThat(spanNearQuery.isInOrder(), equalTo(queryBuilder.inOrder()));
-        assertThat(spanNearQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanNearQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new SpanNearQueryBuilder(null, 1);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // ecpected
-        }
-
-        try {
-            SpanNearQueryBuilder spanNearQueryBuilder = new SpanNearQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, 1);
-            spanNearQueryBuilder.clause(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // ecpected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTests.java
deleted file mode 100644
index 1b711f1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTests.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.Matchers.*;
-
-public class SpanNotQueryBuilderTests extends AbstractQueryTestCase<SpanNotQueryBuilder> {
-
-    @Override
-    protected SpanNotQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(2);
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-        if (randomBoolean()) {
-            // also test negative values, they should implicitly be changed to 0
-            queryBuilder.dist(randomIntBetween(-2, 10));
-        } else {
-            if (randomBoolean()) {
-                queryBuilder.pre(randomIntBetween(-2, 10));
-            }
-            if (randomBoolean()) {
-                queryBuilder.post(randomIntBetween(-2, 10));
-            }
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNotQuery.class));
-        SpanNotQuery spanNotQuery = (SpanNotQuery) query;
-        assertThat(spanNotQuery.getExclude(), equalTo(queryBuilder.excludeQuery().toQuery(context)));
-        assertThat(spanNotQuery.getInclude(), equalTo(queryBuilder.includeQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testIllegalArgument() {
-        try {
-            new SpanNotQueryBuilder(null, SpanTermQueryBuilder.PROTOTYPE);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-        try {
-            new SpanNotQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testDist() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(4);
-        assertThat(builder.pre(), equalTo(4));
-        assertThat(builder.post(), equalTo(4));
-    }
-
-    @Test
-    public void testPrePost() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(-4).post(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(1).post(2);
-        assertThat(builder.pre(), equalTo(1));
-        assertThat(builder.post(), equalTo(2));
-    }
-
-    /**
-     * test correct parsing of `dist` parameter, this should create builder with pre/post set to same value
-     */
-    @Test
-    public void testParseDist() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanNotQueryBuilder.NAME);
-        builder.field("exclude");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.field("include");
-        spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-        builder.field("dist", 3);
-        builder.endObject();
-        builder.endObject();
-        SpanNotQueryBuilder query = (SpanNotQueryBuilder)parseQuery(builder.string());
-        assertThat(query.pre(), equalTo(3));
-        assertThat(query.post(), equalTo(3));
-        assertNotNull(query.includeQuery());
-        assertNotNull(query.excludeQuery());
-    }
-
-    /**
-     * test exceptions for three types of broken json, missing include / exclude and both dist and pre/post specified
-     */
-    @Test
-    public void testParserExceptions() throws IOException {
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("ParsingException should have been caught");
-            } catch (ParsingException e) {
-                assertThat("ParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [include]"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                    .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("ParsingException should have been caught");
-            } catch (ParsingException e) {
-                assertThat("ParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [exclude]"));
-            }
-        }
-
-        {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                    .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.field("pre", 2);
-            builder.endObject();
-            builder.endObject();
-
-            try {
-                parseQuery(builder.string());
-                fail("ParsingException should have been caught");
-            } catch (ParsingException e) {
-                assertThat("ParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTests.java
deleted file mode 100644
index eaa7035..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTests.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanOrQueryBuilderTests extends AbstractQueryTestCase<SpanOrQueryBuilder> {
-
-    @Override
-    protected SpanOrQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder(spanTermQueries[0]);
-        for (int i = 1; i < spanTermQueries.length; i++) {
-            queryBuilder.clause(spanTermQueries[i]);
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanOrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanOrQuery.class));
-        SpanOrQuery spanOrQuery = (SpanOrQuery) query;
-        assertThat(spanOrQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanOrQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new SpanOrQueryBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            SpanOrQueryBuilder spanOrBuilder = new SpanOrQueryBuilder(SpanTermQueryBuilder.PROTOTYPE);
-            spanOrBuilder.clause(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTests.java
deleted file mode 100644
index a51efc6..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanTermQueryBuilderTests extends AbstractTermQueryTestCase<SpanTermQueryBuilder> {
-
-    @Override
-    protected SpanTermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanTermQuery.class));
-        SpanTermQuery spanTermQuery = (SpanTermQuery) query;
-        assertThat(spanTermQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-
-    /**
-     * @param amount the number of clauses that will be returned
-     * @return an array of random {@link SpanTermQueryBuilder} with same field name
-     */
-    public SpanTermQueryBuilder[] createSpanTermQueryBuilders(int amount) {
-        SpanTermQueryBuilder[] clauses = new SpanTermQueryBuilder[amount];
-        SpanTermQueryBuilder first = createTestQueryBuilder();
-        clauses[0] = first;
-        for (int i = 1; i < amount; i++) {
-            // we need same field name in all clauses, so we only randomize value
-            SpanTermQueryBuilder spanTermQuery = new SpanTermQueryBuilder(first.fieldName(), getRandomValueForFieldName(first.fieldName()));
-            if (randomBoolean()) {
-                spanTermQuery.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                spanTermQuery.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-            clauses[i] = spanTermQuery;
-        }
-        return clauses;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTests.java
deleted file mode 100644
index 87b2380..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanWithinQueryBuilderTests extends AbstractQueryTestCase<SpanWithinQueryBuilder> {
-
-    @Override
-    protected SpanWithinQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTests().createSpanTermQueryBuilders(2);
-        return new SpanWithinQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanWithinQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanWithinQuery.class));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            new SpanWithinQueryBuilder(null, SpanTermQueryBuilder.PROTOTYPE);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new SpanWithinQueryBuilder(SpanTermQueryBuilder.PROTOTYPE, null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index 3d89633..647ac44 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -16,62 +16,23 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
-import org.junit.BeforeClass;
+import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQueryBuilder> {
-
-    /**
-     * The query type all template tests will be based on.
-     */
-    private static QueryBuilder<?> templateBase;
-
-    @BeforeClass
-    public static void setupClass() {
-        templateBase = RandomQueryBuilder.createQuery(getRandom());
-    }
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    @Override
-    protected TemplateQueryBuilder doCreateTestQueryBuilder() {
-        return new TemplateQueryBuilder(new Template(templateBase.toString()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TemplateQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertEquals(templateBase.toQuery(context), query);
-    }
-
-    @Test
-    public void testIllegalArgument() {
-        try {
-            new TemplateQueryBuilder(null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Override
-    protected void assertBoost(TemplateQueryBuilder queryBuilder, Query query) throws IOException {
-        //no-op boost is checked already above as part of doAssertLuceneQuery as we rely on lucene equals impl
-    }
+/**
+ * Test building and serialising a template search request.
+ * */
+public class TemplateQueryBuilderTests extends ESTestCase {
 
     @Test
     public void testJSONGeneration() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
index d0ac5cb..6527e11 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.Version;
-import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.ParsingException;
@@ -56,9 +55,6 @@ import org.junit.Before;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.lang.reflect.InvocationHandler;
-import java.lang.reflect.Method;
-import java.lang.reflect.Proxy;
 
 /**
  * Test parsing and executing a template request.
@@ -67,7 +63,7 @@ import java.lang.reflect.Proxy;
 public class TemplateQueryParserTests extends ESTestCase {
 
     private Injector injector;
-    private QueryShardContext context;
+    private QueryParseContext context;
 
     @Before
     public void setup() throws IOException {
@@ -77,11 +73,7 @@ public class TemplateQueryParserTests extends ESTestCase {
                 .put("name", getClass().getName())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
-        final Client proxy = (Client) Proxy.newProxyInstance(
-                Client.class.getClassLoader(),
-                new Class[]{Client.class}, (proxy1, method, args) -> {
-                    throw new UnsupportedOperationException("client is just a dummy");
-                });
+
         Index index = new Index("test");
         injector = new ModulesBuilder().add(
                 new EnvironmentModule(new Environment(settings)),
@@ -103,7 +95,6 @@ public class TemplateQueryParserTests extends ESTestCase {
                 new AbstractModule() {
                     @Override
                     protected void configure() {
-                        bind(Client.class).toInstance(proxy); // not needed here
                         Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
                         bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
                         bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
@@ -112,7 +103,7 @@ public class TemplateQueryParserTests extends ESTestCase {
         ).createInjector();
 
         IndexQueryParserService queryParserService = injector.getInstance(IndexQueryParserService.class);
-        context = new QueryShardContext(index, queryParserService);
+        context = new QueryParseContext(index, queryParserService);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTests.java
deleted file mode 100644
index b2a8410..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTests.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class TermQueryBuilderTests extends AbstractTermQueryTestCase<TermQueryBuilder> {
-
-    /**
-     * @return a TermQuery with random field name and value, optional random boost and queryname
-     */
-    @Override
-    protected TermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(TermQuery.class));
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(termQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(termQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTests.java
deleted file mode 100644
index 70d0c18..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermsQueryBuilderTests.java
+++ /dev/null
@@ -1,280 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.get.GetRequest;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
-import org.hamcrest.Matchers;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class TermsQueryBuilderTests extends AbstractQueryTestCase<TermsQueryBuilder> {
-
-    private List<Object> randomTerms;
-    private String termsPath;
-
-    @Before
-    public void randomTerms() {
-        List<Object> randomTerms = new ArrayList<>();
-        String[] strings = generateRandomStringArray(10, 10, false, true);
-        for (String string : strings) {
-            randomTerms.add(string);
-            if (rarely()) {
-                randomTerms.add(null);
-            }
-        }
-        this.randomTerms = randomTerms;
-        termsPath = randomAsciiOfLength(10).replace('.', '_');
-    }
-
-    @Override
-    protected TermsQueryBuilder doCreateTestQueryBuilder() {
-        TermsQueryBuilder query;
-        // terms query or lookup query
-        if (randomBoolean()) {
-            // make between 0 and 5 different values of the same type
-            String fieldName = getRandomFieldName();
-            Object[] values = new Object[randomInt(5)];
-            for (int i = 0; i < values.length; i++) {
-                values[i] = getRandomValueForFieldName(fieldName);
-            }
-            query = new TermsQueryBuilder(fieldName, values);
-        } else {
-            // right now the mock service returns us a list of strings
-            query = new TermsQueryBuilder(randomBoolean() ? randomAsciiOfLengthBetween(1,10) : STRING_FIELD_NAME, randomTermsLookup());
-        }
-        return query;
-    }
-
-    private TermsLookup randomTermsLookup() {
-        return new TermsLookup(randomBoolean() ? randomAsciiOfLength(10) : null, randomAsciiOfLength(10), randomAsciiOfLength(10),
-                termsPath).routing(randomBoolean() ? randomAsciiOfLength(10) : null);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(BooleanQuery.class));
-        BooleanQuery booleanQuery = (BooleanQuery) query;
-
-        // we only do the check below for string fields (otherwise we'd have to decode the values)
-        if (queryBuilder.fieldName().equals(INT_FIELD_NAME) || queryBuilder.fieldName().equals(DOUBLE_FIELD_NAME)
-                || queryBuilder.fieldName().equals(BOOLEAN_FIELD_NAME) || queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            return;
-        }
-
-        // expected returned terms depending on whether we have a terms query or a terms lookup query
-        List<Object> terms;
-        if (queryBuilder.termsLookup() != null) {
-            terms = randomTerms;
-        } else {
-            terms = queryBuilder.values();
-        }
-
-        // compare whether we have the expected list of terms returned
-        final List<Term> booleanTerms = new ArrayList<>();
-        for (BooleanClause booleanClause : booleanQuery) {
-            assertThat(booleanClause.getQuery(), instanceOf(TermQuery.class));
-            Term term = ((TermQuery) booleanClause.getQuery()).getTerm();
-            booleanTerms.add(term);
-        }
-        CollectionUtil.timSort(booleanTerms);
-        List<Term> expectedTerms = new ArrayList<>();
-        for (Object term : terms) {
-            if (term != null) { // terms lookup filters this out
-                expectedTerms.add(new Term(queryBuilder.fieldName(), term.toString()));
-            }
-        }
-        CollectionUtil.timSort(expectedTerms);
-        assertEquals(expectedTerms + " vs. " + booleanTerms, expectedTerms.size(), booleanTerms.size());
-        assertEquals(expectedTerms + " vs. " + booleanTerms, expectedTerms, booleanTerms);
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testEmtpyFieldName() {
-        if (randomBoolean()) {
-            new TermsQueryBuilder(null, "term");
-        } else {
-            new TermsQueryBuilder("", "term");
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testEmtpyTermsLookup() {
-        new TermsQueryBuilder("field", (TermsLookup) null);
-    }
-
-    @Test
-    public void testNullValues() {
-        try {
-            switch (randomInt(6)) {
-                case 0:
-                    new TermsQueryBuilder("field", (String[]) null);
-                    break;
-                case 1:
-                    new TermsQueryBuilder("field", (int[]) null);
-                    break;
-                case 2:
-                    new TermsQueryBuilder("field", (long[]) null);
-                    break;
-                case 3:
-                    new TermsQueryBuilder("field", (float[]) null);
-                    break;
-                case 4:
-                    new TermsQueryBuilder("field", (double[]) null);
-                    break;
-                case 5:
-                    new TermsQueryBuilder("field", (Object[]) null);
-                    break;
-                default:
-                    new TermsQueryBuilder("field", (Iterable<?>) null);
-                    break;
-            }
-            fail("should have failed with IllegalArgumentException");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.containsString("No value specified for terms query"));
-        }
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testBothValuesAndLookupSet() throws IOException {
-        String query = "{\n" +
-                "  \"terms\": {\n" +
-                "    \"field\": [\n" +
-                "      \"blue\",\n" +
-                "      \"pill\"\n" +
-                "    ],\n" +
-                "    \"field_lookup\": {\n" +
-                "      \"index\": \"pills\",\n" +
-                "      \"type\": \"red\",\n" +
-                "      \"id\": \"3\",\n" +
-                "      \"path\": \"white rabbit\"\n" +
-                "    }\n" +
-                "  }\n" +
-                "}";
-        QueryBuilder termsQueryBuilder = parseQuery(query);
-    }
-
-    public void testDeprecatedXContent() throws IOException {
-        String query = "{\n" +
-                "  \"terms\": {\n" +
-                "    \"field\": [\n" +
-                "      \"blue\",\n" +
-                "      \"pill\"\n" +
-                "    ],\n" +
-                "    \"disable_coord\": true\n" +
-                "  }\n" +
-                "}";
-        try {
-            parseQuery(query);
-            fail("disable_coord is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [disable_coord] used, replaced by [Use [bool] query instead]", ex.getMessage());
-        }
-
-        TermsQueryBuilder queryBuilder = (TermsQueryBuilder) parseQuery(query, ParseFieldMatcher.EMPTY);
-        TermsQueryBuilder copy = assertSerialization(queryBuilder);
-        assertTrue(queryBuilder.disableCoord());
-        assertTrue(copy.disableCoord());
-
-        String randomMinShouldMatch = RandomPicks.randomFrom(random(), Arrays.asList("min_match", "min_should_match", "minimum_should_match"));
-        query = "{\n" +
-                "  \"terms\": {\n" +
-                "    \"field\": [\n" +
-                "      \"blue\",\n" +
-                "      \"pill\"\n" +
-                "    ],\n" +
-                "    \"" + randomMinShouldMatch +"\": \"42%\"\n" +
-                "  }\n" +
-                "}";
-        try {
-            parseQuery(query);
-            fail(randomMinShouldMatch + " is deprecated");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Deprecated field [" + randomMinShouldMatch + "] used, replaced by [Use [bool] query instead]", ex.getMessage());
-        }
-        queryBuilder = (TermsQueryBuilder) parseQuery(query, ParseFieldMatcher.EMPTY);
-        copy = assertSerialization(queryBuilder);
-        assertEquals("42%", queryBuilder.minimumShouldMatch());
-        assertEquals("42%", copy.minimumShouldMatch());
-    }
-
-    @Override
-    public GetResponse executeGet(GetRequest getRequest) {
-        String json;
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-            builder.startObject();
-            builder.array(termsPath, randomTerms.toArray(new Object[0]));
-            builder.endObject();
-            json = builder.string();
-        } catch (IOException ex) {
-            throw new ElasticsearchException("boom", ex);
-        }
-        return new GetResponse(new GetResult(getRequest.index(), getRequest.type(), getRequest.id(), 0, true, new BytesArray(json), null));
-    }
-
-    public void testNumeric() throws IOException {
-        {
-            TermsQueryBuilder builder = new TermsQueryBuilder("foo", new int[]{1, 3, 4});
-            TermsQueryBuilder copy = assertSerialization(builder);
-            List<Object> values = copy.values();
-            assertEquals(Arrays.asList(1, 3, 4), values);
-        }
-        {
-            TermsQueryBuilder builder = new TermsQueryBuilder("foo", new double[]{1, 3, 4});
-            TermsQueryBuilder copy = assertSerialization(builder);
-            List<Object> values = copy.values();
-            assertEquals(Arrays.asList(1d, 3d, 4d), values);
-        }
-        {
-            TermsQueryBuilder builder = new TermsQueryBuilder("foo", new float[]{1, 3, 4});
-            TermsQueryBuilder copy = assertSerialization(builder);
-            List<Object> values = copy.values();
-            assertEquals(Arrays.asList(1f, 3f, 4f), values);
-        }
-        {
-            TermsQueryBuilder builder = new TermsQueryBuilder("foo", new long[]{1, 3, 4});
-            TermsQueryBuilder copy = assertSerialization(builder);
-            List<Object> values = copy.values();
-            assertEquals(Arrays.asList(1l, 3l, 4l), values);
-        }
-    }
-}
-
diff --git a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTests.java
deleted file mode 100644
index af5c63c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTests.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.*;
-
-public class TypeQueryBuilderTests extends AbstractQueryTestCase<TypeQueryBuilder> {
-
-    @Override
-    protected TypeQueryBuilder doCreateTestQueryBuilder() {
-        return new TypeQueryBuilder(getRandomType());
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TypeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(ConstantScoreQuery.class)));
-        if (query instanceof ConstantScoreQuery) {
-            query = ((ConstantScoreQuery) query).getQuery();
-            assertThat(query, instanceOf(TermQuery.class));
-        }
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(TypeFieldMapper.NAME));
-        assertThat(termQuery.getTerm().text(), equalTo(queryBuilder.type()));
-    }
-
-    @Test
-    public void testIllegalArgument() {
-        try {
-            new TypeQueryBuilder((String) null);
-            fail("cannot be null");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTests.java
deleted file mode 100644
index 9f6a564..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-
-public class WildcardQueryBuilderTests extends AbstractQueryTestCase<WildcardQueryBuilder> {
-
-    @Override
-    protected WildcardQueryBuilder doCreateTestQueryBuilder() {
-        WildcardQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new WildcardQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new WildcardQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WildcardQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(WildcardQuery.class));
-    }
-
-    @Test
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new WildcardQueryBuilder(null, "text");
-            } else {
-                new WildcardQueryBuilder("", "text");
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new WildcardQueryBuilder("field", null);
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    @Test
-    public void testEmptyValue() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(getRandomType(), "");
-        assertEquals(wildcardQueryBuilder.toQuery(context).getClass(), WildcardQuery.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java
deleted file mode 100644
index fff8de7..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WrapperQueryBuilderTests.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class WrapperQueryBuilderTests extends AbstractQueryTestCase<WrapperQueryBuilder> {
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    @Override
-    protected WrapperQueryBuilder doCreateTestQueryBuilder() {
-        QueryBuilder wrappedQuery = RandomQueryBuilder.createQuery(random());
-        switch (randomInt(2)) {
-            case 0:
-                return new WrapperQueryBuilder(wrappedQuery.toString());
-            case 1:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes().toBytes());
-            case 2:
-                return new WrapperQueryBuilder(wrappedQuery.buildAsBytes());
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WrapperQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        try (XContentParser qSourceParser = XContentFactory.xContent(queryBuilder.source()).createParser(queryBuilder.source())) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            QueryBuilder<?> innerQuery = contextCopy.parseContext().parseInnerQueryBuilder();
-            Query expected = innerQuery.toQuery(context);
-            assertThat(query, equalTo(expected));
-        }
-    }
-
-    @Override
-    protected void assertBoost(WrapperQueryBuilder queryBuilder, Query query) throws IOException {
-        //no-op boost is checked already above as part of doAssertLuceneQuery as we rely on lucene equals impl
-    }
-
-    @Test
-    public void testIllegalArgument() {
-        try {
-            if (randomBoolean()) {
-                new WrapperQueryBuilder((byte[]) null);
-            } else {
-                new WrapperQueryBuilder(new byte[0]);
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            if (randomBoolean()) {
-                new WrapperQueryBuilder((String) null);
-            } else {
-                new WrapperQueryBuilder("");
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            if (randomBoolean()) {
-                new WrapperQueryBuilder((BytesReference) null);
-            } else {
-                new WrapperQueryBuilder(new BytesArray(new byte[0]));
-            }
-            fail("cannot be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index 7f27ee1..0ad8b53 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -25,7 +25,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.plugins.Plugin;
 
@@ -47,41 +50,24 @@ public class DummyQueryParserPlugin extends Plugin {
         module.registerQueryParser(DummyQueryParser.class);
     }
 
-    public static class DummyQueryBuilder extends AbstractQueryBuilder<DummyQueryBuilder> {
-        private static final String NAME = "dummy";
-
+    public static class DummyQueryBuilder extends QueryBuilder {
         @Override
         protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAME).endObject();
-        }
-
-        @Override
-        protected Query doToQuery(QueryShardContext context) throws IOException {
-            return new DummyQuery(context.isFilter());
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.startObject("dummy").endObject();
         }
     }
 
-    public static class DummyQueryParser extends BaseQueryParser {
+    public static class DummyQueryParser implements QueryParser {
         @Override
         public String[] names() {
-            return new String[]{DummyQueryBuilder.NAME};
+            return new String[]{"dummy"};
         }
 
         @Override
-        public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+        public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
             XContentParser.Token token = parseContext.parser().nextToken();
             assert token == XContentParser.Token.END_OBJECT;
-            return new DummyQueryBuilder();
-        }
-
-        @Override
-        public DummyQueryBuilder getBuilderPrototype() {
-            return new DummyQueryBuilder();
+            return new DummyQuery(parseContext.isFilter());
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/support/QueryInnerHitsTests.java b/core/src/test/java/org/elasticsearch/index/query/support/QueryInnerHitsTests.java
deleted file mode 100644
index 2c4e317..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/support/QueryInnerHitsTests.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query.support;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-public class QueryInnerHitsTests extends ESTestCase {
-
-    public void testSerialize() throws IOException {
-        copyAndAssert(new QueryInnerHits());
-        copyAndAssert(new QueryInnerHits("foo", new InnerHitsBuilder.InnerHit()));
-        copyAndAssert(new QueryInnerHits("foo", null));
-        copyAndAssert(new QueryInnerHits("foo", new InnerHitsBuilder.InnerHit().setSize(randomIntBetween(0, 100))));
-    }
-
-    public void testToXContent() throws IOException {
-        assertJson("{\"inner_hits\":{}}", new QueryInnerHits());
-        assertJson("{\"inner_hits\":{\"name\":\"foo\"}}", new QueryInnerHits("foo", new InnerHitsBuilder.InnerHit()));
-        assertJson("{\"inner_hits\":{\"name\":\"bar\"}}", new QueryInnerHits("bar", null));
-        assertJson("{\"inner_hits\":{\"name\":\"foo\",\"size\":42}}", new QueryInnerHits("foo", new InnerHitsBuilder.InnerHit().setSize(42)));
-        assertJson("{\"inner_hits\":{\"name\":\"boom\",\"from\":66,\"size\":666}}", new QueryInnerHits("boom", new InnerHitsBuilder.InnerHit().setFrom(66).setSize(666)));
-    }
-
-    private void assertJson(String expected, QueryInnerHits hits) throws IOException {
-        QueryInnerHits queryInnerHits = copyAndAssert(hits);
-        String actual;
-        if (randomBoolean()) {
-            actual = oneLineJSON(queryInnerHits);
-        } else {
-            actual = oneLineJSON(hits);
-        }
-        assertEquals(expected, actual);
-        XContentParser parser = hits.getXcontentParser();
-        assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken());
-        QueryInnerHits other = copyAndAssert(new QueryInnerHits(parser));
-        assertEquals(expected, oneLineJSON(other));
-    }
-
-    public QueryInnerHits copyAndAssert(QueryInnerHits hits) throws IOException {
-        BytesStreamOutput out = new BytesStreamOutput();
-        hits.writeTo(out);
-        QueryInnerHits copy = randomBoolean() ? hits.readFrom(StreamInput.wrap(out.bytes())) : new QueryInnerHits(StreamInput.wrap(out.bytes()));
-        assertEquals(copy.toString() + " vs. " + hits.toString(), copy, hits);
-        return copy;
-    }
-
-    private String oneLineJSON(QueryInnerHits hits) throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        hits.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        builder.endObject();
-        return builder.string().trim();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/search/geo/GeoDistanceTests.java b/core/src/test/java/org/elasticsearch/index/search/geo/GeoDistanceTests.java
new file mode 100644
index 0000000..fdb11a0
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/search/geo/GeoDistanceTests.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.search.geo;
+
+import org.elasticsearch.common.geo.GeoDistance;
+import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.unit.DistanceUnit;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.*;
+
+/**
+ */
+public class GeoDistanceTests extends ESTestCase {
+
+    @Test
+    public void testDistanceCheck() {
+        // Note, is within is an approximation, so, even though 0.52 is outside 50mi, we still get "true"
+        GeoDistance.DistanceBoundingCheck check = GeoDistance.distanceBoundingCheck(0, 0, 50, DistanceUnit.MILES);
+        assertThat(check.isWithin(0.5, 0.5), equalTo(true));
+        assertThat(check.isWithin(0.52, 0.52), equalTo(true));
+        assertThat(check.isWithin(1, 1), equalTo(false));
+
+        check = GeoDistance.distanceBoundingCheck(0, 179, 200, DistanceUnit.MILES);
+        assertThat(check.isWithin(0, -179), equalTo(true));
+        assertThat(check.isWithin(0, -178), equalTo(false));
+    }
+
+    @Test
+    public void testArcDistanceVsPlaneInEllipsis() {
+        GeoPoint centre = new GeoPoint(48.8534100, 2.3488000);
+        GeoPoint northernPoint = new GeoPoint(48.8801108681, 2.35152032666);
+        GeoPoint westernPoint = new GeoPoint(48.85265, 2.308896);
+
+        // With GeoDistance.ARC both the northern and western points are within the 4km range
+        assertThat(GeoDistance.ARC.calculate(centre.lat(), centre.lon(), northernPoint.lat(),
+                northernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
+        assertThat(GeoDistance.ARC.calculate(centre.lat(), centre.lon(), westernPoint.lat(),
+                westernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
+
+        // With GeoDistance.PLANE, only the northern point is within the 4km range,
+        // the western point is outside of the range due to the simple math it employs,
+        // meaning results will appear elliptical
+        assertThat(GeoDistance.PLANE.calculate(centre.lat(), centre.lon(), northernPoint.lat(),
+                northernPoint.lon(), DistanceUnit.KILOMETERS), lessThan(4D));
+        assertThat(GeoDistance.PLANE.calculate(centre.lat(), centre.lon(), westernPoint.lat(),
+                westernPoint.lon(), DistanceUnit.KILOMETERS), greaterThan(4D));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 017ce5b..8fd7821 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -85,9 +85,10 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.ToXContent.EMPTY_PARAMS;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java b/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
index fdd8559..d44d7c2 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesModuleTests.java
@@ -23,7 +23,10 @@ import org.apache.lucene.analysis.hunspell.Dictionary;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.ModuleTestCase;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.index.query.TermQueryParser;
 
 import java.io.IOException;
 import java.io.InputStream;
@@ -36,19 +39,8 @@ public class IndicesModuleTests extends ModuleTestCase {
         public String[] names() {
             return new String[] {"fake-query-parser"};
         }
-
-        @Override
-        public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-            return null;
-        }
-
-        @Override
-        public Query parse(QueryShardContext context) throws IOException {
-            return null;
-        }
-
         @Override
-        public QueryBuilder getBuilderPrototype() {
+        public Query parse(QueryParseContext parseContext) throws IOException, ParsingException {
             return null;
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/indices/cache/query/terms/TermsLookupTests.java b/core/src/test/java/org/elasticsearch/indices/cache/query/terms/TermsLookupTests.java
deleted file mode 100644
index 6474547..0000000
--- a/core/src/test/java/org/elasticsearch/indices/cache/query/terms/TermsLookupTests.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.indices.cache.query.terms;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-public class TermsLookupTests extends ESTestCase {
-
-    @Test
-    public void testTermsLookup() {
-        String index = randomAsciiOfLengthBetween(1, 10);
-        String type = randomAsciiOfLengthBetween(1, 10);
-        String id = randomAsciiOfLengthBetween(1, 10);
-        String path = randomAsciiOfLengthBetween(1, 10);
-        String routing = randomAsciiOfLengthBetween(1, 10);
-        TermsLookup termsLookup = new TermsLookup(index, type, id, path);
-        termsLookup.routing(routing);
-        assertEquals(index, termsLookup.index());
-        assertEquals(type, termsLookup.type());
-        assertEquals(id, termsLookup.id());
-        assertEquals(path, termsLookup.path());
-        assertEquals(routing, termsLookup.routing());
-    }
-
-    @Test(expected=IllegalArgumentException.class)
-    public void testIllegalArguments() {
-        String type = randomAsciiOfLength(5);
-        String id = randomAsciiOfLength(5);
-        String path = randomAsciiOfLength(5);
-        switch (randomIntBetween(0, 2)) {
-        case 0:
-            type = null; break;
-        case 1:
-            id = null; break;
-        case 2:
-            path = null; break;
-        }
-        new TermsLookup(null, type, id, path);
-    }
-
-    @Test
-    public void testSerialization() throws IOException {
-        TermsLookup termsLookup = randomTermsLookup();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            termsLookup.writeTo(output);
-            try (StreamInput in = StreamInput.wrap(output.bytes())) {
-                TermsLookup deserializedLookup = TermsLookup.readTermsLookupFrom(in);
-                assertEquals(deserializedLookup, termsLookup);
-                assertEquals(deserializedLookup.hashCode(), termsLookup.hashCode());
-                assertNotSame(deserializedLookup, termsLookup);
-            }
-        }
-    }
-
-    public static TermsLookup randomTermsLookup() {
-        return new TermsLookup(
-                randomBoolean() ? randomAsciiOfLength(10) : null,
-                randomAsciiOfLength(10),
-                randomAsciiOfLength(10),
-                randomAsciiOfLength(10).replace('.', '_')
-        ).routing(randomBoolean() ? randomAsciiOfLength(10) : null);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/nested/SimpleNestedIT.java b/core/src/test/java/org/elasticsearch/nested/SimpleNestedIT.java
index 258c4ad..96db9bf 100644
--- a/core/src/test/java/org/elasticsearch/nested/SimpleNestedIT.java
+++ b/core/src/test/java/org/elasticsearch/nested/SimpleNestedIT.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.nested;
 
 import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
@@ -312,7 +311,7 @@ public class SimpleNestedIT extends ESIntegTestCase {
                 .execute().actionGet();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(nestedQuery("nested1", termQuery("nested1.n_field1", "n_value1")).scoreMode(ScoreMode.Total))
+                .setQuery(nestedQuery("nested1", termQuery("nested1.n_field1", "n_value1")).scoreMode("total"))
                 .setExplain(true)
                 .execute().actionGet();
         assertNoFailures(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
index 2565d5d..0af7be6 100644
--- a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.percolate.MultiPercolateRequestBuilder;
 import org.elasticsearch.action.percolate.MultiPercolateResponse;
@@ -27,7 +26,6 @@ import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -362,7 +360,7 @@ public class MultiPercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode(ScoreMode.Avg)).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
index f250e92..09cda74 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
@@ -23,8 +23,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.percolate.PercolateSourceBuilder;
 import org.elasticsearch.index.percolator.PercolatorException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.index.query.QueryShardException;
 import org.junit.Test;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -67,7 +67,7 @@ public class PercolatorBackwardsCompatibilityIT extends ESIntegTestCase {
             fail();
         } catch (PercolatorException e) {
             e.printStackTrace();
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(ParsingException.class));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index ed31a54..1407162 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-import org.apache.lucene.search.join.ScoreMode;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
@@ -42,12 +41,11 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.engine.DocumentMissingException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.percolator.PercolatorException;
-import org.elasticsearch.index.query.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
-import org.elasticsearch.index.query.support.QueryInnerHits;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
+import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -1739,7 +1737,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(ParsingException.class));
         }
 
         try {
@@ -1748,7 +1746,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(ParsingException.class));
         }
     }
 
@@ -1787,7 +1785,7 @@ public class PercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode(ScoreMode.Avg)).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
@@ -1987,11 +1985,11 @@ public class PercolatorIT extends ESIntegTestCase {
         assertAcked(prepareCreate("index").addMapping("mapping", mapping));
         try {
             client().prepareIndex("index", PercolatorService.TYPE_NAME, "1")
-                    .setSource(jsonBuilder().startObject().field("query", nestedQuery("nested", matchQuery("nested.name", "value")).innerHit(new QueryInnerHits())).endObject())
+                    .setSource(jsonBuilder().startObject().field("query", nestedQuery("nested", matchQuery("nested.name", "value")).innerHit(new QueryInnerHitBuilder())).endObject())
                     .execute().actionGet();
             fail("Expected a parse error, because inner_hits isn't supported in the percolate api");
         } catch (Exception e) {
-            assertThat(e.getCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getCause(), instanceOf(ParsingException.class));
             assertThat(e.getCause().getMessage(), containsString("inner_hits unsupported"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index e293968..d166a15 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -590,7 +590,6 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-phonetic");
         PluginManager.checkForOfficialPlugins("analysis-smartcn");
         PluginManager.checkForOfficialPlugins("analysis-stempel");
-        PluginManager.checkForOfficialPlugins("cloud-azure");
         PluginManager.checkForOfficialPlugins("cloud-gce");
         PluginManager.checkForOfficialPlugins("delete-by-query");
         PluginManager.checkForOfficialPlugins("lang-javascript");
@@ -598,8 +597,11 @@ public class PluginManagerIT extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("mapper-murmur3");
         PluginManager.checkForOfficialPlugins("mapper-size");
         PluginManager.checkForOfficialPlugins("discovery-multicast");
+        PluginManager.checkForOfficialPlugins("discovery-azure");
         PluginManager.checkForOfficialPlugins("discovery-ec2");
+        PluginManager.checkForOfficialPlugins("repository-azure");
         PluginManager.checkForOfficialPlugins("repository-s3");
+        PluginManager.checkForOfficialPlugins("store-smb");
 
         try {
             PluginManager.checkForOfficialPlugins("elasticsearch-mapper-attachment");
diff --git a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
index b8b8551..5637b2d 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
@@ -23,9 +23,6 @@ import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.procedures.IntProcedure;
 import org.apache.lucene.index.IndexFileNames;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
-import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
-import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
@@ -37,20 +34,16 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.recovery.RecoveryFileChunkRequest;
-import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.recovery.RecoveryTarget;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.SearchHit;
@@ -58,14 +51,9 @@ import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.test.BackgroundIndexer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.transport.MockTransportService;
-import org.elasticsearch.transport.Transport;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestOptions;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -85,12 +73,8 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -362,89 +346,6 @@ public class RelocationIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/13542")
-    public void testMoveShardsWhileRelocation() throws Exception {
-        final String indexName = "test";
-
-        InternalTestCluster.Async<String> blueFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "blue").build());
-        InternalTestCluster.Async<String> redFuture = internalCluster().startNodeAsync(Settings.builder().put("node.color", "red").build());
-        internalCluster().startNode(Settings.builder().put("node.color", "green").build());
-        final String blueNodeName = blueFuture.get();
-        final String redNodeName = redFuture.get();
-
-        ClusterHealthResponse response = client().admin().cluster().prepareHealth().setWaitForNodes(">=3").get();
-        assertThat(response.isTimedOut(), is(false));
-
-
-        client().admin().indices().prepareCreate(indexName)
-                .setSettings(
-                        Settings.builder()
-                                .put(FilterAllocationDecider.INDEX_ROUTING_INCLUDE_GROUP + "color", "blue")
-                                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
-                ).get();
-
-        List<IndexRequestBuilder> requests = new ArrayList<>();
-        int numDocs = scaledRandomIntBetween(25, 250);
-        for (int i = 0; i < numDocs; i++) {
-            requests.add(client().prepareIndex(indexName, "type").setCreate(true).setSource("{}"));
-        }
-        indexRandom(true, requests);
-        ensureSearchable(indexName);
-
-        ClusterStateResponse stateResponse = client().admin().cluster().prepareState().get();
-        String blueNodeId = internalCluster().getInstance(DiscoveryService.class, blueNodeName).localNode().id();
-
-        assertFalse(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());
-
-        SearchResponse searchResponse = client().prepareSearch(indexName).get();
-        assertHitCount(searchResponse, numDocs);
-
-        // Slow down recovery in order to make recovery cancellations more likely
-        IndicesStatsResponse statsResponse = client().admin().indices().prepareStats(indexName).get();
-        long chunkSize = Math.max(1, statsResponse.getIndex(indexName).getShards()[0].getStats().getStore().size().bytes() / 10);
-        assertTrue(client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder()
-                                // one chunk per sec..
-                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, chunkSize, ByteSizeUnit.BYTES)
-                                .put(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, chunkSize, ByteSizeUnit.BYTES)
-                )
-                .get().isAcknowledged());
-
-        client().admin().indices().prepareUpdateSettings(indexName).setSettings(
-                Settings.builder().put(FilterAllocationDecider.INDEX_ROUTING_INCLUDE_GROUP + "color", "red")
-        ).get();
-
-        // Lets wait a bit and then move again to hopefully trigger recovery cancellations.
-        boolean applied = awaitBusy(
-                () -> {
-                    RecoveryResponse recoveryResponse =
-                            internalCluster().client(redNodeName).admin().indices().prepareRecoveries(indexName).get();
-                    return !recoveryResponse.shardRecoveryStates().get(indexName).isEmpty();
-                }
-        );
-        assertTrue(applied);
-        client().admin().indices().prepareUpdateSettings(indexName).setSettings(
-                Settings.builder().put(FilterAllocationDecider.INDEX_ROUTING_INCLUDE_GROUP + "color", "green")
-        ).get();
-
-        // Restore the recovery speed to not timeout cluster health call
-        assertTrue(client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder()
-                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, "20mb")
-                                .put(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, "512kb")
-                )
-                .get().isAcknowledged());
-
-        // this also waits for all ongoing recoveries to complete:
-        ensureSearchable(indexName);
-        searchResponse = client().prepareSearch(indexName).get();
-        assertHitCount(searchResponse, numDocs);
-
-        stateResponse = client().admin().cluster().prepareState().get();
-        assertTrue(stateResponse.getState().getRoutingNodes().node(blueNodeId).isEmpty());
-    }
-
-    @Test
     public void testCancellationCleansTempFiles() throws Exception {
         final String indexName = "test";
 
diff --git a/core/src/test/java/org/elasticsearch/rest/util/RestUtilsTests.java b/core/src/test/java/org/elasticsearch/rest/util/RestUtilsTests.java
index c06bf45..f202251 100644
--- a/core/src/test/java/org/elasticsearch/rest/util/RestUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/rest/util/RestUtilsTests.java
@@ -142,6 +142,16 @@ public class RestUtilsTests extends ESTestCase {
         assertThat(RestUtils.getCorsSettingRegex(Settings.EMPTY), is(nullValue()));
     }
 
+    public void testCrazyURL() {
+        Map<String, String> params = new HashMap<>();
+
+        // This is a valid URL
+        String uri = "example.com/:@-._~!$&'()*+,=;:@-._~!$&'()*+,=:@-._~!$&'()*+,==?/?:@-._~!$'()*+,;=/?:@-._~!$'()*+,;==#/?:@-._~!$&'()*+,;=";
+        RestUtils.decodeQueryString(uri, uri.indexOf('?') + 1, params);
+        assertThat(params.get("/?:@-._~!$'()* ,;"), equalTo("/?:@-._~!$'()* ,;=="));
+        assertThat(params.size(), equalTo(1));
+    }
+
     private void assertCorsSettingRegexIsNull(String settingsValue) {
         assertThat(RestUtils.getCorsSettingRegex(settingsBuilder().put("http.cors.allow-origin", settingsValue).build()), is(nullValue()));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
index d5834c9..37fff46 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
@@ -645,7 +645,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     /**
      * The script will change to document date values to the following:
-     * <p/>
+     * <p>
      * doc 1: [ Feb 2, Mar 3]
      * doc 2: [ Mar 2, Apr 3]
      * doc 3: [ Mar 15, Apr 16]
@@ -700,7 +700,7 @@ public class DateHistogramIT extends ESIntegTestCase {
 
     /**
      * The script will change to document date values to the following:
-     * <p/>
+     * <p>
      * doc 1: [ Feb 2, Mar 3]
      * doc 2: [ Mar 2, Apr 3]
      * doc 3: [ Mar 15, Apr 16]
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index d0d91b8..08e0767 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -132,7 +132,6 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
 
     /**
      * Set offset so day buckets start at 6am. Index first 12 hours for two days, with one day gap.
-     * @throws Exception
      */
     @Test
     public void singleValue_WithOffset_MinDocCount() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
index e08b6d7..e2708b1 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridIT.java
@@ -212,7 +212,7 @@ public class GeoHashGridIT extends ESIntegTestCase {
     @Test
     public void filtered() throws Exception {
         GeoBoundingBoxQueryBuilder bbox = new GeoBoundingBoxQueryBuilder("location");
-        bbox.setCorners(smallestGeoHash, smallestGeoHash).queryName("bbox");
+        bbox.topLeft(smallestGeoHash).bottomRight(smallestGeoHash).queryName("bbox");
         for (int precision = 1; precision <= XGeoHashUtils.PRECISION; precision++) {
             SearchResponse response = client().prepareSearch("idx")
                     .addAggregation(
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index 26cb3a9..9660217 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptModule;
@@ -237,7 +237,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
-                    throws IOException, QueryShardException {
+                    throws IOException, ParsingException {
                 parser.nextToken();
                 return new SimpleHeuristic();
             }
@@ -621,4 +621,4 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
         indexRandom(true, indexRequestBuilderList);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index c26ac8d..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -44,7 +44,6 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param size              Size of mock histogram to generate (in buckets)
      * @param gapProbability    Probability of generating an empty bucket. 0.0-1.0 inclusive
      * @param runProbability    Probability of extending a gap once one has been created.  0.0-1.0 inclusive
-     * @return
      */
     public static ArrayList<MockBucket> generateHistogram(int interval, int size, double gapProbability, double runProbability) {
         ArrayList<MockBucket> values = new ArrayList<>(size);
@@ -109,7 +108,6 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      *
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
-     * @return
      */
     public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index d139e38..fe942dc 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -235,7 +235,6 @@ public class MovAvgIT extends ESIntegTestCase {
      * Simple, unweighted moving average
      *
      * @param window Window of values to compute movavg for
-     * @return
      */
     private double simple(Collection<Double> window) {
         double movAvg = 0;
@@ -250,7 +249,6 @@ public class MovAvgIT extends ESIntegTestCase {
      * Linearly weighted moving avg
      *
      * @param window Window of values to compute movavg for
-     * @return
      */
     private double linear(Collection<Double> window) {
         double avg = 0;
@@ -269,7 +267,6 @@ public class MovAvgIT extends ESIntegTestCase {
      * Exponentionally weighted (EWMA, Single exponential) moving avg
      *
      * @param window Window of values to compute movavg for
-     * @return
      */
     private double ewma(Collection<Double> window) {
         double avg = 0;
@@ -289,7 +286,6 @@ public class MovAvgIT extends ESIntegTestCase {
     /**
      * Holt-Linear (Double exponential) moving avg
      * @param window Window of values to compute movavg for
-     * @return
      */
     private double holt(Collection<Double> window) {
         double s = 0;
@@ -323,7 +319,6 @@ public class MovAvgIT extends ESIntegTestCase {
     /**
      * Holt winters (triple exponential) moving avg
      * @param window Window of values to compute movavg for
-     * @return
      */
     private double holtWinters(Collection<Double> window) {
         // Smoothed value
@@ -1346,11 +1341,6 @@ public class MovAvgIT extends ESIntegTestCase {
      * Better floating point comparisons courtesy of https://github.com/brazzy/floating-point-gui.de
      *
      * Snippet adapted to use doubles instead of floats
-     *
-     * @param a
-     * @param b
-     * @param epsilon
-     * @return
      */
     private static boolean nearlyEqual(double a, double b, double epsilon) {
         final double absA = Math.abs(a);
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index 2fbf587..43dd815 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -27,8 +27,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
-
 import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -52,15 +52,42 @@ import org.hamcrest.Matchers;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
+import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
+import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
+import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
+import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
+import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
+import static org.elasticsearch.index.query.QueryBuilders.notQuery;
+import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
+import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.termQuery;
+import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.hamcrest.Matchers.anyOf;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThanOrEqualTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
 
 /**
  *
@@ -267,11 +294,11 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         for (int i = 1; i <= 10; i++) {
             logger.info("Round {}", i);
             SearchResponse searchResponse = client().prepareSearch("test")
-                    .setQuery(constantScoreQuery(hasChildQuery("child", matchAllQuery()).scoreMode(ScoreMode.Max)))
+                    .setQuery(constantScoreQuery(hasChildQuery("child", matchAllQuery()).scoreMode("max")))
                     .get();
             assertNoFailures(searchResponse);
             searchResponse = client().prepareSearch("test")
-                    .setQuery(constantScoreQuery(hasParentQuery("parent", matchAllQuery()).score(true)))
+                    .setQuery(constantScoreQuery(hasParentQuery("parent", matchAllQuery()).scoreMode("score")))
                     .get();
             assertNoFailures(searchResponse);
         }
@@ -529,11 +556,11 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "child", "c1").setSource("c_field", "1").setParent(parentId).get();
         refresh();
 
-        CountResponse countResponse = client().prepareCount("test").setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode(ScoreMode.Max))
+        CountResponse countResponse = client().prepareCount("test").setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode("max"))
                 .get();
         assertHitCount(countResponse, 1l);
 
-        countResponse = client().prepareCount("test").setQuery(hasParentQuery("parent", termQuery("p_field", "1")).score(true))
+        countResponse = client().prepareCount("test").setQuery(hasParentQuery("parent", termQuery("p_field", "1")).scoreMode("score"))
                 .get();
         assertHitCount(countResponse, 1l);
 
@@ -560,20 +587,20 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setExplain(true)
-                .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode(ScoreMode.Max))
+                .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode("max"))
                 .get();
         assertHitCount(searchResponse, 1l);
         assertThat(searchResponse.getHits().getAt(0).explanation().getDescription(), equalTo("Score based on join value p1"));
 
         searchResponse = client().prepareSearch("test")
                 .setExplain(true)
-                .setQuery(hasParentQuery("parent", termQuery("p_field", "1")).score(true))
+                .setQuery(hasParentQuery("parent", termQuery("p_field", "1")).scoreMode("score"))
                 .get();
         assertHitCount(searchResponse, 1l);
         assertThat(searchResponse.getHits().getAt(0).explanation().getDescription(), equalTo("Score based on join value p1"));
 
         ExplainResponse explainResponse = client().prepareExplain("test", "parent", parentId)
-                .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode(ScoreMode.Max))
+                .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode("max"))
                 .get();
         assertThat(explainResponse.isExists(), equalTo(true));
         assertThat(explainResponse.getExplanation().getDetails()[0].getDescription(), equalTo("Score based on join value p1"));
@@ -651,7 +678,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                                 "child",
                                 QueryBuilders.functionScoreQuery(matchQuery("c_field2", 0),
                                         scriptFunction(new Script("doc['c_field1'].value")))
-                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode(ScoreMode.Total)).get();
+                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode("total")).get();
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("1"));
@@ -668,7 +695,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                                 "child",
                                 QueryBuilders.functionScoreQuery(matchQuery("c_field2", 0),
                                         scriptFunction(new Script("doc['c_field1'].value")))
-                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode(ScoreMode.Max)).get();
+                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode("max")).get();
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -685,7 +712,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                                 "child",
                                 QueryBuilders.functionScoreQuery(matchQuery("c_field2", 0),
                                         scriptFunction(new Script("doc['c_field1'].value")))
-                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode(ScoreMode.Avg)).get();
+                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode("avg")).get();
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -702,7 +729,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                                 "parent",
                                 QueryBuilders.functionScoreQuery(matchQuery("p_field1", "p_value3"),
                                         scriptFunction(new Script("doc['p_field2'].value")))
-                                        .boostMode(CombineFunction.REPLACE.getName())).score(true))
+                                        .boostMode(CombineFunction.REPLACE.getName())).scoreMode("score"))
                 .addSort(SortBuilders.fieldSort("c_field3")).addSort(SortBuilders.scoreSort()).get();
 
         assertThat(response.getHits().totalHits(), equalTo(7l));
@@ -742,7 +769,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertNoFailures(response);
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = client().prepareSearch("test").setQuery(QueryBuilders.hasChildQuery("child", matchQuery("text", "value")).scoreMode(ScoreMode.Max))
+        response = client().prepareSearch("test").setQuery(QueryBuilders.hasChildQuery("child", matchQuery("text", "value")).scoreMode("max"))
                 .get();
         assertNoFailures(response);
         assertThat(response.getHits().totalHits(), equalTo(0l));
@@ -751,7 +778,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertNoFailures(response);
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = client().prepareSearch("test").setQuery(QueryBuilders.hasParentQuery("child", matchQuery("text", "value")).score(true))
+        response = client().prepareSearch("test").setQuery(QueryBuilders.hasParentQuery("child", matchQuery("text", "value")).scoreMode("score"))
                 .get();
         assertNoFailures(response);
         assertThat(response.getHits().totalHits(), equalTo(0l));
@@ -838,7 +865,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         SearchType[] searchTypes = new SearchType[]{SearchType.QUERY_THEN_FETCH, SearchType.DFS_QUERY_THEN_FETCH};
         for (SearchType searchType : searchTypes) {
             SearchResponse searchResponse = client().prepareSearch("test").setSearchType(searchType)
-                    .setQuery(hasChildQuery("child", prefixQuery("c_field", "c")).scoreMode(ScoreMode.Max)).addSort("p_field", SortOrder.ASC)
+                    .setQuery(hasChildQuery("child", prefixQuery("c_field", "c")).scoreMode("max")).addSort("p_field", SortOrder.ASC)
                     .setSize(5).get();
             assertNoFailures(searchResponse);
             assertThat(searchResponse.getHits().totalHits(), equalTo(10L));
@@ -849,7 +876,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
             assertThat(searchResponse.getHits().hits()[4].id(), equalTo("p004"));
 
             searchResponse = client().prepareSearch("test").setSearchType(searchType)
-                    .setQuery(hasParentQuery("parent", prefixQuery("p_field", "p")).score(true)).addSort("c_field", SortOrder.ASC)
+                    .setQuery(hasParentQuery("parent", prefixQuery("p_field", "p")).scoreMode("score")).addSort("c_field", SortOrder.ASC)
                     .setSize(5).get();
             assertNoFailures(searchResponse);
             assertThat(searchResponse.getHits().totalHits(), equalTo(500L));
@@ -881,7 +908,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(hasChildQuery("child", termQuery("c_field", "yellow")).scoreMode(ScoreMode.Total)).get();
+                .setQuery(hasChildQuery("child", termQuery("c_field", "yellow")).scoreMode("total")).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("p1"));
@@ -891,7 +918,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                 .prepareSearch("test")
                 .setQuery(
                         boolQuery().must(matchQuery("c_field", "x")).must(
-                                hasParentQuery("parent", termQuery("p_field", "p_value2")).score(true))).get();
+                                hasParentQuery("parent", termQuery("p_field", "p_value2")).scoreMode("score"))).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("c3"));
@@ -906,7 +933,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
             client().admin().indices().prepareRefresh("test").get();
         }
 
-        searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", termQuery("c_field", "yellow")).scoreMode(ScoreMode.Total))
+        searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", termQuery("c_field", "yellow")).scoreMode("total"))
                 .get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
@@ -917,7 +944,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
                 .prepareSearch("test")
                 .setQuery(
                         boolQuery().must(matchQuery("c_field", "x")).must(
-                                hasParentQuery("parent", termQuery("p_field", "p_value2")).score(true))).get();
+                                hasParentQuery("parent", termQuery("p_field", "p_value2")).scoreMode("score"))).get();
         assertNoFailures(searchResponse);
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         assertThat(searchResponse.getHits().getAt(0).id(), Matchers.anyOf(equalTo("c3"), equalTo("c4")));
@@ -942,7 +969,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "child", "c5").setSource("c_field", "x").setParent("p2").get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", matchAllQuery()).scoreMode(ScoreMode.Total))
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", matchAllQuery()).scoreMode("total"))
                 .setMinScore(3) // Score needs to be 3 or above!
                 .get();
         assertNoFailures(searchResponse);
@@ -1211,7 +1238,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "child", "c3").setParent("p2").setSource("c_field", "red").get();
         refresh();
 
-        ScoreMode scoreMode = randomFrom(ScoreMode.values());
+        String scoreMode = ScoreMode.values()[getRandom().nextInt(ScoreMode.values().length)].name().toLowerCase(Locale.ROOT);
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(boolQuery().must(QueryBuilders.hasChildQuery("child", termQuery("c_field", "blue")).scoreMode(scoreMode)).filter(notQuery(termQuery("p_field", "3"))))
                 .get();
@@ -1237,13 +1264,13 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "child", "c1").setSource("c_field", "1").setParent(parentId).get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode(ScoreMode.Max).queryName("test"))
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode("max").queryName("test"))
                 .get();
         assertHitCount(searchResponse, 1l);
         assertThat(searchResponse.getHits().getAt(0).getMatchedQueries().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).getMatchedQueries()[0], equalTo("test"));
 
-        searchResponse = client().prepareSearch("test").setQuery(hasParentQuery("parent", termQuery("p_field", "1")).score(true).queryName("test"))
+        searchResponse = client().prepareSearch("test").setQuery(hasParentQuery("parent", termQuery("p_field", "1")).scoreMode("score").queryName("test"))
                 .get();
         assertHitCount(searchResponse, 1l);
         assertThat(searchResponse.getHits().getAt(0).getMatchedQueries().length, equalTo(1));
@@ -1285,7 +1312,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         try {
             client().prepareSearch("test")
-                    .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode(ScoreMode.Max))
+                    .setQuery(hasChildQuery("child", termQuery("c_field", "1")).scoreMode("max"))
                     .get();
             fail();
         } catch (SearchPhaseExecutionException e) {
@@ -1303,7 +1330,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         try {
             client().prepareSearch("test")
-                    .setQuery(hasParentQuery("parent", termQuery("p_field", "1")).score(true))
+                    .setQuery(hasParentQuery("parent", termQuery("p_field", "1")).scoreMode("score"))
                     .get();
             fail();
         } catch (SearchPhaseExecutionException e) {
@@ -1553,7 +1580,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         return indexBuilders;
     }
 
-    private SearchResponse minMaxQuery(ScoreMode scoreMode, int minChildren, Integer maxChildren) throws SearchPhaseExecutionException {
+    private SearchResponse minMaxQuery(String scoreMode, int minChildren, Integer maxChildren) throws SearchPhaseExecutionException {
         HasChildQueryBuilder hasChildQuery = hasChildQuery(
                 "child",
                 QueryBuilders.functionScoreQuery(constantScoreQuery(QueryBuilders.termQuery("foo", "two"))).boostMode("replace").scoreMode("sum")
@@ -1583,7 +1610,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         SearchResponse response;
 
         // Score mode = NONE
-        response = minMaxQuery(ScoreMode.None, 0, 0);
+        response = minMaxQuery("none", 0, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("2"));
@@ -1593,7 +1620,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("4"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 1, 0);
+        response = minMaxQuery("none", 1, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("2"));
@@ -1603,7 +1630,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("4"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 2, 0);
+        response = minMaxQuery("none", 2, null);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -1611,17 +1638,17 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("4"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 3, 0);
+        response = minMaxQuery("none", 3, null);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
         assertThat(response.getHits().hits()[0].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 4, 0);
+        response = minMaxQuery("none", 4, null);
 
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = minMaxQuery(ScoreMode.None, 0, 4);
+        response = minMaxQuery("none", 0, 4);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("2"));
@@ -1631,7 +1658,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("4"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 0, 3);
+        response = minMaxQuery("none", 0, 3);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("2"));
@@ -1641,7 +1668,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("4"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 0, 2);
+        response = minMaxQuery("none", 0, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("2"));
@@ -1649,21 +1676,21 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("3"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.None, 2, 2);
+        response = minMaxQuery("none", 2, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
         assertThat(response.getHits().hits()[0].score(), equalTo(1f));
 
         try {
-            response = minMaxQuery(ScoreMode.None, 3, 2);
+            response = minMaxQuery("none", 3, 2);
             fail();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.toString(), containsString("[has_child] 'max_children' is less than 'min_children'"));
         }
 
-        // Score mode = SUM
-        response = minMaxQuery(ScoreMode.Total, 0, 0);
+        // Score mode = TOTAL
+        response = minMaxQuery("total", 0, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1673,7 +1700,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Total, 1, 0);
+        response = minMaxQuery("total", 1, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1683,7 +1710,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Total, 2, 0);
+        response = minMaxQuery("total", 2, null);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1691,17 +1718,17 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("3"));
         assertThat(response.getHits().hits()[1].score(), equalTo(3f));
 
-        response = minMaxQuery(ScoreMode.Total, 3, 0);
+        response = minMaxQuery("total", 3, null);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
         assertThat(response.getHits().hits()[0].score(), equalTo(6f));
 
-        response = minMaxQuery(ScoreMode.Total, 4, 0);
+        response = minMaxQuery("total", 4, null);
 
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = minMaxQuery(ScoreMode.Total, 0, 4);
+        response = minMaxQuery("total", 0, 4);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1711,7 +1738,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Total, 0, 3);
+        response = minMaxQuery("total", 0, 3);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1721,7 +1748,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Total, 0, 2);
+        response = minMaxQuery("total", 0, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -1729,21 +1756,21 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("2"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Total, 2, 2);
+        response = minMaxQuery("total", 2, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
         assertThat(response.getHits().hits()[0].score(), equalTo(3f));
 
         try {
-            response = minMaxQuery(ScoreMode.Total, 3, 2);
+            response = minMaxQuery("total", 3, 2);
             fail();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.toString(), containsString("[has_child] 'max_children' is less than 'min_children'"));
         }
 
         // Score mode = MAX
-        response = minMaxQuery(ScoreMode.Max, 0, 0);
+        response = minMaxQuery("max", 0, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1753,7 +1780,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Max, 1, 0);
+        response = minMaxQuery("max", 1, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1763,7 +1790,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Max, 2, 0);
+        response = minMaxQuery("max", 2, null);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1771,17 +1798,17 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("3"));
         assertThat(response.getHits().hits()[1].score(), equalTo(2f));
 
-        response = minMaxQuery(ScoreMode.Max, 3, 0);
+        response = minMaxQuery("max", 3, null);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
         assertThat(response.getHits().hits()[0].score(), equalTo(3f));
 
-        response = minMaxQuery(ScoreMode.Max, 4, 0);
+        response = minMaxQuery("max", 4, null);
 
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = minMaxQuery(ScoreMode.Max, 0, 4);
+        response = minMaxQuery("max", 0, 4);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1791,7 +1818,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Max, 0, 3);
+        response = minMaxQuery("max", 0, 3);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1801,7 +1828,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Max, 0, 2);
+        response = minMaxQuery("max", 0, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -1809,21 +1836,21 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("2"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Max, 2, 2);
+        response = minMaxQuery("max", 2, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
         assertThat(response.getHits().hits()[0].score(), equalTo(2f));
 
         try {
-            response = minMaxQuery(ScoreMode.Max, 3, 2);
+            response = minMaxQuery("max", 3, 2);
             fail();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.toString(), containsString("[has_child] 'max_children' is less than 'min_children'"));
         }
 
         // Score mode = AVG
-        response = minMaxQuery(ScoreMode.Avg, 0, 0);
+        response = minMaxQuery("avg", 0, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1833,7 +1860,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Avg, 1, 0);
+        response = minMaxQuery("avg", 1, null);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1843,7 +1870,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Avg, 2, 0);
+        response = minMaxQuery("avg", 2, null);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1851,17 +1878,17 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("3"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1.5f));
 
-        response = minMaxQuery(ScoreMode.Avg, 3, 0);
+        response = minMaxQuery("avg", 3, null);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
         assertThat(response.getHits().hits()[0].score(), equalTo(2f));
 
-        response = minMaxQuery(ScoreMode.Avg, 4, 0);
+        response = minMaxQuery("avg", 4, null);
 
         assertThat(response.getHits().totalHits(), equalTo(0l));
 
-        response = minMaxQuery(ScoreMode.Avg, 0, 4);
+        response = minMaxQuery("avg", 0, 4);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1871,7 +1898,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Avg, 0, 3);
+        response = minMaxQuery("avg", 0, 3);
 
         assertThat(response.getHits().totalHits(), equalTo(3l));
         assertThat(response.getHits().hits()[0].id(), equalTo("4"));
@@ -1881,7 +1908,7 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[2].id(), equalTo("2"));
         assertThat(response.getHits().hits()[2].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Avg, 0, 2);
+        response = minMaxQuery("avg", 0, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(2l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
@@ -1889,14 +1916,14 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
         assertThat(response.getHits().hits()[1].id(), equalTo("2"));
         assertThat(response.getHits().hits()[1].score(), equalTo(1f));
 
-        response = minMaxQuery(ScoreMode.Avg, 2, 2);
+        response = minMaxQuery("avg", 2, 2);
 
         assertThat(response.getHits().totalHits(), equalTo(1l));
         assertThat(response.getHits().hits()[0].id(), equalTo("3"));
         assertThat(response.getHits().hits()[0].score(), equalTo(1.5f));
 
         try {
-            response = minMaxQuery(ScoreMode.Avg, 3, 2);
+            response = minMaxQuery("avg", 3, 2);
             fail();
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.toString(), containsString("[has_child] 'max_children' is less than 'min_children'"));
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
index 1f4e953..cb79168 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java
@@ -91,7 +91,7 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh().execute().actionGet();
 
         SearchResponse searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoBoundingBoxQuery("location").setCorners(40.73, -74.1, 40.717, -73.99))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(40.73, -74.1).bottomRight(40.717, -73.99))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
         assertThat(searchResponse.getHits().hits().length, equalTo(2));
@@ -100,7 +100,7 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
         }
 
         searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoBoundingBoxQuery("location").setCorners(40.73, -74.1, 40.717, -73.99).type("indexed"))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(40.73, -74.1).bottomRight(40.717, -73.99).type("indexed"))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(2l));
         assertThat(searchResponse.getHits().hits().length, equalTo(2));
@@ -160,52 +160,52 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(41, -11, 40, 9))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(41, -11).bottomRight(40, 9))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("2"));
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(41, -11, 40, 9).type("indexed"))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(41, -11).bottomRight(40, 9).type("indexed"))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("2"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(41, -9, 40, 11))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(41, -9).bottomRight(40, 11))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("3"));
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(41, -9, 40, 11).type("indexed"))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(41, -9).bottomRight(40, 11).type("indexed"))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("3"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(11, 171, 1, -169))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(11, 171).bottomRight(1, -169))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("5"));
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(11, 171, 1, -169).type("indexed"))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(11, 171).bottomRight(1, -169).type("indexed"))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("5"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(9, 169, -1, -171))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(9, 169).bottomRight(-1, -171))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
         assertThat(searchResponse.getHits().getAt(0).id(), equalTo("9"));
         searchResponse = client().prepareSearch()
-                .setQuery(geoBoundingBoxQuery("location").setCorners(9, 169, -1, -171).type("indexed"))
+                .setQuery(geoBoundingBoxQuery("location").topLeft(9, 169).bottomRight(-1, -171).type("indexed"))
                 .execute().actionGet();
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(1l));
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
@@ -239,26 +239,26 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(
                         boolQuery().must(termQuery("userid", 880)).filter(
-                                geoBoundingBoxQuery("location").setCorners(74.579421999999994, 143.5, -66.668903999999998, 113.96875))
+                                geoBoundingBoxQuery("location").topLeft(74.579421999999994, 143.5).bottomRight(-66.668903999999998, 113.96875))
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
                         boolQuery().must(termQuery("userid", 880)).filter(
-                                geoBoundingBoxQuery("location").setCorners(74.579421999999994, 143.5, -66.668903999999998, 113.96875).type("indexed"))
+                                geoBoundingBoxQuery("location").topLeft(74.579421999999994, 143.5).bottomRight(-66.668903999999998, 113.96875).type("indexed"))
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
 
         searchResponse = client().prepareSearch()
                 .setQuery(
                         boolQuery().must(termQuery("userid", 534)).filter(
-                                geoBoundingBoxQuery("location").setCorners(74.579421999999994, 143.5, -66.668903999999998, 113.96875))
+                                geoBoundingBoxQuery("location").topLeft(74.579421999999994, 143.5).bottomRight(-66.668903999999998, 113.96875))
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
                         boolQuery().must(termQuery("userid", 534)).filter(
-                                geoBoundingBoxQuery("location").setCorners(74.579421999999994, 143.5, -66.668903999999998, 113.96875).type("indexed"))
+                                geoBoundingBoxQuery("location").topLeft(74.579421999999994, 143.5).bottomRight(-66.668903999999998, 113.96875).type("indexed"))
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
     }
@@ -289,43 +289,43 @@ public class GeoBoundingBoxIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, -180, -50, 180)
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(50, -180).bottomRight(-50, 180)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, -180, -50, 180).type("indexed")
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(50, -180).bottomRight(-50, 180).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, -180, -90, 180)
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(90, -180).bottomRight(-90, 180)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, -180, -90, 180).type("indexed")
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(90, -180).bottomRight(-90, 180).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
 
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, 0, -50, 360)
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(50, 0).bottomRight(-50, 360)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(50, 0, -50, 360).type("indexed")
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(50, 0).bottomRight(-50, 360).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, 0, -90, 360)
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(90, 0).bottomRight(-90, 360)
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
         searchResponse = client().prepareSearch()
                 .setQuery(
-                        geoBoundingBoxQuery("location").coerce(true).setCorners(90, 0, -90, 360).type("indexed")
+                        geoBoundingBoxQuery("location").coerce(true).topLeft(90, 0).bottomRight(-90, 360).type("indexed")
                 ).execute().actionGet();
         assertThat(searchResponse.getHits().totalHits(), equalTo(2l));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
index 809de30..d59baf1 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoDistanceIT.java
@@ -75,7 +75,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
         indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject()
                 .field("name", "New York")
                 .startObject("location").field("lat", 40.7143528).field("lon", -74.0059731).endObject()
-                .endObject()),
+                .endObject()), 
         // to NY: 5.286 km
         client().prepareIndex("test", "type1", "2").setSource(jsonBuilder().startObject()
                 .field("name", "Times Square")
@@ -171,7 +171,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
         }
 
         searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoDistanceRangeQuery("location", 40.7143528, -74.0059731).from("1.0km").to("2.0km"))
+                .setQuery(geoDistanceRangeQuery("location").from("1.0km").to("2.0km").point(40.7143528, -74.0059731))
                 .execute().actionGet();
         assertHitCount(searchResponse, 2);
         assertThat(searchResponse.getHits().hits().length, equalTo(2));
@@ -179,7 +179,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
             assertThat(hit.id(), anyOf(equalTo("4"), equalTo("5")));
         }
         searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoDistanceRangeQuery("location", 40.7143528, -74.0059731).from("1.0km").to("2.0km").optimizeBbox("indexed"))
+                .setQuery(geoDistanceRangeQuery("location").from("1.0km").to("2.0km").point(40.7143528, -74.0059731).optimizeBbox("indexed"))
                 .execute().actionGet();
         assertHitCount(searchResponse, 2);
         assertThat(searchResponse.getHits().hits().length, equalTo(2));
@@ -188,13 +188,13 @@ public class GeoDistanceIT extends ESIntegTestCase {
         }
 
         searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoDistanceRangeQuery("location", 40.7143528, -74.0059731).to("2.0km"))
+                .setQuery(geoDistanceRangeQuery("location").to("2.0km").point(40.7143528, -74.0059731))
                 .execute().actionGet();
         assertHitCount(searchResponse, 4);
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
 
         searchResponse = client().prepareSearch() // from NY
-                .setQuery(geoDistanceRangeQuery("location", 40.7143528, -74.0059731).from("2.0km"))
+                .setQuery(geoDistanceRangeQuery("location").from("2.0km").point(40.7143528, -74.0059731))
                 .execute().actionGet();
         assertHitCount(searchResponse, 3);
         assertThat(searchResponse.getHits().hits().length, equalTo(3));
@@ -394,7 +394,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
 
         // Doc with missing geo point is first, is consistent with 0.20.x
         assertHitCount(searchResponse, 2);
-        assertOrderedSearchHits(searchResponse, "2", "1");
+        assertOrderedSearchHits(searchResponse, "2", "1");        
         assertThat(((Number) searchResponse.getHits().getAt(0).sortValues()[0]).doubleValue(), equalTo(Double.MAX_VALUE));
         assertThat(((Number) searchResponse.getHits().getAt(1).sortValues()[0]).doubleValue(), closeTo(5286d, 10d));
     }
@@ -508,7 +508,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
                         .startObject("location").field("lat", 40.7143528).field("lon", -74.0059731).endObject()
                     .endObject()
                 .endArray()
-                .endObject()),
+                .endObject()), 
         client().prepareIndex("companies", "company", "2").setSource(jsonBuilder().startObject()
                 .field("name", "company 2")
                 .startArray("branches")
@@ -641,7 +641,7 @@ public class GeoDistanceIT extends ESIntegTestCase {
                 RestStatus.BAD_REQUEST,
                 containsString("sort_mode [sum] isn't supported for sorting by geo distance"));
     }
-
+    
     /**
      * Issue 3073
      */
@@ -681,12 +681,12 @@ public class GeoDistanceIT extends ESIntegTestCase {
                 .setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(QueryBuilders.geoDistanceQuery("pin")
                         .geoDistance(GeoDistance.ARC)
-                        .point(lat, lon)
+                        .lat(lat).lon(lon)
                         .distance("1m"))
                 .execute().actionGet();
 
         assertHitCount(result, 1);
-    }
+    } 
 
     private double randomLon() {
         return randomDouble() * 360 - 180;
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
index 4548fbd..4d5ae30 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
@@ -434,7 +434,9 @@ public class GeoFilterIT extends ESIntegTestCase {
         }
 
         SearchResponse world = client().prepareSearch().addField("pin").setQuery(
-                geoBoundingBoxQuery("pin").setCorners(90, -179.99999, -90, 179.99999)
+                geoBoundingBoxQuery("pin")
+                        .topLeft(90, -179.99999)
+                        .bottomRight(-90, 179.99999)
         ).execute().actionGet();
 
         assertHitCount(world, 53);
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoPolygonIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoPolygonIT.java
index 248c62b..d841701 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoPolygonIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoPolygonIT.java
@@ -20,16 +20,12 @@
 package org.elasticsearch.search.geo;
 
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.List;
-
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
 import static org.elasticsearch.index.query.QueryBuilders.geoPolygonQuery;
@@ -53,7 +49,7 @@ public class GeoPolygonIT extends ESIntegTestCase {
         indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject()
                 .field("name", "New York")
                 .startObject("location").field("lat", 40.714).field("lon", -74.006).endObject()
-                .endObject()),
+                .endObject()), 
         // to NY: 5.286 km
         client().prepareIndex("test", "type1", "2").setSource(jsonBuilder().startObject()
                 .field("name", "Times Square")
@@ -89,14 +85,14 @@ public class GeoPolygonIT extends ESIntegTestCase {
 
     @Test
     public void simplePolygonTest() throws Exception {
-        List<GeoPoint> points = new ArrayList<>();
-        points.add(new GeoPoint(40.7, -74.0));
-        points.add(new GeoPoint(40.7, -74.1));
-        points.add(new GeoPoint(40.8, -74.1));
-        points.add(new GeoPoint(40.8, -74.0));
-        points.add(new GeoPoint(40.7, -74.0));
+
         SearchResponse searchResponse = client().prepareSearch("test") // from NY
-                .setQuery(boolQuery().must(geoPolygonQuery("location", points)))
+                .setQuery(boolQuery().must(geoPolygonQuery("location")
+                        .addPoint(40.7, -74.0)
+                        .addPoint(40.7, -74.1)
+                        .addPoint(40.8, -74.1)
+                        .addPoint(40.8, -74.0)
+                        .addPoint(40.7, -74.0)))
                 .execute().actionGet();
         assertHitCount(searchResponse, 4);
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -107,13 +103,13 @@ public class GeoPolygonIT extends ESIntegTestCase {
 
     @Test
     public void simpleUnclosedPolygon() throws Exception {
-        List<GeoPoint> points = new ArrayList<>();
-        points.add(new GeoPoint(40.7, -74.0));
-        points.add(new GeoPoint(40.7, -74.1));
-        points.add(new GeoPoint(40.8, -74.1));
-        points.add(new GeoPoint(40.8, -74.0));
         SearchResponse searchResponse = client().prepareSearch("test") // from NY
-                .setQuery(boolQuery().must(geoPolygonQuery("location", points))).execute().actionGet();
+                .setQuery(boolQuery().must(geoPolygonQuery("location")
+                        .addPoint(40.7, -74.0)
+                        .addPoint(40.7, -74.1)
+                        .addPoint(40.8, -74.1)
+                        .addPoint(40.8, -74.0)))
+                .execute().actionGet();
         assertHitCount(searchResponse, 4);
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
         for (SearchHit hit : searchResponse.getHits()) {
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
index 33593e9..feb3322 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
@@ -49,10 +49,7 @@ import static org.elasticsearch.index.query.QueryBuilders.geoShapeQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class GeoShapeIntegrationIT extends ESIntegTestCase {
 
@@ -289,28 +286,28 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
                         .endObject().endObject()));
         ensureSearchable("test", "shapes");
 
-        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery("location", "1", "type").relation(ShapeRelation.INTERSECTS)
+        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery("location", "1", "type", ShapeRelation.INTERSECTS)
                 .indexedShapeIndex("shapes")
                 .indexedShapePath("location");
         SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
         assertHitCount(result, 1);
-        filter = QueryBuilders.geoShapeQuery("location", "1", "type").relation(ShapeRelation.INTERSECTS)
+        filter = QueryBuilders.geoShapeQuery("location", "1", "type", ShapeRelation.INTERSECTS)
                 .indexedShapeIndex("shapes")
                 .indexedShapePath("1.location");
         result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
         assertHitCount(result, 1);
-        filter = QueryBuilders.geoShapeQuery("location", "1", "type").relation(ShapeRelation.INTERSECTS)
+        filter = QueryBuilders.geoShapeQuery("location", "1", "type", ShapeRelation.INTERSECTS)
                 .indexedShapeIndex("shapes")
                 .indexedShapePath("1.2.location");
         result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
         assertHitCount(result, 1);
-        filter = QueryBuilders.geoShapeQuery("location", "1", "type").relation(ShapeRelation.INTERSECTS)
+        filter = QueryBuilders.geoShapeQuery("location", "1", "type", ShapeRelation.INTERSECTS)
                 .indexedShapeIndex("shapes")
                 .indexedShapePath("1.2.3.location");
         result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
@@ -363,8 +360,7 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
 
         ShapeBuilder filterShape = (gcb.getShapeAt(randomIntBetween(0, gcb.numShapes() - 1)));
 
-        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery("location", filterShape);
-        filter.relation(ShapeRelation.INTERSECTS);
+        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery("location", filterShape, ShapeRelation.INTERSECTS);
         SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
@@ -403,30 +399,19 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
                 .setSource(docSource));
         ensureSearchable("test");
 
-        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery(
-                "location",
-                ShapeBuilder.newGeometryCollection()
-                        .polygon(
-                                ShapeBuilder.newPolygon().point(99.0, -1.0).point(99.0, 3.0).point(103.0, 3.0).point(103.0, -1.0)
-                                        .point(99.0, -1.0))).relation(ShapeRelation.INTERSECTS);
+        GeoShapeQueryBuilder filter = QueryBuilders.geoShapeQuery("location", ShapeBuilder.newGeometryCollection().polygon(ShapeBuilder.newPolygon().point(99.0, -1.0).point(99.0, 3.0).point(103.0, 3.0).point(103.0, -1.0).point(99.0, -1.0)), ShapeRelation.INTERSECTS);
         SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
         assertHitCount(result, 1);
-        filter = QueryBuilders.geoShapeQuery(
-                "location",
-                ShapeBuilder.newGeometryCollection().polygon(
-                        ShapeBuilder.newPolygon().point(199.0, -11.0).point(199.0, 13.0).point(193.0, 13.0).point(193.0, -11.0)
-                                .point(199.0, -11.0))).relation(ShapeRelation.INTERSECTS);
+        filter = QueryBuilders.geoShapeQuery("location", ShapeBuilder.newGeometryCollection().polygon(ShapeBuilder.newPolygon().point(199.0, -11.0).point(199.0, 13.0).point(193.0, 13.0).point(193.0, -11.0).point(199.0, -11.0)), ShapeRelation.INTERSECTS);
         result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
         assertHitCount(result, 0);
         filter = QueryBuilders.geoShapeQuery("location", ShapeBuilder.newGeometryCollection()
                 .polygon(ShapeBuilder.newPolygon().point(99.0, -1.0).point(99.0, 3.0).point(103.0, 3.0).point(103.0, -1.0).point(99.0, -1.0))
-                        .polygon(
-                                ShapeBuilder.newPolygon().point(199.0, -11.0).point(199.0, 13.0).point(193.0, 13.0).point(193.0, -11.0)
-                                        .point(199.0, -11.0))).relation(ShapeRelation.INTERSECTS);
+                .polygon(ShapeBuilder.newPolygon().point(199.0, -11.0).point(199.0, 13.0).point(193.0, 13.0).point(193.0, -11.0).point(199.0, -11.0)), ShapeRelation.INTERSECTS);
         result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery())
                 .setPostFilter(filter).get();
         assertSearchResponse(result);
@@ -435,7 +420,6 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
 
     /**
      * Test that orientation parameter correctly persists across cluster restart
-     * @throws IOException
      */
     public void testOrientationPersistence() throws Exception {
         String idxName = "orientation";
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 4134c4f..93449c9 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -26,13 +26,8 @@ import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.*;
-import org.elasticsearch.index.query.IdsQueryBuilder;
-import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.search.MatchQuery.Type;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.MatchQueryBuilder.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder.Type;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -943,12 +938,12 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
 
         // which can also be written by searching on the subfield
-        resp = req.setQuery(queryStringQuery("cats").field("foo").field("foo.plain", 5)).get();
+        resp = req.setQuery(queryStringQuery("cats").field("foo").field("foo.plain^5")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
 
         // Speaking of two fields, you can have two fields, only one of which has matchedFields enabled
-        QueryBuilder twoFieldsQuery = queryStringQuery("cats").field("foo").field("foo.plain", 5)
-                .field("bar").field("bar.plain", 5);
+        QueryBuilder twoFieldsQuery = queryStringQuery("cats").field("foo").field("foo.plain^5")
+                .field("bar").field("bar.plain^5");
         resp = req.setQuery(twoFieldsQuery).addHighlightedField(barField).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
@@ -1370,7 +1365,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
@@ -1389,7 +1384,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
@@ -1515,7 +1510,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse response = client().prepareSearch("test")
-                .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
+                .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQueryBuilder.Type.PHRASE))
                 .addHighlightedField(new HighlightBuilder.Field("tags")
                         .fragmentSize(-1).numOfFragments(2).fragmenter("simple")).get();
 
@@ -1523,7 +1518,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         response = client().prepareSearch("test")
-                .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
+                .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQueryBuilder.Type.PHRASE))
                 .addHighlightedField(new HighlightBuilder.Field("tags")
                         .fragmentSize(-1).numOfFragments(2).fragmenter("span")).get();
 
@@ -1531,7 +1526,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         assertFailures(client().prepareSearch("test")
-                        .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
+                        .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQueryBuilder.Type.PHRASE))
                         .addHighlightedField(new HighlightBuilder.Field("tags")
                                 .fragmentSize(-1).numOfFragments(2).fragmenter("invalid")),
                 RestStatus.BAD_REQUEST,
@@ -1586,7 +1581,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         // This query used to fail when the field to highlight was absent
         SearchResponse response = client().prepareSearch("test")
-                .setQuery(QueryBuilders.matchQuery("field", "highlight").type(MatchQuery.Type.BOOLEAN))
+                .setQuery(QueryBuilders.matchQuery("field", "highlight").type(MatchQueryBuilder.Type.BOOLEAN))
                 .addHighlightedField(new HighlightBuilder.Field("highlight_field")
                         .fragmentSize(-1).numOfFragments(1).fragmenter("simple")).get();
         assertThat(response.getHits().hits()[0].highlightFields().isEmpty(), equalTo(true));
@@ -1606,7 +1601,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse response = client().prepareSearch("test")
-                .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
+                .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQueryBuilder.Type.BOOLEAN))
                 .addHighlightedField("text")
                 .addHighlightedField("byte")
                 .addHighlightedField("short")
@@ -1636,7 +1631,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse response = client().prepareSearch("test")
-                .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
+                .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQueryBuilder.Type.BOOLEAN))
                 .addHighlightedField("text").execute().actionGet();
         // PatternAnalyzer will throw an exception if it is resetted twice
         assertHitCount(response, 1l);
@@ -2119,7 +2114,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             } else {
                 supportedQueryTypes = MultiMatchQueryBuilder.Type.values();
             }
-            MultiMatchQueryBuilder.Type matchQueryType = RandomPicks.randomFrom(getRandom(), supportedQueryTypes);
+            MultiMatchQueryBuilder.Type matchQueryType = rarely() ? null : RandomPicks.randomFrom(getRandom(), supportedQueryTypes);
             final MultiMatchQueryBuilder multiMatchQueryBuilder = multiMatchQuery("the quick brown fox", "field1", "field2").type(matchQueryType);
 
             SearchSourceBuilder source = searchSource()
@@ -2303,7 +2298,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2589,10 +2584,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Query string boosting the field
         phraseBoostTestCaseForClauses(highlighterType, 1f,
                 queryStringQuery("highlight words together").field("field1"),
-                queryStringQuery("\"highlight words together\"").field("field1", 100).autoGeneratePhraseQueries(true));
+                queryStringQuery("\"highlight words together\"").field("field1^100").autoGeneratePhraseQueries(true));
     }
 
-    private <P extends AbstractQueryBuilder<P>> void
+    private <P extends QueryBuilder & BoostableQueryBuilder<?>> void
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
@@ -2606,10 +2601,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         phrase.boost(1);
         // Try with a boosting query
-        response = search.setQuery(boostingQuery(phrase, terms).boost(boost).negativeBoost(1)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(boost).negativeBoost(1)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         // Try with a boosting query using a negative boost
-        response = search.setQuery(boostingQuery(phrase, terms).boost(1).negativeBoost(1/boost)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(1).negativeBoost(1/boost)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index 16c54c4..ba43286 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -28,7 +28,7 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.support.QueryInnerHits;
+import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
@@ -88,9 +88,9 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         // Inner hits can be defined in two ways: 1) with the query 2) as seperate inner_hit definition
         SearchRequest[] searchRequests = new SearchRequest[]{
-                client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits("comment", null))).request(),
+                client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder().setName("comment"))).request(),
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox"))).request()
+                        .addInnerHit("comment", new InnerHitsBuilder.InnerHit().setPath("comments").setQuery(matchQuery("comments.message", "fox"))).request()
         };
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -112,11 +112,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")))
-                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant"))).request(),
+                        .addInnerHit("comment", new InnerHitsBuilder.InnerHit().setPath("comments").setQuery(matchQuery("comments.message", "elephant"))).request(),
                 client().prepareSearch("articles")
-                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHits("comment", null))).request(),
+                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHitBuilder().setName("comment"))).request(),
                 client().prepareSearch("articles")
-                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHits("comment", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC)))).request()
+                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHitBuilder().setName("comment").addSort("_doc", SortOrder.DESC))).request()
         };
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -138,24 +138,24 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).getNestedIdentity().getField().string(), equalTo("comments"));
             assertThat(innerHits.getAt(2).getNestedIdentity().getOffset(), equalTo(2));
         }
-        InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlightBuilder().field("comments.message");
-        innerHit.setExplain(true);
-        innerHit.addFieldDataField("comments.message");
-        innerHit.addScriptField("script", new Script("doc['comments.message'].value"));
-        innerHit.setSize(1);
+
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
+                        .addInnerHit("comments", new InnerHitsBuilder.InnerHit().setPath("comments")
                                 .setQuery(matchQuery("comments.message", "fox"))
                                 .addHighlightedField("comments.message")
                                 .setExplain(true)
                                 .addFieldDataField("comments.message")
-                                .addScriptField("script", new Script("doc['comments.message'].value"))
+                                        .addScriptField("script", new Script("doc['comments.message'].value"))
                                 .setSize(1)).request(),
                 client().prepareSearch("articles")
-                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, innerHit))).request()
+                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder()
+                                .addHighlightedField("comments.message")
+                                .setExplain(true)
+                                .addFieldDataField("comments.message")
+                                                .addScriptField("script", new Script("doc['comments.message'].value"))
+                                .setSize(1))).request()
         };
 
         for (SearchRequest searchRequest : searchRequests) {
@@ -201,17 +201,17 @@ public class InnerHitsIT extends ESIntegTestCase {
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .addSort("_uid", SortOrder.ASC)
-                    .addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)) // Sort order is DESC, because we reverse the inner objects during indexing!
-                    .addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))
+                    .addInnerHit("a", new InnerHitsBuilder.InnerHit().setPath("field1").addSort("_doc", SortOrder.DESC).setSize(size)) // Sort order is DESC, because we reverse the inner objects during indexing!
+                    .addInnerHit("b", new InnerHitsBuilder.InnerHit().setPath("field2").addSort("_doc", SortOrder.DESC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
             if (randomBoolean()) {
-                boolQuery.should(nestedQuery("field1", matchAllQuery()).innerHit(new QueryInnerHits("a", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))));
-                boolQuery.should(nestedQuery("field2", matchAllQuery()).innerHit(new QueryInnerHits("b", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))));
+                boolQuery.should(nestedQuery("field1", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("a").addSort("_doc", SortOrder.DESC).setSize(size)));
+                boolQuery.should(nestedQuery("field2", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("b").addSort("_doc", SortOrder.DESC).setSize(size)));
             } else {
-                boolQuery.should(constantScoreQuery(nestedQuery("field1", matchAllQuery()).innerHit(new QueryInnerHits("a", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)))));
-                boolQuery.should(constantScoreQuery(nestedQuery("field2", matchAllQuery()).innerHit(new QueryInnerHits("b", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)))));
+                boolQuery.should(constantScoreQuery(nestedQuery("field1", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("a").addSort("_doc", SortOrder.DESC).setSize(size))));
+                boolQuery.should(constantScoreQuery(nestedQuery("field2", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("b").addSort("_doc", SortOrder.DESC).setSize(size))));
             }
             searchResponse = client().prepareSearch("idx")
                     .setQuery(boolQuery)
@@ -267,10 +267,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")))
+                        .addInnerHit("comment", new InnerHitsBuilder.InnerHit().setType("comment").setQuery(matchQuery("message", "fox")))
                         .request(),
                 client().prepareSearch("articles")
-                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")).innerHit(new QueryInnerHits("comment", null)))
+                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")).innerHit(new QueryInnerHitBuilder().setName("comment")))
                         .request()
         };
         for (SearchRequest searchRequest : searchRequests) {
@@ -293,10 +293,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")))
-                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")))
+                        .addInnerHit("comment", new InnerHitsBuilder.InnerHit().setType("comment").setQuery(matchQuery("message", "elephant")))
                         .request(),
                 client().prepareSearch("articles")
-                        .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")).innerHit(new QueryInnerHits()))
+                        .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")).innerHit(new QueryInnerHitBuilder()))
                         .request()
         };
         for (SearchRequest searchRequest : searchRequests) {
@@ -316,16 +316,11 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).getId(), equalTo("6"));
             assertThat(innerHits.getAt(2).type(), equalTo("comment"));
         }
-        InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlightBuilder().field("message");
-        innerHit.setExplain(true);
-        innerHit.addFieldDataField("message");
-        innerHit.addScriptField("script", new Script("doc['message'].value"));
-        innerHit.setSize(1);
+
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
+                        .addInnerHit("comment", new InnerHitsBuilder.InnerHit().setType("comment")
                                         .setQuery(matchQuery("message", "fox"))
                                         .addHighlightedField("message")
                                         .setExplain(true)
@@ -333,11 +328,12 @@ public class InnerHitsIT extends ESIntegTestCase {
                                         .addScriptField("script", new Script("doc['message'].value"))
                                         .setSize(1)
                         ).request(),
-
                 client().prepareSearch("articles")
                         .setQuery(
                                 hasChildQuery("comment", matchQuery("message", "fox")).innerHit(
-                                        new QueryInnerHits(null, innerHit))).request() };
+                                        new QueryInnerHitBuilder().addHighlightedField("message").setExplain(true)
+                                                .addFieldDataField("message").addScriptField("script", new Script("doc['message'].value"))
+                                                .setSize(1))).request() };
 
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -389,17 +385,17 @@ public class InnerHitsIT extends ESIntegTestCase {
                     .setSize(numDocs)
                     .setTypes("parent")
                     .addSort("_uid", SortOrder.ASC)
-                    .addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
-                    .addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
+                    .addInnerHit("a", new InnerHitsBuilder.InnerHit().setType("child1").addSort("_uid", SortOrder.ASC).setSize(size))
+                    .addInnerHit("b", new InnerHitsBuilder.InnerHit().setType("child2").addSort("_uid", SortOrder.ASC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
             if (randomBoolean()) {
-                boolQuery.should(hasChildQuery("child1", matchAllQuery()).innerHit(new QueryInnerHits("a", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))));
-                boolQuery.should(hasChildQuery("child2", matchAllQuery()).innerHit(new QueryInnerHits("b", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))));
+                boolQuery.should(hasChildQuery("child1", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("a").addSort("_uid", SortOrder.ASC).setSize(size)));
+                boolQuery.should(hasChildQuery("child2", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("b").addSort("_uid", SortOrder.ASC).setSize(size)));
             } else {
-                boolQuery.should(constantScoreQuery(hasChildQuery("child1", matchAllQuery()).innerHit(new QueryInnerHits("a", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size)))));
-                boolQuery.should(constantScoreQuery(hasChildQuery("child2", matchAllQuery()).innerHit(new QueryInnerHits("b", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size)))));
+                boolQuery.should(constantScoreQuery(hasChildQuery("child1", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("a").addSort("_uid", SortOrder.ASC).setSize(size))));
+                boolQuery.should(constantScoreQuery(hasChildQuery("child2", matchAllQuery()).innerHit(new QueryInnerHitBuilder().setName("b").addSort("_uid", SortOrder.ASC).setSize(size))));
             }
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
@@ -451,7 +447,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         ensureGreen("articles");
         try {
             client().prepareSearch("articles")
-                    .addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit())
+                    .addInnerHit("comment", new InnerHitsBuilder.InnerHit())
                     .get();
         } catch (Exception e) {
             assertThat(e.getMessage(), containsString("Failed to build"));
@@ -478,7 +474,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .setQuery(
                         boolQuery()
                                 .must(matchQuery("body", "fail2ban"))
-                                .must(hasParentQuery("question", matchAllQuery()).innerHit(new QueryInnerHits()))
+                                .must(hasParentQuery("question", matchAllQuery()).innerHit(new QueryInnerHitBuilder()))
                 ).get();
         assertNoFailures(response);
         assertHitCount(response, 2);
@@ -517,10 +513,10 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "good"))))
-                .addParentChildInnerHits("comment", "comment",
-                        new InnerHitsBuilder.InnerHit()
+                .addInnerHit("comment",
+                        new InnerHitsBuilder.InnerHit().setType("comment")
                                 .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
-                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")))
+                                .addInnerHit("remark", new InnerHitsBuilder.InnerHit().setType("remark").setQuery(matchQuery("message", "good")))
                 )
                 .get();
 
@@ -541,10 +537,10 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "bad"))))
-                .addParentChildInnerHits("comment", "comment",
-                        new InnerHitsBuilder.InnerHit()
+                .addInnerHit("comment",
+                        new InnerHitsBuilder.InnerHit().setType("comment")
                                 .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
-                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")))
+                                .addInnerHit("remark", new InnerHitsBuilder.InnerHit().setType("remark").setQuery(matchQuery("message", "bad")))
                 )
                 .get();
 
@@ -609,9 +605,10 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good"))))
-                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                .addInnerHit("comment", new InnerHitsBuilder.InnerHit()
+                                .setPath("comments")
                                 .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
-                                .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")))
+                                .addInnerHit("remark", new InnerHitsBuilder.InnerHit().setPath("comments.remarks").setQuery(matchQuery("comments.remarks.message", "good")))
                 ).get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -634,7 +631,7 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         // Directly refer to the second level:
         response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")).innerHit(new QueryInnerHits()))
+                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")).innerHit(new QueryInnerHitBuilder()))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -651,9 +648,10 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad"))))
-                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                        .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
-                        .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad"))))
+                .addInnerHit("comment", new InnerHitsBuilder.InnerHit()
+                                .setPath("comments")
+                                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
+                                .addInnerHit("remark", new InnerHitsBuilder.InnerHit().setPath("comments.remarks").setQuery(matchQuery("comments.remarks.message", "bad"))))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -688,7 +686,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requests);
 
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits()))
+                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder()))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -726,8 +724,8 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requests);
 
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, new InnerHitsBuilder.InnerHit().field("comments.message"))))
-                        .get();
+                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder().field("comments.message")))
+                .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
         assertThat(response.getHits().getAt(0).id(), equalTo("1"));
@@ -763,10 +761,9 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .startObject("comments").field("message", "fox eat quick").endObject()
                 .endObject()));
         indexRandom(true, requests);
-        InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlightBuilder().field("comments.message");
+
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
+                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder().addHighlightedField("comments.message")))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -785,13 +782,13 @@ public class InnerHitsIT extends ESIntegTestCase {
                         .addMapping("article", jsonBuilder().startObject()
                                         .startObject("_source").field("excludes", new String[]{"comments"}).endObject()
                                         .startObject("properties")
-                                        .startObject("comments")
-                                        .field("type", "nested")
-                                        .startObject("properties")
-                                        .startObject("message").field("type", "string").field("store", "yes").endObject()
-                                        .endObject()
-                                        .endObject()
-                                        .endObject()
+                                            .startObject("comments")
+                                                .field("type", "nested")
+                                                .startObject("properties")
+                                                    .startObject("message").field("type", "string").field("store", "yes").endObject()
+                                                .endObject()
+                                                .endObject()
+                                            .endObject()
                                         .endObject()
                         )
         );
@@ -802,11 +799,9 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .startObject("comments").field("message", "fox eat quick").endObject()
                 .endObject()));
         indexRandom(true, requests);
-        InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.field("comments.message");
-        builder.setFetchSource(true);
+
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
+                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder().field("comments.message").setFetchSource(true)))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -842,11 +837,10 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .startObject("comments").field("message", "fox eat quick").endObject()
                 .endObject()));
         indexRandom(true, requests);
-        InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlightBuilder().field("comments.message");
+
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
-                        .get();
+                .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHitBuilder().addHighlightedField("comments.message")))
+                .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
         assertThat(response.getHits().getAt(0).id(), equalTo("1"));
@@ -887,7 +881,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requests);
 
         SearchResponse response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "fox")).innerHit(new QueryInnerHits()))
+                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "fox")).innerHit(new QueryInnerHitBuilder()))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -899,7 +893,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments.messages").getAt(0).getNestedIdentity().getChild(), nullValue());
 
         response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "bear")).innerHit(new QueryInnerHits()))
+                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "bear")).innerHit(new QueryInnerHitBuilder()))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -918,7 +912,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         response = client().prepareSearch("articles")
-                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "fox")).innerHit(new QueryInnerHits()))
+                .setQuery(nestedQuery("comments.messages", matchQuery("comments.messages.message", "fox")).innerHit(new QueryInnerHitBuilder()))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -934,11 +928,11 @@ public class InnerHitsIT extends ESIntegTestCase {
     public void testRoyals() throws Exception {
         assertAcked(
                 prepareCreate("royals")
-                        .addMapping("king")
-                        .addMapping("prince", "_parent", "type=king")
-                        .addMapping("duke", "_parent", "type=prince")
-                        .addMapping("earl", "_parent", "type=duke")
-                        .addMapping("baron", "_parent", "type=earl")
+                .addMapping("king")
+                .addMapping("prince", "_parent", "type=king")
+                .addMapping("duke", "_parent", "type=prince")
+                .addMapping("earl", "_parent", "type=duke")
+                .addMapping("baron", "_parent", "type=earl")
         );
 
         List<IndexRequestBuilder> requests = new ArrayList<>();
@@ -957,14 +951,15 @@ public class InnerHitsIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("royals")
                 .setTypes("duke")
-                .addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
+                .addInnerHit("earls", new InnerHitsBuilder.InnerHit()
+                                .setType("earl")
                                 .addSort(SortBuilders.fieldSort("_uid").order(SortOrder.ASC))
                                 .setSize(4)
-                                .addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit())
+                                .addInnerHit("barons", new InnerHitsBuilder.InnerHit().setType("baron"))
                 )
-                .addParentChildInnerHits("princes", "prince",
-                        new InnerHitsBuilder.InnerHit()
-                        .addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit())
+                .addInnerHit("princes",
+                        new InnerHitsBuilder.InnerHit().setType("prince")
+                        .addInnerHit("kings", new InnerHitsBuilder.InnerHit().setType("king"))
                 )
                 .get();
         assertHitCount(response, 1);
@@ -1072,7 +1067,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                                 .should(termQuery("nested1.n_field1", "n_value1_1").queryName("test1"))
                                 .should(termQuery("nested1.n_field1", "n_value1_3").queryName("test2"))
                                 .should(termQuery("nested1.n_field2", "n_value2_2").queryName("test3"))
-                ).innerHit(new QueryInnerHits(null, new InnerHitsBuilder.InnerHit().addSort("nested1.n_field1", SortOrder.ASC))))
+                ).innerHit(new QueryInnerHitBuilder().addSort("nested1.n_field1", SortOrder.ASC)))
                 .setSize(numDocs)
                 .addSort("field1", SortOrder.ASC)
                 .get();
@@ -1112,7 +1107,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requests);
 
         SearchResponse response = client().prepareSearch("index")
-                .setQuery(hasChildQuery("child", matchQuery("field", "value1").queryName("_name1")).innerHit(new QueryInnerHits()))
+                .setQuery(hasChildQuery("child", matchQuery("field", "value1").queryName("_name1")).innerHit(new QueryInnerHitBuilder()))
                 .addSort("_uid", SortOrder.ASC)
                 .get();
         assertHitCount(response, 2);
@@ -1127,7 +1122,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(1).getInnerHits().get("child").getAt(0).getMatchedQueries()[0], equalTo("_name1"));
 
         response = client().prepareSearch("index")
-                .setQuery(hasChildQuery("child", matchQuery("field", "value2").queryName("_name2")).innerHit(new QueryInnerHits()))
+                .setQuery(hasChildQuery("child", matchQuery("field", "value2").queryName("_name2")).innerHit(new QueryInnerHitBuilder()))
                 .addSort("_id", SortOrder.ASC)
                 .get();
         assertHitCount(response, 1);
@@ -1146,7 +1141,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requests);
 
         SearchResponse response = client().prepareSearch("index1")
-                .setQuery(hasChildQuery("child", matchQuery("field", "value1")).innerHit(new QueryInnerHits(null, new InnerHitsBuilder.InnerHit().setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1))))
+                .setQuery(hasChildQuery("child", matchQuery("field", "value1")).innerHit(new QueryInnerHitBuilder().setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1)))
                 .addSort("_uid", SortOrder.ASC)
                 .get();
         assertNoFailures(response);
@@ -1164,7 +1159,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         .get();
 
         response = client().prepareSearch("index2")
-                .setQuery(nestedQuery("nested", matchQuery("nested.field", "value1")).innerHit(new QueryInnerHits(null, new InnerHitsBuilder.InnerHit().setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1))))
+                .setQuery(nestedQuery("nested", matchQuery("nested.field", "value1")).innerHit(new QueryInnerHitBuilder().setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1)))
                 .addSort("_uid", SortOrder.ASC)
                 .get();
         assertNoFailures(response);
diff --git a/core/src/test/java/org/elasticsearch/search/morelikethis/ItemSerializationTests.java b/core/src/test/java/org/elasticsearch/search/morelikethis/ItemSerializationTests.java
new file mode 100644
index 0000000..5f5f42a
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/morelikethis/ItemSerializationTests.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.morelikethis;
+
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.VersionType;
+import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.util.Random;
+
+public class ItemSerializationTests extends ESTestCase {
+
+    private Item generateRandomItem(int arraySize, int stringSize) {
+        String index = randomAsciiOfLength(stringSize);
+        String type = randomAsciiOfLength(stringSize);
+        String id = String.valueOf(Math.abs(randomInt()));
+        String[] fields = generateRandomStringArray(arraySize, stringSize, true);
+        String routing = randomBoolean() ? randomAsciiOfLength(stringSize) : null;
+        long version = Math.abs(randomLong());
+        VersionType versionType = RandomPicks.randomFrom(new Random(), VersionType.values());
+        return new Item(index, type, id).fields(fields).routing(routing).version(version).versionType(versionType);
+    }
+
+    @Test
+    public void testItemSerialization() throws Exception {
+        int numOfTrials = 100;
+        int maxArraySize = 7;
+        int maxStringSize = 8;
+        for (int i = 0; i < numOfTrials; i++) {
+            Item item1 = generateRandomItem(maxArraySize, maxStringSize);
+            String json = item1.toXContent(XContentFactory.jsonBuilder(), ToXContent.EMPTY_PARAMS).string();
+            XContentParser parser = XContentFactory.xContent(json).createParser(json);
+            Item item2 = Item.parse(parser, ParseFieldMatcher.STRICT, new Item());
+            assertEquals(item1, item2);
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java b/core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java
index dc98d0d..bbc992f 100644
--- a/core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java
+++ b/core/src/test/java/org/elasticsearch/search/morelikethis/MoreLikeThisIT.java
@@ -72,7 +72,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         logger.info("Running moreLikeThis");
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 1l);
     }
 
@@ -92,7 +92,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         logger.info("Running moreLikeThis");
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 0l);
     }
 
@@ -119,24 +119,24 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         logger.info("Running moreLikeThis on index");
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 2l);
 
         logger.info("Running moreLikeThis on beta shard");
         response = client().prepareSearch("beta").setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 1l);
         assertThat(response.getHits().getAt(0).id(), equalTo("3"));
 
         logger.info("Running moreLikeThis on release shard");
         response = client().prepareSearch("release").setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 1l);
         assertThat(response.getHits().getAt(0).id(), equalTo("2"));
 
         logger.info("Running moreLikeThis on alias with node client");
         response = internalCluster().clientNodeClient().prepareSearch("beta").setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(response, 1l);
         assertThat(response.getHits().getAt(0).id(), equalTo("3"));
     }
@@ -156,11 +156,11 @@ public class MoreLikeThisIT extends ESIntegTestCase {
         assertThat(ensureGreen(), equalTo(ClusterHealthStatus.GREEN));
 
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("foo", "bar", "1"))).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("foo", "bar", "1"))).get();
         assertNoFailures(response);
         assertThat(response, notNullValue());
         response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("foo", "bar", "1"))).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("foo", "bar", "1"))).get();
         assertNoFailures(response);
         assertThat(response, notNullValue());
     }
@@ -182,7 +182,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh("foo").execute().actionGet();
 
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("foo", "bar", "1").routing("2"))).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("foo", "bar", "1").routing("2"))).get();
         assertNoFailures(response);
         assertThat(response, notNullValue());
     }
@@ -205,7 +205,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
                 .execute().actionGet();
         client().admin().indices().prepareRefresh("foo").execute().actionGet();
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("foo", "bar", "1").routing("4000"))).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("foo", "bar", "1").routing("4000"))).get();
         assertNoFailures(response);
         assertThat(response, notNullValue());
     }
@@ -233,12 +233,12 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         // Implicit list of fields -> ignore numeric fields
         SearchResponse searchResponse = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type", "1")).minTermFreq(1).minDocFreq(1)).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type", "1")).minTermFreq(1).minDocFreq(1)).get();
         assertHitCount(searchResponse, 1l);
 
         // Explicit list of fields including numeric fields -> fail
         assertThrows(client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder("string_value", "int_value").like(new Item("test", "type", "1")).minTermFreq(1).minDocFreq(1)), SearchPhaseExecutionException.class);
+                new MoreLikeThisQueryBuilder("string_value", "int_value").addLikeItem(new Item("test", "type", "1")).minTermFreq(1).minDocFreq(1)), SearchPhaseExecutionException.class);
 
         // mlt query with no field -> OK
         searchResponse = client().prepareSearch().setQuery(moreLikeThisQuery().likeText("index").minTermFreq(1).minDocFreq(1)).execute().actionGet();
@@ -295,16 +295,16 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         logger.info("Running More Like This with include true");
         SearchResponse response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1).include(true).minimumShouldMatch("0%")).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1).include(true).minimumShouldMatch("0%")).get();
         assertOrderedSearchHits(response, "1", "2");
 
         response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "2")).minTermFreq(1).minDocFreq(1).include(true).minimumShouldMatch("0%")).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "2")).minTermFreq(1).minDocFreq(1).include(true).minimumShouldMatch("0%")).get();
         assertOrderedSearchHits(response, "2", "1");
 
         logger.info("Running More Like This with include false");
         response = client().prepareSearch().setQuery(
-                new MoreLikeThisQueryBuilder().like(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1).minimumShouldMatch("0%")).get();
+                new MoreLikeThisQueryBuilder().addLikeItem(new Item("test", "type1", "1")).minTermFreq(1).minDocFreq(1).minimumShouldMatch("0%")).get();
         assertSearchHits(response, "2");
     }
 
@@ -355,7 +355,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
 
         logger.info("Running MoreLikeThis");
         MoreLikeThisQueryBuilder queryBuilder = QueryBuilders.moreLikeThisQuery("text").include(true).minTermFreq(1).minDocFreq(1)
-                .like(new Item("test", "type0", "0"));
+                .addLikeItem(new Item("test", "type0", "0"));
 
         String[] types = new String[numOfTypes];
         for (int i = 0; i < numOfTypes; i++) {
@@ -573,7 +573,7 @@ public class MoreLikeThisIT extends ESIntegTestCase {
             docs.add(new Item("test", "type1", i+""));
             mltQuery = moreLikeThisQuery()
                     .like(new Item("test", "type1", doc))
-                    .unlike(docs.toArray(Item.EMPTY_ARRAY))
+                    .ignoreLike(docs.toArray(Item.EMPTY_ARRAY))
                     .minTermFreq(0)
                     .minDocFreq(0)
                     .maxQueryTerms(100)
diff --git a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
index 4d85f8b..c232a7c 100644
--- a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
@@ -23,6 +23,7 @@ import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.action.explain.ExplainResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
@@ -157,19 +158,19 @@ public class ExistsMissingIT extends ESIntegTestCase {
                 client().prepareIndex("idx", "type", "3").setSource("g", "bar"),
                 client().prepareIndex("idx", "type", "4").setSource("f", "bar"));
 
-        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, true)).get();
+        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(true).nullValue(true)).get();
         assertSearchHits(resp, "2", "3");
 
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, true)).get();
+        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(true).nullValue(false)).get();
         assertSearchHits(resp, "2", "3");
 
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, false)).get();
+        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(false).nullValue(true)).get();
         assertSearchHits(resp);
 
         try {
-            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, false)).get();
+            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(false).nullValue(false)).get();
             fail("both existence and null_value can't be false");
-        } catch (IllegalArgumentException e) {
+        } catch (SearchPhaseExecutionException e) {
             // expected
         }
     }
@@ -182,19 +183,19 @@ public class ExistsMissingIT extends ESIntegTestCase {
                 client().prepareIndex("idx", "type", "3").setSource("g", "bar"),
                 client().prepareIndex("idx", "type", "4").setSource("f", "bar"));
 
-        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, true)).get();
+        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(true).nullValue(true)).get();
         assertSearchHits(resp, "2", "3", "4");
 
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, true)).get();
+        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(true).nullValue(false)).get();
         assertSearchHits(resp, "3");
 
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, false)).get();
+        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(false).nullValue(true)).get();
         assertSearchHits(resp, "2", "4");
 
         try {
-            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, false)).get();
+            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f").existence(false).nullValue(false)).get();
             fail("both existence and null_value can't be false");
-        } catch (IllegalArgumentException e) {
+        } catch (SearchPhaseExecutionException e) {
             // expected
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
index 28a3664..24eb8cb 100644
--- a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
@@ -27,9 +27,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.sort.SortBuilders;
@@ -171,10 +169,10 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
     @Test
     public void testDefaults() throws ExecutionException, InterruptedException {
-        MatchQuery.Type type = randomBoolean() ? MatchQueryBuilder.DEFAULT_TYPE : MatchQuery.Type.BOOLEAN;
+        MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -186,25 +184,25 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
     }
@@ -213,18 +211,18 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     public void testPhraseType() {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Man the Ultimate", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQuery.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Captain", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQuery.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertThat(searchResponse.getHits().getTotalHits(), greaterThan(1l));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the Ul", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQuery.Type.PHRASE_PREFIX))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
         assertSearchHits(searchResponse, "ultimate2", "ultimate1");
         assertHitCount(searchResponse, 2l);
     }
@@ -258,7 +256,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                     .setQuery(multiMatchQueryBuilder).get();
             MatchQueryBuilder matchQueryBuilder = QueryBuilders.matchQuery(field, builder.toString());
             if (getType(multiMatchQueryBuilder) != null) {
-                matchQueryBuilder.type(MatchQuery.Type.valueOf(getType(multiMatchQueryBuilder).matchQueryType().toString()));
+                matchQueryBuilder.type(MatchQueryBuilder.Type.valueOf(getType(multiMatchQueryBuilder).matchQueryType().toString()));
             }
             SearchResponse matchResp = client().prepareSearch("test")
                     // _uid tie sort
@@ -279,11 +277,11 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     public void testCutoffFreq() throws ExecutionException, InterruptedException {
         final long numDocs = client().prepareCount("test")
                 .setQuery(matchAllQuery()).get().getCount();
-        MatchQuery.Type type = randomBoolean() ? MatchQueryBuilder.DEFAULT_TYPE : MatchQuery.Type.BOOLEAN;
+        MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
         Float cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -296,39 +294,39 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
         long size = searchResponse.getHits().getTotalHits();
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat("common terms expected to be a way smaller result set", size, lessThan(searchResponse.getHits().getTotalHits()));
 
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency)
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency)
                         .analyzer("category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS))).get();
         assertHitCount(searchResponse, 1l);
@@ -344,13 +342,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         int numIters = scaledRandomIntBetween(5, 10);
         for (int i = 0; i < numIters; i++) {
             {
-                MatchQuery.Type type = randomBoolean() ? MatchQueryBuilder.DEFAULT_TYPE : MatchQuery.Type.BOOLEAN;
+                MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
                 MultiMatchQueryBuilder multiMatchQueryBuilder = randomBoolean() ? multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category") :
                         multiMatchQuery("marvel hero captain america", "*_name", randomBoolean() ? "category" : "categ*");
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQueryBuilder
-                                .operator(Operator.OR).type(type))).get();
+                                .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
 
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -364,9 +362,9 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             }
 
             {
-                MatchQuery.Type type = randomBoolean() ? MatchQueryBuilder.DEFAULT_TYPE : MatchQuery.Type.BOOLEAN;
+                MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 MultiMatchQueryBuilder multiMatchQueryBuilder = randomBoolean() ? multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category") :
                         multiMatchQuery("captain america", "*_name", randomBoolean() ? "category" : "categ*");
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
@@ -387,11 +385,11 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQuery("capta", "full_name", "first_name", "last_name", "category")
-                                .type(MatchQuery.Type.PHRASE_PREFIX).useDisMax(false).minimumShouldMatch(minShouldMatch))).get();
+                                .type(MatchQueryBuilder.Type.PHRASE_PREFIX).useDisMax(false).minimumShouldMatch(minShouldMatch))).get();
 
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -405,18 +403,18 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             }
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left;
                 if (randomBoolean()) {
                     left = client().prepareSearch("test").setSize(numDocs)
                             .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                             .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                                    .type(MatchQuery.Type.PHRASE).useDisMax(false).minimumShouldMatch(minShouldMatch))).get();
+                                    .type(MatchQueryBuilder.Type.PHRASE).useDisMax(false).minimumShouldMatch(minShouldMatch))).get();
                 } else {
                     left = client().prepareSearch("test").setSize(numDocs)
                             .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                             .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                                    .type(MatchQuery.Type.PHRASE).tieBreaker(1.0f).minimumShouldMatch(minShouldMatch))).get();
+                                    .type(MatchQueryBuilder.Type.PHRASE).tieBreaker(1.0f).minimumShouldMatch(minShouldMatch))).get();
                 }
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -436,13 +434,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
         assertSecondHit(searchResponse, hasId("theother"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -450,13 +448,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -464,7 +462,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america 15", "full_name", "first_name", "last_name", "category", "skill")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -485,7 +483,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .cutoffFrequency(0.1f)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theother"), hasId("theone")));
         long numResults = searchResponse.getHits().totalHits();
 
@@ -493,7 +491,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertThat(numResults, lessThan(searchResponse.getHits().getTotalHits()));
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -503,28 +501,28 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
-                        .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : MultiMatchQueryBuilder.DEFAULT_TYPE)
-                        .operator(Operator.AND))).get();
+                        .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
-                        .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : MultiMatchQueryBuilder.DEFAULT_TYPE)
-                        .operator(Operator.AND))).get();
+                        .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // test if boosts work
         searchResponse = client().prepareSearch("test")
-                .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name", "category").field("last_name", 2)
+                .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name^2", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate1"));   // has ultimate in the last_name and that is boosted
         assertSecondHit(searchResponse, hasId("ultimate2"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -534,7 +532,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertSecondHit(searchResponse, hasId("ultimate1"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -560,6 +558,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         }
     }
 
+
     public static List<String> fill(List<String> list, String value, int times) {
         for (int i = 0; i < times; i++) {
             list.add(value);
@@ -600,24 +599,24 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 switch (type) {
                     case BEST_FIELDS:
                         if (randomBoolean()) {
-                            oType = MatchQuery.Type.BOOLEAN;
+                            oType = MatchQueryBuilder.Type.BOOLEAN;
                         }
                         break;
                     case MOST_FIELDS:
                         if (randomBoolean()) {
-                            oType = MatchQuery.Type.BOOLEAN;
+                            oType = MatchQueryBuilder.Type.BOOLEAN;
                         }
                         break;
                     case CROSS_FIELDS:
                         break;
                     case PHRASE:
                         if (randomBoolean()) {
-                            oType = MatchQuery.Type.PHRASE;
+                            oType = MatchQueryBuilder.Type.PHRASE;
                         }
                         break;
                     case PHRASE_PREFIX:
                         if (randomBoolean()) {
-                            oType = MatchQuery.Type.PHRASE_PREFIX;
+                            oType = MatchQueryBuilder.Type.PHRASE_PREFIX;
                         }
                         break;
                 }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 83a8008..8ee123f 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.query;
 
 import org.apache.lucene.util.English;
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -31,10 +32,15 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.*;
-import org.elasticsearch.index.search.MatchQuery.Type;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
+import org.elasticsearch.index.query.BoolQueryBuilder;
+import org.elasticsearch.index.query.CommonTermsQueryBuilder.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.query.MatchQueryBuilder.Type;
+import org.elasticsearch.index.query.MultiMatchQueryBuilder;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.QueryStringQueryBuilder;
+import org.elasticsearch.index.query.TermQueryBuilder;
+import org.elasticsearch.index.query.WrapperQueryBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
@@ -56,8 +62,24 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
+import static org.hamcrest.Matchers.allOf;
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.is;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -163,7 +185,7 @@ public class SearchQueryIT extends ESIntegTestCase {
                 client().prepareIndex("test", "type1", "1").setSource("field1", "quick brown fox", "field2", "quick brown fox"),
                 client().prepareIndex("test", "type1", "2").setSource("field1", "quick lazy huge brown fox", "field2", "quick lazy huge brown fox"));
 
-        SearchResponse searchResponse = client().prepareSearch().setQuery(matchQuery("field2", "quick brown").type(Type.PHRASE).slop(0)).get();
+        SearchResponse searchResponse = client().prepareSearch().setQuery(matchQuery("field2", "quick brown").type(MatchQueryBuilder.Type.PHRASE).slop(0)).get();
         assertHitCount(searchResponse, 1l);
 
         assertFailures(client().prepareSearch().setQuery(matchQuery("field1", "quick brown").type(Type.PHRASE).slop(0)),
@@ -327,18 +349,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -346,7 +368,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -419,18 +441,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -443,7 +465,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSecondHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -467,10 +489,10 @@ public class SearchQueryIT extends ESIntegTestCase {
                         client().prepareIndex("test", "type1", "2").setSource("field1", "quick lazy huge brown fox", "field2", "quick lazy huge brown fox"));
 
 
-                SearchResponse searchResponse = client().prepareSearch().setQuery(matchQuery("field2", "quick brown").type(Type.PHRASE).slop(0)).get();
+                SearchResponse searchResponse = client().prepareSearch().setQuery(matchQuery("field2", "quick brown").type(MatchQueryBuilder.Type.PHRASE).slop(0)).get();
                 assertHitCount(searchResponse, 1l);
                 try {
-                    client().prepareSearch().setQuery(matchQuery("field1", "quick brown").type(Type.PHRASE).slop(0)).get();
+                    client().prepareSearch().setQuery(matchQuery("field1", "quick brown").type(MatchQueryBuilder.Type.PHRASE).slop(0)).get();
                     fail("SearchPhaseExecutionException should have been thrown");
                 } catch (SearchPhaseExecutionException e) {
                     assertTrue(e.toString().contains("IllegalStateException[field \"field1\" was indexed without position data; cannot run PhraseQuery"));
@@ -901,7 +923,7 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1", "field1", "field2")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch()
                 .setQuery(builder)
                 .get();
@@ -909,15 +931,15 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("1"));
 
         refresh();
-        builder = multiMatchQuery("value1", "field1").field("field3", 1.5f)
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+        builder = multiMatchQuery("value1", "field1", "field3^1.5")
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1").field("field1").field("field3", 1.5f)
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
@@ -947,18 +969,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         BoolQueryBuilder boolQuery = boolQuery()
-                .must(matchQuery("field1", "a").zeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE))
-                .must(matchQuery("field1", "value1").zeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE));
+                .must(matchQuery("field1", "a").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.NONE))
+                .must(matchQuery("field1", "value1").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.NONE));
         SearchResponse searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 0l);
 
         boolQuery = boolQuery()
-                .must(matchQuery("field1", "a").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL))
-                .must(matchQuery("field1", "value1").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL));
+                .must(matchQuery("field1", "a").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL))
+                .must(matchQuery("field1", "value1").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL));
         searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 1l);
 
-        boolQuery = boolQuery().must(matchQuery("field1", "a").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL));
+        boolQuery = boolQuery().must(matchQuery("field1", "a").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL));
         searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 2l);
     }
@@ -972,18 +994,18 @@ public class SearchQueryIT extends ESIntegTestCase {
 
 
         BoolQueryBuilder boolQuery = boolQuery()
-                .must(multiMatchQuery("a", "field1", "field2").zeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE))
-                .must(multiMatchQuery("value1", "field1", "field2").zeroTermsQuery(MatchQuery.ZeroTermsQuery.NONE)); // Fields are ORed together
+                .must(multiMatchQuery("a", "field1", "field2").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.NONE))
+                .must(multiMatchQuery("value1", "field1", "field2").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.NONE)); // Fields are ORed together
         SearchResponse searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 0l);
 
         boolQuery = boolQuery()
-                .must(multiMatchQuery("a", "field1", "field2").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL))
-                .must(multiMatchQuery("value4", "field1", "field2").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL));
+                .must(multiMatchQuery("a", "field1", "field2").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL))
+                .must(multiMatchQuery("value4", "field1", "field2").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL));
         searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 1l);
 
-        boolQuery = boolQuery().must(multiMatchQuery("a", "field1").zeroTermsQuery(MatchQuery.ZeroTermsQuery.ALL));
+        boolQuery = boolQuery().must(multiMatchQuery("a", "field1").zeroTermsQuery(MatchQueryBuilder.ZeroTermsQuery.ALL));
         searchResponse = client().prepareSearch().setQuery(boolQuery).get();
         assertHitCount(searchResponse, 2l);
     }
@@ -1232,54 +1254,63 @@ public class SearchQueryIT extends ESIntegTestCase {
                 client().prepareIndex("test", "type", "4").setSource("term", "4") );
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term" , new TermsLookup("lookup", "type", "1", "terms"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup").lookupType("type").lookupId("1").lookupPath("terms")
+                ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "1", "3");
 
         // same as above, just on the _id...
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("_id", new TermsLookup("lookup", "type", "1", "terms"))
+                .setQuery(termsLookupQuery("_id").lookupIndex("lookup").lookupType("type").lookupId("1").lookupPath("terms")
                 ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "1", "3");
 
         // another search with same parameters...
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup", "type", "1", "terms"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup").lookupType("type").lookupId("1").lookupPath("terms")
+                ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "1", "3");
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup", "type", "2", "terms"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup").lookupType("type").lookupId("2").lookupPath("terms")
+                ).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup", "type", "3", "terms"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup").lookupType("type").lookupId("3").lookupPath("terms")
+                ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "2", "4");
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup", "type", "4", "terms"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup").lookupType("type").lookupId("4").lookupPath("terms")
+                ).get();
         assertHitCount(searchResponse, 0l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup2", "type", "1", "arr.term"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup2").lookupType("type").lookupId("1").lookupPath("arr.term")
+                ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "1", "3");
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup2", "type", "2", "arr.term"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup2").lookupType("type").lookupId("2").lookupPath("arr.term")
+                ).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("term", new TermsLookup("lookup2", "type", "3", "arr.term"))).get();
+                .setQuery(termsLookupQuery("term").lookupIndex("lookup2").lookupType("type").lookupId("3").lookupPath("arr.term")
+                ).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "2", "4");
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(termsLookupQuery("not_exists", new TermsLookup("lookup2", "type", "3", "arr.term"))).get();
+                .setQuery(termsLookupQuery("not_exists").lookupIndex("lookup2").lookupType("type").lookupId("3").lookupPath("arr.term")
+                ).get();
         assertHitCount(searchResponse, 0l);
     }
 
@@ -1548,12 +1579,14 @@ public class SearchQueryIT extends ESIntegTestCase {
                 client().prepareIndex("test", "test", "4").setSource("description", "foo"));
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanTermQuery("description", "bar"))).get();
+                .setQuery(spanOrQuery().clause(spanTermQuery("description", "bar"))).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test").setQuery(
-                spanNearQuery(spanTermQuery("description", "foo"), 3)
-                        .clause(spanTermQuery("description", "other"))).get();
+                spanNearQuery()
+                        .clause(spanTermQuery("description", "foo"))
+                        .clause(spanTermQuery("description", "other"))
+                        .slop(3)).get();
         assertHitCount(searchResponse, 3l);
     }
 
@@ -1568,24 +1601,24 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse response = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanMultiTermQueryBuilder(fuzzyQuery("description", "fop")))).get();
+                .setQuery(spanOrQuery().clause(spanMultiTermQueryBuilder(fuzzyQuery("description", "fop")))).get();
         assertHitCount(response, 4);
 
         response = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanMultiTermQueryBuilder(prefixQuery("description", "fo")))).get();
+                .setQuery(spanOrQuery().clause(spanMultiTermQueryBuilder(prefixQuery("description", "fo")))).get();
         assertHitCount(response, 4);
 
         response = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanMultiTermQueryBuilder(wildcardQuery("description", "oth*")))).get();
+                .setQuery(spanOrQuery().clause(spanMultiTermQueryBuilder(wildcardQuery("description", "oth*")))).get();
         assertHitCount(response, 3);
 
         response = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanMultiTermQueryBuilder(QueryBuilders.rangeQuery("description").from("ffa").to("foo"))))
+                .setQuery(spanOrQuery().clause(spanMultiTermQueryBuilder(QueryBuilders.rangeQuery("description").from("ffa").to("foo"))))
                 .execute().actionGet();
         assertHitCount(response, 3);
 
         response = client().prepareSearch("test")
-                .setQuery(spanOrQuery(spanMultiTermQueryBuilder(regexpQuery("description", "fo{2}")))).get();
+                .setQuery(spanOrQuery().clause(spanMultiTermQueryBuilder(regexpQuery("description", "fo{2}")))).get();
         assertHitCount(response, 3);
     }
 
@@ -1598,19 +1631,33 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "brown"))).get();
+                .setQuery(spanNotQuery().include(spanNearQuery()
+                        .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "brown"))).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "sleeping")).dist(5)).get();
+                .setQuery(spanNotQuery().include(spanNearQuery()
+                        .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "sleeping")).dist(5)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(QueryBuilders.spanTermQuery("description", "quick"), 1)
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "jumped")).pre(1).post(1)).get();
+                .setQuery(spanNotQuery().include(spanNearQuery()
+                        .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).pre(1).post(1)).get();
         assertHitCount(searchResponse, 1l);
+
+        try {
+            client().prepareSearch("test")
+                    .setQuery(spanNotQuery().include(spanNearQuery()
+                            .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                            .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).dist(2).pre(2)
+                    ).get();
+            fail("ElasticsearchIllegalArgumentException should have been caught");
+        } catch (ElasticsearchException e) {
+            assertThat("ElasticsearchIllegalArgumentException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
+        }
     }
 
     @Test
@@ -1706,18 +1753,18 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1737,12 +1784,12 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "the fox runs across the street").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "run fox run").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1763,19 +1810,19 @@ public class SearchQueryIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
 
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1801,7 +1848,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("test")
                 .setQuery(
-                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(Operator.AND)
+                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(QueryStringQueryBuilder.Operator.AND)
                                 .field("field1").field("field2")).get();
         assertHitCount(response, 1l);
     }
@@ -1814,15 +1861,15 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(multiMatchQuery("value2", "field2").field("field1", 2).lenient(true).useDisMax(false)).get();
+                .setQuery(multiMatchQuery("value2", "field1^2", "field2").lenient(true).useDisMax(false)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(multiMatchQuery("value2", "field2").field("field1", 2).lenient(true).useDisMax(true)).get();
+                .setQuery(multiMatchQuery("value2", "field1^2", "field2").lenient(true).useDisMax(true)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(multiMatchQuery("value2").field("field2", 2).lenient(true)).get();
+                .setQuery(multiMatchQuery("value2", "field2^2").lenient(true)).get();
         assertHitCount(searchResponse, 1l);
     }
 
@@ -1867,7 +1914,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSearchHits(searchResponse, "1", "2", "3");
         searchResponse = client().prepareSearch("index1", "index2", "index3")
                 .setQuery(indicesQuery(matchQuery("text", "value1"), "index1")
-                        .noMatchQuery(QueryBuilders.matchAllQuery())).get();
+                        .noMatchQuery("all")).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
@@ -1898,7 +1945,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         } catch (SearchPhaseExecutionException e) {
             assertThat(e.shardFailures().length, greaterThan(0));
             for (ShardSearchFailure shardSearchFailure : e.shardFailures()) {
-                assertThat(shardSearchFailure.reason(), containsString("no mapping found for type [child]"));
+                assertThat(shardSearchFailure.reason(), containsString("No mapping for for type [child]"));
             }
         }
 
@@ -2105,7 +2152,7 @@ functionScoreQuery(scriptFunction(new Script("_doc['score'].value")))).setMinSco
             client().prepareSearch("test")
                     .setQuery(QueryBuilders.rangeQuery("date").from(1388534400000L).to(1388537940999L).timeZone("+01:00"))
                     .get();
-            fail("A Range Filter using ms since epoch with a TimeZone should raise a ParsingException");
+            fail("A Range Filter using ms since epoch with a TimeZone should raise a QueryParsingException");
         } catch (SearchPhaseExecutionException e) {
             // We expect it
         }
@@ -2127,7 +2174,7 @@ functionScoreQuery(scriptFunction(new Script("_doc['score'].value")))).setMinSco
             client().prepareSearch("test")
                     .setQuery(QueryBuilders.rangeQuery("num").from("0").to("4").timeZone("-01:00"))
                     .get();
-            fail("A Range Filter on a numeric field with a TimeZone should raise a ParsingException");
+            fail("A Range Filter on a numeric field with a TimeZone should raise a QueryParsingException");
         } catch (SearchPhaseExecutionException e) {
             // We expect it
         }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index bf3e458..3a857bf 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -24,7 +24,7 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.Operator;
+import org.elasticsearch.index.query.SimpleQueryStringBuilder;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -34,7 +34,10 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
+import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -68,7 +71,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
-                simpleQueryStringQuery("foo bar").defaultOperator(Operator.AND)).get();
+                simpleQueryStringQuery("foo bar").defaultOperator(SimpleQueryStringBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
@@ -249,21 +252,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
@@ -280,7 +283,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
index 5b559da..6aa31ca 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.script.Script;
@@ -117,7 +116,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         ensureYellow();
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -127,7 +126,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThat(searchResponse.getHits().getHits()[2].getId(), equalTo("2"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -137,7 +136,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -180,7 +179,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh("test").execute().actionGet();
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -195,7 +194,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -212,7 +211,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Make sure non-zero from works:
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(2)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -321,7 +320,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
             .setSize(5).execute().actionGet();
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -334,7 +333,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Now, penalizing rescore (nothing matches the rescore query):
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -426,7 +425,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -441,7 +440,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
             
             // check equivalence
@@ -451,7 +450,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -469,7 +468,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -504,7 +503,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
                                     .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
@@ -542,7 +541,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
@@ -565,7 +564,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 searchResponse = client()
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                         .addRescorer(innerRescoreQuery).setRescoreWindow(5)
                         .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
index 1359f97..e55a736 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
@@ -869,7 +869,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
     }
 
     /**
-     * Searching for a rare phrase shouldn't provide any suggestions if confidence > 1.  This was possible before we rechecked the cutoff
+     * Searching for a rare phrase shouldn't provide any suggestions if confidence &gt; 1.  This was possible before we rechecked the cutoff
      * score during the reduce phase.  Failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
      */
     @Test
@@ -937,7 +937,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
     }
 
     /**
-     * If the suggester finds tons of options then picking the right one is slow without <<<INSERT SOLUTION HERE>>>.
+     * If the suggester finds tons of options then picking the right one is slow without &lt;&lt;&lt;INSERT SOLUTION HERE&gt;&gt;&gt;.
      */
     @Test
     @Nightly
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 4d14f49..40b9025 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -160,9 +160,9 @@ import static org.hamcrest.Matchers.*;
  * <li>{@link Scope#TEST} - uses a new cluster for each individual test method.</li>
  * <li>{@link Scope#SUITE} - uses a cluster shared across all test methods in the same suite</li>
  * </ul>
- * <p/>
+ * <p>
  * The most common test scope is {@link Scope#SUITE} which shares a cluster per test suite.
- * <p/>
+ * <p>
  * If the test methods need specific node settings or change persistent and/or transient cluster settings {@link Scope#TEST}
  * should be used. To configure a scope for the test cluster the {@link ClusterScope} annotation
  * should be used, here is an example:
@@ -172,25 +172,23 @@ import static org.hamcrest.Matchers.*;
  * @Test public void testMethod() {}
  * }
  * </pre>
- * <p/>
+ * <p>
  * If no {@link ClusterScope} annotation is present on an integration test the default scope is {@link Scope#SUITE}
- * <p/>
+ * <p>
  * A test cluster creates a set of nodes in the background before the test starts. The number of nodes in the cluster is
  * determined at random and can change across tests. The {@link ClusterScope} allows configuring the initial number of nodes
  * that are created before the tests start.
- * <p/>
  *  <pre>
  * @ClusterScope(scope=Scope.SUITE, numDataNodes=3)
  * public class SomeIT extends ESIntegTestCase {
  * @Test public void testMethod() {}
  * }
  * </pre>
- * <p/>
+ * <p>
  * Note, the {@link ESIntegTestCase} uses randomized settings on a cluster and index level. For instance
  * each test might use different directory implementation for each test or will return a random client to one of the
  * nodes in the cluster for each call to {@link #client()}. Test failures might only be reproducible if the correct
  * system properties are passed to the test execution environment.
- * <p/>
  * <p>
  * This class supports the following system properties (passed with -Dkey=value to the application)
  * <ul>
@@ -199,7 +197,6 @@ import static org.hamcrest.Matchers.*;
  * useful to test the system without asserting modules that to make sure they don't hide any bugs in production.</li>
  * <li> - a random seed used to initialize the index random context.
  * </ul>
- * </p>
  */
 @LuceneTestCase.SuppressFileSystems("ExtrasFS") // doesn't work with potential multi data path from test cluster yet
 public abstract class ESIntegTestCase extends ESTestCase {
@@ -211,7 +208,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     /**
      * Annotation for third-party integration tests.
-     * <p/>
+     * <p>
      * These are tests the require a third-party service in order to run. They
      * may require the user to manually configure an external process (such as rabbitmq),
      * or may additionally require some external configuration (e.g. AWS credentials)
@@ -964,7 +961,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
      *
      * @param numDocs number of documents to wait for.
      * @return the actual number of docs seen.
-     * @throws InterruptedException
      */
     public long waitForDocs(final long numDocs) throws InterruptedException {
         return waitForDocs(numDocs, null);
@@ -977,7 +973,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
      * @param indexer a {@link org.elasticsearch.test.BackgroundIndexer}. If supplied it will be first checked for documents indexed.
      *                This saves on unneeded searches.
      * @return the actual number of docs seen.
-     * @throws InterruptedException
      */
     public long waitForDocs(final long numDocs, final @Nullable BackgroundIndexer indexer) throws InterruptedException {
         // indexing threads can wait for up to ~1m before retrying when they first try to index into a shard which is not STARTED.
@@ -993,7 +988,6 @@ public abstract class ESIntegTestCase extends ESTestCase {
      * @param indexer         a {@link org.elasticsearch.test.BackgroundIndexer}. If supplied it will be first checked for documents indexed.
      *                        This saves on unneeded searches.
      * @return the actual number of docs seen.
-     * @throws InterruptedException
      */
     public long waitForDocs(final long numDocs, int maxWaitTime, TimeUnit maxWaitTimeUnit, final @Nullable BackgroundIndexer indexer)
             throws InterruptedException {
@@ -1219,11 +1213,10 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     /**
      * Syntactic sugar for:
-     * <p/>
      * <pre>
      *   return client().prepareIndex(index, type, id).setSource(source).execute().actionGet();
      * </pre>
-     * <p/>
+     * <p>
      * where source is a String.
      */
     protected final IndexResponse index(String index, String type, String id, String source) {
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 7f82ac9..a1c511d 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -225,7 +225,7 @@ public abstract class ESTestCase extends LuceneTestCase {
     }
 
     // -----------------------------------------------------------------
-    // Test facilities and facades for subclasses.
+    // Test facilities and facades for subclasses. 
     // -----------------------------------------------------------------
 
     // TODO: replaces uses of getRandom() with random()
@@ -241,7 +241,7 @@ public abstract class ESTestCase extends LuceneTestCase {
     /**
      * Returns a "scaled" random number between min and max (inclusive).
      *
-     * @see RandomizedTest#scaledRandomIntBetween(int, int);
+     * @see RandomizedTest#scaledRandomIntBetween(int, int)
      */
     public static int scaledRandomIntBetween(int min, int max) {
         return RandomizedTest.scaledRandomIntBetween(min, max);
@@ -305,35 +305,6 @@ public abstract class ESTestCase extends LuceneTestCase {
         return random().nextDouble();
     }
 
-    /**
-     * Returns a double value in the interval [start, end) if lowerInclusive is
-     * set to true, (start, end) otherwise.
-     *
-     * @param start lower bound of interval to draw uniformly distributed random numbers from
-     * @param end upper bound
-     * @param lowerInclusive whether or not to include lower end of the interval
-     * */
-    public static double randomDoubleBetween(double start, double end, boolean lowerInclusive) {
-        double result = 0.0;
-
-        if (start == -Double.MAX_VALUE || end == Double.MAX_VALUE) {
-            // formula below does not work with very large doubles
-            result = Double.longBitsToDouble(randomLong());
-            while (result < start || result > end || Double.isNaN(result)) {
-                result = Double.longBitsToDouble(randomLong());
-            }
-        } else {
-            result = randomDouble();
-            if (lowerInclusive == false) {
-                while (result <= 0.0) {
-                    result = randomDouble();
-                }
-            }
-            result = result * end + (1.0 - result) * start;
-        }
-        return result;
-    }
-
     public static long randomLong() {
         return random().nextLong();
     }
@@ -393,27 +364,17 @@ public abstract class ESTestCase extends LuceneTestCase {
         return RandomizedTest.randomRealisticUnicodeOfCodepointLength(codePoints);
     }
 
-    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull, boolean allowEmpty) {
+    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull) {
         if (allowNull && random().nextBoolean()) {
             return null;
         }
-        int arraySize = randomIntBetween(allowEmpty ? 0 : 1, maxArraySize);
-        String[] array = new String[arraySize];
-        for (int i = 0; i < arraySize; i++) {
+        String[] array = new String[random().nextInt(maxArraySize)]; // allow empty arrays
+        for (int i = 0; i < array.length; i++) {
             array[i] = RandomStrings.randomAsciiOfLength(random(), maxStringSize);
         }
         return array;
     }
 
-    public static String[] generateRandomStringArray(int maxArraySize, int maxStringSize, boolean allowNull) {
-        return generateRandomStringArray(maxArraySize, maxStringSize, allowNull, true);
-    }
-
-    public static String randomTimeValue() {
-        final String[] values = new String[]{"d", "H", "ms", "s", "S", "w"};
-        return randomIntBetween(0, 1000) + randomFrom(values);
-    }
-
     /**
      * Runs the code block for 10 seconds waiting for no assertion to trip.
      */
@@ -510,7 +471,7 @@ public abstract class ESTestCase extends LuceneTestCase {
      */
     @Override
     public Path getDataPath(String relativePath) {
-        // we override LTC behavior here: wrap even resources with mockfilesystems,
+        // we override LTC behavior here: wrap even resources with mockfilesystems, 
         // because some code is buggy when it comes to multiple nio.2 filesystems
         // (e.g. FileSystemUtils, and likely some tests)
         try {
diff --git a/core/src/test/java/org/elasticsearch/test/geo/RandomShapeGenerator.java b/core/src/test/java/org/elasticsearch/test/geo/RandomShapeGenerator.java
index 2bf231e..7144ab7 100644
--- a/core/src/test/java/org/elasticsearch/test/geo/RandomShapeGenerator.java
+++ b/core/src/test/java/org/elasticsearch/test/geo/RandomShapeGenerator.java
@@ -242,7 +242,7 @@ public class RandomShapeGenerator {
         }
     }
 
-    public static Point xRandomPoint(Random r) {
+    protected static Point xRandomPoint(Random r) {
         return xRandomPointIn(r, ctx.getWorldBounds());
     }
 
@@ -256,7 +256,7 @@ public class RandomShapeGenerator {
         return p;
     }
 
-    public static Rectangle xRandomRectangle(Random r, Point nearP) {
+    protected static Rectangle xRandomRectangle(Random r, Point nearP) {
         Rectangle bounds = ctx.getWorldBounds();
         if (nearP == null)
             nearP = xRandomPointIn(r, bounds);
diff --git a/core/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java b/core/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java
index a136056..ade7fbd 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java
@@ -28,7 +28,7 @@ import static org.junit.Assert.fail;
 
 /**
  * Represents a gt assert section:
- * <p/>
+ * <p>
  * - gt:    { fields._ttl: 0}
  */
 public class GreaterThanAssertion extends Assertion {
diff --git a/core/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java b/core/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java
index 4e81618..265487a 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java
@@ -30,7 +30,7 @@ import static org.junit.Assert.assertThat;
 
 /**
  * Represents a length assert section:
- * <p/>
+ * <p>
  * - length:   { hits.hits: 1  }
  */
 public class LengthAssertion extends Assertion {
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index c253a75..d9b9b49 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -80,7 +80,7 @@ public class AssertingLocalTransport extends LocalTransport {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
-
+    
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 04e4154..2810c09 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -54,11 +54,11 @@ import org.elasticsearch.index.query.GeoShapeQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.TermsQueryBuilder;
-import org.elasticsearch.indices.cache.query.terms.TermsLookup;
+import org.elasticsearch.index.query.TermsLookupQueryBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
@@ -161,7 +161,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("username", "foo").endObject()).get();
         transportClient().admin().indices().prepareRefresh(queryIndex, lookupIndex).get();
 
-        TermsQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username", new TermsLookup(lookupIndex, "type", "1", "followers"));
+        TermsLookupQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
         BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).must(termsLookupFilterBuilder);
 
         SearchResponse searchResponse = transportClient()
@@ -230,7 +230,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
         transportClient().admin().indices().prepareRefresh(lookupIndex, queryIndex).get();
 
         MoreLikeThisQueryBuilder moreLikeThisQueryBuilder = QueryBuilders.moreLikeThisQuery("name")
-                .like(new Item(lookupIndex, "type", "1"))
+                .addLikeItem(new Item(lookupIndex, "type", "1"))
                 .minTermFreq(1)
                 .minDocFreq(1);
 
diff --git a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
index 505d416..044bf30 100644
--- a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
@@ -236,7 +236,7 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
                 containsString("+field:pidgin (field:huge field:brown)"), true);
         assertExplanation(QueryBuilders.commonTermsQuery("field", "the brown").analyzer("stop"),
                 containsString("field:brown"), true);
-
+        
         // match queries with cutoff frequency
         assertExplanation(QueryBuilders.matchQuery("field", "huge brown pidgin").cutoffFrequency(1),
                 containsString("+field:pidgin (field:huge field:brown)"), true);
@@ -276,7 +276,11 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareValidateQuery("test").setSource(new BytesArray("{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}")).get().isValid(), equalTo(false));
     }
 
-    private static void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher) {
+        assertExplanation(queryBuilder, matcher, false);
+    }
+
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
         ValidateQueryResponse response = client().admin().indices().prepareValidateQuery("test")
                 .setTypes("type1")
                 .setQuery(queryBuilder)
diff --git a/core/src/test/resources/org/elasticsearch/index/query/has-child-with-inner-hits.json b/core/src/test/resources/org/elasticsearch/index/query/has-child-with-inner-hits.json
deleted file mode 100644
index 38d4483..0000000
--- a/core/src/test/resources/org/elasticsearch/index/query/has-child-with-inner-hits.json
+++ /dev/null
@@ -1,30 +0,0 @@
-{
-  "has_child" : {
-    "query" : {
-      "range" : {
-        "mapped_string" : {
-          "from" : "agJhRET",
-          "to" : "zvqIq",
-          "include_lower" : true,
-          "include_upper" : true,
-          "boost" : 1.0
-        }
-      }
-    },
-    "child_type" : "child",
-    "score_mode" : "avg",
-    "min_children" : 883170873,
-    "max_children" : 1217235442,
-    "boost" : 2.0,
-    "_name" : "WNzYMJKRwePuRBh",
-    "inner_hits" : {
-      "name" : "inner_hits_name",
-      "size" : 100,
-      "sort" : [ {
-        "mapped_string" : {
-          "order" : "asc"
-        }
-      } ]
-    }
-  }
-}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/simple-query-string.json b/core/src/test/resources/org/elasticsearch/index/query/simple-query-string.json
new file mode 100644
index 0000000..9208e88
--- /dev/null
+++ b/core/src/test/resources/org/elasticsearch/index/query/simple-query-string.json
@@ -0,0 +1,8 @@
+{
+  "simple_query_string": {
+    "query": "foo bar",
+    "analyzer": "keyword",
+    "fields": ["body^5","_all"],
+    "default_operator": "and"
+  }
+}
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index 2678024..8e7cf3a 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -62,15 +62,18 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "analysis-phonetic",
                    "analysis-smartcn",
                    "analysis-stempel",
-                   "cloud-aws",
-                   "cloud-azure",
                    "cloud-gce",
                    "delete-by-query",
+                   "discovery-azure",
+                   "discovery-ec2",
                    "discovery-multicast",
                    "lang-javascript",
                    "lang-python",
                    "mapper-murmur3",
-                   "mapper-size"]
+                   "mapper-size",
+                   "repository-azure",
+                   "repository-s3",
+                   "store-smb"]
 
 try:
   JAVA_HOME = os.environ['JAVA_HOME']
diff --git a/distribution/src/main/resources/bin/plugin b/distribution/src/main/resources/bin/plugin
index d2d112c..0eef371 100755
--- a/distribution/src/main/resources/bin/plugin
+++ b/distribution/src/main/resources/bin/plugin
@@ -65,6 +65,11 @@ else
     JAVA=`which java`
 fi
 
+if [ ! -x "$JAVA" ]; then
+    echo "Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME"
+    exit 1
+fi
+
 # real getopt cannot be used because we need to hand options over to the PluginManager
 while [ $# -gt 0 ]; do
   case $1 in
diff --git a/docs/plugins/cloud-azure.asciidoc b/docs/plugins/cloud-azure.asciidoc
deleted file mode 100644
index 5427450..0000000
--- a/docs/plugins/cloud-azure.asciidoc
+++ /dev/null
@@ -1,683 +0,0 @@
-[[cloud-azure]]
-=== Azure Cloud Plugin
-
-The Azure Cloud plugin uses the Azure API for unicast discovery, and adds
-support for using Azure as a repository for
-{ref}/modules-snapshots.html[Snapshot/Restore].
-
-[[cloud-azure-install]]
-[float]
-==== Installation
-
-This plugin can be installed using the plugin manager:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin install cloud-azure
-----------------------------------------------------------------
-
-The plugin must be installed on every node in the cluster, and each node must
-be restarted after installation.
-
-[[cloud-azure-remove]]
-[float]
-==== Removal
-
-The plugin can be removed with the following command:
-
-[source,sh]
-----------------------------------------------------------------
-sudo bin/plugin remove cloud-azure
-----------------------------------------------------------------
-
-The node must be stopped before removing the plugin.
-
-[[cloud-azure-discovery]]
-==== Azure Virtual Machine Discovery
-
-Azure VM discovery allows to use the azure APIs to perform automatic discovery (similar to multicast in non hostile
-multicast environments). Here is a simple sample configuration:
-
-[source,yaml]
-----
-cloud:
-    azure:
-        management:
-             subscription.id: XXX-XXX-XXX-XXX
-             cloud.service.name: es-demo-app
-             keystore:
-                   path: /path/to/azurekeystore.pkcs12
-                   password: WHATEVER
-                   type: pkcs12
-
-discovery:
-    type: azure
-----
-
-
-[IMPORTANT]
-.Binding the network host
-==============================================
-
-It's important to define `network.host` as by default it's bound to `localhost`.
-
-You can use {ref}/modules-network.html[core network host settings]. For example `_non_loopback_` or `_en0_`.
-
-==============================================
-
-
-[[cloud-azure-discovery-short]]
-===== How to start (short story)
-
-* Create Azure instances
-* Install Elasticsearch
-* Install Azure plugin
-* Modify `elasticsearch.yml` file
-* Start Elasticsearch
-
-[[cloud-azure-discovery-settings]]
-===== Azure credential API settings
-
-The following are a list of settings that can further control the credential API:
-
-[horizontal]
-`cloud.azure.management.keystore.path`::
-
-    /path/to/keystore
-
-`cloud.azure.management.keystore.type`::
-
-    `pkcs12`, `jceks` or `jks`. Defaults to `pkcs12`.
-
-`cloud.azure.management.keystore.password`::
-
-    your_password for the keystore
-
-`cloud.azure.management.subscription.id`::
-
-    your_azure_subscription_id
-
-`cloud.azure.management.cloud.service.name`::
-
-    your_azure_cloud_service_name
-
-
-[[cloud-azure-discovery-settings-advanced]]
-===== Advanced settings
-
-The following are a list of settings that can further control the discovery:
-
-`discovery.azure.host.type`::
-
-    Either `public_ip` or `private_ip` (default). Azure discovery will use the
-    one you set to ping other nodes.
-
-`discovery.azure.endpoint.name`::
-
-    When using `public_ip` this setting is used to identify the endpoint name
-    used to forward requests to elasticsearch (aka transport port name).
-    Defaults to `elasticsearch`. In Azure management console, you could define
-    an endpoint `elasticsearch` forwarding for example requests on public IP
-    on port 8100 to the virtual machine on port 9300.
-
-`discovery.azure.deployment.name`::
-
-    Deployment name if any. Defaults to the value set with
-    `cloud.azure.management.cloud.service.name`.
-
-`discovery.azure.deployment.slot`::
-
-    Either `staging` or `production` (default).
-
-For example:
-
-[source,yaml]
-----
-discovery:
-    type: azure
-    azure:
-        host:
-            type: private_ip
-        endpoint:
-            name: elasticsearch
-        deployment:
-            name: your_azure_cloud_service_name
-            slot: production
-----
-
-[[cloud-azure-discovery-long]]
-==== Setup process for Azure Discovery
-
-We will expose here one strategy which is to hide our Elasticsearch cluster from outside.
-
-With this strategy, only VMs behind the same virtual port can talk to each
-other.  That means that with this mode, you can use elasticsearch unicast
-discovery to build a cluster, using the Azure API to retrieve information
-about your nodes.
-
-[[cloud-azure-discovery-long-prerequisites]]
-===== Prerequisites
-
-Before starting, you need to have:
-
-* A http://www.windowsazure.com/[Windows Azure account]
-* OpenSSL that isn't from MacPorts, specifically `OpenSSL 1.0.1f 6 Jan
-  2014` doesn't seem to create a valid keypair for ssh. FWIW,
- `OpenSSL 1.0.1c 10 May 2012` on Ubuntu 12.04 LTS is known to work.
-* SSH keys and certificate
-+
---
-
-You should follow http://azure.microsoft.com/en-us/documentation/articles/linux-use-ssh-key/[this guide] to learn
-how to create or use existing SSH keys. If you have already did it, you can skip the following.
-
-Here is a description on how to generate SSH keys using `openssl`:
-
-[source,sh]
-----
-# You may want to use another dir than /tmp
-cd /tmp
-openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout azure-private.key -out azure-certificate.pem
-chmod 600 azure-private.key azure-certificate.pem
-openssl x509 -outform der -in azure-certificate.pem -out azure-certificate.cer
-----
-
-Generate a keystore which will be used by the plugin to authenticate with a certificate
-all Azure API calls.
-
-[source,sh]
-----
-# Generate a keystore (azurekeystore.pkcs12)
-# Transform private key to PEM format
-openssl pkcs8 -topk8 -nocrypt -in azure-private.key -inform PEM -out azure-pk.pem -outform PEM
-# Transform certificate to PEM format
-openssl x509 -inform der -in azure-certificate.cer -out azure-cert.pem
-cat azure-cert.pem azure-pk.pem > azure.pem.txt
-# You MUST enter a password!
-openssl pkcs12 -export -in azure.pem.txt -out azurekeystore.pkcs12 -name azure -noiter -nomaciter
-----
-
-Upload the `azure-certificate.cer` file both in the elasticsearch Cloud Service (under `Manage Certificates`),
-and under `Settings -> Manage Certificates`.
-
-IMPORTANT: When prompted for a password, you need to enter a non empty one.
-
-See this http://www.windowsazure.com/en-us/manage/linux/how-to-guides/ssh-into-linux/[guide] for
-more details about how to create keys for Azure.
-
-Once done, you need to upload your certificate in Azure:
-
-* Go to the https://account.windowsazure.com/[management console].
-* Sign in using your account.
-* Click on `Portal`.
-* Go to Settings (bottom of the left list)
-* On the bottom bar, click on `Upload` and upload your `azure-certificate.cer` file.
-
-You may want to use
-http://www.windowsazure.com/en-us/develop/nodejs/how-to-guides/command-line-tools/[Windows Azure Command-Line Tool]:
-
---
-
-* Install https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager[NodeJS], for example using
-homebrew on MacOS X:
-+
-[source,sh]
-----
-brew install node
-----
-
-* Install Azure tools
-+
-[source,sh]
-----
-sudo npm install azure-cli -g
-----
-
-* Download and import your azure settings:
-+
-[source,sh]
-----
-# This will open a browser and will download a .publishsettings file
-azure account download
-
-# Import this file (we have downloaded it to /tmp)
-# Note, it will create needed files in ~/.azure. You can remove azure.publishsettings when done.
-azure account import /tmp/azure.publishsettings
-----
-
-[[cloud-azure-discovery-long-instance]]
-===== Creating your first instance
-
-You need to have a storage account available. Check http://www.windowsazure.com/en-us/develop/net/how-to-guides/blob-storage/#create-account[Azure Blob Storage documentation]
-for more information.
-
-You will need to choose the operating system you want to run on. To get a list of official available images, run:
-
-[source,sh]
-----
-azure vm image list
-----
-
-Let's say we are going to deploy an Ubuntu image on an extra small instance in West Europe:
-
-[horizontal]
-Azure cluster name::
-
-    `azure-elasticsearch-cluster`
-
-Image::
-
-    `b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB`
-
-VM Name::
-
-    `myesnode1`
-
-VM Size::
-
-    `extrasmall`
-
-Location::
-
-    `West Europe`
-
-Login::
-
-    `elasticsearch`
-
-Password::
-
-    `password1234!!`
-
-
-Using command line:
-
-[source,sh]
-----
-azure vm create azure-elasticsearch-cluster \
-                b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB \
-                --vm-name myesnode1 \
-                --location "West Europe" \
-                --vm-size extrasmall \
-                --ssh 22 \
-                --ssh-cert /tmp/azure-certificate.pem \
-                elasticsearch password1234\!\!
-----
-
-You should see something like:
-
-[source,text]
-----
-info:    Executing command vm create
-+ Looking up image
-+ Looking up cloud service
-+ Creating cloud service
-+ Retrieving storage accounts
-+ Configuring certificate
-+ Creating VM
-info:    vm create command OK
-----
-
-Now, your first instance is started.
-
-[TIP]
-.Working with SSH
-===============================================
-
-You need to give the private key and username each time you log on your instance:
-
-[source,sh]
-----
-ssh -i ~/.ssh/azure-private.key elasticsearch@myescluster.cloudapp.net
-----
-
-But you can also define it once in `~/.ssh/config` file:
-
-[source,text]
-----
-Host *.cloudapp.net
- User elasticsearch
- StrictHostKeyChecking no
- UserKnownHostsFile=/dev/null
- IdentityFile ~/.ssh/azure-private.key
-----
-===============================================
-
-Next, you need to install Elasticsearch on your new instance. First, copy your
-keystore to the instance, then connect to the instance using SSH:
-
-[source,sh]
-----
-scp /tmp/azurekeystore.pkcs12 azure-elasticsearch-cluster.cloudapp.net:/home/elasticsearch
-ssh azure-elasticsearch-cluster.cloudapp.net
-----
-
-Once connected, install Elasticsearch:
-
-[source,sh]
-----
-# Install Latest Java version
-# Read http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html for details
-sudo add-apt-repository ppa:webupd8team/java
-sudo apt-get update
-sudo apt-get install oracle-java7-installer
-
-# If you want to install OpenJDK instead
-# sudo apt-get update
-# sudo apt-get install openjdk-7-jre-headless
-
-# Download Elasticsearch
-curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb -o elasticsearch-2.0.0.deb
-
-# Prepare Elasticsearch installation
-sudo dpkg -i elasticsearch-2.0.0.deb
-----
-
-Check that elasticsearch is running:
-
-[source,sh]
-----
-curl http://localhost:9200/
-----
-
-This command should give you a JSON result:
-
-[source,javascript]
-----
-{
-  "status" : 200,
-  "name" : "Living Colossus",
-  "version" : {
-    "number" : "2.0.0",
-    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
-    "build_timestamp" : "2014-02-12T16:18:34Z",
-    "build_snapshot" : false,
-    "lucene_version" : "5.1"
-  },
-  "tagline" : "You Know, for Search"
-}
-----
-
-[[cloud-azure-discovery-long-plugin]]
-===== Install elasticsearch cloud azure plugin
-
-[source,sh]
-----
-# Stop elasticsearch
-sudo service elasticsearch stop
-
-# Install the plugin
-sudo /usr/share/elasticsearch/bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.6.1
-
-# Configure it
-sudo vi /etc/elasticsearch/elasticsearch.yml
-----
-
-And add the following lines:
-
-[source,yaml]
-----
-# If you don't remember your account id, you may get it with `azure account list`
-cloud:
-    azure:
-        management:
-             subscription.id: your_azure_subscription_id
-             cloud.service.name: your_azure_cloud_service_name
-             keystore:
-                   path: /home/elasticsearch/azurekeystore.pkcs12
-                   password: your_password_for_keystore
-
-discovery:
-    type: azure
-
-# Recommended (warning: non durable disk)
-# path.data: /mnt/resource/elasticsearch/data
-----
-
-Restart elasticsearch:
-
-[source,sh]
-----
-sudo service elasticsearch start
-----
-
-If anything goes wrong, check your logs in `/var/log/elasticsearch`.
-
-[[cloud-azure-discovery-scale]]
-==== Scaling Out!
-
-You need first to create an image of your previous machine.
-Disconnect from your machine and run locally the following commands:
-
-[source,sh]
-----
-# Shutdown the instance
-azure vm shutdown myesnode1
-
-# Create an image from this instance (it could take some minutes)
-azure vm capture myesnode1 esnode-image --delete
-
-# Note that the previous instance has been deleted (mandatory)
-# So you need to create it again and BTW create other instances.
-
-azure vm create azure-elasticsearch-cluster \
-                esnode-image \
-                --vm-name myesnode1 \
-                --location "West Europe" \
-                --vm-size extrasmall \
-                --ssh 22 \
-                --ssh-cert /tmp/azure-certificate.pem \
-                elasticsearch password1234\!\!
-----
-
-
-[TIP]
-=========================================
-It could happen that azure changes the endpoint public IP address.
-DNS propagation could take some minutes before you can connect again using
-name. You can get from azure the IP address if needed, using:
-
-[source,sh]
-----
-# Look at Network `Endpoints 0 Vip`
-azure vm show myesnode1
-----
-
-=========================================
-
-Let's start more instances!
-
-[source,sh]
-----
-for x in $(seq  2 10)
-	do
-		echo "Launching azure instance #$x..."
-		azure vm create azure-elasticsearch-cluster \
-		                esnode-image \
-		                --vm-name myesnode$x \
-		                --vm-size extrasmall \
-		                --ssh $((21 + $x)) \
-		                --ssh-cert /tmp/azure-certificate.pem \
-		                --connect \
-		                elasticsearch password1234\!\!
-	done
-----
-
-If you want to remove your running instances:
-
-[source,sh]
-----
-azure vm delete myesnode1
-----
-
-[[cloud-azure-repository]]
-==== Azure Repository
-
-To enable Azure repositories, you have first to set your azure storage settings in `elasticsearch.yml` file:
-
-[source,yaml]
-----
-cloud:
-    azure:
-        storage:
-            account: your_azure_storage_account
-            key: your_azure_storage_key
-----
-
-For information, in previous version of the azure plugin, settings were:
-
-[source,yaml]
-----
-cloud:
-    azure:
-        storage_account: your_azure_storage_account
-        storage_key: your_azure_storage_key
-----
-
-The Azure repository supports following settings:
-
-`container`::
-
-    Container name. Defaults to `elasticsearch-snapshots`
-
-`base_path`::
-
-    Specifies the path within container to repository data. Defaults to empty
-    (root directory).
-
-`chunk_size`::
-
-    Big files can be broken down into chunks during snapshotting if needed.
-    The chunk size can be specified in bytes or by using size value notation,
-    i.e. `1g`, `10m`, `5k`. Defaults to `64m` (64m max)
-
-`compress`::
-
-    When set to `true` metadata files are stored in compressed format. This
-    setting doesn't affect index files that are already compressed by default.
-    Defaults to `false`.
-
-`read_only`::
-
-    Makes repository read-only. coming[2.1.0]  Defaults to `false`.
-
-Some examples, using scripts:
-
-[source,json]
-----
-# The simpliest one
-PUT _snapshot/my_backup1
-{
-    "type": "azure"
-}
-
-# With some settings
-PUT _snapshot/my_backup2
-{
-    "type": "azure",
-    "settings": {
-        "container": "backup_container",
-        "base_path": "backups",
-        "chunk_size": "32m",
-        "compress": true
-    }
-}
-----
-// AUTOSENSE
-
-Example using Java:
-
-[source,java]
-----
-client.admin().cluster().preparePutRepository("my_backup3")
-    .setType("azure").setSettings(Settings.settingsBuilder()
-        .put(Storage.CONTAINER, "backup_container")
-        .put(Storage.CHUNK_SIZE, new ByteSizeValue(32, ByteSizeUnit.MB))
-    ).get();
-----
-
-[[cloud-azure-repository-validation]]
-===== Repository validation rules
-
-According to the http://msdn.microsoft.com/en-us/library/dd135715.aspx[containers naming guide], a container name must
-be a valid DNS name, conforming to the following naming rules:
-
-* Container names must start with a letter or number, and can contain only letters, numbers, and the dash (-) character.
-* Every dash (-) character must be immediately preceded and followed by a letter or number; consecutive dashes are not
-permitted in container names.
-* All letters in a container name must be lowercase.
-* Container names must be from 3 through 63 characters long.
-
-[[cloud-azure-testing]]
-==== Testing Azure
-
-Integrations tests in this plugin require working Azure configuration and therefore disabled by default.
-To enable tests prepare a config file `elasticsearch.yml` with the following content:
-
-[source,yaml]
-----
-cloud:
-  azure:
-    storage:
-      account: "YOUR-AZURE-STORAGE-NAME"
-      key: "YOUR-AZURE-STORAGE-KEY"
-----
-
-Replaces `account`, `key` with your settings. Please, note that the test will delete all snapshot/restore related
-files in the specified bucket.
-
-To run test:
-
-[source,sh]
-----
-mvn -Dtests.azure=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
-----
-
-[[cloud-azure-smb-workaround]]
-==== Working around a bug in Windows SMB and Java on windows
-
-When using a shared file system based on the SMB protocol (like Azure File Service) to store indices, the way Lucene
-open index segment files is with a write only flag. This is the _correct_ way to open the files, as they will only be
-used for writes and allows different FS implementations to optimize for it. Sadly, in windows with SMB, this disables
-the cache manager, causing writes to be slow. This has been described in
-https://issues.apache.org/jira/browse/LUCENE-6176[LUCENE-6176], but it affects each and every Java program out there!.
-This need and must be fixed outside of ES and/or Lucene, either in windows or OpenJDK. For now, we are providing an
-experimental support to open the files with read flag, but this should be considered experimental and the correct way
-to fix it is in OpenJDK or Windows.
-
-The Azure Cloud plugin provides two storage types optimized for SMB:
-
-`smb_mmap_fs`::
-
-    a SMB specific implementation of the default
-    {ref}/index-modules-store.html#mmapfs[mmap fs]
-
-`smb_simple_fs`::
-
-    a SMB specific implementation of the default
-    {ref}/index-modules-store.html#simplefs[simple fs]
-
-To use one of these specific storage types, you need to install the Azure Cloud plugin and restart the node.
-Then configure Elasticsearch to set the storage type you want.
-
-This can be configured for all indices by adding this to the `elasticsearch.yml` file:
-
-[source,yaml]
-----
-index.store.type: smb_simple_fs
-----
-
-Note that setting will be applied for newly created indices.
-
-It can also be set on a per-index basis at index creation time:
-
-[source,json]
-----
-PUT my_index
-{
-   "settings": {
-       "index.store.type": "smb_mmap_fs"
-   }
-}
-----
-// AUTOSENSE
diff --git a/docs/plugins/discovery-azure.asciidoc b/docs/plugins/discovery-azure.asciidoc
new file mode 100644
index 0000000..05b195a
--- /dev/null
+++ b/docs/plugins/discovery-azure.asciidoc
@@ -0,0 +1,507 @@
+[[discovery-azure]]
+=== Azure Discovery Plugin
+
+The Azure Discovery plugin uses the Azure API for unicast discovery.
+
+[[discovery-azure-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install discovery-azure
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[discovery-azure-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove discovery-azure
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[discovery-azure-usage]]
+==== Azure Virtual Machine Discovery
+
+Azure VM discovery allows to use the azure APIs to perform automatic discovery (similar to multicast in non hostile
+multicast environments). Here is a simple sample configuration:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        management:
+             subscription.id: XXX-XXX-XXX-XXX
+             cloud.service.name: es-demo-app
+             keystore:
+                   path: /path/to/azurekeystore.pkcs12
+                   password: WHATEVER
+                   type: pkcs12
+
+discovery:
+    type: azure
+----
+
+[IMPORTANT]
+.Binding the network host
+==============================================
+
+It's important to define `network.host` as by default it's bound to `localhost`.
+
+You can use {ref}/modules-network.html[core network host settings]. For example `_non_loopback_` or `_en0_`.
+
+==============================================
+
+[[discovery-azure-short]]
+===== How to start (short story)
+
+* Create Azure instances
+* Install Elasticsearch
+* Install Azure plugin
+* Modify `elasticsearch.yml` file
+* Start Elasticsearch
+
+[[discovery-azure-settings]]
+===== Azure credential API settings
+
+The following are a list of settings that can further control the credential API:
+
+[horizontal]
+`cloud.azure.management.keystore.path`::
+
+    /path/to/keystore
+
+`cloud.azure.management.keystore.type`::
+
+    `pkcs12`, `jceks` or `jks`. Defaults to `pkcs12`.
+
+`cloud.azure.management.keystore.password`::
+
+    your_password for the keystore
+
+`cloud.azure.management.subscription.id`::
+
+    your_azure_subscription_id
+
+`cloud.azure.management.cloud.service.name`::
+
+    your_azure_cloud_service_name
+
+
+[[discovery-azure-settings-advanced]]
+===== Advanced settings
+
+The following are a list of settings that can further control the discovery:
+
+`discovery.azure.host.type`::
+
+    Either `public_ip` or `private_ip` (default). Azure discovery will use the
+    one you set to ping other nodes.
+
+`discovery.azure.endpoint.name`::
+
+    When using `public_ip` this setting is used to identify the endpoint name
+    used to forward requests to elasticsearch (aka transport port name).
+    Defaults to `elasticsearch`. In Azure management console, you could define
+    an endpoint `elasticsearch` forwarding for example requests on public IP
+    on port 8100 to the virtual machine on port 9300.
+
+`discovery.azure.deployment.name`::
+
+    Deployment name if any. Defaults to the value set with
+    `cloud.azure.management.cloud.service.name`.
+
+`discovery.azure.deployment.slot`::
+
+    Either `staging` or `production` (default).
+
+For example:
+
+[source,yaml]
+----
+discovery:
+    type: azure
+    azure:
+        host:
+            type: private_ip
+        endpoint:
+            name: elasticsearch
+        deployment:
+            name: your_azure_cloud_service_name
+            slot: production
+----
+
+[[discovery-azure-long]]
+==== Setup process for Azure Discovery
+
+We will expose here one strategy which is to hide our Elasticsearch cluster from outside.
+
+With this strategy, only VMs behind the same virtual port can talk to each
+other.  That means that with this mode, you can use elasticsearch unicast
+discovery to build a cluster, using the Azure API to retrieve information
+about your nodes.
+
+[[discovery-azure-long-prerequisites]]
+===== Prerequisites
+
+Before starting, you need to have:
+
+* A http://www.windowsazure.com/[Windows Azure account]
+* OpenSSL that isn't from MacPorts, specifically `OpenSSL 1.0.1f 6 Jan
+  2014` doesn't seem to create a valid keypair for ssh. FWIW,
+ `OpenSSL 1.0.1c 10 May 2012` on Ubuntu 12.04 LTS is known to work.
+* SSH keys and certificate
++
+--
+
+You should follow http://azure.microsoft.com/en-us/documentation/articles/linux-use-ssh-key/[this guide] to learn
+how to create or use existing SSH keys. If you have already did it, you can skip the following.
+
+Here is a description on how to generate SSH keys using `openssl`:
+
+[source,sh]
+----
+# You may want to use another dir than /tmp
+cd /tmp
+openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout azure-private.key -out azure-certificate.pem
+chmod 600 azure-private.key azure-certificate.pem
+openssl x509 -outform der -in azure-certificate.pem -out azure-certificate.cer
+----
+
+Generate a keystore which will be used by the plugin to authenticate with a certificate
+all Azure API calls.
+
+[source,sh]
+----
+# Generate a keystore (azurekeystore.pkcs12)
+# Transform private key to PEM format
+openssl pkcs8 -topk8 -nocrypt -in azure-private.key -inform PEM -out azure-pk.pem -outform PEM
+# Transform certificate to PEM format
+openssl x509 -inform der -in azure-certificate.cer -out azure-cert.pem
+cat azure-cert.pem azure-pk.pem > azure.pem.txt
+# You MUST enter a password!
+openssl pkcs12 -export -in azure.pem.txt -out azurekeystore.pkcs12 -name azure -noiter -nomaciter
+----
+
+Upload the `azure-certificate.cer` file both in the elasticsearch Cloud Service (under `Manage Certificates`),
+and under `Settings -> Manage Certificates`.
+
+IMPORTANT: When prompted for a password, you need to enter a non empty one.
+
+See this http://www.windowsazure.com/en-us/manage/linux/how-to-guides/ssh-into-linux/[guide] for
+more details about how to create keys for Azure.
+
+Once done, you need to upload your certificate in Azure:
+
+* Go to the https://account.windowsazure.com/[management console].
+* Sign in using your account.
+* Click on `Portal`.
+* Go to Settings (bottom of the left list)
+* On the bottom bar, click on `Upload` and upload your `azure-certificate.cer` file.
+
+You may want to use
+http://www.windowsazure.com/en-us/develop/nodejs/how-to-guides/command-line-tools/[Windows Azure Command-Line Tool]:
+
+--
+
+* Install https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager[NodeJS], for example using
+homebrew on MacOS X:
++
+[source,sh]
+----
+brew install node
+----
+
+* Install Azure tools
++
+[source,sh]
+----
+sudo npm install azure-cli -g
+----
+
+* Download and import your azure settings:
++
+[source,sh]
+----
+# This will open a browser and will download a .publishsettings file
+azure account download
+
+# Import this file (we have downloaded it to /tmp)
+# Note, it will create needed files in ~/.azure. You can remove azure.publishsettings when done.
+azure account import /tmp/azure.publishsettings
+----
+
+[[discovery-azure-long-instance]]
+===== Creating your first instance
+
+You need to have a storage account available. Check http://www.windowsazure.com/en-us/develop/net/how-to-guides/blob-storage/#create-account[Azure Blob Storage documentation]
+for more information.
+
+You will need to choose the operating system you want to run on. To get a list of official available images, run:
+
+[source,sh]
+----
+azure vm image list
+----
+
+Let's say we are going to deploy an Ubuntu image on an extra small instance in West Europe:
+
+[horizontal]
+Azure cluster name::
+
+    `azure-elasticsearch-cluster`
+
+Image::
+
+    `b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB`
+
+VM Name::
+
+    `myesnode1`
+
+VM Size::
+
+    `extrasmall`
+
+Location::
+
+    `West Europe`
+
+Login::
+
+    `elasticsearch`
+
+Password::
+
+    `password1234!!`
+
+
+Using command line:
+
+[source,sh]
+----
+azure vm create azure-elasticsearch-cluster \
+                b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB \
+                --vm-name myesnode1 \
+                --location "West Europe" \
+                --vm-size extrasmall \
+                --ssh 22 \
+                --ssh-cert /tmp/azure-certificate.pem \
+                elasticsearch password1234\!\!
+----
+
+You should see something like:
+
+[source,text]
+----
+info:    Executing command vm create
++ Looking up image
++ Looking up cloud service
++ Creating cloud service
++ Retrieving storage accounts
++ Configuring certificate
++ Creating VM
+info:    vm create command OK
+----
+
+Now, your first instance is started.
+
+[TIP]
+.Working with SSH
+===============================================
+
+You need to give the private key and username each time you log on your instance:
+
+[source,sh]
+----
+ssh -i ~/.ssh/azure-private.key elasticsearch@myescluster.cloudapp.net
+----
+
+But you can also define it once in `~/.ssh/config` file:
+
+[source,text]
+----
+Host *.cloudapp.net
+ User elasticsearch
+ StrictHostKeyChecking no
+ UserKnownHostsFile=/dev/null
+ IdentityFile ~/.ssh/azure-private.key
+----
+===============================================
+
+Next, you need to install Elasticsearch on your new instance. First, copy your
+keystore to the instance, then connect to the instance using SSH:
+
+[source,sh]
+----
+scp /tmp/azurekeystore.pkcs12 azure-elasticsearch-cluster.cloudapp.net:/home/elasticsearch
+ssh azure-elasticsearch-cluster.cloudapp.net
+----
+
+Once connected, install Elasticsearch:
+
+[source,sh]
+----
+# Install Latest Java version
+# Read http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html for details
+sudo add-apt-repository ppa:webupd8team/java
+sudo apt-get update
+sudo apt-get install oracle-java7-installer
+
+# If you want to install OpenJDK instead
+# sudo apt-get update
+# sudo apt-get install openjdk-7-jre-headless
+
+# Download Elasticsearch
+curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb -o elasticsearch-2.0.0.deb
+
+# Prepare Elasticsearch installation
+sudo dpkg -i elasticsearch-2.0.0.deb
+----
+
+Check that elasticsearch is running:
+
+[source,sh]
+----
+curl http://localhost:9200/
+----
+
+This command should give you a JSON result:
+
+[source,javascript]
+----
+{
+  "status" : 200,
+  "name" : "Living Colossus",
+  "version" : {
+    "number" : "2.0.0",
+    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
+    "build_timestamp" : "2014-02-12T16:18:34Z",
+    "build_snapshot" : false,
+    "lucene_version" : "5.1"
+  },
+  "tagline" : "You Know, for Search"
+}
+----
+
+[[discovery-azure-long-plugin]]
+===== Install elasticsearch cloud azure plugin
+
+[source,sh]
+----
+# Stop elasticsearch
+sudo service elasticsearch stop
+
+# Install the plugin
+sudo /usr/share/elasticsearch/bin/plugin install discovery-azure
+
+# Configure it
+sudo vi /etc/elasticsearch/elasticsearch.yml
+----
+
+And add the following lines:
+
+[source,yaml]
+----
+# If you don't remember your account id, you may get it with `azure account list`
+cloud:
+    azure:
+        management:
+             subscription.id: your_azure_subscription_id
+             cloud.service.name: your_azure_cloud_service_name
+             keystore:
+                   path: /home/elasticsearch/azurekeystore.pkcs12
+                   password: your_password_for_keystore
+
+discovery:
+    type: azure
+
+# Recommended (warning: non durable disk)
+# path.data: /mnt/resource/elasticsearch/data
+----
+
+Restart elasticsearch:
+
+[source,sh]
+----
+sudo service elasticsearch start
+----
+
+If anything goes wrong, check your logs in `/var/log/elasticsearch`.
+
+[[discovery-azure-scale]]
+==== Scaling Out!
+
+You need first to create an image of your previous machine.
+Disconnect from your machine and run locally the following commands:
+
+[source,sh]
+----
+# Shutdown the instance
+azure vm shutdown myesnode1
+
+# Create an image from this instance (it could take some minutes)
+azure vm capture myesnode1 esnode-image --delete
+
+# Note that the previous instance has been deleted (mandatory)
+# So you need to create it again and BTW create other instances.
+
+azure vm create azure-elasticsearch-cluster \
+                esnode-image \
+                --vm-name myesnode1 \
+                --location "West Europe" \
+                --vm-size extrasmall \
+                --ssh 22 \
+                --ssh-cert /tmp/azure-certificate.pem \
+                elasticsearch password1234\!\!
+----
+
+
+[TIP]
+=========================================
+It could happen that azure changes the endpoint public IP address.
+DNS propagation could take some minutes before you can connect again using
+name. You can get from azure the IP address if needed, using:
+
+[source,sh]
+----
+# Look at Network `Endpoints 0 Vip`
+azure vm show myesnode1
+----
+
+=========================================
+
+Let's start more instances!
+
+[source,sh]
+----
+for x in $(seq  2 10)
+	do
+		echo "Launching azure instance #$x..."
+		azure vm create azure-elasticsearch-cluster \
+		                esnode-image \
+		                --vm-name myesnode$x \
+		                --vm-size extrasmall \
+		                --ssh $((21 + $x)) \
+		                --ssh-cert /tmp/azure-certificate.pem \
+		                --connect \
+		                elasticsearch password1234\!\!
+	done
+----
+
+If you want to remove your running instances:
+
+[source,sh]
+----
+azure vm delete myesnode1
+----
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
index 0e39d29..289a020 100644
--- a/docs/plugins/discovery.asciidoc
+++ b/docs/plugins/discovery.asciidoc
@@ -13,11 +13,9 @@ The core discovery plugins are:
 
 The EC2 discovery plugin uses the https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery.
 
-<<cloud-azure,Azure Cloud>>::
+<<discovery-azure,Azure discovery>>::
 
-The Azure Cloud plugin uses the Azure API for unicast discovery, and adds
-support for using Azure as a repository for
-{ref}/modules-snapshots.html[Snapshot/Restore].
+The Azure discovery plugin uses the Azure API for unicast discovery.
 
 <<cloud-gce,GCE Cloud>>::
 
@@ -38,7 +36,7 @@ A number of discovery plugins have been contributed by our community:
 
 include::discovery-ec2.asciidoc[]
 
-include::cloud-azure.asciidoc[]
+include::discovery-azure.asciidoc[]
 
 include::cloud-gce.asciidoc[]
 
diff --git a/docs/plugins/index.asciidoc b/docs/plugins/index.asciidoc
index 2ebe5d2..bbc9065 100644
--- a/docs/plugins/index.asciidoc
+++ b/docs/plugins/index.asciidoc
@@ -57,6 +57,8 @@ include::security.asciidoc[]
 
 include::repository.asciidoc[]
 
+include::store.asciidoc[]
+
 include::transport.asciidoc[]
 
 include::integrations.asciidoc[]
diff --git a/docs/plugins/repository-azure.asciidoc b/docs/plugins/repository-azure.asciidoc
new file mode 100644
index 0000000..fee308b
--- /dev/null
+++ b/docs/plugins/repository-azure.asciidoc
@@ -0,0 +1,155 @@
+[[repository-azure]]
+=== Azure Repository Plugin
+
+The Azure Repository plugin adds support for using Azure as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+[[repository-azure-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install repository-azure
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[repository-azure-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove repository-azure
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[repository-azure-usage]]
+==== Azure Repository
+
+To enable Azure repositories, you have first to set your azure storage settings in `elasticsearch.yml` file:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        storage:
+            account: your_azure_storage_account
+            key: your_azure_storage_key
+----
+
+For information, in previous version of the azure plugin, settings were:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        storage_account: your_azure_storage_account
+        storage_key: your_azure_storage_key
+----
+
+The Azure repository supports following settings:
+
+`container`::
+
+    Container name. Defaults to `elasticsearch-snapshots`
+
+`base_path`::
+
+    Specifies the path within container to repository data. Defaults to empty
+    (root directory).
+
+`chunk_size`::
+
+    Big files can be broken down into chunks during snapshotting if needed.
+    The chunk size can be specified in bytes or by using size value notation,
+    i.e. `1g`, `10m`, `5k`. Defaults to `64m` (64m max)
+
+`compress`::
+
+    When set to `true` metadata files are stored in compressed format. This
+    setting doesn't affect index files that are already compressed by default.
+    Defaults to `false`.
+
+`read_only`::
+
+    Makes repository read-only. coming[2.1.0]  Defaults to `false`.
+
+Some examples, using scripts:
+
+[source,json]
+----
+# The simpliest one
+PUT _snapshot/my_backup1
+{
+    "type": "azure"
+}
+
+# With some settings
+PUT _snapshot/my_backup2
+{
+    "type": "azure",
+    "settings": {
+        "container": "backup_container",
+        "base_path": "backups",
+        "chunk_size": "32m",
+        "compress": true
+    }
+}
+----
+// AUTOSENSE
+
+Example using Java:
+
+[source,java]
+----
+client.admin().cluster().preparePutRepository("my_backup3")
+    .setType("azure").setSettings(Settings.settingsBuilder()
+        .put(Storage.CONTAINER, "backup_container")
+        .put(Storage.CHUNK_SIZE, new ByteSizeValue(32, ByteSizeUnit.MB))
+    ).get();
+----
+
+[[repository-azure-validation]]
+===== Repository validation rules
+
+According to the http://msdn.microsoft.com/en-us/library/dd135715.aspx[containers naming guide], a container name must
+be a valid DNS name, conforming to the following naming rules:
+
+* Container names must start with a letter or number, and can contain only letters, numbers, and the dash (-) character.
+* Every dash (-) character must be immediately preceded and followed by a letter or number; consecutive dashes are not
+permitted in container names.
+* All letters in a container name must be lowercase.
+* Container names must be from 3 through 63 characters long.
+
+[[repository-azure-testing]]
+==== Testing Azure
+
+Integrations tests in this plugin require working Azure configuration and therefore disabled by default.
+To enable tests prepare a config file `elasticsearch.yml` with the following content:
+
+[source,yaml]
+----
+cloud:
+  azure:
+    storage:
+      account: "YOUR-AZURE-STORAGE-NAME"
+      key: "YOUR-AZURE-STORAGE-KEY"
+----
+
+Replaces `account`, `key` with your settings. Please, note that the test will delete all snapshot/restore related
+files in the specified bucket.
+
+To run test:
+
+[source,sh]
+----
+mvn -Dtests.azure=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+----
diff --git a/docs/plugins/repository.asciidoc b/docs/plugins/repository.asciidoc
index cddde5f..554fa34 100644
--- a/docs/plugins/repository.asciidoc
+++ b/docs/plugins/repository.asciidoc
@@ -14,9 +14,9 @@ The core repository plugins are:
 
 The S3 repository plugin adds support for using S3 as a repository.
 
-<<cloud-azure,Azure Cloud>>::
+<<repository-azure,Azure Repository>>::
 
-The Azure Cloud plugin adds support for using Azure as a repository.
+The Azure repository plugin adds support for using Azure as a repository.
 
 https://github.com/elastic/elasticsearch-hadoop/tree/master/repository-hdfs[Hadoop HDFS Repository]::
 
@@ -36,5 +36,7 @@ This community plugin appears to have been abandoned:
 * https://github.com/kzwang/elasticsearch-repository-gridfs[GridFS] Repository (by Kevin Wang)
 
 
+include::repository-azure.asciidoc[]
+
 include::repository-s3.asciidoc[]
 
diff --git a/docs/plugins/store-smb.asciidoc b/docs/plugins/store-smb.asciidoc
new file mode 100644
index 0000000..b8cbe57
--- /dev/null
+++ b/docs/plugins/store-smb.asciidoc
@@ -0,0 +1,80 @@
+[[store-smb]]
+=== Store SMB Plugin
+
+The Store SMB plugin works around for a bug in Windows SMB and Java on windows.
+
+[[store-smb-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install store-smb
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[store-smb-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove store-smb
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[store-smb-usage]]
+==== Working around a bug in Windows SMB and Java on windows
+
+When using a shared file system based on the SMB protocol (like Azure File Service) to store indices, the way Lucene
+open index segment files is with a write only flag. This is the _correct_ way to open the files, as they will only be
+used for writes and allows different FS implementations to optimize for it. Sadly, in windows with SMB, this disables
+the cache manager, causing writes to be slow. This has been described in
+https://issues.apache.org/jira/browse/LUCENE-6176[LUCENE-6176], but it affects each and every Java program out there!.
+This need and must be fixed outside of ES and/or Lucene, either in windows or OpenJDK. For now, we are providing an
+experimental support to open the files with read flag, but this should be considered experimental and the correct way
+to fix it is in OpenJDK or Windows.
+
+The Store SMB plugin provides two storage types optimized for SMB:
+
+`smb_mmap_fs`::
+
+    a SMB specific implementation of the default
+    {ref}/index-modules-store.html#mmapfs[mmap fs]
+
+`smb_simple_fs`::
+
+    a SMB specific implementation of the default
+    {ref}/index-modules-store.html#simplefs[simple fs]
+
+To use one of these specific storage types, you need to install the Store SMB plugin and restart the node.
+Then configure Elasticsearch to set the storage type you want.
+
+This can be configured for all indices by adding this to the `elasticsearch.yml` file:
+
+[source,yaml]
+----
+index.store.type: smb_simple_fs
+----
+
+Note that setting will be applied for newly created indices.
+
+It can also be set on a per-index basis at index creation time:
+
+[source,json]
+----
+PUT my_index
+{
+   "settings": {
+       "index.store.type": "smb_mmap_fs"
+   }
+}
+----
+// AUTOSENSE
diff --git a/docs/plugins/store.asciidoc b/docs/plugins/store.asciidoc
new file mode 100644
index 0000000..8a4a520
--- /dev/null
+++ b/docs/plugins/store.asciidoc
@@ -0,0 +1,17 @@
+[[store]]
+== Store Plugins
+
+Store plugins offer alternatives to default Lucene stores.
+
+[float]
+=== Core store plugins
+
+The core store plugins are:
+
+<<store-smb,Store SMB>>::
+
+The Store SMB plugin works around for a bug in Windows SMB and Java on windows.
+
+
+include::store-smb.asciidoc[]
+
diff --git a/docs/reference/cat/plugins.asciidoc b/docs/reference/cat/plugins.asciidoc
index a970463..81df5cf 100644
--- a/docs/reference/cat/plugins.asciidoc
+++ b/docs/reference/cat/plugins.asciidoc
@@ -7,8 +7,7 @@ The `plugins` command provides a view per node of running plugins. This informat
 ------------------------------------------------------------------------------
 % curl 'localhost:9200/_cat/plugins?v'
 name    component       version        type isolation url
-Abraxas cloud-azure     2.1.0-SNAPSHOT j    x
-Abraxas lang-groovy     2.0.0          j    x
+Abraxas discovery-azure 2.1.0-SNAPSHOT j    x
 Abraxas lang-javascript 2.0.0-SNAPSHOT j    x
 Abraxas marvel          NA             j/s  x         /_plugin/marvel/
 Abraxas lang-python     2.0.0-SNAPSHOT j    x
diff --git a/docs/reference/migration/migrate_query_refactoring.asciidoc b/docs/reference/migration/migrate_query_refactoring.asciidoc
deleted file mode 100644
index c7e005b..0000000
--- a/docs/reference/migration/migrate_query_refactoring.asciidoc
+++ /dev/null
@@ -1,147 +0,0 @@
-[[breaking-changes query-refactoring]]
-== Breaking changes on the query-refactoring branch
-
-This section discusses changes that are breaking to the current rest or java-api
-on the query-refactoring feature branch.
-
-=== Plugins
-
-Plugins implementing custom queries need to implement the `fromXContent(QueryParseContext)` method in their
-`QueryParser` subclass rather than `parse`. This method will take care of parsing the query from `XContent` format
-into an intermediate query representation that can be streamed between the nodes in binary format, effectively the
-query object used in the java api. Also, the query parser needs to implement the `getBuilderPrototype` method that
-returns a prototype of the streamable query, which allows to deserialize an incoming query by calling
-`readFrom(StreamInput)` against it, which will create a new object, see usages of `Writeable`. The `QueryParser`
-also needs to declare the generic type of the query that it supports and it's able to parse.
-The query object can then transform itself into a lucene query through the new `toQuery(QueryShardContext)` method,
-which returns a lucene query to be executed on the data node. The query implementation also needs to implement the
-`validate` method that allows to validate the content of the query, no matter whether it came in through the java api
-directly or through the REST layer.
-
-=== Java-API
-
-==== BoostingQueryBuilder
-
-Removed setters for mandatory positive/negative query. Both arguments now have
-to be supplied at construction time already and have to be non-null.
-
-==== SpanContainingQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== SpanOrQueryBuilder
-
-Making sure that query contains at least one clause by making initial clause mandatory
-in constructor.
-
-==== SpanNearQueryBuilder
-
-Removed setter for mandatory slop parameter, needs to be set in constructor now. Also
-making sure that query contains at least one clause by making initial clause mandatory
-in constructor. Updated the static factory methods in QueryBuilders accordingly.
-
-==== SpanNotQueryBuilder
-
-Removed setter for mandatory include/exclude span query clause, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders and tests accordingly.
-
-==== SpanWithinQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== QueryFilterBuilder
-
-Removed the setter `queryName(String queryName)` since this field is not supported
-in this type of query. Use `FQueryFilterBuilder.queryName(String queryName)` instead 
-when in need to wrap a named query as a filter.
-
-==== WrapperQueryBuilder
-
-Removed `wrapperQueryBuilder(byte[] source, int offset, int length)`. Instead simply
-use  `wrapperQueryBuilder(byte[] source)`. Updated the static factory methods in
-QueryBuilders accordingly.
-
-==== QueryStringQueryBuilder
-
-Removed ability to pass in boost value using `field(String field)` method in form e.g. `field^2`.
-Use the `field(String, float)` method instead.
-
-==== Operator
-
-Removed the enums called `Operator` from `MatchQueryBuilder`, `QueryStringQueryBuilder`,
-`SimpleQueryStringBuilder`, and `CommonTermsQueryBuilder` in favour of using the enum
-defined in `org.elasticsearch.index.query.Operator` in an effort to consolidate the
-codebase and avoid duplication.
-
-==== queryName and boost support
-
-Support for `queryName` and `boost` has been streamlined to all of the queries. That is
-a breaking change till queries get sent over the network as serialized json rather
-than in `Streamable` format. In fact whenever additional fields are added to the json
-representation of the query, older nodes might throw error when they find unknown fields.
-
-==== InnerHitsBuilder
-
-InnerHitsBuilder now has a dedicated addParentChildInnerHits and addNestedInnerHits methods
-to differentiate between inner hits for nested vs. parent / child documents. This change
-makes the type / path parameter mandatory.
-
-==== MatchQueryBuilder
-
-Moving MatchQueryBuilder.Type and MatchQueryBuilder.ZeroTermsQuery enum to MatchQuery.Type.
-Also reusing new Operator enum.
-
-==== MoreLikeThisQueryBuilder
-
-Removed `MoreLikeThisQueryBuilder.Item#id(String id)`, `Item#doc(BytesReference doc)`,
-`Item#doc(XContentBuilder doc)`. Use provided constructors instead.
-
-Removed `MoreLikeThisQueryBuilder#addLike` and `addUnlike` in favor to using the `like`
-and `unlike` methods.
-
-The deprecated `docs(Item... docs)`, `ignoreLike(Item... docs)`,
-`ignoreLike(String... likeText)`, `addItem(Item... likeItems)` have been removed.
-
-==== GeoDistanceQueryBuilder
-
-Removing individual setters for lon() and lat() values, both values should be set together
- using point(lon, lat).
-
-==== GeoDistanceRangeQueryBuilder
-
-Removing setters for to(Object ...) and from(Object ...) in favour of the only two allowed input
-arguments (String, Number). Removing setter for center point (point(), geohash()) because parameter 
-is mandatory and should already be set in constructor. 
-Also removing setters for lt(), lte(), gt(), gte() since they can all be replaced by equivallent 
-calls to to/from() and inludeLower()/includeUpper().
-
-==== GeoPolygonQueryBuilder
-
-Require shell of polygon already to be specified in constructor instead of adding it pointwise.
-This enables validation, but makes it necessary to remove the addPoint() methods. 
-
-==== MultiMatchQueryBuilder
-
-Moving MultiMatchQueryBuilder.ZeroTermsQuery enum to MatchQuery.ZeroTermsQuery.
-Also reusing new Operator enum.
-
-Removed ability to pass in boost value using `field(String field)` method in form e.g. `field^2`.
-Use the `field(String, float)` method instead.
-
-==== MissingQueryBuilder
-
-The two individual setters for existence() and nullValue() were removed in favour of
-optional constructor settings in order to better capture and validate their interdependent
-settings at construction time.
-
-==== TermsQueryBuilder
-
-Remove the setter for `termsLookup()`, making it only possible to either use a TermsLookUp object or
-individual values at constrution time. Also moving individual settings for the TermsLookUp (lookupIndex,
-lookupType, lookupId, lookupPath) to the separate TermsLookUp class, using construtor only and moving
-checks for validation there.
-
diff --git a/docs/reference/modules/discovery/azure.asciidoc b/docs/reference/modules/discovery/azure.asciidoc
index bb6fdc8..87d0725 100644
--- a/docs/reference/modules/discovery/azure.asciidoc
+++ b/docs/reference/modules/discovery/azure.asciidoc
@@ -2,5 +2,4 @@
 === Azure Discovery
 
 Azure discovery allows to use the Azure APIs to perform automatic discovery (similar to multicast).
-Please check the https://github.com/elasticsearch/elasticsearch-cloud-azure[plugin website]
-to find the full documentation.
+It is available as a plugin. See {plugins}/discovery-azure.html[discovery-azure] for more information.
diff --git a/docs/reference/modules/http.asciidoc b/docs/reference/modules/http.asciidoc
index 3255361..cf3780e 100644
--- a/docs/reference/modules/http.asciidoc
+++ b/docs/reference/modules/http.asciidoc
@@ -18,7 +18,9 @@ http://en.wikipedia.org/wiki/Chunked_transfer_encoding[HTTP chunking].
 [float]
 === Settings
 
-The following are the settings that can be configured for HTTP:
+The settings in the table below can be configured for HTTP. Note that none of
+them are dynamically updatable so for them to take effect they should be set in
+`elasticsearch.yml`.
 
 [cols="<,<",options="header",]
 |=======================================================================
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 449ca4b..4830e43 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -152,7 +152,7 @@ Other repository backends are available in these official plugins:
 
 * {plugins}/repository-s3.html[repository-s3] for S3 repository support
 * https://github.com/elasticsearch/elasticsearch-hadoop/tree/master/repository-hdfs[HDFS Plugin] for Hadoop environments
-* https://github.com/elasticsearch/elasticsearch-cloud-azure#azure-repository[Azure Cloud Plugin] for Azure storage repositories
+* {plugins}/repository-azure.html[repository-azure] for Azure storage repositories
 
 [float]
 ===== Repository Verification
diff --git a/docs/reference/query-dsl/has-parent-query.asciidoc b/docs/reference/query-dsl/has-parent-query.asciidoc
index 19958bf..5f12e44 100644
--- a/docs/reference/query-dsl/has-parent-query.asciidoc
+++ b/docs/reference/query-dsl/has-parent-query.asciidoc
@@ -24,10 +24,11 @@ in the same manner as the `has_child` query.
 [float]
 ==== Scoring capabilities
 
-The `has_parent` also has scoring support. The default is `false` which
-ignores the score from the parent document. The score is in this
+The `has_parent` also has scoring support. The
+supported score types are `score` or `none`. The default is `none` and
+this ignores the score from the parent document. The score is in this
 case equal to the boost on the `has_parent` query (Defaults to 1). If
-the score is set to `true`, then the score of the matching parent
+the score type is set to `score`, then the score of the matching parent
 document is aggregated into the child documents belonging to the
 matching parent document. The score mode can be specified with the
 `score_mode` field inside the `has_parent` query:
@@ -37,7 +38,7 @@ matching parent document. The score mode can be specified with the
 {
     "has_parent" : {
         "parent_type" : "blog",
-        "score" : true,
+        "score_mode" : "score",
         "query" : {
             "term" : {
                 "tag" : "something"
diff --git a/plugins/analysis-icu/LICENSE.txt b/plugins/analysis-icu/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/analysis-icu/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/analysis-icu/NOTICE.txt b/plugins/analysis-icu/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/analysis-icu/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/ICUCollationKeyFilter.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/ICUCollationKeyFilter.java
index d55be92..674ae8b 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/ICUCollationKeyFilter.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/ICUCollationKeyFilter.java
@@ -23,6 +23,7 @@ import com.ibm.icu.text.RawCollationKey;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.collation.ICUCollationDocValuesField;
 
 import java.io.IOException;
 
@@ -63,7 +64,7 @@ import java.io.IOException;
  *   generation timing and key length comparisons between ICU4J and
  *   java.text.Collator over several languages.
  * </p>
- *  @deprecated Use {@link ICUCollationAttributeFactory} instead, which encodes
+ *  @deprecated Use {@link ICUCollationDocValuesField} instead, which encodes
  *  terms directly as bytes. This filter WAS removed in Lucene 5.0
  */
 @Deprecated
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
index ec720f7..ca4be80 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
@@ -37,11 +37,9 @@ import java.nio.file.Files;
 
 /**
  * An ICU based collation token filter. There are two ways to configure collation:
- * <p/>
  * <p>The first is simply specifying the locale (defaults to the default locale). The <tt>language</tt>
  * parameter is the lowercase two-letter ISO-639 code. An additional <tt>country</tt> and <tt>variant</tt>
  * can be provided.
- * <p/>
  * <p>The second option is to specify collation rules as defined in the <a href="http://www.icu-project.org/userguide/Collate_Customization.html">
  * Collation customization</a> chapter in icu docs. The <tt>rules</tt> parameter can either embed the rules definition
  * in the settings or refer to an external location (preferable located under the <tt>config</tt> location, relative to it).
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerCharFilterFactory.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerCharFilterFactory.java
index 337461c..d8fec09 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerCharFilterFactory.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerCharFilterFactory.java
@@ -33,7 +33,6 @@ import java.io.Reader;
 
 /**
  * Uses the {@link org.apache.lucene.analysis.icu.ICUNormalizer2CharFilter} to normalize character.
- * <p/>
  * <p>The <tt>name</tt> can be used to provide the type of normalization to perform.</p>
  * <p>The <tt>mode</tt> can be used to provide 'compose' or 'decompose'. Default is compose.</p>
  */
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java
index 7f25886..c27fc1d 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java
@@ -30,7 +30,6 @@ import org.elasticsearch.index.settings.IndexSettings;
 
 /**
  * Uses the {@link org.apache.lucene.analysis.icu.ICUNormalizer2Filter} to normalize tokens.
- * <p/>
  * <p>The <tt>name</tt> can be used to provide the type of normalization to perform.
  *
  *
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IndexableBinaryStringTools.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IndexableBinaryStringTools.java
index b8ae222..a114a34 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IndexableBinaryStringTools.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IndexableBinaryStringTools.java
@@ -22,20 +22,20 @@ import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute; // ja
 /**
  * Provides support for converting byte sequences to Strings and back again.
  * The resulting Strings preserve the original byte sequences' sort order.
- * <p/>
+ * <p>
  * The Strings are constructed using a Base 8000h encoding of the original
  * binary data - each char of an encoded String represents a 15-bit chunk
  * from the byte sequence.  Base 8000h was chosen because it allows for all
  * lower 15 bits of char to be used without restriction; the surrogate range 
  * [U+D8000-U+DFFF] does not represent valid chars, and would require
  * complicated handling to avoid them and allow use of char's high bit.
- * <p/>
+ * <p>
  * Although unset bits are used as padding in the final char, the original
  * byte sequence could contain trailing bytes with no set bits (null bytes):
  * padding is indistinguishable from valid information.  To overcome this
  * problem, a char is appended, indicating the number of encoded bytes in the
  * final content char.
- * <p/>
+ * <p>
  *
  * @lucene.experimental
  * @deprecated Implement {@link TermToBytesRefAttribute} and store bytes directly
diff --git a/plugins/analysis-kuromoji/LICENSE.txt b/plugins/analysis-kuromoji/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/analysis-kuromoji/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/analysis-kuromoji/NOTICE.txt b/plugins/analysis-kuromoji/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/analysis-kuromoji/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/analysis-phonetic/LICENSE.txt b/plugins/analysis-phonetic/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/analysis-phonetic/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/analysis-phonetic/NOTICE.txt b/plugins/analysis-phonetic/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/analysis-phonetic/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/HaasePhonetik.java b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/HaasePhonetik.java
index 880bc00..728a935 100644
--- a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/HaasePhonetik.java
+++ b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/HaasePhonetik.java
@@ -42,28 +42,16 @@ public class HaasePhonetik extends KoelnerPhonetik {
     private final static String[] HAASE_VARIATIONS_REPLACEMENTS = {"AUN", "RW", "RSK", "AR", "OW", "CH",
         "LI", "O", "SCH", "O", "O", "I"};
 
-    /**
-     *
-     * @return
-     */
     @Override
     protected String[] getPatterns() {
         return HAASE_VARIATIONS_PATTERNS;
     }
 
-    /**
-     * 
-     * @return
-     */
     @Override
     protected String[] getReplacements() {
         return HAASE_VARIATIONS_REPLACEMENTS;
     }
 
-    /**
-     *
-     * @return
-     */
     @Override
     protected char getCode() {
         return '9';
diff --git a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/KoelnerPhonetik.java b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/KoelnerPhonetik.java
index a3190fa..e70722c 100644
--- a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/KoelnerPhonetik.java
+++ b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/KoelnerPhonetik.java
@@ -60,10 +60,6 @@ public class KoelnerPhonetik implements StringEncoder {
         init();
     }
     
-    /**
-     *
-     * @param useOnlyPrimaryCode
-     */
     public KoelnerPhonetik(boolean useOnlyPrimaryCode) {
         this();
         this.primary = useOnlyPrimaryCode;
@@ -78,28 +74,14 @@ public class KoelnerPhonetik implements StringEncoder {
         return POSTEL_VARIATIONS_PATTERNS;
     }
 
-    /**
-     *
-     * @return
-     */
     protected String[] getReplacements() {
         return POSTEL_VARIATIONS_REPLACEMENTS;
     }
 
-    /**
-     *
-     * @return
-     */
     protected char getCode() {
         return '0';
     }
 
-    /**
-     *
-     * @param o1
-     * @param o2
-     * @return
-     */
     public double getRelativeValue(Object o1, Object o2) {
         String[] kopho1 = code(expandUmlauts(o1.toString().toUpperCase(Locale.GERMANY)));
         String[] kopho2 = code(expandUmlauts(o2.toString().toUpperCase(Locale.GERMANY)));
@@ -290,20 +272,10 @@ public class KoelnerPhonetik implements StringEncoder {
         return s;
     }
 
-    /**
-     *
-     * @param str
-     * @return
-     */
     private String expandUmlauts(String str) {
         return str.replaceAll("\u00C4", "AE").replaceAll("\u00D6", "OE").replaceAll("\u00DC", "UE");
     }
 
-    /**
-     *
-     * @param str
-     * @return
-     */
     private String removeSequences(String str) {
         if (str == null || str.length() == 0) {
             return "";
diff --git a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/Nysiis.java b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/Nysiis.java
index 3b85ef4..894d5d8 100644
--- a/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/Nysiis.java
+++ b/plugins/analysis-phonetic/src/main/java/org/elasticsearch/index/analysis/phonetic/Nysiis.java
@@ -38,33 +38,33 @@ import java.util.regex.Pattern;
  * <p>Algorithm description:
  * <pre>
  * 1. Transcode first characters of name
- *   1a. MAC ->   MCC
- *   1b. KN  ->   NN
- *   1c. K   ->   C
- *   1d. PH  ->   FF
- *   1e. PF  ->   FF
- *   1f. SCH ->   SSS
+ *   1a. MAC -&gt;   MCC
+ *   1b. KN  -&gt;   NN
+ *   1c. K   -&gt;   C
+ *   1d. PH  -&gt;   FF
+ *   1e. PF  -&gt;   FF
+ *   1f. SCH -&gt;   SSS
  * 2. Transcode last characters of name
- *   2a. EE, IE          ->   Y
- *   2b. DT,RT,RD,NT,ND  ->   D
+ *   2a. EE, IE          -&gt;   Y
+ *   2b. DT,RT,RD,NT,ND  -&gt;   D
  * 3. First character of key = first character of name
  * 4. Transcode remaining characters by following these rules, incrementing by one character each time
- *   4a. EV  ->   AF  else A,E,I,O,U -> A
- *   4b. Q   ->   G
- *   4c. Z   ->   S
- *   4d. M   ->   N
- *   4e. KN  ->   N   else K -> C
- *   4f. SCH ->   SSS
- *   4g. PH  ->   FF
- *   4h. H   ->   If previous or next is nonvowel, previous
- *   4i. W   ->   If previous is vowel, previous
+ *   4a. EV  -&gt;   AF  else A,E,I,O,U -&gt; A
+ *   4b. Q   -&gt;   G
+ *   4c. Z   -&gt;   S
+ *   4d. M   -&gt;   N
+ *   4e. KN  -&gt;   N   else K -&gt; C
+ *   4f. SCH -&gt;   SSS
+ *   4g. PH  -&gt;   FF
+ *   4h. H   -&gt;   If previous or next is nonvowel, previous
+ *   4i. W   -&gt;   If previous is vowel, previous
  *   4j. Add current to key if current != last key character
  * 5. If last character is S, remove it
  * 6. If last characters are AY, replace with Y
  * 7. If last character is A, remove it
  * 8. Collapse all strings of repeated characters
  * 9. Add original first character of name as first character of key
- * </pre></p>
+ * </pre>
  *
  * @see <a href="http://en.wikipedia.org/wiki/NYSIIS">NYSIIS on Wikipedia</a>
  * @see <a href="http://www.dropby.com/NYSIIS.html">NYSIIS on dropby.com</a>
diff --git a/plugins/analysis-smartcn/LICENSE.txt b/plugins/analysis-smartcn/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/analysis-smartcn/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/analysis-smartcn/NOTICE.txt b/plugins/analysis-smartcn/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/analysis-smartcn/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/analysis-stempel/LICENSE.txt b/plugins/analysis-stempel/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/analysis-stempel/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/analysis-stempel/NOTICE.txt b/plugins/analysis-stempel/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/analysis-stempel/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/cloud-azure/LICENSE.txt b/plugins/cloud-azure/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/activation-1.1.jar.sha1 b/plugins/cloud-azure/licenses/activation-1.1.jar.sha1
deleted file mode 100644
index c4ee8fa..0000000
--- a/plugins/cloud-azure/licenses/activation-1.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-e6cb541461c2834bdea3eb920f1884d1eb508b50
diff --git a/plugins/cloud-azure/licenses/activation-LICENSE.txt b/plugins/cloud-azure/licenses/activation-LICENSE.txt
deleted file mode 100644
index 3be7a13..0000000
--- a/plugins/cloud-azure/licenses/activation-LICENSE.txt
+++ /dev/null
@@ -1,119 +0,0 @@
-COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0
-
-1. Definitions.
-
-1.1. Contributor means each individual or entity that creates or contributes to the creation of Modifications.
-
-1.2. Contributor Version means the combination of the Original Software, prior Modifications used by a Contributor (if any), and the Modifications made by that particular Contributor.
-
-1.3. Covered Software means (a) the Original Software, or (b) Modifications, or (c) the combination of files containing Original Software with files containing Modifications, in each case including portions thereof.
-
-1.4. Executable means the Covered Software in any form other than Source Code.
-
-1.5. Initial Developer means the individual or entity that first makes Original Software available under this License.
-
-1.6. Larger Work means a work which combines Covered Software or portions thereof with code not governed by the terms of this License.
-
-1.7. License means this document.
-
-1.8. Licensable means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently acquired, any and all of the rights conveyed herein.
-
-1.9. Modifications means the Source Code and Executable form of any of the following:
-
-A. Any file that results from an addition to, deletion from or modification of the contents of a file containing Original Software or previous Modifications;
-
-B. Any new file that contains any part of the Original Software or previous Modification; or
-
-C. Any new file that is contributed or otherwise made available under the terms of this License.
-
-1.10. Original Software means the Source Code and Executable form of computer software code that is originally released under this License.
-
-1.11. Patent Claims means any patent claim(s), now owned or hereafter acquired, including without limitation, method, process, and apparatus claims, in any patent Licensable by grantor.
-
-1.12. Source Code means (a) the common form of computer software code in which modifications are made and (b) associated documentation included in or with such code.
-
-1.13. You (or Your) means an individual or a legal entity exercising rights under, and complying with all of the terms of, this License. For legal entities, You includes any entity which controls, is controlled by, or is under common control with You. For purposes of this definition, control means (a)the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b)ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.
-
-2. License Grants.
-
-2.1. The Initial Developer Grant.
-Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, the Initial Developer hereby grants You a world-wide, royalty-free, non-exclusive license:
-(a) under intellectual property rights (other than patent or trademark) Licensable by Initial Developer, to use, reproduce, modify, display, perform, sublicense and distribute the Original Software (or portions thereof), with or without Modifications, and/or as part of a Larger Work; and
-(b) under Patent Claims infringed by the making, using or selling of Original Software, to make, have made, use, practice, sell, and offer for sale, and/or otherwise dispose of the Original Software (or portions thereof).
-(c) The licenses granted in Sections2.1(a) and (b) are effective on the date Initial Developer first distributes or otherwise makes the Original Software available to a third party under the terms of this License.
-(d) Notwithstanding Section2.1(b) above, no patent license is granted: (1)for code that You delete from the Original Software, or (2)for infringements caused by: (i)the modification of the Original Software, or (ii)the combination of the Original Software with other software or devices.
-
-2.2. Contributor Grant.
-Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:
-(a) under intellectual property rights (other than patent or trademark) Licensable by Contributor to use, reproduce, modify, display, perform, sublicense and distribute the Modifications created by such Contributor (or portions thereof), either on an unmodified basis, with other Modifications, as Covered Software and/or as part of a Larger Work; and
-(b) under Patent Claims infringed by the making, using, or selling of Modifications made by that Contributor either alone and/or in combination with its Contributor Version (or portions of such combination), to make, use, sell, offer for sale, have made, and/or otherwise dispose of: (1)Modifications made by that Contributor (or portions thereof); and (2)the combination of Modifications made by that Contributor with its Contributor Version (or portions of such combination).
-(c) The licenses granted in Sections2.2(a) and 2.2(b) are effective on the date Contributor first distributes or otherwise makes the Modifications available to a third party.
-(d) Notwithstanding Section2.2(b) above, no patent license is granted: (1)for any code that Contributor has deleted from the Contributor Version; (2)for infringements caused by: (i)third party modifications of Contributor Version, or (ii)the combination of Modifications made by that Contributor with other software (except as part of the Contributor Version) or other devices; or (3)under Patent Claims infringed by Covered Software in the absence of Modifications made by that Contributor.
-
-3. Distribution Obligations.
-
-3.1. Availability of Source Code.
-
-Any Covered Software that You distribute or otherwise make available in Executable form must also be made available in Source Code form and that Source Code form must be distributed only under the terms of this License. You must include a copy of this License with every copy of the Source Code form of the Covered Software You distribute or otherwise make available. You must inform recipients of any such Covered Software in Executable form as to how they can obtain such Covered Software in Source Code form in a reasonable manner on or through a medium customarily used for software exchange.
-
-3.2. Modifications.
-
-The Modifications that You create or to which You contribute are governed by the terms of this License. You represent that You believe Your Modifications are Your original creation(s) and/or You have sufficient rights to grant the rights conveyed by this License.
-
-3.3. Required Notices.
-You must include a notice in each of Your Modifications that identifies You as the Contributor of the Modification. You may not remove or alter any copyright, patent or trademark notices contained within the Covered Software, or any notices of licensing or any descriptive text giving attribution to any Contributor or the Initial Developer.
-
-3.4. Application of Additional Terms.
-You may not offer or impose any terms on any Covered Software in Source Code form that alters or restricts the applicable version of this License or the recipients rights hereunder. You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, you may do so only on Your own behalf, and not on behalf of the Initial Developer or any Contributor. You must make it absolutely clear that any such warranty, support, indemnity or liability obligation is offered by You alone, and You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of warranty, support, indemnity or liability terms You offer.
-
-3.5. Distribution of Executable Versions.
-You may distribute the Executable form of the Covered Software under the terms of this License or under the terms of a license of Your choice, which may contain terms different from this License, provided that You are in compliance with the terms of this License and that the license for the Executable form does not attempt to limit or alter the recipients rights in the Source Code form from the rights set forth in this License. If You distribute the Covered Software in Executable form under a different license, You must make it absolutely clear that any terms which differ from this License are offered by You alone, not by the Initial Developer or Contributor. You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of any such terms You offer.
-
-3.6. Larger Works.
-You may create a Larger Work by combining Covered Software with other code not governed by the terms of this License and distribute the Larger Work as a single product. In such a case, You must make sure the requirements of this License are fulfilled for the Covered Software.
-
-4. Versions of the License.
-
-4.1. New Versions.
-Sun Microsystems, Inc. is the initial license steward and may publish revised and/or new versions of this License from time to time. Each version will be given a distinguishing version number. Except as provided in Section 4.3, no one other than the license steward has the right to modify this License.
-
-4.2. Effect of New Versions.
-
-You may always continue to use, distribute or otherwise make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. If the Initial Developer includes a notice in the Original Software prohibiting it from being distributed or otherwise made available under any subsequent version of the License, You must distribute and make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. Otherwise, You may also choose to use, distribute or otherwise make the Covered Software available under the terms of any subsequent version of the License published by the license steward.
-4.3. Modified Versions.
-
-When You are an Initial Developer and You want to create a new license for Your Original Software, You may create and use a modified version of this License if You: (a)rename the license and remove any references to the name of the license steward (except to note that the license differs from this License); and (b)otherwise make it clear that the license contains terms which differ from this License.
-
-5. DISCLAIMER OF WARRANTY.
-
-COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN AS IS BASIS, WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, WITHOUT LIMITATION, WARRANTIES THAT THE COVERED SOFTWARE IS FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE COVERED SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME THE COST OF ANY NECESSARY SERVICING, REPAIR OR CORRECTION. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.
-
-6. TERMINATION.
-
-6.1. This License and the rights granted hereunder will terminate automatically if You fail to comply with terms herein and fail to cure such breach within 30 days of becoming aware of the breach. Provisions which, by their nature, must remain in effect beyond the termination of this License shall survive.
-
-6.2. If You assert a patent infringement claim (excluding declaratory judgment actions) against Initial Developer or a Contributor (the Initial Developer or Contributor against whom You assert such claim is referred to as Participant) alleging that the Participant Software (meaning the Contributor Version where the Participant is a Contributor or the Original Software where the Participant is the Initial Developer) directly or indirectly infringes any patent, then any and all rights granted directly or indirectly to You by such Participant, the Initial Developer (if the Initial Developer is not the Participant) and all Contributors under Sections2.1 and/or 2.2 of this License shall, upon 60 days notice from Participant terminate prospectively and automatically at the expiration of such 60 day notice period, unless if within such 60 day period You withdraw Your claim with respect to the Participant Software against such Participant either unilaterally or pursuant to a written agreement with Participant.
-
-6.3. In the event of termination under Sections6.1 or 6.2 above, all end user licenses that have been validly granted by You or any distributor hereunder prior to termination (excluding licenses granted to You by any distributor) shall survive termination.
-
-7. LIMITATION OF LIABILITY.
-
-UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE, OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTYS NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
-
-8. U.S. GOVERNMENT END USERS.
-
-The Covered Software is a commercial item, as that term is defined in 48C.F.R.2.101 (Oct. 1995), consisting of commercial computer software (as that term is defined at 48 C.F.R. 252.227-7014(a)(1)) and commercial computer software documentation as such terms are used in 48C.F.R.12.212 (Sept. 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government End Users acquire Covered Software with only those rights set forth herein. This U.S. Government Rights clause is in lieu of, and supersedes, any other FAR, DFAR, or other clause or provision that addresses Government rights in computer software under this License.
-
-9. MISCELLANEOUS.
-
-This License represents the complete agreement concerning subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. This License shall be governed by the law of the jurisdiction specified in a notice contained within the Original Software (except to the extent applicable law, if any, provides otherwise), excluding such jurisdictions conflict-of-law provisions. Any litigation relating to this License shall be subject to the jurisdiction of the courts located in the jurisdiction and venue specified in a notice contained within the Original Software, with the losing party responsible for costs, including, without limitation, court costs and reasonable attorneys fees and expenses. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not apply to this License. You agree that You alone are responsible for compliance with the United States export administration regulations (and the export control laws and regulation of any other countries) when You use, distribute or otherwise make available any Covered Software.
-
-10. RESPONSIBILITY FOR CLAIMS.
-
-As between Initial Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.
-
-NOTICE PURSUANT TO SECTION 9 OF THE COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)
-The GlassFish code released under the CDDL shall be governed by the laws of the State of California (excluding conflict-of-law provisions). Any litigation relating to this License shall be subject to the jurisdiction of the Federal Courts of the Northern District of California and the state courts of the State of California, with venue lying in Santa Clara County, California. 
-
-
-
diff --git a/plugins/cloud-azure/licenses/activation-NOTICE.txt b/plugins/cloud-azure/licenses/activation-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/activation-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/azure-LICENSE.txt b/plugins/cloud-azure/licenses/azure-LICENSE.txt
deleted file mode 100644
index c763ea5..0000000
--- a/plugins/cloud-azure/licenses/azure-LICENSE.txt
+++ /dev/null
@@ -1,1565 +0,0 @@
-
-
-
-<!DOCTYPE html>
-<html lang="en" class="">
-  <head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# object: http://ogp.me/ns/object# article: http://ogp.me/ns/article# profile: http://ogp.me/ns/profile#">
-    <meta charset='utf-8'>
-    <meta http-equiv="X-UA-Compatible" content="IE=edge">
-    <meta http-equiv="Content-Language" content="en">
-    
-    
-    <title>azure-sdk-for-java/LICENSE.txt at master  Azure/azure-sdk-for-java  GitHub</title>
-    <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">
-    <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
-    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-114.png">
-    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114.png">
-    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-144.png">
-    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144.png">
-    <meta property="fb:app_id" content="1401488693436528">
-
-      <meta content="@github" name="twitter:site" /><meta content="summary" name="twitter:card" /><meta content="Azure/azure-sdk-for-java" name="twitter:title" /><meta content="azure-sdk-for-java - Microsoft Azure SDK for Java" name="twitter:description" /><meta content="https://avatars2.githubusercontent.com/u/6844498?v=3&amp;s=400" name="twitter:image:src" />
-      <meta content="GitHub" property="og:site_name" /><meta content="object" property="og:type" /><meta content="https://avatars2.githubusercontent.com/u/6844498?v=3&amp;s=400" property="og:image" /><meta content="Azure/azure-sdk-for-java" property="og:title" /><meta content="https://github.com/Azure/azure-sdk-for-java" property="og:url" /><meta content="azure-sdk-for-java - Microsoft Azure SDK for Java" property="og:description" />
-      <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">
-    <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">
-    <link rel="assets" href="https://assets-cdn.github.com/">
-    
-    <meta name="pjax-timeout" content="1000">
-    
-
-    <meta name="msapplication-TileImage" content="/windows-tile.png">
-    <meta name="msapplication-TileColor" content="#ffffff">
-    <meta name="selected-link" value="repo_source" data-pjax-transient>
-
-        <meta name="google-analytics" content="UA-3769691-2">
-
-    <meta content="collector.githubapp.com" name="octolytics-host" /><meta content="collector-cdn.github.com" name="octolytics-script-host" /><meta content="github" name="octolytics-app-id" /><meta content="5F179209:67DA:4BEC236:55846EE2" name="octolytics-dimension-request_id" />
-    
-    <meta content="Rails, view, blob#show" name="analytics-event" />
-    <meta class="js-ga-set" name="dimension1" content="Logged Out">
-    <meta name="is-dotcom" content="true">
-      <meta name="hostname" content="github.com">
-    <meta name="user-login" content="">
-
-      <link rel="icon" sizes="any" mask href="https://assets-cdn.github.com/pinned-octocat.svg">
-      <meta name="theme-color" content="#4078c0">
-      <link rel="icon" type="image/x-icon" href="https://assets-cdn.github.com/favicon.ico">
-
-
-    <meta content="authenticity_token" name="csrf-param" />
-<meta content="RrOhsOCPIj1a2I38Uy+kMyrEl0XTjx+qKrnM9z9il+AaIjMmv5FPMEbdUboERp/0E//crprf8ZZiqGpy3Zgrag==" name="csrf-token" />
-
-    <link crossorigin="anonymous" href="https://assets-cdn.github.com/assets/github/index-805b1dc56a27171cceb8daae5c9e50c759789b9473bca4278a8145697ca3e05b.css" media="all" rel="stylesheet" />
-    <link crossorigin="anonymous" href="https://assets-cdn.github.com/assets/github2/index-7fed6e419518134cd22bb335cba08692014caf252dd8266ee4122c0baeacd7ce.css" media="all" rel="stylesheet" />
-    
-    
-
-
-    <meta http-equiv="x-pjax-version" content="107482bfddb41cfc3cfa2df392aed5a1">
-
-      
-  <meta name="description" content="azure-sdk-for-java - Microsoft Azure SDK for Java">
-  <meta name="go-import" content="github.com/Azure/azure-sdk-for-java git https://github.com/Azure/azure-sdk-for-java.git">
-
-  <meta content="6844498" name="octolytics-dimension-user_id" /><meta content="Azure" name="octolytics-dimension-user_login" /><meta content="2928948" name="octolytics-dimension-repository_id" /><meta content="Azure/azure-sdk-for-java" name="octolytics-dimension-repository_nwo" /><meta content="true" name="octolytics-dimension-repository_public" /><meta content="false" name="octolytics-dimension-repository_is_fork" /><meta content="2928948" name="octolytics-dimension-repository_network_root_id" /><meta content="Azure/azure-sdk-for-java" name="octolytics-dimension-repository_network_root_nwo" />
-  <link href="https://github.com/Azure/azure-sdk-for-java/commits/master.atom" rel="alternate" title="Recent Commits to azure-sdk-for-java:master" type="application/atom+xml">
-
-  </head>
-
-
-  <body class="logged_out  env-production  vis-public page-blob">
-    <a href="#start-of-content" tabindex="1" class="accessibility-aid js-skip-to-content">Skip to content</a>
-    <div class="wrapper">
-      
-      
-      
-
-
-        
-        <div class="header header-logged-out" role="banner">
-  <div class="container clearfix">
-
-    <a class="header-logo-wordmark" href="https://github.com/" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark">
-      <span class="mega-octicon octicon-logo-github"></span>
-    </a>
-
-    <div class="header-actions" role="navigation">
-        <a class="btn btn-primary" href="/join" data-ga-click="(Logged out) Header, clicked Sign up, text:sign-up">Sign up</a>
-      <a class="btn" href="/login?return_to=%2FAzure%2Fazure-sdk-for-java%2Fblob%2Fmaster%2FLICENSE.txt" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in">Sign in</a>
-    </div>
-
-    <div class="site-search repo-scope js-site-search" role="search">
-      <form accept-charset="UTF-8" action="/Azure/azure-sdk-for-java/search" class="js-site-search-form" data-global-search-url="/search" data-repo-search-url="/Azure/azure-sdk-for-java/search" method="get"><div style="margin:0;padding:0;display:inline"><input name="utf8" type="hidden" value="&#x2713;" /></div>
-  <label class="js-chromeless-input-container form-control">
-    <div class="scope-badge">This repository</div>
-    <input type="text"
-      class="js-site-search-focus js-site-search-field is-clearable chromeless-input"
-      data-hotkey="s"
-      name="q"
-      placeholder="Search"
-      data-global-scope-placeholder="Search GitHub"
-      data-repo-scope-placeholder="Search"
-      tabindex="1"
-      autocapitalize="off">
-  </label>
-</form>
-    </div>
-
-      <ul class="header-nav left" role="navigation">
-          <li class="header-nav-item">
-            <a class="header-nav-link" href="/explore" data-ga-click="(Logged out) Header, go to explore, text:explore">Explore</a>
-          </li>
-          <li class="header-nav-item">
-            <a class="header-nav-link" href="/features" data-ga-click="(Logged out) Header, go to features, text:features">Features</a>
-          </li>
-          <li class="header-nav-item">
-            <a class="header-nav-link" href="https://enterprise.github.com/" data-ga-click="(Logged out) Header, go to enterprise, text:enterprise">Enterprise</a>
-          </li>
-          <li class="header-nav-item">
-            <a class="header-nav-link" href="/blog" data-ga-click="(Logged out) Header, go to blog, text:blog">Blog</a>
-          </li>
-      </ul>
-
-  </div>
-</div>
-
-
-
-      <div id="start-of-content" class="accessibility-aid"></div>
-          <div class="site" itemscope itemtype="http://schema.org/WebPage">
-    <div id="js-flash-container">
-      
-    </div>
-    <div class="pagehead repohead instapaper_ignore readability-menu">
-      <div class="container">
-
-        
-<ul class="pagehead-actions">
-
-  <li>
-      <a href="/login?return_to=%2FAzure%2Fazure-sdk-for-java"
-    class="btn btn-sm btn-with-count tooltipped tooltipped-n"
-    aria-label="You must be signed in to watch a repository" rel="nofollow">
-    <span class="octicon octicon-eye"></span>
-    Watch
-  </a>
-  <a class="social-count" href="/Azure/azure-sdk-for-java/watchers">
-    83
-  </a>
-
-  </li>
-
-  <li>
-      <a href="/login?return_to=%2FAzure%2Fazure-sdk-for-java"
-    class="btn btn-sm btn-with-count tooltipped tooltipped-n"
-    aria-label="You must be signed in to star a repository" rel="nofollow">
-    <span class="octicon octicon-star"></span>
-    Star
-  </a>
-
-    <a class="social-count js-social-count" href="/Azure/azure-sdk-for-java/stargazers">
-      184
-    </a>
-
-  </li>
-
-    <li>
-      <a href="/login?return_to=%2FAzure%2Fazure-sdk-for-java"
-        class="btn btn-sm btn-with-count tooltipped tooltipped-n"
-        aria-label="You must be signed in to fork a repository" rel="nofollow">
-        <span class="octicon octicon-repo-forked"></span>
-        Fork
-      </a>
-      <a href="/Azure/azure-sdk-for-java/network" class="social-count">
-        124
-      </a>
-    </li>
-</ul>
-
-        <h1 itemscope itemtype="http://data-vocabulary.org/Breadcrumb" class="entry-title public">
-          <span class="mega-octicon octicon-repo"></span>
-          <span class="author"><a href="/Azure" class="url fn" itemprop="url" rel="author"><span itemprop="title">Azure</span></a></span><!--
-       --><span class="path-divider">/</span><!--
-       --><strong><a href="/Azure/azure-sdk-for-java" data-pjax="#js-repo-pjax-container">azure-sdk-for-java</a></strong>
-
-          <span class="page-context-loader">
-            <img alt="" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-          </span>
-
-        </h1>
-      </div><!-- /.container -->
-    </div><!-- /.repohead -->
-
-    <div class="container">
-      <div class="repository-with-sidebar repo-container new-discussion-timeline  ">
-        <div class="repository-sidebar clearfix">
-            
-<nav class="sunken-menu repo-nav js-repo-nav js-sidenav-container-pjax js-octicon-loaders"
-     role="navigation"
-     data-pjax="#js-repo-pjax-container"
-     data-issue-count-url="/Azure/azure-sdk-for-java/issues/counts">
-  <ul class="sunken-menu-group">
-    <li class="tooltipped tooltipped-w" aria-label="Code">
-      <a href="/Azure/azure-sdk-for-java" aria-label="Code" class="selected js-selected-navigation-item sunken-menu-item" data-hotkey="g c" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches /Azure/azure-sdk-for-java">
-        <span class="octicon octicon-code"></span> <span class="full-word">Code</span>
-        <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>    </li>
-
-      <li class="tooltipped tooltipped-w" aria-label="Issues">
-        <a href="/Azure/azure-sdk-for-java/issues" aria-label="Issues" class="js-selected-navigation-item sunken-menu-item" data-hotkey="g i" data-selected-links="repo_issues repo_labels repo_milestones /Azure/azure-sdk-for-java/issues">
-          <span class="octicon octicon-issue-opened"></span> <span class="full-word">Issues</span>
-          <span class="js-issue-replace-counter"></span>
-          <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>      </li>
-
-    <li class="tooltipped tooltipped-w" aria-label="Pull requests">
-      <a href="/Azure/azure-sdk-for-java/pulls" aria-label="Pull requests" class="js-selected-navigation-item sunken-menu-item" data-hotkey="g p" data-selected-links="repo_pulls /Azure/azure-sdk-for-java/pulls">
-          <span class="octicon octicon-git-pull-request"></span> <span class="full-word">Pull requests</span>
-          <span class="js-pull-replace-counter"></span>
-          <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>    </li>
-
-      <li class="tooltipped tooltipped-w" aria-label="Wiki">
-        <a href="/Azure/azure-sdk-for-java/wiki" aria-label="Wiki" class="js-selected-navigation-item sunken-menu-item" data-hotkey="g w" data-selected-links="repo_wiki /Azure/azure-sdk-for-java/wiki">
-          <span class="octicon octicon-book"></span> <span class="full-word">Wiki</span>
-          <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>      </li>
-  </ul>
-  <div class="sunken-menu-separator"></div>
-  <ul class="sunken-menu-group">
-
-    <li class="tooltipped tooltipped-w" aria-label="Pulse">
-      <a href="/Azure/azure-sdk-for-java/pulse" aria-label="Pulse" class="js-selected-navigation-item sunken-menu-item" data-selected-links="pulse /Azure/azure-sdk-for-java/pulse">
-        <span class="octicon octicon-pulse"></span> <span class="full-word">Pulse</span>
-        <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>    </li>
-
-    <li class="tooltipped tooltipped-w" aria-label="Graphs">
-      <a href="/Azure/azure-sdk-for-java/graphs" aria-label="Graphs" class="js-selected-navigation-item sunken-menu-item" data-selected-links="repo_graphs repo_contributors /Azure/azure-sdk-for-java/graphs">
-        <span class="octicon octicon-graph"></span> <span class="full-word">Graphs</span>
-        <img alt="" class="mini-loader" height="16" src="https://assets-cdn.github.com/images/spinners/octocat-spinner-32.gif" width="16" />
-</a>    </li>
-  </ul>
-
-
-</nav>
-
-              <div class="only-with-full-nav">
-                  
-<div class="js-clone-url clone-url open"
-  data-protocol-type="http">
-  <h3><span class="text-emphasized">HTTPS</span> clone URL</h3>
-  <div class="input-group js-zeroclipboard-container">
-    <input type="text" class="input-mini input-monospace js-url-field js-zeroclipboard-target"
-           value="https://github.com/Azure/azure-sdk-for-java.git" readonly="readonly">
-    <span class="input-group-button">
-      <button aria-label="Copy to clipboard" class="js-zeroclipboard btn btn-sm zeroclipboard-button tooltipped tooltipped-s" data-copied-hint="Copied!" type="button"><span class="octicon octicon-clippy"></span></button>
-    </span>
-  </div>
-</div>
-
-  
-<div class="js-clone-url clone-url "
-  data-protocol-type="subversion">
-  <h3><span class="text-emphasized">Subversion</span> checkout URL</h3>
-  <div class="input-group js-zeroclipboard-container">
-    <input type="text" class="input-mini input-monospace js-url-field js-zeroclipboard-target"
-           value="https://github.com/Azure/azure-sdk-for-java" readonly="readonly">
-    <span class="input-group-button">
-      <button aria-label="Copy to clipboard" class="js-zeroclipboard btn btn-sm zeroclipboard-button tooltipped tooltipped-s" data-copied-hint="Copied!" type="button"><span class="octicon octicon-clippy"></span></button>
-    </span>
-  </div>
-</div>
-
-
-
-<div class="clone-options">You can clone with
-  <form accept-charset="UTF-8" action="/users/set_protocol?protocol_selector=http&amp;protocol_type=clone" class="inline-form js-clone-selector-form " data-remote="true" method="post"><div style="margin:0;padding:0;display:inline"><input name="utf8" type="hidden" value="&#x2713;" /><input name="authenticity_token" type="hidden" value="sQeAtFSDwUHuIPCbyPRYFzJ4SAZt+azdZR5yzQVf0I6ZL3Bm2VZfPKzR1aKPLbzkE2wBBFJoogda8KfpDZNyTg==" /></div><button class="btn-link js-clone-selector" data-protocol="http" type="submit">HTTPS</button></form> or <form accept-charset="UTF-8" action="/users/set_protocol?protocol_selector=subversion&amp;protocol_type=clone" class="inline-form js-clone-selector-form " data-remote="true" method="post"><div style="margin:0;padding:0;display:inline"><input name="utf8" type="hidden" value="&#x2713;" /><input name="authenticity_token" type="hidden" value="Siqfm6TQo4Mhog02TUC0AGjQ4aE0j6qP01/z7+aXZfpatXF2jL1lIEyzQaD0Y3jH4BfDRzJjX+becJSM31/+3g==" /></div><button class="btn-link js-clone-selector" data-protocol="subversion" type="submit">Subversion</button></form>.
-  <a href="https://help.github.com/articles/which-remote-url-should-i-use" class="help tooltipped tooltipped-n" aria-label="Get help on which URL is right for you.">
-    <span class="octicon octicon-question"></span>
-  </a>
-</div>
-
-
-
-
-                <a href="/Azure/azure-sdk-for-java/archive/master.zip"
-                   class="btn btn-sm sidebar-button"
-                   aria-label="Download the contents of Azure/azure-sdk-for-java as a zip file"
-                   title="Download the contents of Azure/azure-sdk-for-java as a zip file"
-                   rel="nofollow">
-                  <span class="octicon octicon-cloud-download"></span>
-                  Download ZIP
-                </a>
-              </div>
-        </div><!-- /.repository-sidebar -->
-
-        <div id="js-repo-pjax-container" class="repository-content context-loader-container" data-pjax-container>
-
-          
-
-<a href="/Azure/azure-sdk-for-java/blob/97b4f2a28f212d76fa8fdaea77f59358fabbd955/LICENSE.txt" class="hidden js-permalink-shortcut" data-hotkey="y">Permalink</a>
-
-<!-- blob contrib key: blob_contributors:v21:263852e7dddc976ec11940a5f67703dc -->
-
-<div class="file-navigation js-zeroclipboard-container">
-  
-<div class="select-menu js-menu-container js-select-menu left">
-  <span class="btn btn-sm select-menu-button js-menu-target css-truncate" data-hotkey="w"
-    data-ref="master"
-    title="master"
-    role="button" aria-label="Switch branches or tags" tabindex="0" aria-haspopup="true">
-    <span class="octicon octicon-git-branch"></span>
-    <i>branch:</i>
-    <span class="js-select-button css-truncate-target">master</span>
-  </span>
-
-  <div class="select-menu-modal-holder js-menu-content js-navigation-container" data-pjax aria-hidden="true">
-
-    <div class="select-menu-modal">
-      <div class="select-menu-header">
-        <span class="select-menu-title">Switch branches/tags</span>
-        <span class="octicon octicon-x js-menu-close" role="button" aria-label="Close"></span>
-      </div>
-
-      <div class="select-menu-filters">
-        <div class="select-menu-text-filter">
-          <input type="text" aria-label="Filter branches/tags" id="context-commitish-filter-field" class="js-filterable-field js-navigation-enable" placeholder="Filter branches/tags">
-        </div>
-        <div class="select-menu-tabs">
-          <ul>
-            <li class="select-menu-tab">
-              <a href="#" data-tab-filter="branches" data-filter-placeholder="Filter branches/tags" class="js-select-menu-tab">Branches</a>
-            </li>
-            <li class="select-menu-tab">
-              <a href="#" data-tab-filter="tags" data-filter-placeholder="Find a tag" class="js-select-menu-tab">Tags</a>
-            </li>
-          </ul>
-        </div>
-      </div>
-
-      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="branches">
-
-        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">
-
-
-            <a class="select-menu-item js-navigation-item js-navigation-open "
-               href="/Azure/azure-sdk-for-java/blob/dev/LICENSE.txt"
-               data-name="dev"
-               data-skip-pjax="true"
-               rel="nofollow">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <span class="select-menu-item-text css-truncate-target" title="dev">
-                dev
-              </span>
-            </a>
-            <a class="select-menu-item js-navigation-item js-navigation-open selected"
-               href="/Azure/azure-sdk-for-java/blob/master/LICENSE.txt"
-               data-name="master"
-               data-skip-pjax="true"
-               rel="nofollow">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <span class="select-menu-item-text css-truncate-target" title="master">
-                master
-              </span>
-            </a>
-        </div>
-
-          <div class="select-menu-no-results">Nothing to show</div>
-      </div>
-
-      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="tags">
-        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">
-
-
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.7.0/LICENSE.txt"
-                 data-name="v0.7.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.7.0">v0.7.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.6.0/LICENSE.txt"
-                 data-name="v0.6.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.6.0">v0.6.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.5.0/LICENSE.txt"
-                 data-name="v0.5.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.5.0">v0.5.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.6/LICENSE.txt"
-                 data-name="v0.4.6"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.6">v0.4.6</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.5/LICENSE.txt"
-                 data-name="v0.4.5"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.5">v0.4.5</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.4/LICENSE.txt"
-                 data-name="v0.4.4"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.4">v0.4.4</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.3/LICENSE.txt"
-                 data-name="v0.4.3"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.3">v0.4.3</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.2/LICENSE.txt"
-                 data-name="v0.4.2"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.2">v0.4.2</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.1/LICENSE.txt"
-                 data-name="v0.4.1"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.1">v0.4.1</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.4.0/LICENSE.txt"
-                 data-name="v0.4.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.4.0">v0.4.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.3.3/LICENSE.txt"
-                 data-name="v0.3.3"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.3.3">v0.3.3</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.3.2/LICENSE.txt"
-                 data-name="v0.3.2"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.3.2">v0.3.2</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.3.1/LICENSE.txt"
-                 data-name="v0.3.1"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.3.1">v0.3.1</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.3.0/LICENSE.txt"
-                 data-name="v0.3.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.3.0">v0.3.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.2.2/LICENSE.txt"
-                 data-name="v0.2.2"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.2.2">v0.2.2</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.2.1/LICENSE.txt"
-                 data-name="v0.2.1"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.2.1">v0.2.1</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.2.0/LICENSE.txt"
-                 data-name="v0.2.0"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.2.0">v0.2.0</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.1.3/LICENSE.txt"
-                 data-name="v0.1.3"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.1.3">v0.1.3</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.1.2/LICENSE.txt"
-                 data-name="v0.1.2"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.1.2">v0.1.2</a>
-            </div>
-            <div class="select-menu-item js-navigation-item ">
-              <span class="select-menu-item-icon octicon octicon-check"></span>
-              <a href="/Azure/azure-sdk-for-java/tree/v0.1.0-December2011-CTP/LICENSE.txt"
-                 data-name="v0.1.0-December2011-CTP"
-                 data-skip-pjax="true"
-                 rel="nofollow"
-                 class="js-navigation-open select-menu-item-text css-truncate-target"
-                 title="v0.1.0-December2011-CTP">v0.1.0-December2011-CTP</a>
-            </div>
-        </div>
-
-        <div class="select-menu-no-results">Nothing to show</div>
-      </div>
-
-    </div>
-  </div>
-</div>
-
-  <div class="btn-group right">
-    <a href="/Azure/azure-sdk-for-java/find/master"
-          class="js-show-file-finder btn btn-sm empty-icon tooltipped tooltipped-s"
-          data-pjax
-          data-hotkey="t"
-          aria-label="Quickly jump between files">
-      <span class="octicon octicon-list-unordered"></span>
-    </a>
-    <button aria-label="Copy file path to clipboard" class="js-zeroclipboard btn btn-sm zeroclipboard-button tooltipped tooltipped-s" data-copied-hint="Copied!" type="button"><span class="octicon octicon-clippy"></span></button>
-  </div>
-
-  <div class="breadcrumb js-zeroclipboard-target">
-    <span class="repo-root js-repo-root"><span itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="/Azure/azure-sdk-for-java" class="" data-branch="master" data-pjax="true" itemscope="url"><span itemprop="title">azure-sdk-for-java</span></a></span></span><span class="separator">/</span><strong class="final-path">LICENSE.txt</strong>
-  </div>
-</div>
-
-
-  <div class="commit file-history-tease">
-    <div class="file-history-tease-header">
-        <img alt="" class="avatar" height="24" src="https://0.gravatar.com/avatar/0fcca41cd4fe378f0584ca1476fe1fa5?d=https%3A%2F%2Fassets-cdn.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&amp;r=x&amp;s=140" width="24" />
-        <span class="author"><span>Renaud Paquay</span></span>
-        <time datetime="2011-12-10T03:41:06Z" is="relative-time">Dec 9, 2011</time>
-        <div class="commit-title">
-            <a href="/Azure/azure-sdk-for-java/commit/98b4695bf5868b546f57e30eec93ee45ac7a0009" class="message" data-pjax="true" title="Initial commit, v0.1.0 December 2011 CTP">Initial commit, v0.1.0 December 2011 CTP</a>
-        </div>
-    </div>
-
-    <div class="participation">
-      <p class="quickstat">
-        <a href="#blob_contributors_box" rel="facebox">
-          <strong>0</strong>
-           contributors
-        </a>
-      </p>
-      
-    </div>
-    <div id="blob_contributors_box" style="display:none">
-      <h2 class="facebox-header">Users who have contributed to this file</h2>
-      <ul class="facebox-user-list">
-      </ul>
-    </div>
-  </div>
-
-<div class="file">
-  <div class="file-header">
-    <div class="file-actions">
-
-      <div class="btn-group">
-        <a href="/Azure/azure-sdk-for-java/raw/master/LICENSE.txt" class="btn btn-sm " id="raw-url">Raw</a>
-          <a href="/Azure/azure-sdk-for-java/blame/master/LICENSE.txt" class="btn btn-sm js-update-url-with-hash">Blame</a>
-        <a href="/Azure/azure-sdk-for-java/commits/master/LICENSE.txt" class="btn btn-sm " rel="nofollow">History</a>
-      </div>
-
-
-          <button type="button" class="octicon-btn disabled tooltipped tooltipped-n" aria-label="You must be signed in to make or propose changes">
-            <span class="octicon octicon-pencil"></span>
-          </button>
-
-        <button type="button" class="octicon-btn octicon-btn-danger disabled tooltipped tooltipped-n" aria-label="You must be signed in to make or propose changes">
-          <span class="octicon octicon-trashcan"></span>
-        </button>
-    </div>
-
-    <div class="file-info">
-        203 lines (169 sloc)
-        <span class="file-info-divider"></span>
-      11.358 kB
-    </div>
-  </div>
-  
-  <div class="blob-wrapper data type-text">
-      <table class="highlight tab-size js-file-line-container" data-tab-size="8">
-      <tr>
-        <td id="L1" class="blob-num js-line-number" data-line-number="1"></td>
-        <td id="LC1" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L2" class="blob-num js-line-number" data-line-number="2"></td>
-        <td id="LC2" class="blob-code blob-code-inner js-file-line">                                 Apache License</td>
-      </tr>
-      <tr>
-        <td id="L3" class="blob-num js-line-number" data-line-number="3"></td>
-        <td id="LC3" class="blob-code blob-code-inner js-file-line">                           Version 2.0, January 2004</td>
-      </tr>
-      <tr>
-        <td id="L4" class="blob-num js-line-number" data-line-number="4"></td>
-        <td id="LC4" class="blob-code blob-code-inner js-file-line">                        http://www.apache.org/licenses/</td>
-      </tr>
-      <tr>
-        <td id="L5" class="blob-num js-line-number" data-line-number="5"></td>
-        <td id="LC5" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L6" class="blob-num js-line-number" data-line-number="6"></td>
-        <td id="LC6" class="blob-code blob-code-inner js-file-line">   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</td>
-      </tr>
-      <tr>
-        <td id="L7" class="blob-num js-line-number" data-line-number="7"></td>
-        <td id="LC7" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L8" class="blob-num js-line-number" data-line-number="8"></td>
-        <td id="LC8" class="blob-code blob-code-inner js-file-line">   1. Definitions.</td>
-      </tr>
-      <tr>
-        <td id="L9" class="blob-num js-line-number" data-line-number="9"></td>
-        <td id="LC9" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L10" class="blob-num js-line-number" data-line-number="10"></td>
-        <td id="LC10" class="blob-code blob-code-inner js-file-line">      &quot;License&quot; shall mean the terms and conditions for use, reproduction,</td>
-      </tr>
-      <tr>
-        <td id="L11" class="blob-num js-line-number" data-line-number="11"></td>
-        <td id="LC11" class="blob-code blob-code-inner js-file-line">      and distribution as defined by Sections 1 through 9 of this document.</td>
-      </tr>
-      <tr>
-        <td id="L12" class="blob-num js-line-number" data-line-number="12"></td>
-        <td id="LC12" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L13" class="blob-num js-line-number" data-line-number="13"></td>
-        <td id="LC13" class="blob-code blob-code-inner js-file-line">      &quot;Licensor&quot; shall mean the copyright owner or entity authorized by</td>
-      </tr>
-      <tr>
-        <td id="L14" class="blob-num js-line-number" data-line-number="14"></td>
-        <td id="LC14" class="blob-code blob-code-inner js-file-line">      the copyright owner that is granting the License.</td>
-      </tr>
-      <tr>
-        <td id="L15" class="blob-num js-line-number" data-line-number="15"></td>
-        <td id="LC15" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L16" class="blob-num js-line-number" data-line-number="16"></td>
-        <td id="LC16" class="blob-code blob-code-inner js-file-line">      &quot;Legal Entity&quot; shall mean the union of the acting entity and all</td>
-      </tr>
-      <tr>
-        <td id="L17" class="blob-num js-line-number" data-line-number="17"></td>
-        <td id="LC17" class="blob-code blob-code-inner js-file-line">      other entities that control, are controlled by, or are under common</td>
-      </tr>
-      <tr>
-        <td id="L18" class="blob-num js-line-number" data-line-number="18"></td>
-        <td id="LC18" class="blob-code blob-code-inner js-file-line">      control with that entity. For the purposes of this definition,</td>
-      </tr>
-      <tr>
-        <td id="L19" class="blob-num js-line-number" data-line-number="19"></td>
-        <td id="LC19" class="blob-code blob-code-inner js-file-line">      &quot;control&quot; means (i) the power, direct or indirect, to cause the</td>
-      </tr>
-      <tr>
-        <td id="L20" class="blob-num js-line-number" data-line-number="20"></td>
-        <td id="LC20" class="blob-code blob-code-inner js-file-line">      direction or management of such entity, whether by contract or</td>
-      </tr>
-      <tr>
-        <td id="L21" class="blob-num js-line-number" data-line-number="21"></td>
-        <td id="LC21" class="blob-code blob-code-inner js-file-line">      otherwise, or (ii) ownership of fifty percent (50%) or more of the</td>
-      </tr>
-      <tr>
-        <td id="L22" class="blob-num js-line-number" data-line-number="22"></td>
-        <td id="LC22" class="blob-code blob-code-inner js-file-line">      outstanding shares, or (iii) beneficial ownership of such entity.</td>
-      </tr>
-      <tr>
-        <td id="L23" class="blob-num js-line-number" data-line-number="23"></td>
-        <td id="LC23" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L24" class="blob-num js-line-number" data-line-number="24"></td>
-        <td id="LC24" class="blob-code blob-code-inner js-file-line">      &quot;You&quot; (or &quot;Your&quot;) shall mean an individual or Legal Entity</td>
-      </tr>
-      <tr>
-        <td id="L25" class="blob-num js-line-number" data-line-number="25"></td>
-        <td id="LC25" class="blob-code blob-code-inner js-file-line">      exercising permissions granted by this License.</td>
-      </tr>
-      <tr>
-        <td id="L26" class="blob-num js-line-number" data-line-number="26"></td>
-        <td id="LC26" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L27" class="blob-num js-line-number" data-line-number="27"></td>
-        <td id="LC27" class="blob-code blob-code-inner js-file-line">      &quot;Source&quot; form shall mean the preferred form for making modifications,</td>
-      </tr>
-      <tr>
-        <td id="L28" class="blob-num js-line-number" data-line-number="28"></td>
-        <td id="LC28" class="blob-code blob-code-inner js-file-line">      including but not limited to software source code, documentation</td>
-      </tr>
-      <tr>
-        <td id="L29" class="blob-num js-line-number" data-line-number="29"></td>
-        <td id="LC29" class="blob-code blob-code-inner js-file-line">      source, and configuration files.</td>
-      </tr>
-      <tr>
-        <td id="L30" class="blob-num js-line-number" data-line-number="30"></td>
-        <td id="LC30" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L31" class="blob-num js-line-number" data-line-number="31"></td>
-        <td id="LC31" class="blob-code blob-code-inner js-file-line">      &quot;Object&quot; form shall mean any form resulting from mechanical</td>
-      </tr>
-      <tr>
-        <td id="L32" class="blob-num js-line-number" data-line-number="32"></td>
-        <td id="LC32" class="blob-code blob-code-inner js-file-line">      transformation or translation of a Source form, including but</td>
-      </tr>
-      <tr>
-        <td id="L33" class="blob-num js-line-number" data-line-number="33"></td>
-        <td id="LC33" class="blob-code blob-code-inner js-file-line">      not limited to compiled object code, generated documentation,</td>
-      </tr>
-      <tr>
-        <td id="L34" class="blob-num js-line-number" data-line-number="34"></td>
-        <td id="LC34" class="blob-code blob-code-inner js-file-line">      and conversions to other media types.</td>
-      </tr>
-      <tr>
-        <td id="L35" class="blob-num js-line-number" data-line-number="35"></td>
-        <td id="LC35" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L36" class="blob-num js-line-number" data-line-number="36"></td>
-        <td id="LC36" class="blob-code blob-code-inner js-file-line">      &quot;Work&quot; shall mean the work of authorship, whether in Source or</td>
-      </tr>
-      <tr>
-        <td id="L37" class="blob-num js-line-number" data-line-number="37"></td>
-        <td id="LC37" class="blob-code blob-code-inner js-file-line">      Object form, made available under the License, as indicated by a</td>
-      </tr>
-      <tr>
-        <td id="L38" class="blob-num js-line-number" data-line-number="38"></td>
-        <td id="LC38" class="blob-code blob-code-inner js-file-line">      copyright notice that is included in or attached to the work</td>
-      </tr>
-      <tr>
-        <td id="L39" class="blob-num js-line-number" data-line-number="39"></td>
-        <td id="LC39" class="blob-code blob-code-inner js-file-line">      (an example is provided in the Appendix below).</td>
-      </tr>
-      <tr>
-        <td id="L40" class="blob-num js-line-number" data-line-number="40"></td>
-        <td id="LC40" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L41" class="blob-num js-line-number" data-line-number="41"></td>
-        <td id="LC41" class="blob-code blob-code-inner js-file-line">      &quot;Derivative Works&quot; shall mean any work, whether in Source or Object</td>
-      </tr>
-      <tr>
-        <td id="L42" class="blob-num js-line-number" data-line-number="42"></td>
-        <td id="LC42" class="blob-code blob-code-inner js-file-line">      form, that is based on (or derived from) the Work and for which the</td>
-      </tr>
-      <tr>
-        <td id="L43" class="blob-num js-line-number" data-line-number="43"></td>
-        <td id="LC43" class="blob-code blob-code-inner js-file-line">      editorial revisions, annotations, elaborations, or other modifications</td>
-      </tr>
-      <tr>
-        <td id="L44" class="blob-num js-line-number" data-line-number="44"></td>
-        <td id="LC44" class="blob-code blob-code-inner js-file-line">      represent, as a whole, an original work of authorship. For the purposes</td>
-      </tr>
-      <tr>
-        <td id="L45" class="blob-num js-line-number" data-line-number="45"></td>
-        <td id="LC45" class="blob-code blob-code-inner js-file-line">      of this License, Derivative Works shall not include works that remain</td>
-      </tr>
-      <tr>
-        <td id="L46" class="blob-num js-line-number" data-line-number="46"></td>
-        <td id="LC46" class="blob-code blob-code-inner js-file-line">      separable from, or merely link (or bind by name) to the interfaces of,</td>
-      </tr>
-      <tr>
-        <td id="L47" class="blob-num js-line-number" data-line-number="47"></td>
-        <td id="LC47" class="blob-code blob-code-inner js-file-line">      the Work and Derivative Works thereof.</td>
-      </tr>
-      <tr>
-        <td id="L48" class="blob-num js-line-number" data-line-number="48"></td>
-        <td id="LC48" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L49" class="blob-num js-line-number" data-line-number="49"></td>
-        <td id="LC49" class="blob-code blob-code-inner js-file-line">      &quot;Contribution&quot; shall mean any work of authorship, including</td>
-      </tr>
-      <tr>
-        <td id="L50" class="blob-num js-line-number" data-line-number="50"></td>
-        <td id="LC50" class="blob-code blob-code-inner js-file-line">      the original version of the Work and any modifications or additions</td>
-      </tr>
-      <tr>
-        <td id="L51" class="blob-num js-line-number" data-line-number="51"></td>
-        <td id="LC51" class="blob-code blob-code-inner js-file-line">      to that Work or Derivative Works thereof, that is intentionally</td>
-      </tr>
-      <tr>
-        <td id="L52" class="blob-num js-line-number" data-line-number="52"></td>
-        <td id="LC52" class="blob-code blob-code-inner js-file-line">      submitted to Licensor for inclusion in the Work by the copyright owner</td>
-      </tr>
-      <tr>
-        <td id="L53" class="blob-num js-line-number" data-line-number="53"></td>
-        <td id="LC53" class="blob-code blob-code-inner js-file-line">      or by an individual or Legal Entity authorized to submit on behalf of</td>
-      </tr>
-      <tr>
-        <td id="L54" class="blob-num js-line-number" data-line-number="54"></td>
-        <td id="LC54" class="blob-code blob-code-inner js-file-line">      the copyright owner. For the purposes of this definition, &quot;submitted&quot;</td>
-      </tr>
-      <tr>
-        <td id="L55" class="blob-num js-line-number" data-line-number="55"></td>
-        <td id="LC55" class="blob-code blob-code-inner js-file-line">      means any form of electronic, verbal, or written communication sent</td>
-      </tr>
-      <tr>
-        <td id="L56" class="blob-num js-line-number" data-line-number="56"></td>
-        <td id="LC56" class="blob-code blob-code-inner js-file-line">      to the Licensor or its representatives, including but not limited to</td>
-      </tr>
-      <tr>
-        <td id="L57" class="blob-num js-line-number" data-line-number="57"></td>
-        <td id="LC57" class="blob-code blob-code-inner js-file-line">      communication on electronic mailing lists, source code control systems,</td>
-      </tr>
-      <tr>
-        <td id="L58" class="blob-num js-line-number" data-line-number="58"></td>
-        <td id="LC58" class="blob-code blob-code-inner js-file-line">      and issue tracking systems that are managed by, or on behalf of, the</td>
-      </tr>
-      <tr>
-        <td id="L59" class="blob-num js-line-number" data-line-number="59"></td>
-        <td id="LC59" class="blob-code blob-code-inner js-file-line">      Licensor for the purpose of discussing and improving the Work, but</td>
-      </tr>
-      <tr>
-        <td id="L60" class="blob-num js-line-number" data-line-number="60"></td>
-        <td id="LC60" class="blob-code blob-code-inner js-file-line">      excluding communication that is conspicuously marked or otherwise</td>
-      </tr>
-      <tr>
-        <td id="L61" class="blob-num js-line-number" data-line-number="61"></td>
-        <td id="LC61" class="blob-code blob-code-inner js-file-line">      designated in writing by the copyright owner as &quot;Not a Contribution.&quot;</td>
-      </tr>
-      <tr>
-        <td id="L62" class="blob-num js-line-number" data-line-number="62"></td>
-        <td id="LC62" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L63" class="blob-num js-line-number" data-line-number="63"></td>
-        <td id="LC63" class="blob-code blob-code-inner js-file-line">      &quot;Contributor&quot; shall mean Licensor and any individual or Legal Entity</td>
-      </tr>
-      <tr>
-        <td id="L64" class="blob-num js-line-number" data-line-number="64"></td>
-        <td id="LC64" class="blob-code blob-code-inner js-file-line">      on behalf of whom a Contribution has been received by Licensor and</td>
-      </tr>
-      <tr>
-        <td id="L65" class="blob-num js-line-number" data-line-number="65"></td>
-        <td id="LC65" class="blob-code blob-code-inner js-file-line">      subsequently incorporated within the Work.</td>
-      </tr>
-      <tr>
-        <td id="L66" class="blob-num js-line-number" data-line-number="66"></td>
-        <td id="LC66" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L67" class="blob-num js-line-number" data-line-number="67"></td>
-        <td id="LC67" class="blob-code blob-code-inner js-file-line">   2. Grant of Copyright License. Subject to the terms and conditions of</td>
-      </tr>
-      <tr>
-        <td id="L68" class="blob-num js-line-number" data-line-number="68"></td>
-        <td id="LC68" class="blob-code blob-code-inner js-file-line">      this License, each Contributor hereby grants to You a perpetual,</td>
-      </tr>
-      <tr>
-        <td id="L69" class="blob-num js-line-number" data-line-number="69"></td>
-        <td id="LC69" class="blob-code blob-code-inner js-file-line">      worldwide, non-exclusive, no-charge, royalty-free, irrevocable</td>
-      </tr>
-      <tr>
-        <td id="L70" class="blob-num js-line-number" data-line-number="70"></td>
-        <td id="LC70" class="blob-code blob-code-inner js-file-line">      copyright license to reproduce, prepare Derivative Works of,</td>
-      </tr>
-      <tr>
-        <td id="L71" class="blob-num js-line-number" data-line-number="71"></td>
-        <td id="LC71" class="blob-code blob-code-inner js-file-line">      publicly display, publicly perform, sublicense, and distribute the</td>
-      </tr>
-      <tr>
-        <td id="L72" class="blob-num js-line-number" data-line-number="72"></td>
-        <td id="LC72" class="blob-code blob-code-inner js-file-line">      Work and such Derivative Works in Source or Object form.</td>
-      </tr>
-      <tr>
-        <td id="L73" class="blob-num js-line-number" data-line-number="73"></td>
-        <td id="LC73" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L74" class="blob-num js-line-number" data-line-number="74"></td>
-        <td id="LC74" class="blob-code blob-code-inner js-file-line">   3. Grant of Patent License. Subject to the terms and conditions of</td>
-      </tr>
-      <tr>
-        <td id="L75" class="blob-num js-line-number" data-line-number="75"></td>
-        <td id="LC75" class="blob-code blob-code-inner js-file-line">      this License, each Contributor hereby grants to You a perpetual,</td>
-      </tr>
-      <tr>
-        <td id="L76" class="blob-num js-line-number" data-line-number="76"></td>
-        <td id="LC76" class="blob-code blob-code-inner js-file-line">      worldwide, non-exclusive, no-charge, royalty-free, irrevocable</td>
-      </tr>
-      <tr>
-        <td id="L77" class="blob-num js-line-number" data-line-number="77"></td>
-        <td id="LC77" class="blob-code blob-code-inner js-file-line">      (except as stated in this section) patent license to make, have made,</td>
-      </tr>
-      <tr>
-        <td id="L78" class="blob-num js-line-number" data-line-number="78"></td>
-        <td id="LC78" class="blob-code blob-code-inner js-file-line">      use, offer to sell, sell, import, and otherwise transfer the Work,</td>
-      </tr>
-      <tr>
-        <td id="L79" class="blob-num js-line-number" data-line-number="79"></td>
-        <td id="LC79" class="blob-code blob-code-inner js-file-line">      where such license applies only to those patent claims licensable</td>
-      </tr>
-      <tr>
-        <td id="L80" class="blob-num js-line-number" data-line-number="80"></td>
-        <td id="LC80" class="blob-code blob-code-inner js-file-line">      by such Contributor that are necessarily infringed by their</td>
-      </tr>
-      <tr>
-        <td id="L81" class="blob-num js-line-number" data-line-number="81"></td>
-        <td id="LC81" class="blob-code blob-code-inner js-file-line">      Contribution(s) alone or by combination of their Contribution(s)</td>
-      </tr>
-      <tr>
-        <td id="L82" class="blob-num js-line-number" data-line-number="82"></td>
-        <td id="LC82" class="blob-code blob-code-inner js-file-line">      with the Work to which such Contribution(s) was submitted. If You</td>
-      </tr>
-      <tr>
-        <td id="L83" class="blob-num js-line-number" data-line-number="83"></td>
-        <td id="LC83" class="blob-code blob-code-inner js-file-line">      institute patent litigation against any entity (including a</td>
-      </tr>
-      <tr>
-        <td id="L84" class="blob-num js-line-number" data-line-number="84"></td>
-        <td id="LC84" class="blob-code blob-code-inner js-file-line">      cross-claim or counterclaim in a lawsuit) alleging that the Work</td>
-      </tr>
-      <tr>
-        <td id="L85" class="blob-num js-line-number" data-line-number="85"></td>
-        <td id="LC85" class="blob-code blob-code-inner js-file-line">      or a Contribution incorporated within the Work constitutes direct</td>
-      </tr>
-      <tr>
-        <td id="L86" class="blob-num js-line-number" data-line-number="86"></td>
-        <td id="LC86" class="blob-code blob-code-inner js-file-line">      or contributory patent infringement, then any patent licenses</td>
-      </tr>
-      <tr>
-        <td id="L87" class="blob-num js-line-number" data-line-number="87"></td>
-        <td id="LC87" class="blob-code blob-code-inner js-file-line">      granted to You under this License for that Work shall terminate</td>
-      </tr>
-      <tr>
-        <td id="L88" class="blob-num js-line-number" data-line-number="88"></td>
-        <td id="LC88" class="blob-code blob-code-inner js-file-line">      as of the date such litigation is filed.</td>
-      </tr>
-      <tr>
-        <td id="L89" class="blob-num js-line-number" data-line-number="89"></td>
-        <td id="LC89" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L90" class="blob-num js-line-number" data-line-number="90"></td>
-        <td id="LC90" class="blob-code blob-code-inner js-file-line">   4. Redistribution. You may reproduce and distribute copies of the</td>
-      </tr>
-      <tr>
-        <td id="L91" class="blob-num js-line-number" data-line-number="91"></td>
-        <td id="LC91" class="blob-code blob-code-inner js-file-line">      Work or Derivative Works thereof in any medium, with or without</td>
-      </tr>
-      <tr>
-        <td id="L92" class="blob-num js-line-number" data-line-number="92"></td>
-        <td id="LC92" class="blob-code blob-code-inner js-file-line">      modifications, and in Source or Object form, provided that You</td>
-      </tr>
-      <tr>
-        <td id="L93" class="blob-num js-line-number" data-line-number="93"></td>
-        <td id="LC93" class="blob-code blob-code-inner js-file-line">      meet the following conditions:</td>
-      </tr>
-      <tr>
-        <td id="L94" class="blob-num js-line-number" data-line-number="94"></td>
-        <td id="LC94" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L95" class="blob-num js-line-number" data-line-number="95"></td>
-        <td id="LC95" class="blob-code blob-code-inner js-file-line">      (a) You must give any other recipients of the Work or</td>
-      </tr>
-      <tr>
-        <td id="L96" class="blob-num js-line-number" data-line-number="96"></td>
-        <td id="LC96" class="blob-code blob-code-inner js-file-line">          Derivative Works a copy of this License; and</td>
-      </tr>
-      <tr>
-        <td id="L97" class="blob-num js-line-number" data-line-number="97"></td>
-        <td id="LC97" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L98" class="blob-num js-line-number" data-line-number="98"></td>
-        <td id="LC98" class="blob-code blob-code-inner js-file-line">      (b) You must cause any modified files to carry prominent notices</td>
-      </tr>
-      <tr>
-        <td id="L99" class="blob-num js-line-number" data-line-number="99"></td>
-        <td id="LC99" class="blob-code blob-code-inner js-file-line">          stating that You changed the files; and</td>
-      </tr>
-      <tr>
-        <td id="L100" class="blob-num js-line-number" data-line-number="100"></td>
-        <td id="LC100" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L101" class="blob-num js-line-number" data-line-number="101"></td>
-        <td id="LC101" class="blob-code blob-code-inner js-file-line">      (c) You must retain, in the Source form of any Derivative Works</td>
-      </tr>
-      <tr>
-        <td id="L102" class="blob-num js-line-number" data-line-number="102"></td>
-        <td id="LC102" class="blob-code blob-code-inner js-file-line">          that You distribute, all copyright, patent, trademark, and</td>
-      </tr>
-      <tr>
-        <td id="L103" class="blob-num js-line-number" data-line-number="103"></td>
-        <td id="LC103" class="blob-code blob-code-inner js-file-line">          attribution notices from the Source form of the Work,</td>
-      </tr>
-      <tr>
-        <td id="L104" class="blob-num js-line-number" data-line-number="104"></td>
-        <td id="LC104" class="blob-code blob-code-inner js-file-line">          excluding those notices that do not pertain to any part of</td>
-      </tr>
-      <tr>
-        <td id="L105" class="blob-num js-line-number" data-line-number="105"></td>
-        <td id="LC105" class="blob-code blob-code-inner js-file-line">          the Derivative Works; and</td>
-      </tr>
-      <tr>
-        <td id="L106" class="blob-num js-line-number" data-line-number="106"></td>
-        <td id="LC106" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L107" class="blob-num js-line-number" data-line-number="107"></td>
-        <td id="LC107" class="blob-code blob-code-inner js-file-line">      (d) If the Work includes a &quot;NOTICE&quot; text file as part of its</td>
-      </tr>
-      <tr>
-        <td id="L108" class="blob-num js-line-number" data-line-number="108"></td>
-        <td id="LC108" class="blob-code blob-code-inner js-file-line">          distribution, then any Derivative Works that You distribute must</td>
-      </tr>
-      <tr>
-        <td id="L109" class="blob-num js-line-number" data-line-number="109"></td>
-        <td id="LC109" class="blob-code blob-code-inner js-file-line">          include a readable copy of the attribution notices contained</td>
-      </tr>
-      <tr>
-        <td id="L110" class="blob-num js-line-number" data-line-number="110"></td>
-        <td id="LC110" class="blob-code blob-code-inner js-file-line">          within such NOTICE file, excluding those notices that do not</td>
-      </tr>
-      <tr>
-        <td id="L111" class="blob-num js-line-number" data-line-number="111"></td>
-        <td id="LC111" class="blob-code blob-code-inner js-file-line">          pertain to any part of the Derivative Works, in at least one</td>
-      </tr>
-      <tr>
-        <td id="L112" class="blob-num js-line-number" data-line-number="112"></td>
-        <td id="LC112" class="blob-code blob-code-inner js-file-line">          of the following places: within a NOTICE text file distributed</td>
-      </tr>
-      <tr>
-        <td id="L113" class="blob-num js-line-number" data-line-number="113"></td>
-        <td id="LC113" class="blob-code blob-code-inner js-file-line">          as part of the Derivative Works; within the Source form or</td>
-      </tr>
-      <tr>
-        <td id="L114" class="blob-num js-line-number" data-line-number="114"></td>
-        <td id="LC114" class="blob-code blob-code-inner js-file-line">          documentation, if provided along with the Derivative Works; or,</td>
-      </tr>
-      <tr>
-        <td id="L115" class="blob-num js-line-number" data-line-number="115"></td>
-        <td id="LC115" class="blob-code blob-code-inner js-file-line">          within a display generated by the Derivative Works, if and</td>
-      </tr>
-      <tr>
-        <td id="L116" class="blob-num js-line-number" data-line-number="116"></td>
-        <td id="LC116" class="blob-code blob-code-inner js-file-line">          wherever such third-party notices normally appear. The contents</td>
-      </tr>
-      <tr>
-        <td id="L117" class="blob-num js-line-number" data-line-number="117"></td>
-        <td id="LC117" class="blob-code blob-code-inner js-file-line">          of the NOTICE file are for informational purposes only and</td>
-      </tr>
-      <tr>
-        <td id="L118" class="blob-num js-line-number" data-line-number="118"></td>
-        <td id="LC118" class="blob-code blob-code-inner js-file-line">          do not modify the License. You may add Your own attribution</td>
-      </tr>
-      <tr>
-        <td id="L119" class="blob-num js-line-number" data-line-number="119"></td>
-        <td id="LC119" class="blob-code blob-code-inner js-file-line">          notices within Derivative Works that You distribute, alongside</td>
-      </tr>
-      <tr>
-        <td id="L120" class="blob-num js-line-number" data-line-number="120"></td>
-        <td id="LC120" class="blob-code blob-code-inner js-file-line">          or as an addendum to the NOTICE text from the Work, provided</td>
-      </tr>
-      <tr>
-        <td id="L121" class="blob-num js-line-number" data-line-number="121"></td>
-        <td id="LC121" class="blob-code blob-code-inner js-file-line">          that such additional attribution notices cannot be construed</td>
-      </tr>
-      <tr>
-        <td id="L122" class="blob-num js-line-number" data-line-number="122"></td>
-        <td id="LC122" class="blob-code blob-code-inner js-file-line">          as modifying the License.</td>
-      </tr>
-      <tr>
-        <td id="L123" class="blob-num js-line-number" data-line-number="123"></td>
-        <td id="LC123" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L124" class="blob-num js-line-number" data-line-number="124"></td>
-        <td id="LC124" class="blob-code blob-code-inner js-file-line">      You may add Your own copyright statement to Your modifications and</td>
-      </tr>
-      <tr>
-        <td id="L125" class="blob-num js-line-number" data-line-number="125"></td>
-        <td id="LC125" class="blob-code blob-code-inner js-file-line">      may provide additional or different license terms and conditions</td>
-      </tr>
-      <tr>
-        <td id="L126" class="blob-num js-line-number" data-line-number="126"></td>
-        <td id="LC126" class="blob-code blob-code-inner js-file-line">      for use, reproduction, or distribution of Your modifications, or</td>
-      </tr>
-      <tr>
-        <td id="L127" class="blob-num js-line-number" data-line-number="127"></td>
-        <td id="LC127" class="blob-code blob-code-inner js-file-line">      for any such Derivative Works as a whole, provided Your use,</td>
-      </tr>
-      <tr>
-        <td id="L128" class="blob-num js-line-number" data-line-number="128"></td>
-        <td id="LC128" class="blob-code blob-code-inner js-file-line">      reproduction, and distribution of the Work otherwise complies with</td>
-      </tr>
-      <tr>
-        <td id="L129" class="blob-num js-line-number" data-line-number="129"></td>
-        <td id="LC129" class="blob-code blob-code-inner js-file-line">      the conditions stated in this License.</td>
-      </tr>
-      <tr>
-        <td id="L130" class="blob-num js-line-number" data-line-number="130"></td>
-        <td id="LC130" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L131" class="blob-num js-line-number" data-line-number="131"></td>
-        <td id="LC131" class="blob-code blob-code-inner js-file-line">   5. Submission of Contributions. Unless You explicitly state otherwise,</td>
-      </tr>
-      <tr>
-        <td id="L132" class="blob-num js-line-number" data-line-number="132"></td>
-        <td id="LC132" class="blob-code blob-code-inner js-file-line">      any Contribution intentionally submitted for inclusion in the Work</td>
-      </tr>
-      <tr>
-        <td id="L133" class="blob-num js-line-number" data-line-number="133"></td>
-        <td id="LC133" class="blob-code blob-code-inner js-file-line">      by You to the Licensor shall be under the terms and conditions of</td>
-      </tr>
-      <tr>
-        <td id="L134" class="blob-num js-line-number" data-line-number="134"></td>
-        <td id="LC134" class="blob-code blob-code-inner js-file-line">      this License, without any additional terms or conditions.</td>
-      </tr>
-      <tr>
-        <td id="L135" class="blob-num js-line-number" data-line-number="135"></td>
-        <td id="LC135" class="blob-code blob-code-inner js-file-line">      Notwithstanding the above, nothing herein shall supersede or modify</td>
-      </tr>
-      <tr>
-        <td id="L136" class="blob-num js-line-number" data-line-number="136"></td>
-        <td id="LC136" class="blob-code blob-code-inner js-file-line">      the terms of any separate license agreement you may have executed</td>
-      </tr>
-      <tr>
-        <td id="L137" class="blob-num js-line-number" data-line-number="137"></td>
-        <td id="LC137" class="blob-code blob-code-inner js-file-line">      with Licensor regarding such Contributions.</td>
-      </tr>
-      <tr>
-        <td id="L138" class="blob-num js-line-number" data-line-number="138"></td>
-        <td id="LC138" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L139" class="blob-num js-line-number" data-line-number="139"></td>
-        <td id="LC139" class="blob-code blob-code-inner js-file-line">   6. Trademarks. This License does not grant permission to use the trade</td>
-      </tr>
-      <tr>
-        <td id="L140" class="blob-num js-line-number" data-line-number="140"></td>
-        <td id="LC140" class="blob-code blob-code-inner js-file-line">      names, trademarks, service marks, or product names of the Licensor,</td>
-      </tr>
-      <tr>
-        <td id="L141" class="blob-num js-line-number" data-line-number="141"></td>
-        <td id="LC141" class="blob-code blob-code-inner js-file-line">      except as required for reasonable and customary use in describing the</td>
-      </tr>
-      <tr>
-        <td id="L142" class="blob-num js-line-number" data-line-number="142"></td>
-        <td id="LC142" class="blob-code blob-code-inner js-file-line">      origin of the Work and reproducing the content of the NOTICE file.</td>
-      </tr>
-      <tr>
-        <td id="L143" class="blob-num js-line-number" data-line-number="143"></td>
-        <td id="LC143" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L144" class="blob-num js-line-number" data-line-number="144"></td>
-        <td id="LC144" class="blob-code blob-code-inner js-file-line">   7. Disclaimer of Warranty. Unless required by applicable law or</td>
-      </tr>
-      <tr>
-        <td id="L145" class="blob-num js-line-number" data-line-number="145"></td>
-        <td id="LC145" class="blob-code blob-code-inner js-file-line">      agreed to in writing, Licensor provides the Work (and each</td>
-      </tr>
-      <tr>
-        <td id="L146" class="blob-num js-line-number" data-line-number="146"></td>
-        <td id="LC146" class="blob-code blob-code-inner js-file-line">      Contributor provides its Contributions) on an &quot;AS IS&quot; BASIS,</td>
-      </tr>
-      <tr>
-        <td id="L147" class="blob-num js-line-number" data-line-number="147"></td>
-        <td id="LC147" class="blob-code blob-code-inner js-file-line">      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or</td>
-      </tr>
-      <tr>
-        <td id="L148" class="blob-num js-line-number" data-line-number="148"></td>
-        <td id="LC148" class="blob-code blob-code-inner js-file-line">      implied, including, without limitation, any warranties or conditions</td>
-      </tr>
-      <tr>
-        <td id="L149" class="blob-num js-line-number" data-line-number="149"></td>
-        <td id="LC149" class="blob-code blob-code-inner js-file-line">      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A</td>
-      </tr>
-      <tr>
-        <td id="L150" class="blob-num js-line-number" data-line-number="150"></td>
-        <td id="LC150" class="blob-code blob-code-inner js-file-line">      PARTICULAR PURPOSE. You are solely responsible for determining the</td>
-      </tr>
-      <tr>
-        <td id="L151" class="blob-num js-line-number" data-line-number="151"></td>
-        <td id="LC151" class="blob-code blob-code-inner js-file-line">      appropriateness of using or redistributing the Work and assume any</td>
-      </tr>
-      <tr>
-        <td id="L152" class="blob-num js-line-number" data-line-number="152"></td>
-        <td id="LC152" class="blob-code blob-code-inner js-file-line">      risks associated with Your exercise of permissions under this License.</td>
-      </tr>
-      <tr>
-        <td id="L153" class="blob-num js-line-number" data-line-number="153"></td>
-        <td id="LC153" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L154" class="blob-num js-line-number" data-line-number="154"></td>
-        <td id="LC154" class="blob-code blob-code-inner js-file-line">   8. Limitation of Liability. In no event and under no legal theory,</td>
-      </tr>
-      <tr>
-        <td id="L155" class="blob-num js-line-number" data-line-number="155"></td>
-        <td id="LC155" class="blob-code blob-code-inner js-file-line">      whether in tort (including negligence), contract, or otherwise,</td>
-      </tr>
-      <tr>
-        <td id="L156" class="blob-num js-line-number" data-line-number="156"></td>
-        <td id="LC156" class="blob-code blob-code-inner js-file-line">      unless required by applicable law (such as deliberate and grossly</td>
-      </tr>
-      <tr>
-        <td id="L157" class="blob-num js-line-number" data-line-number="157"></td>
-        <td id="LC157" class="blob-code blob-code-inner js-file-line">      negligent acts) or agreed to in writing, shall any Contributor be</td>
-      </tr>
-      <tr>
-        <td id="L158" class="blob-num js-line-number" data-line-number="158"></td>
-        <td id="LC158" class="blob-code blob-code-inner js-file-line">      liable to You for damages, including any direct, indirect, special,</td>
-      </tr>
-      <tr>
-        <td id="L159" class="blob-num js-line-number" data-line-number="159"></td>
-        <td id="LC159" class="blob-code blob-code-inner js-file-line">      incidental, or consequential damages of any character arising as a</td>
-      </tr>
-      <tr>
-        <td id="L160" class="blob-num js-line-number" data-line-number="160"></td>
-        <td id="LC160" class="blob-code blob-code-inner js-file-line">      result of this License or out of the use or inability to use the</td>
-      </tr>
-      <tr>
-        <td id="L161" class="blob-num js-line-number" data-line-number="161"></td>
-        <td id="LC161" class="blob-code blob-code-inner js-file-line">      Work (including but not limited to damages for loss of goodwill,</td>
-      </tr>
-      <tr>
-        <td id="L162" class="blob-num js-line-number" data-line-number="162"></td>
-        <td id="LC162" class="blob-code blob-code-inner js-file-line">      work stoppage, computer failure or malfunction, or any and all</td>
-      </tr>
-      <tr>
-        <td id="L163" class="blob-num js-line-number" data-line-number="163"></td>
-        <td id="LC163" class="blob-code blob-code-inner js-file-line">      other commercial damages or losses), even if such Contributor</td>
-      </tr>
-      <tr>
-        <td id="L164" class="blob-num js-line-number" data-line-number="164"></td>
-        <td id="LC164" class="blob-code blob-code-inner js-file-line">      has been advised of the possibility of such damages.</td>
-      </tr>
-      <tr>
-        <td id="L165" class="blob-num js-line-number" data-line-number="165"></td>
-        <td id="LC165" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L166" class="blob-num js-line-number" data-line-number="166"></td>
-        <td id="LC166" class="blob-code blob-code-inner js-file-line">   9. Accepting Warranty or Additional Liability. While redistributing</td>
-      </tr>
-      <tr>
-        <td id="L167" class="blob-num js-line-number" data-line-number="167"></td>
-        <td id="LC167" class="blob-code blob-code-inner js-file-line">      the Work or Derivative Works thereof, You may choose to offer,</td>
-      </tr>
-      <tr>
-        <td id="L168" class="blob-num js-line-number" data-line-number="168"></td>
-        <td id="LC168" class="blob-code blob-code-inner js-file-line">      and charge a fee for, acceptance of support, warranty, indemnity,</td>
-      </tr>
-      <tr>
-        <td id="L169" class="blob-num js-line-number" data-line-number="169"></td>
-        <td id="LC169" class="blob-code blob-code-inner js-file-line">      or other liability obligations and/or rights consistent with this</td>
-      </tr>
-      <tr>
-        <td id="L170" class="blob-num js-line-number" data-line-number="170"></td>
-        <td id="LC170" class="blob-code blob-code-inner js-file-line">      License. However, in accepting such obligations, You may act only</td>
-      </tr>
-      <tr>
-        <td id="L171" class="blob-num js-line-number" data-line-number="171"></td>
-        <td id="LC171" class="blob-code blob-code-inner js-file-line">      on Your own behalf and on Your sole responsibility, not on behalf</td>
-      </tr>
-      <tr>
-        <td id="L172" class="blob-num js-line-number" data-line-number="172"></td>
-        <td id="LC172" class="blob-code blob-code-inner js-file-line">      of any other Contributor, and only if You agree to indemnify,</td>
-      </tr>
-      <tr>
-        <td id="L173" class="blob-num js-line-number" data-line-number="173"></td>
-        <td id="LC173" class="blob-code blob-code-inner js-file-line">      defend, and hold each Contributor harmless for any liability</td>
-      </tr>
-      <tr>
-        <td id="L174" class="blob-num js-line-number" data-line-number="174"></td>
-        <td id="LC174" class="blob-code blob-code-inner js-file-line">      incurred by, or claims asserted against, such Contributor by reason</td>
-      </tr>
-      <tr>
-        <td id="L175" class="blob-num js-line-number" data-line-number="175"></td>
-        <td id="LC175" class="blob-code blob-code-inner js-file-line">      of your accepting any such warranty or additional liability.</td>
-      </tr>
-      <tr>
-        <td id="L176" class="blob-num js-line-number" data-line-number="176"></td>
-        <td id="LC176" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L177" class="blob-num js-line-number" data-line-number="177"></td>
-        <td id="LC177" class="blob-code blob-code-inner js-file-line">   END OF TERMS AND CONDITIONS</td>
-      </tr>
-      <tr>
-        <td id="L178" class="blob-num js-line-number" data-line-number="178"></td>
-        <td id="LC178" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L179" class="blob-num js-line-number" data-line-number="179"></td>
-        <td id="LC179" class="blob-code blob-code-inner js-file-line">   APPENDIX: How to apply the Apache License to your work.</td>
-      </tr>
-      <tr>
-        <td id="L180" class="blob-num js-line-number" data-line-number="180"></td>
-        <td id="LC180" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L181" class="blob-num js-line-number" data-line-number="181"></td>
-        <td id="LC181" class="blob-code blob-code-inner js-file-line">      To apply the Apache License to your work, attach the following</td>
-      </tr>
-      <tr>
-        <td id="L182" class="blob-num js-line-number" data-line-number="182"></td>
-        <td id="LC182" class="blob-code blob-code-inner js-file-line">      boilerplate notice, with the fields enclosed by brackets &quot;[]&quot;</td>
-      </tr>
-      <tr>
-        <td id="L183" class="blob-num js-line-number" data-line-number="183"></td>
-        <td id="LC183" class="blob-code blob-code-inner js-file-line">      replaced with your own identifying information. (Don&#39;t include</td>
-      </tr>
-      <tr>
-        <td id="L184" class="blob-num js-line-number" data-line-number="184"></td>
-        <td id="LC184" class="blob-code blob-code-inner js-file-line">      the brackets!)  The text should be enclosed in the appropriate</td>
-      </tr>
-      <tr>
-        <td id="L185" class="blob-num js-line-number" data-line-number="185"></td>
-        <td id="LC185" class="blob-code blob-code-inner js-file-line">      comment syntax for the file format. We also recommend that a</td>
-      </tr>
-      <tr>
-        <td id="L186" class="blob-num js-line-number" data-line-number="186"></td>
-        <td id="LC186" class="blob-code blob-code-inner js-file-line">      file or class name and description of purpose be included on the</td>
-      </tr>
-      <tr>
-        <td id="L187" class="blob-num js-line-number" data-line-number="187"></td>
-        <td id="LC187" class="blob-code blob-code-inner js-file-line">      same &quot;printed page&quot; as the copyright notice for easier</td>
-      </tr>
-      <tr>
-        <td id="L188" class="blob-num js-line-number" data-line-number="188"></td>
-        <td id="LC188" class="blob-code blob-code-inner js-file-line">      identification within third-party archives.</td>
-      </tr>
-      <tr>
-        <td id="L189" class="blob-num js-line-number" data-line-number="189"></td>
-        <td id="LC189" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L190" class="blob-num js-line-number" data-line-number="190"></td>
-        <td id="LC190" class="blob-code blob-code-inner js-file-line">   Copyright [yyyy] [name of copyright owner]</td>
-      </tr>
-      <tr>
-        <td id="L191" class="blob-num js-line-number" data-line-number="191"></td>
-        <td id="LC191" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L192" class="blob-num js-line-number" data-line-number="192"></td>
-        <td id="LC192" class="blob-code blob-code-inner js-file-line">   Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</td>
-      </tr>
-      <tr>
-        <td id="L193" class="blob-num js-line-number" data-line-number="193"></td>
-        <td id="LC193" class="blob-code blob-code-inner js-file-line">   you may not use this file except in compliance with the License.</td>
-      </tr>
-      <tr>
-        <td id="L194" class="blob-num js-line-number" data-line-number="194"></td>
-        <td id="LC194" class="blob-code blob-code-inner js-file-line">   You may obtain a copy of the License at</td>
-      </tr>
-      <tr>
-        <td id="L195" class="blob-num js-line-number" data-line-number="195"></td>
-        <td id="LC195" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L196" class="blob-num js-line-number" data-line-number="196"></td>
-        <td id="LC196" class="blob-code blob-code-inner js-file-line">       http://www.apache.org/licenses/LICENSE-2.0</td>
-      </tr>
-      <tr>
-        <td id="L197" class="blob-num js-line-number" data-line-number="197"></td>
-        <td id="LC197" class="blob-code blob-code-inner js-file-line">
-</td>
-      </tr>
-      <tr>
-        <td id="L198" class="blob-num js-line-number" data-line-number="198"></td>
-        <td id="LC198" class="blob-code blob-code-inner js-file-line">   Unless required by applicable law or agreed to in writing, software</td>
-      </tr>
-      <tr>
-        <td id="L199" class="blob-num js-line-number" data-line-number="199"></td>
-        <td id="LC199" class="blob-code blob-code-inner js-file-line">   distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</td>
-      </tr>
-      <tr>
-        <td id="L200" class="blob-num js-line-number" data-line-number="200"></td>
-        <td id="LC200" class="blob-code blob-code-inner js-file-line">   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</td>
-      </tr>
-      <tr>
-        <td id="L201" class="blob-num js-line-number" data-line-number="201"></td>
-        <td id="LC201" class="blob-code blob-code-inner js-file-line">   See the License for the specific language governing permissions and</td>
-      </tr>
-      <tr>
-        <td id="L202" class="blob-num js-line-number" data-line-number="202"></td>
-        <td id="LC202" class="blob-code blob-code-inner js-file-line">   limitations under the License.</td>
-      </tr>
-</table>
-
-  </div>
-
-</div>
-
-<a href="#jump-to-line" rel="facebox[.linejump]" data-hotkey="l" style="display:none">Jump to Line</a>
-<div id="jump-to-line" style="display:none">
-  <form accept-charset="UTF-8" action="" class="js-jump-to-line-form" method="get"><div style="margin:0;padding:0;display:inline"><input name="utf8" type="hidden" value="&#x2713;" /></div>
-    <input class="linejump-input js-jump-to-line-field" type="text" placeholder="Jump to line&hellip;" autofocus>
-    <button type="submit" class="btn">Go</button>
-</form></div>
-
-        </div>
-
-      </div><!-- /.repo-container -->
-      <div class="modal-backdrop"></div>
-    </div><!-- /.container -->
-  </div><!-- /.site -->
-
-
-    </div><!-- /.wrapper -->
-
-      <div class="container">
-  <div class="site-footer" role="contentinfo">
-    <ul class="site-footer-links right">
-        <li><a href="https://status.github.com/" data-ga-click="Footer, go to status, text:status">Status</a></li>
-      <li><a href="https://developer.github.com" data-ga-click="Footer, go to api, text:api">API</a></li>
-      <li><a href="https://training.github.com" data-ga-click="Footer, go to training, text:training">Training</a></li>
-      <li><a href="https://shop.github.com" data-ga-click="Footer, go to shop, text:shop">Shop</a></li>
-        <li><a href="https://github.com/blog" data-ga-click="Footer, go to blog, text:blog">Blog</a></li>
-        <li><a href="https://github.com/about" data-ga-click="Footer, go to about, text:about">About</a></li>
-      <li><a href="https://help.github.com" data-ga-click="Footer, go to help, text:help">Help</a></li>
-
-    </ul>
-
-    <a href="https://github.com" aria-label="Homepage">
-      <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
-</a>
-    <ul class="site-footer-links">
-      <li>&copy; 2015 <span title="0.03628s from github-fe132-cp1-prd.iad.github.net">GitHub</span>, Inc.</li>
-        <li><a href="https://github.com/site/terms" data-ga-click="Footer, go to terms, text:terms">Terms</a></li>
-        <li><a href="https://github.com/site/privacy" data-ga-click="Footer, go to privacy, text:privacy">Privacy</a></li>
-        <li><a href="https://github.com/security" data-ga-click="Footer, go to security, text:security">Security</a></li>
-        <li><a href="https://github.com/contact" data-ga-click="Footer, go to contact, text:contact">Contact</a></li>
-    </ul>
-  </div>
-</div>
-
-
-    <div class="fullscreen-overlay js-fullscreen-overlay" id="fullscreen_overlay">
-  <div class="fullscreen-container js-suggester-container">
-    <div class="textarea-wrap">
-      <textarea name="fullscreen-contents" id="fullscreen-contents" class="fullscreen-contents js-fullscreen-contents" placeholder=""></textarea>
-      <div class="suggester-container">
-        <div class="suggester fullscreen-suggester js-suggester js-navigation-container"></div>
-      </div>
-    </div>
-  </div>
-  <div class="fullscreen-sidebar">
-    <a href="#" class="exit-fullscreen js-exit-fullscreen tooltipped tooltipped-w" aria-label="Exit Zen Mode">
-      <span class="mega-octicon octicon-screen-normal"></span>
-    </a>
-    <a href="#" class="theme-switcher js-theme-switcher tooltipped tooltipped-w"
-      aria-label="Switch themes">
-      <span class="octicon octicon-color-mode"></span>
-    </a>
-  </div>
-</div>
-
-
-
-    
-    
-
-    <div id="ajax-error-message" class="flash flash-error">
-      <span class="octicon octicon-alert"></span>
-      <a href="#" class="octicon octicon-x flash-close js-ajax-error-dismiss" aria-label="Dismiss error"></a>
-      Something went wrong with that request. Please try again.
-    </div>
-
-
-      <script crossorigin="anonymous" src="https://assets-cdn.github.com/assets/frameworks-808fcfcd63c9ecba3e84453f540cb1cbafde48c6b30c1d51ebd4e67e88ff66bd.js"></script>
-      <script async="async" crossorigin="anonymous" src="https://assets-cdn.github.com/assets/github/index-cf8b63745014f69875d29516fa9721c8ae5238584f174d34aa45b92aaa11fd21.js"></script>
-      
-      
-  </body>
-</html>
-
diff --git a/plugins/cloud-azure/licenses/azure-NOTICE.txt b/plugins/cloud-azure/licenses/azure-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/azure-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/azure-core-0.7.0.jar.sha1 b/plugins/cloud-azure/licenses/azure-core-0.7.0.jar.sha1
deleted file mode 100644
index f7d0b7c..0000000
--- a/plugins/cloud-azure/licenses/azure-core-0.7.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-feed802efe8a7a83d15962d11c6780c63997c528
diff --git a/plugins/cloud-azure/licenses/azure-management-0.7.0.jar.sha1 b/plugins/cloud-azure/licenses/azure-management-0.7.0.jar.sha1
deleted file mode 100644
index f69856a..0000000
--- a/plugins/cloud-azure/licenses/azure-management-0.7.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0dfdd1c3a9bd783b087050e979f6ba34f06a68f3
diff --git a/plugins/cloud-azure/licenses/azure-management-compute-0.7.0.jar.sha1 b/plugins/cloud-azure/licenses/azure-management-compute-0.7.0.jar.sha1
deleted file mode 100644
index bcab189..0000000
--- a/plugins/cloud-azure/licenses/azure-management-compute-0.7.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b945fc3968a4e5a64bbde419c14d92a4a53fa7a1
diff --git a/plugins/cloud-azure/licenses/azure-storage-2.0.0.jar.sha1 b/plugins/cloud-azure/licenses/azure-storage-2.0.0.jar.sha1
deleted file mode 100644
index 9e72b7b..0000000
--- a/plugins/cloud-azure/licenses/azure-storage-2.0.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b970c65a38da0569013e0c76de7c404f842496c2
diff --git a/plugins/cloud-azure/licenses/commons-codec-1.6.jar.sha1 b/plugins/cloud-azure/licenses/commons-codec-1.6.jar.sha1
deleted file mode 100644
index bf78aff..0000000
--- a/plugins/cloud-azure/licenses/commons-codec-1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/cloud-azure/licenses/commons-codec-LICENSE.txt b/plugins/cloud-azure/licenses/commons-codec-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/commons-codec-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/commons-codec-NOTICE.txt b/plugins/cloud-azure/licenses/commons-codec-NOTICE.txt
deleted file mode 100644
index 5691644..0000000
--- a/plugins/cloud-azure/licenses/commons-codec-NOTICE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Apache Commons Codec
-Copyright 2002-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
-contains test data from http://aspell.net/test/orig/batch0.tab.
-Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
-
-===============================================================================
-
-The content of package org.apache.commons.codec.language.bm has been translated
-from the original php source code available at http://stevemorse.org/phoneticinfo.htm
-with permission from the original authors.
-Original source copyright:
-Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/cloud-azure/licenses/commons-lang3-3.3.2.jar.sha1 b/plugins/cloud-azure/licenses/commons-lang3-3.3.2.jar.sha1
deleted file mode 100644
index bdd913c..0000000
--- a/plugins/cloud-azure/licenses/commons-lang3-3.3.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-90a3822c38ec8c996e84c16a3477ef632cbc87a3
diff --git a/plugins/cloud-azure/licenses/commons-lang3-LICENSE.txt b/plugins/cloud-azure/licenses/commons-lang3-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/commons-lang3-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/commons-lang3-NOTICE.txt b/plugins/cloud-azure/licenses/commons-lang3-NOTICE.txt
deleted file mode 100644
index 0782824..0000000
--- a/plugins/cloud-azure/licenses/commons-lang3-NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Apache Commons Lang
-Copyright 2001-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-This product includes software from the Spring Framework,
-under the Apache License 2.0 (see: StringUtils.containsWhitespace())
diff --git a/plugins/cloud-azure/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/cloud-azure/licenses/commons-logging-1.1.3.jar.sha1
deleted file mode 100644
index c8756c4..0000000
--- a/plugins/cloud-azure/licenses/commons-logging-1.1.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/cloud-azure/licenses/commons-logging-LICENSE.txt b/plugins/cloud-azure/licenses/commons-logging-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/commons-logging-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/commons-logging-NOTICE.txt b/plugins/cloud-azure/licenses/commons-logging-NOTICE.txt
deleted file mode 100644
index d3d6e14..0000000
--- a/plugins/cloud-azure/licenses/commons-logging-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache Commons Logging
-Copyright 2003-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-azure/licenses/httpclient-4.3.6.jar.sha1 b/plugins/cloud-azure/licenses/httpclient-4.3.6.jar.sha1
deleted file mode 100644
index 3d35ee9..0000000
--- a/plugins/cloud-azure/licenses/httpclient-4.3.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/cloud-azure/licenses/httpclient-LICENSE.txt b/plugins/cloud-azure/licenses/httpclient-LICENSE.txt
deleted file mode 100644
index 32f01ed..0000000
--- a/plugins/cloud-azure/licenses/httpclient-LICENSE.txt
+++ /dev/null
@@ -1,558 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project includes Public Suffix List copied from
-<https://publicsuffix.org/list/effective_tld_names.dat>
-licensed under the terms of the Mozilla Public License, v. 2.0
-
-Full license text: <http://mozilla.org/MPL/2.0/>
-
-Mozilla Public License Version 2.0
-==================================
-
-1. Definitions
---------------
-
-1.1. "Contributor"
-    means each individual or legal entity that creates, contributes to
-    the creation of, or owns Covered Software.
-
-1.2. "Contributor Version"
-    means the combination of the Contributions of others (if any) used
-    by a Contributor and that particular Contributor's Contribution.
-
-1.3. "Contribution"
-    means Covered Software of a particular Contributor.
-
-1.4. "Covered Software"
-    means Source Code Form to which the initial Contributor has attached
-    the notice in Exhibit A, the Executable Form of such Source Code
-    Form, and Modifications of such Source Code Form, in each case
-    including portions thereof.
-
-1.5. "Incompatible With Secondary Licenses"
-    means
-
-    (a) that the initial Contributor has attached the notice described
-        in Exhibit B to the Covered Software; or
-
-    (b) that the Covered Software was made available under the terms of
-        version 1.1 or earlier of the License, but not also under the
-        terms of a Secondary License.
-
-1.6. "Executable Form"
-    means any form of the work other than Source Code Form.
-
-1.7. "Larger Work"
-    means a work that combines Covered Software with other material, in
-    a separate file or files, that is not Covered Software.
-
-1.8. "License"
-    means this document.
-
-1.9. "Licensable"
-    means having the right to grant, to the maximum extent possible,
-    whether at the time of the initial grant or subsequently, any and
-    all of the rights conveyed by this License.
-
-1.10. "Modifications"
-    means any of the following:
-
-    (a) any file in Source Code Form that results from an addition to,
-        deletion from, or modification of the contents of Covered
-        Software; or
-
-    (b) any new file in Source Code Form that contains any Covered
-        Software.
-
-1.11. "Patent Claims" of a Contributor
-    means any patent claim(s), including without limitation, method,
-    process, and apparatus claims, in any patent Licensable by such
-    Contributor that would be infringed, but for the grant of the
-    License, by the making, using, selling, offering for sale, having
-    made, import, or transfer of either its Contributions or its
-    Contributor Version.
-
-1.12. "Secondary License"
-    means either the GNU General Public License, Version 2.0, the GNU
-    Lesser General Public License, Version 2.1, the GNU Affero General
-    Public License, Version 3.0, or any later versions of those
-    licenses.
-
-1.13. "Source Code Form"
-    means the form of the work preferred for making modifications.
-
-1.14. "You" (or "Your")
-    means an individual or a legal entity exercising rights under this
-    License. For legal entities, "You" includes any entity that
-    controls, is controlled by, or is under common control with You. For
-    purposes of this definition, "control" means (a) the power, direct
-    or indirect, to cause the direction or management of such entity,
-    whether by contract or otherwise, or (b) ownership of more than
-    fifty percent (50%) of the outstanding shares or beneficial
-    ownership of such entity.
-
-2. License Grants and Conditions
---------------------------------
-
-2.1. Grants
-
-Each Contributor hereby grants You a world-wide, royalty-free,
-non-exclusive license:
-
-(a) under intellectual property rights (other than patent or trademark)
-    Licensable by such Contributor to use, reproduce, make available,
-    modify, display, perform, distribute, and otherwise exploit its
-    Contributions, either on an unmodified basis, with Modifications, or
-    as part of a Larger Work; and
-
-(b) under Patent Claims of such Contributor to make, use, sell, offer
-    for sale, have made, import, and otherwise transfer either its
-    Contributions or its Contributor Version.
-
-2.2. Effective Date
-
-The licenses granted in Section 2.1 with respect to any Contribution
-become effective for each Contribution on the date the Contributor first
-distributes such Contribution.
-
-2.3. Limitations on Grant Scope
-
-The licenses granted in this Section 2 are the only rights granted under
-this License. No additional rights or licenses will be implied from the
-distribution or licensing of Covered Software under this License.
-Notwithstanding Section 2.1(b) above, no patent license is granted by a
-Contributor:
-
-(a) for any code that a Contributor has removed from Covered Software;
-    or
-
-(b) for infringements caused by: (i) Your and any other third party's
-    modifications of Covered Software, or (ii) the combination of its
-    Contributions with other software (except as part of its Contributor
-    Version); or
-
-(c) under Patent Claims infringed by Covered Software in the absence of
-    its Contributions.
-
-This License does not grant any rights in the trademarks, service marks,
-or logos of any Contributor (except as may be necessary to comply with
-the notice requirements in Section 3.4).
-
-2.4. Subsequent Licenses
-
-No Contributor makes additional grants as a result of Your choice to
-distribute the Covered Software under a subsequent version of this
-License (see Section 10.2) or under the terms of a Secondary License (if
-permitted under the terms of Section 3.3).
-
-2.5. Representation
-
-Each Contributor represents that the Contributor believes its
-Contributions are its original creation(s) or it has sufficient rights
-to grant the rights to its Contributions conveyed by this License.
-
-2.6. Fair Use
-
-This License is not intended to limit any rights You have under
-applicable copyright doctrines of fair use, fair dealing, or other
-equivalents.
-
-2.7. Conditions
-
-Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
-in Section 2.1.
-
-3. Responsibilities
--------------------
-
-3.1. Distribution of Source Form
-
-All distribution of Covered Software in Source Code Form, including any
-Modifications that You create or to which You contribute, must be under
-the terms of this License. You must inform recipients that the Source
-Code Form of the Covered Software is governed by the terms of this
-License, and how they can obtain a copy of this License. You may not
-attempt to alter or restrict the recipients' rights in the Source Code
-Form.
-
-3.2. Distribution of Executable Form
-
-If You distribute Covered Software in Executable Form then:
-
-(a) such Covered Software must also be made available in Source Code
-    Form, as described in Section 3.1, and You must inform recipients of
-    the Executable Form how they can obtain a copy of such Source Code
-    Form by reasonable means in a timely manner, at a charge no more
-    than the cost of distribution to the recipient; and
-
-(b) You may distribute such Executable Form under the terms of this
-    License, or sublicense it under different terms, provided that the
-    license for the Executable Form does not attempt to limit or alter
-    the recipients' rights in the Source Code Form under this License.
-
-3.3. Distribution of a Larger Work
-
-You may create and distribute a Larger Work under terms of Your choice,
-provided that You also comply with the requirements of this License for
-the Covered Software. If the Larger Work is a combination of Covered
-Software with a work governed by one or more Secondary Licenses, and the
-Covered Software is not Incompatible With Secondary Licenses, this
-License permits You to additionally distribute such Covered Software
-under the terms of such Secondary License(s), so that the recipient of
-the Larger Work may, at their option, further distribute the Covered
-Software under the terms of either this License or such Secondary
-License(s).
-
-3.4. Notices
-
-You may not remove or alter the substance of any license notices
-(including copyright notices, patent notices, disclaimers of warranty,
-or limitations of liability) contained within the Source Code Form of
-the Covered Software, except that You may alter any license notices to
-the extent required to remedy known factual inaccuracies.
-
-3.5. Application of Additional Terms
-
-You may choose to offer, and to charge a fee for, warranty, support,
-indemnity or liability obligations to one or more recipients of Covered
-Software. However, You may do so only on Your own behalf, and not on
-behalf of any Contributor. You must make it absolutely clear that any
-such warranty, support, indemnity, or liability obligation is offered by
-You alone, and You hereby agree to indemnify every Contributor for any
-liability incurred by such Contributor as a result of warranty, support,
-indemnity or liability terms You offer. You may include additional
-disclaimers of warranty and limitations of liability specific to any
-jurisdiction.
-
-4. Inability to Comply Due to Statute or Regulation
----------------------------------------------------
-
-If it is impossible for You to comply with any of the terms of this
-License with respect to some or all of the Covered Software due to
-statute, judicial order, or regulation then You must: (a) comply with
-the terms of this License to the maximum extent possible; and (b)
-describe the limitations and the code they affect. Such description must
-be placed in a text file included with all distributions of the Covered
-Software under this License. Except to the extent prohibited by statute
-or regulation, such description must be sufficiently detailed for a
-recipient of ordinary skill to be able to understand it.
-
-5. Termination
---------------
-
-5.1. The rights granted under this License will terminate automatically
-if You fail to comply with any of its terms. However, if You become
-compliant, then the rights granted under this License from a particular
-Contributor are reinstated (a) provisionally, unless and until such
-Contributor explicitly and finally terminates Your grants, and (b) on an
-ongoing basis, if such Contributor fails to notify You of the
-non-compliance by some reasonable means prior to 60 days after You have
-come back into compliance. Moreover, Your grants from a particular
-Contributor are reinstated on an ongoing basis if such Contributor
-notifies You of the non-compliance by some reasonable means, this is the
-first time You have received notice of non-compliance with this License
-from such Contributor, and You become compliant prior to 30 days after
-Your receipt of the notice.
-
-5.2. If You initiate litigation against any entity by asserting a patent
-infringement claim (excluding declaratory judgment actions,
-counter-claims, and cross-claims) alleging that a Contributor Version
-directly or indirectly infringes any patent, then the rights granted to
-You by any and all Contributors for the Covered Software under Section
-2.1 of this License shall terminate.
-
-5.3. In the event of termination under Sections 5.1 or 5.2 above, all
-end user license agreements (excluding distributors and resellers) which
-have been validly granted by You or Your distributors under this License
-prior to termination shall survive termination.
-
-************************************************************************
-*                                                                      *
-*  6. Disclaimer of Warranty                                           *
-*  -------------------------                                           *
-*                                                                      *
-*  Covered Software is provided under this License on an "as is"       *
-*  basis, without warranty of any kind, either expressed, implied, or  *
-*  statutory, including, without limitation, warranties that the       *
-*  Covered Software is free of defects, merchantable, fit for a        *
-*  particular purpose or non-infringing. The entire risk as to the     *
-*  quality and performance of the Covered Software is with You.        *
-*  Should any Covered Software prove defective in any respect, You     *
-*  (not any Contributor) assume the cost of any necessary servicing,   *
-*  repair, or correction. This disclaimer of warranty constitutes an   *
-*  essential part of this License. No use of any Covered Software is   *
-*  authorized under this License except under this disclaimer.         *
-*                                                                      *
-************************************************************************
-
-************************************************************************
-*                                                                      *
-*  7. Limitation of Liability                                          *
-*  --------------------------                                          *
-*                                                                      *
-*  Under no circumstances and under no legal theory, whether tort      *
-*  (including negligence), contract, or otherwise, shall any           *
-*  Contributor, or anyone who distributes Covered Software as          *
-*  permitted above, be liable to You for any direct, indirect,         *
-*  special, incidental, or consequential damages of any character      *
-*  including, without limitation, damages for lost profits, loss of    *
-*  goodwill, work stoppage, computer failure or malfunction, or any    *
-*  and all other commercial damages or losses, even if such party      *
-*  shall have been informed of the possibility of such damages. This   *
-*  limitation of liability shall not apply to liability for death or   *
-*  personal injury resulting from such party's negligence to the       *
-*  extent applicable law prohibits such limitation. Some               *
-*  jurisdictions do not allow the exclusion or limitation of           *
-*  incidental or consequential damages, so this exclusion and          *
-*  limitation may not apply to You.                                    *
-*                                                                      *
-************************************************************************
-
-8. Litigation
--------------
-
-Any litigation relating to this License may be brought only in the
-courts of a jurisdiction where the defendant maintains its principal
-place of business and such litigation shall be governed by laws of that
-jurisdiction, without reference to its conflict-of-law provisions.
-Nothing in this Section shall prevent a party's ability to bring
-cross-claims or counter-claims.
-
-9. Miscellaneous
-----------------
-
-This License represents the complete agreement concerning the subject
-matter hereof. If any provision of this License is held to be
-unenforceable, such provision shall be reformed only to the extent
-necessary to make it enforceable. Any law or regulation which provides
-that the language of a contract shall be construed against the drafter
-shall not be used to construe this License against a Contributor.
-
-10. Versions of the License
----------------------------
-
-10.1. New Versions
-
-Mozilla Foundation is the license steward. Except as provided in Section
-10.3, no one other than the license steward has the right to modify or
-publish new versions of this License. Each version will be given a
-distinguishing version number.
-
-10.2. Effect of New Versions
-
-You may distribute the Covered Software under the terms of the version
-of the License under which You originally received the Covered Software,
-or under the terms of any subsequent version published by the license
-steward.
-
-10.3. Modified Versions
-
-If you create software not governed by this License, and you want to
-create a new license for such software, you may create and use a
-modified version of this License if you rename the license and remove
-any references to the name of the license steward (except to note that
-such modified license differs from this License).
-
-10.4. Distributing Source Code Form that is Incompatible With Secondary
-Licenses
-
-If You choose to distribute Source Code Form that is Incompatible With
-Secondary Licenses under the terms of this version of the License, the
-notice described in Exhibit B of this License must be attached.
-
-Exhibit A - Source Code Form License Notice
--------------------------------------------
-
-  This Source Code Form is subject to the terms of the Mozilla Public
-  License, v. 2.0. If a copy of the MPL was not distributed with this
-  file, You can obtain one at http://mozilla.org/MPL/2.0/.
-
-If it is not possible or desirable to put the notice in a particular
-file, then You may include the notice in a location (such as a LICENSE
-file in a relevant directory) where a recipient would be likely to look
-for such a notice.
-
-You may add additional accurate notices of copyright ownership.
-
-Exhibit B - "Incompatible With Secondary Licenses" Notice
----------------------------------------------------------
-
-  This Source Code Form is "Incompatible With Secondary Licenses", as
-  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/cloud-azure/licenses/httpclient-NOTICE.txt b/plugins/cloud-azure/licenses/httpclient-NOTICE.txt
deleted file mode 100644
index 4f60581..0000000
--- a/plugins/cloud-azure/licenses/httpclient-NOTICE.txt
+++ /dev/null
@@ -1,5 +0,0 @@
-Apache HttpComponents Client
-Copyright 1999-2015 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/cloud-azure/licenses/httpcore-4.3.3.jar.sha1 b/plugins/cloud-azure/licenses/httpcore-4.3.3.jar.sha1
deleted file mode 100644
index 5d9c0e2..0000000
--- a/plugins/cloud-azure/licenses/httpcore-4.3.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/cloud-azure/licenses/httpcore-LICENSE.txt b/plugins/cloud-azure/licenses/httpcore-LICENSE.txt
deleted file mode 100644
index 72819a9..0000000
--- a/plugins/cloud-azure/licenses/httpcore-LICENSE.txt
+++ /dev/null
@@ -1,241 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-=========================================================================
-
-This project contains annotations in the package org.apache.http.annotation
-which are derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls.
-See http://www.jcip.net and the Creative Commons Attribution License
-(http://creativecommons.org/licenses/by/2.5)
-Full text: http://creativecommons.org/licenses/by/2.5/legalcode
-
-License
-
-THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
-
-BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
-
-1. Definitions
-
-    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
-    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
-    "Licensor" means the individual or entity that offers the Work under the terms of this License.
-    "Original Author" means the individual or entity who created the Work.
-    "Work" means the copyrightable work of authorship offered under the terms of this License.
-    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
-
-2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
-
-3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
-
-    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
-    to create and reproduce Derivative Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
-    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
-
-    For the avoidance of doubt, where the work is a musical composition:
-        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
-        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
-    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
-
-The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
-
-4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
-
-    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
-    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
-
-5. Representations, Warranties and Disclaimer
-
-UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
-
-6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-7. Termination
-
-    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
-    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
-
-8. Miscellaneous
-
-    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
-    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
-    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
-    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
-    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/cloud-azure/licenses/httpcore-NOTICE.txt b/plugins/cloud-azure/licenses/httpcore-NOTICE.txt
deleted file mode 100644
index c0be50a..0000000
--- a/plugins/cloud-azure/licenses/httpcore-NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Apache HttpComponents Core
-Copyright 2005-2014 The Apache Software Foundation
-
-This product includes software developed at
-The Apache Software Foundation (http://www.apache.org/).
-
-This project contains annotations derived from JCIP-ANNOTATIONS
-Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/cloud-azure/licenses/jackson-LICENSE b/plugins/cloud-azure/licenses/jackson-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/cloud-azure/licenses/jackson-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/cloud-azure/licenses/jackson-NOTICE b/plugins/cloud-azure/licenses/jackson-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/cloud-azure/licenses/jackson-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/cloud-azure/licenses/jackson-core-asl-1.9.2.jar.sha1 b/plugins/cloud-azure/licenses/jackson-core-asl-1.9.2.jar.sha1
deleted file mode 100644
index a608bd1..0000000
--- a/plugins/cloud-azure/licenses/jackson-core-asl-1.9.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-8493982bba1727106d767034bd0d8e77bc1931a9
diff --git a/plugins/cloud-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1 b/plugins/cloud-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1
deleted file mode 100644
index a3dc0aa..0000000
--- a/plugins/cloud-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-aedf43f1d5005561e531b6bf0d067e4d20f58aba
diff --git a/plugins/cloud-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1 b/plugins/cloud-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1
deleted file mode 100644
index fd88504..0000000
--- a/plugins/cloud-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-95400a7922ce75383866eb72f6ef4a7897923945
diff --git a/plugins/cloud-azure/licenses/jackson-xc-1.9.2.jar.sha1 b/plugins/cloud-azure/licenses/jackson-xc-1.9.2.jar.sha1
deleted file mode 100644
index f823e61..0000000
--- a/plugins/cloud-azure/licenses/jackson-xc-1.9.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-437c991a8eb2c8b69ef1dba2eba27fccb9b98448
diff --git a/plugins/cloud-azure/licenses/javax.inject-1.jar.sha1 b/plugins/cloud-azure/licenses/javax.inject-1.jar.sha1
deleted file mode 100644
index 7ef3c70..0000000
--- a/plugins/cloud-azure/licenses/javax.inject-1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-6975da39a7040257bd51d21a231b76c915872d38
diff --git a/plugins/cloud-azure/licenses/javax.inject-LICENSE.txt b/plugins/cloud-azure/licenses/javax.inject-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/javax.inject-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/javax.inject-NOTICE.txt b/plugins/cloud-azure/licenses/javax.inject-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/javax.inject-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/jaxb-LICENSE.txt b/plugins/cloud-azure/licenses/jaxb-LICENSE.txt
deleted file mode 100644
index a3e62b0..0000000
--- a/plugins/cloud-azure/licenses/jaxb-LICENSE.txt
+++ /dev/null
@@ -1,705 +0,0 @@
-COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0
-
-1. Definitions.
-
-   1.1. Contributor. means each individual or  entity  that  creates  or
-contributes to the creation of Modifications.
-
-   1.2. Contributor Version.  means  the  combination  of  the  Original
-Software,  prior  Modifications  used by a Contributor (if any), and the
-Modifications made by that particular Contributor.
-
-   1.3. Covered Software.  means  (a)  the  Original  Software,  or  (b)
-Modifications,  or  (c)  the  combination  of  files containing Original
-Software with files containing Modifications,  in  each  case  including
-portions thereof.
-
-   1.4. Executable. means the Covered Software in any  form  other  than
-Source Code.
-
-   1.5. Initial Developer. means the individual  or  entity  that  first
-makes Original Software available under this License.
-
-   1.6. Larger Work. means a work which  combines  Covered  Software  or
-portions thereof with code not governed by the terms of this License.
-
-   1.7. License. means this document.
-
-   1.8. Licensable. means having the right  to  grant,  to  the  maximum
-extent   possible,   whether  at  the  time  of  the  initial  grant  or
-subsequently acquired, any and all of the rights conveyed herein.
-
-   1.9. Modifications. means the Source Code and Executable form of  any
-of the following:
-
-        A. Any file that results from an addition to, deletion  from  or
-modification  of  the contents of a file containing Original Software or
-previous Modifications;
-
-        B. Any new file that contains any part of the Original  Software
-or previous Modification; or
-
-        C. Any new file that is contributed or otherwise made  available
-under the terms of this License.
-
-   1.10. Original Software. means the Source Code and Executable form of
-computer software code that is originally released under this License.
-
-   1.11.  Patent  Claims.  means  any  patent  claim(s),  now  owned  or
-hereafter  acquired,  including without limitation, method, process, and
-apparatus claims, in any patent Licensable by grantor.
-
-   1.12. Source Code. means (a) the common  form  of  computer  software
-code  in  which  modifications are made and (b) associated documentation
-included in or with such code.
-
-   1.13. You.  (or  .Your.)  means  an  individual  or  a  legal  entity
-exercising  rights  under,  and complying with all of the terms of, this
-License. For legal entities, .You. includes any entity  which  controls,
-is  controlled  by, or is under common control with You. For purposes of
-this definition, .control. means (a) the power, direct or  indirect,  to
-cause the direction or management of such entity, whether by contract or
-otherwise,  or  (b)  ownership  of  more than fifty percent (50%) of the
-outstanding shares or beneficial ownership of such entity.
-
-2. License Grants.
-
-      2.1. The Initial Developer Grant.
-
-      Conditioned upon  Your  compliance  with  Section  3.1  below  and
-subject  to  third  party  intellectual  property  claims,  the  Initial
-Developer hereby grants You a  world-wide,  royalty-free,  non-exclusive
-license:
-
-         (a) under intellectual property rights (other  than  patent  or
-trademark)  Licensable  by Initial Developer, to use, reproduce, modify,
-display, perform, sublicense and distribute the  Original  Software  (or
-portions  thereof),  with  or without Modifications, and/or as part of a
-Larger Work; and
-
-         (b) under Patent Claims  infringed  by  the  making,  using  or
-selling  of  Original Software, to make, have made, use, practice, sell,
-and offer for sale, and/or otherwise dispose of  the  Original  Software
-(or portions thereof).
-
-        (c)  The  licenses  granted  in  Sections  2.1(a)  and  (b)  are
-effective  on  the date Initial Developer first distributes or otherwise
-makes the Original Software available to a third party under  the  terms
-of this License.
-
-        (d) Notwithstanding Section 2.1(b) above, no patent  license  is
-granted: (1) for code that You delete from the Original Software, or (2)
-for  infringements  caused  by:  (i)  the  modification  of the Original
-Software, or (ii) the combination of the Original  Software  with  other
-software or devices.
-
-    2.2. Contributor Grant.
-
-    Conditioned upon Your compliance with Section 3.1 below and  subject
-to  third  party  intellectual  property claims, each Contributor hereby
-grants You a world-wide, royalty-free, non-exclusive license:
-
-        (a) under intellectual property rights  (other  than  patent  or
-trademark) Licensable by Contributor to use, reproduce, modify, display,
-perform,  sublicense  and  distribute  the Modifications created by such
-Contributor (or portions thereof), either on an unmodified  basis,  with
-other Modifications, as Covered Software and/or as part of a Larger Work;
-and
-
-        (b) under Patent Claims  infringed  by  the  making,  using,  or
-selling of Modifications made by that Contributor either alone and/or in
-combination   with   its   Contributor  Version  (or  portions  of  such
-combination), to make, use, sell, offer  for  sale,  have  made,  and/or
-otherwise  dispose  of:  (1)  Modifications made by that Contributor (or
-portions thereof); and (2) the combination of Modifications made by that
-Contributor  with  its  Contributor  Version  (or   portions   of   such
-combination).
-
-        (c) The licenses granted  in  Sections  2.2(a)  and  2.2(b)  are
-effective  on  the date Contributor first distributes or otherwise makes
-the Modifications available to a third party.
-
-        (d) Notwithstanding Section 2.2(b) above, no patent  license  is
-granted:  (1)  for  any  code  that  Contributor  has  deleted  from the
-Contributor Version; (2) for infringements caused by:  (i)  third  party
-modifications  of  Contributor  Version,  or  (ii)  the  combination  of
-Modifications made by that Contributor with other  software  (except  as
-part  of  the Contributor Version) or other devices; or (3) under Patent
-Claims infringed by Covered Software in  the  absence  of  Modifications
-made by that Contributor.
-
-3. Distribution Obligations.
-
-      3.1. Availability of Source Code.
-      Any  Covered  Software  that  You  distribute  or  otherwise  make
-available  in Executable form must also be made available in Source Code
-form and that Source Code form must be distributed only under the  terms
-of this License. You must include a copy of this License with every copy
-of  the  Source  Code  form  of  the  Covered Software You distribute or
-otherwise make available. You must inform recipients of any such Covered
-Software in Executable form as to  how  they  can  obtain  such  Covered
-Software  in  Source  Code  form  in a reasonable manner on or through a
-medium customarily used for software exchange.
-
-      3.2. Modifications.
-      The Modifications that You create or to which You  contribute  are
-governed  by  the  terms of this License. You represent that You believe
-Your  Modifications  are  Your  original  creation(s)  and/or  You  have
-sufficient rights to grant the rights conveyed by this License.
-
-      3.3. Required Notices.
-      You must include a notice  in  each  of  Your  Modifications  that
-identifies  You  as  the  Contributor  of  the Modification. You may not
-remove or alter any copyright, patent  or  trademark  notices  contained
-within  the  Covered  Software,  or  any  notices  of  licensing  or any
-descriptive text giving attribution to any Contributor  or  the  Initial
-Developer.
-
-      3.4. Application of Additional Terms.
-      You may not offer or impose any terms on any Covered  Software  in
-Source Code form that alters or restricts the applicable version of this
-License  or  the  recipients. rights hereunder. You may choose to offer,
-and to charge a fee  for,  warranty,  support,  indemnity  or  liability
-obligations  to one or more recipients of Covered Software. However, you
-may do so only on Your own behalf, and not  on  behalf  of  the  Initial
-Developer or any Contributor. You must make it absolutely clear that any
-such  warranty, support, indemnity or liability obligation is offered by
-You alone, and You hereby agree to indemnify the Initial  Developer  and
-every Contributor for any liability incurred by the Initial Developer or
-such  Contributor  as  a  result  of  warranty,  support,  indemnity  or
-liability terms You offer.
-
-      3.5. Distribution of Executable Versions.
-      You may distribute the Executable form  of  the  Covered  Software
-under  the terms of this License or under the terms of a license of Your
-choice, which may contain terms different from  this  License,  provided
-that  You  are in compliance with the terms of this License and that the
-license for the Executable form does not attempt to limit or  alter  the
-recipient.s  rights in the Source Code form from the rights set forth in
-this License. If You distribute the Covered Software in Executable  form
-under  a  different  license, You must make it absolutely clear that any
-terms which differ from this License are offered by You  alone,  not  by
-the  Initial Developer or Contributor. You hereby agree to indemnify the
-Initial Developer and every Contributor for any  liability  incurred  by
-the  Initial Developer or such Contributor as a result of any such terms
-You offer.
-
-      3.6. Larger Works.
-      You may create a Larger Work by combining  Covered  Software  with
-other  code not governed by the terms of this License and distribute the
-Larger Work as a single product. In such a case, You must make sure  the
-requirements of this License are fulfilled for the Covered Software.
-
-4. Versions of the License.
-
-      4.1. New Versions.
-      Sun Microsystems, Inc. is the  initial  license  steward  and  may
-publish  revised  and/or new versions of this License from time to time.
-Each version will be given a distinguishing version  number.  Except  as
-provided  in  Section 4.3, no one other than the license steward has the
-right to modify this License.
-
-      4.2. Effect of New Versions.
-      You may always continue to use, distribute or otherwise  make  the
-Covered Software available under the terms of the version of the License
-under which You originally received the Covered Software. If the Initial
-Developer includes a notice in the Original Software prohibiting it from
-being  distributed  or  otherwise  made  available  under any subsequent
-version of the  License,  You  must  distribute  and  make  the  Covered
-Software  available  under the terms of the version of the License under
-which You originally received the Covered Software. Otherwise,  You  may
-also  choose  to  use, distribute or otherwise make the Covered Software
-available under the terms of  any  subsequent  version  of  the  License
-published by the license steward.
-
-      4.3. Modified Versions.
-      When You are an Initial Developer and You want  to  create  a  new
-license  for  Your  Original Software, You may create and use a modified
-version of this License if You: (a) rename the license  and  remove  any
-references  to  the name of the license steward (except to note that the
-license differs from this License); and (b) otherwise make it clear that
-the license contains terms which differ from this License.
-
-5. DISCLAIMER OF WARRANTY.
-
-   COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN .AS IS.  BASIS,
-WITHOUT  WARRANTY  OF  ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING,
-WITHOUT LIMITATION, WARRANTIES THAT THE  COVERED  SOFTWARE  IS  FREE  OF
-DEFECTS,  MERCHANTABLE,  FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING.
-THE ENTIRE RISK AS  TO  THE  QUALITY  AND  PERFORMANCE  OF  THE  COVERED
-SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY
-RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME
-THE  COST  OF  ANY  NECESSARY  SERVICING,  REPAIR  OR  CORRECTION.  THIS
-DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO
-USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER  EXCEPT  UNDER  THIS
-DISCLAIMER.
-
-6. TERMINATION.
-
-      6.1. This License and the rights granted hereunder will  terminate
-automatically  if  You fail to comply with terms herein and fail to cure
-such breach within 30 days of becoming aware of the  breach.  Provisions
-which,  by their nature, must remain in effect beyond the termination of
-this License shall survive.
-
-      6.2.  If  You  assert  a  patent  infringement  claim   (excluding
-declaratory judgment actions) against Initial Developer or a Contributor
-(the Initial Developer or Contributor against whom You assert such claim
-is  referred to as .Participant.) alleging that the Participant Software
-(meaning the Contributor Version where the Participant is a  Contributor
-or the Original Software where the Participant is the Initial Developer)
-directly  or  indirectly  infringes  any patent, then any and all rights
-granted directly or indirectly to You by such Participant,  the  Initial
-Developer  (if  the  Initial  Developer  is not the Participant) and all
-Contributors under Sections 2.1 and/or 2.2 of this License  shall,  upon
-60   days   notice   from   Participant   terminate   prospectively  and
-automatically at the expiration of such 60 day notice period, unless  if
-within  such  60  day period You withdraw Your claim with respect to the
-Participant Software against such  Participant  either  unilaterally  or
-pursuant to a written agreement with Participant.
-
-      6.3. In the event of termination under Sections 6.1 or 6.2  above,
-all  end  user  licenses  that  have  been validly granted by You or any
-distributor hereunder prior to termination (excluding  licenses  granted
-to You by any distributor) shall survive termination.
-
-7. LIMITATION OF LIABILITY.
-
-   UNDER NO CIRCUMSTANCES  AND  UNDER  NO  LEGAL  THEORY,  WHETHER  TORT
-(INCLUDING  NEGLIGENCE),  CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL
-DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE,
-OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR  ANY
-INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER
-INCLUDING,  WITHOUT  LIMITATION,  DAMAGES  FOR  LOST  PROFITS,  LOSS  OF
-GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL
-OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL  HAVE  BEEN
-INFORMED  OF  THE  POSSIBILITY  OF  SUCH  DAMAGES.  THIS  LIMITATION  OF
-LIABILITY SHALL NOT APPLY TO LIABILITY  FOR  DEATH  OR  PERSONAL  INJURY
-RESULTING  FROM  SUCH  PARTY.S  NEGLIGENCE  TO THE EXTENT APPLICABLE LAW
-PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION
-OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS  EXCLUSION
-AND LIMITATION MAY NOT APPLY TO YOU.
-
-8. U.S. GOVERNMENT END USERS.
-
-   The Covered Software is a .commercial item,. as that term is  defined
-in  48  C.F.R.  2.101  (Oct.  1995),  consisting of .commercial computer
-software. (as that term is defined at 48  C.F.R.    252.227-7014(a)(1))
-and  .commercial computer software documentation. as such terms are used
-in 48 C.F.R. 12.212 (Sept. 1995). Consistent with 48 C.F.R.  12.212  and
-48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government
-End  Users  acquire  Covered  Software  with only those rights set forth
-herein. This U.S. Government Rights clause is in lieu of, and supersedes,
-any other FAR,  DFAR,  or  other  clause  or  provision  that  addresses
-Government rights in computer software under this License.
-
-9. MISCELLANEOUS.
-
-   This License represents the  complete  agreement  concerning  subject
-matter  hereof.  If  any  provision  of  this  License  is  held  to  be
-unenforceable, such provision shall  be  reformed  only  to  the  extent
-necessary  to make it enforceable. This License shall be governed by the
-law of the jurisdiction specified  in  a  notice  contained  within  the
-Original Software (except to the extent applicable law, if any, provides
-otherwise),  excluding  such  jurisdiction.s conflict-of-law provisions.
-Any litigation  relating  to  this  License  shall  be  subject  to  the
-jurisdiction  of  the  courts  located  in  the  jurisdiction  and venue
-specified in a notice contained within the Original Software,  with  the
-losing party responsible for costs, including, without limitation, court
-costs  and  reasonable  attorneys. fees and expenses. The application of
-the United Nations Convention on Contracts for the International Sale of
-Goods is expressly excluded. Any law or regulation which  provides  that
-the  language of a contract shall be construed against the drafter shall
-not apply to this License. You agree that You alone are responsible  for
-compliance with the United States export administration regulations (and
-the  export control laws and regulation of any other countries) when You
-use, distribute or otherwise make available any Covered Software.
-
-10. RESPONSIBILITY FOR CLAIMS.
-
-   As between Initial Developer and  the  Contributors,  each  party  is
-responsible  for claims and damages arising, directly or indirectly, out
-of its utilization of rights under this License and You  agree  to  work
-with   Initial   Developer   and   Contributors   to   distribute   such
-responsibility on an equitable basis.  Nothing  herein  is  intended  or
-shall be deemed to constitute any admission of liability.
-
-   NOTICE  PURSUANT  TO  SECTION  9  OF  THE  COMMON   DEVELOPMENT   AND
-DISTRIBUTION LICENSE (CDDL)
-
-   The code released under the CDDL shall be governed by the laws of the
-State  of  California  (excluding   conflict-of-law   provisions).   Any
-litigation relating to this License shall be subject to the jurisdiction
-of  the  Federal  Courts  of the Northern District of California and the
-state courts of the State of California, with venue lying in Santa Clara
-County, California.
-
-
-The GNU General Public License (GPL) Version 2, June 1991
-
-
-Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place,
-Suite 330, Boston, MA 02111-1307 USA
-
-Everyone is permitted to copy and distribute  verbatim  copies  of  this
-license document, but changing it is not allowed.
-
-Preamble
-
-The licenses for most software are designed to take away your freedom to
-share and change it. By contrast, the  GNU  General  Public  License  is
-intended to guarantee your freedom to share and change free software--to
-make  sure  the  software is free for all its users. This General Public
-License applies to most of the Free Software Foundation's  software  and
-to  any other program whose authors commit to using it. (Some other Free
-Software Foundation software is  covered  by  the  GNU  Library  General
-Public License instead.) You can apply it to your programs, too.
-
-When we speak of free software, we are referring to freedom, not  price.
-Our  General Public Licenses are designed to make sure that you have the
-freedom to distribute copies of  free  software  (and  charge  for  this
-service  if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces  of  it  in  new
-free programs; and that you know you can do these things.
-
-To protect your rights, we need to make restrictions that forbid  anyone
-to  deny  you  these rights or to ask you to surrender the rights. These
-restrictions translate  to  certain  responsibilities  for  you  if  you
-distribute copies of the software, or if you modify it.
-
-For example, if you distribute copies of such a program, whether  gratis
-or for a fee, you must give the recipients all the rights that you have.
-You  must  make sure that they, too, receive or can get the source code.
-And you must show them these terms so they know their rights.
-
-We protect your rights with two steps: (1) copyright the  software,  and
-(2)  offer  you  this  license which gives you legal permission to copy,
-distribute and/or modify the software.
-
-Also, for each author's protection and ours, we  want  to  make  certain
-that  everyone  understands  that  there  is  no  warranty for this free
-software. If the software is modified by someone else and passed on,  we
-want  its recipients to know that what they have is not the original, so
-that any problems introduced by others will not reflect on the  original
-authors' reputations.
-
-Finally, any free program is threatened constantly by software  patents.
-We  wish  to avoid the danger that redistributors of a free program will
-individually obtain  patent  licenses,  in  effect  making  the  program
-proprietary. To prevent this, we have made it clear that any patent must
-be licensed for everyone's free use or not licensed at all.
-
-The  precise  terms  and  conditions  for  copying,   distribution   and
-modification follow.
-
-
-TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
-
-0. This License applies to any program or other work  which  contains  a
-notice placed by the copyright holder saying it may be distributed under
-the  terms  of this General Public License. The "Program", below, refers
-to any such program or work, and a "work based  on  the  Program"  means
-either  the  Program or any derivative work under copyright law: that is
-to say, a work containing  the  Program  or  a  portion  of  it,  either
-verbatim  or with modifications and/or translated into another language.
-(Hereinafter, translation is included without  limitation  in  the  term
-"modification".) Each licensee is addressed as "you".
-
-Activities other than copying, distribution  and  modification  are  not
-covered  by this License; they are outside its scope. The act of running
-the Program is not restricted,  and  the  output  from  the  Program  is
-covered  only  if  its  contents  constitute a work based on the Program
-(independent of having been made by running the Program).  Whether  that
-is true depends on what the Program does.
-
-1. You may copy and distribute verbatim copies of the  Program's  source
-code  as  you receive it, in any medium, provided that you conspicuously
-and appropriately publish on each copy an appropriate  copyright  notice
-and  disclaimer  of  warranty; keep intact all the notices that refer to
-this License and to the absence of any  warranty;  and  give  any  other
-recipients of the Program a copy of this License along with the Program.
-
-You may charge a fee for the physical act of transferring  a  copy,  and
-you may at your option offer warranty protection in exchange for a fee.
-
-2. You may modify your copy or copies of the Program or any  portion  of
-it,  thus  forming  a work based on the Program, and copy and distribute
-such modifications or work under the terms of Section 1 above,  provided
-that you also meet all of these conditions:
-
-   a) You must cause the  modified  files  to  carry  prominent  notices
-stating that you changed the files and the date of any change.
-
-   b) You must cause any work that you distribute or  publish,  that  in
-whole  or  in  part  contains or is derived from the Program or any part
-thereof, to be licensed as a whole at no charge  to  all  third  parties
-under the terms of this License.
-
-   c) If the modified program normally reads commands interactively when
-run, you must cause it, when started running for such interactive use in
-the most ordinary way, to print or display an announcement including  an
-appropriate  copyright notice and a notice that there is no warranty (or
-else,  saying  that  you  provide  a  warranty)  and  that   users   may
-redistribute  the  program  under these conditions, and telling the user
-how to view a copy of this License. (Exception: if the Program itself is
-interactive but does not normally print such an announcement, your  work
-based on the Program is not required to print an announcement.)
-
-These  requirements  apply  to  the  modified  work  as  a   whole.   If
-identifiable sections of that work are not derived from the Program, and
-can   be   reasonably  considered  independent  and  separate  works  in
-themselves, then this License, and its terms,  do  not  apply  to  those
-sections  when  you  distribute  them  as  separate  works. But when you
-distribute the same sections as part of a whole which is a work based on
-the Program, the distribution of the whole must be on the terms of  this
-License,  whose  permissions  for  other  licensees extend to the entire
-whole, and thus to each and every part regardless of who wrote it.
-
-Thus, it is not the intent of this section to claim  rights  or  contest
-your  rights  to  work written entirely by you; rather, the intent is to
-exercise  the  right  to  control  the  distribution  of  derivative  or
-collective works based on the Program.
-
-In addition, mere aggregation of another work not based on  the  Program
-with  the Program (or with a work based on the Program) on a volume of a
-storage or distribution medium does not bring the other work  under  the
-scope of this License.
-
-3. You may copy and distribute the Program (or a work based on it, under
-Section 2) in object code or executable form under the terms of Sections
-1 and 2 above provided that you also do one of the following:
-
-   a) Accompany it  with  the  complete  corresponding  machine-readable
-source code, which must be distributed under the terms of Sections 1 and
-2 above on a medium customarily used for software interchange; or,
-
-   b) Accompany it with a written offer, valid for at least three years,
-to give any third party,  for  a  charge  no  more  than  your  cost  of
-physically  performing  source distribution, a complete machine-readable
-copy of the corresponding source code, to be distributed under the terms
-of Sections 1 and 2 above on a  medium  customarily  used  for  software
-interchange; or,
-
-   c) Accompany it with the information you received as to the offer  to
-distribute  corresponding source code. (This alternative is allowed only
-for noncommercial distribution and only if you received the  program  in
-object  code  or  executable  form  with  such  an offer, in accord with
-Subsection b above.)
-
-The source code for a work means the preferred  form  of  the  work  for
-making modifications to it. For an executable work, complete source code
-means  all  the  source  code  for  all  modules  it  contains, plus any
-associated interface definition files, plus the scripts used to  control
-compilation  and  installation  of the executable. However, as a special
-exception, the source code distributed need not include anything that is
-normally distributed (in either source or binary form)  with  the  major
-components  (compiler,  kernel,  and  so  on) of the operating system on
-which the executable runs, unless that component itself accompanies  the
-executable.
-
-If distribution of executable or object code is made by offering  access
-to copy from a designated place, then offering equivalent access to copy
-the source code from the same place counts as distribution of the source
-code,  even  though  third  parties are not compelled to copy the source
-along with the object code.
-
-4. You may not copy,  modify,  sublicense,  or  distribute  the  Program
-except  as  expressly provided under this License. Any attempt otherwise
-to copy, modify, sublicense or distribute the Program is void, and  will
-automatically terminate your rights under this License. However, parties
-who  have  received  copies, or rights, from you under this License will
-not have their licenses terminated so long as  such  parties  remain  in
-full compliance.
-
-5. You are not required to accept  this  License,  since  you  have  not
-signed  it.  However,  nothing  else  grants you permission to modify or
-distribute the Program  or  its  derivative  works.  These  actions  are
-prohibited  by  law  if  you  do  not accept this License. Therefore, by
-modifying or distributing the Program (or any work based on the Program),
-you indicate your acceptance of this License to do so, and all its terms
-and conditions for copying, distributing or  modifying  the  Program  or
-works based on it.
-
-6. Each time you redistribute the Program (or  any  work  based  on  the
-Program),  the  recipient  automatically  receives  a  license  from the
-original licensor to copy, distribute or modify the Program  subject  to
-these  terms and conditions. You may not impose any further restrictions
-on the recipients' exercise of the rights granted herein.  You  are  not
-responsible for enforcing compliance by third parties to this License.
-
-7. If, as a consequence of a court  judgment  or  allegation  of  patent
-infringement  or  for  any  other reason (not limited to patent issues),
-conditions are imposed on you (whether  by  court  order,  agreement  or
-otherwise)  that  contradict the conditions of this License, they do not
-excuse you from the conditions of this License. If you cannot distribute
-so as to satisfy simultaneously your obligations under this License  and
-any  other  pertinent  obligations,  then  as  a consequence you may not
-distribute the Program at all. For example, if a  patent  license  would
-not  permit  royalty-free redistribution of the Program by all those who
-receive copies directly or indirectly through you, then the only way you
-could satisfy both it and this License would be to refrain entirely from
-distribution of the Program.
-
-If any portion of this section is held invalid  or  unenforceable  under
-any  particular  circumstance, the balance of the section is intended to
-apply and the  section  as  a  whole  is  intended  to  apply  in  other
-circumstances.
-
-It is not the purpose of this section to  induce  you  to  infringe  any
-patents  or  other  property  right claims or to contest validity of any
-such claims; this  section  has  the  sole  purpose  of  protecting  the
-integrity of the free software distribution system, which is implemented
-by   public   license   practices.   Many   people  have  made  generous
-contributions to the wide range of  software  distributed  through  that
-system in reliance on consistent application of that system; it is up to
-the  author/donor  to  decide  if  he  or  she  is willing to distribute
-software through any other system and  a  licensee  cannot  impose  that
-choice.
-
-This section is intended to make thoroughly clear what is believed to be
-a consequence of the rest of this License.
-
-8. If the distribution and/or  use  of  the  Program  is  restricted  in
-certain  countries  either  by patents or by copyrighted interfaces, the
-original copyright holder who places the Program under this License  may
-add  an  explicit  geographical  distribution limitation excluding those
-countries, so that distribution is permitted only in or among  countries
-not   thus  excluded.  In  such  case,  this  License  incorporates  the
-limitation as if written in the body of this License.
-
-9. The Free Software Foundation may publish revised and/or new  versions
-of  the General Public License from time to time. Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-Each version is given a distinguishing version number.  If  the  Program
-specifies  a version number of this License which applies to it and "any
-later  version",  you  have  the  option  of  following  the  terms  and
-conditions  either  of that version or of any later version published by
-the Free Software Foundation. If the Program does not specify a  version
-number of this License, you may choose any version ever published by the
-Free Software Foundation.
-
-10. If you wish to incorporate parts of  the  Program  into  other  free
-programs  whose  distribution  conditions  are  different,  write to the
-author to ask for permission. For software which is copyrighted  by  the
-Free  Software  Foundation,  write  to  the Free Software Foundation; we
-sometimes make exceptions for this. Our decision will be guided  by  the
-two  goals  of preserving the free status of all derivatives of our free
-software and of promoting the sharing and reuse of software generally.
-
-NO WARRANTY
-
-11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
-FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT  WHEN
-OTHERWISE  STATED  IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
-PROVIDE THE PROGRAM  "AS  IS"  WITHOUT  WARRANTY  OF  ANY  KIND,  EITHER
-EXPRESSED  OR  IMPLIED,  INCLUDING,  BUT  NOT  LIMITED  TO,  THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE
-ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.
-SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY
-SERVICING, REPAIR OR CORRECTION.
-
-12. IN NO EVENT UNLESS REQUIRED  BY  APPLICABLE  LAW  OR  AGREED  TO  IN
-WRITING  WILL  ANY  COPYRIGHT  HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
-AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
-DAMAGES, INCLUDING ANY GENERAL,  SPECIAL,  INCIDENTAL  OR  CONSEQUENTIAL
-DAMAGES  ARISING  OUT  OF  THE  USE  OR  INABILITY  TO  USE  THE PROGRAM
-(INCLUDING BUT NOT LIMITED TO  LOSS  OF  DATA  OR  DATA  BEING  RENDERED
-INACCURATE  OR  LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF
-THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER  OR
-OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-END OF TERMS AND CONDITIONS
-
-
-How to Apply These Terms to Your New Programs
-
-If you develop a new program, and you want it  to  be  of  the  greatest
-possible  use  to the public, the best way to achieve this is to make it
-free software which everyone can redistribute  and  change  under  these
-terms.
-
-To do so, attach the following notices to the program. It is  safest  to
-attach  them to the start of each source file to most effectively convey
-the exclusion of warranty; and  each  file  should  have  at  least  the
-"copyright" line and a pointer to where the full notice is found.
-
-   One line to give the program's name and a brief idea of what it does.
-
-   Copyright (C)
-
-   This program is free software; you can redistribute it and/or  modify
-it under the terms of the GNU General Public License as published by the
-Free  Software  Foundation; either version 2 of the License, or (at your
-option) any later version.
-
-   This program is distributed in the hope that it will be  useful,  but
-WITHOUT   ANY   WARRANTY;   without   even   the   implied  warranty  of
-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
-Public License for more details.
-
-   You should have received a copy of the  GNU  General  Public  License
-along  with this program; if not, write to the Free Software Foundation,
-Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
-
-Also add information on how to contact you by electronic and paper mail.
-
-If the program is interactive, make it output a short notice  like  this
-when it starts in an interactive mode:
-
-   Gnomovision version 69, Copyright (C) year name of author
-   Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show
-w'. This is free software, and you are welcome to redistribute it  under
-certain conditions; type `show c' for details.
-
-The hypothetical  commands  `show  w'  and  `show  c'  should  show  the
-appropriate parts of the General Public License. Of course, the commands
-you  use  may be called something other than `show w' and `show c'; they
-could even be mouse-clicks or menu items--whatever suits your program.
-
-You should also get your employer (if you work as a programmer) or  your
-school,  if  any,  to  sign a "copyright disclaimer" for the program, if
-necessary. Here is a sample; alter the names:
-
-   Yoyodyne, Inc.,  hereby  disclaims  all  copyright  interest  in  the
-program `Gnomovision' (which makes passes at compilers) written by James
-Hacker.
-
-   signature of Ty Coon, 1 April 1989
-   Ty Coon, President of Vice
-
-This General Public License does not permit incorporating  your  program
-into  proprietary programs. If your program is a subroutine library, you
-may consider it more useful to permit linking  proprietary  applications
-with  the  library.  If this is what you want to do, use the GNU Library
-General Public License instead of this License.
-
-
-"CLASSPATH" EXCEPTION TO THE GPL VERSION 2
-
-Certain source files distributed by Sun Microsystems, Inc.  are  subject
-to  the following clarification and special exception to the GPL Version
-2, but only where Sun has expressly included in  the  particular  source
-file's header the words
-
-"Sun designates this particular  file  as  subject  to  the  "Classpath"
-exception  as  provided by Sun in the License file that accompanied this
-code."
-
-Linking this library statically or dynamically  with  other  modules  is
-making  a  combined  work  based  on  this  library. Thus, the terms and
-conditions of the GNU General Public License Version 2 cover  the  whole
-combination.
-
-As a special exception, the copyright holders of this library  give  you
-permission  to  link this library with independent modules to produce an
-executable, regardless of the license terms of these independent modules,
-and to copy and distribute the resulting executable under terms of  your
-choice, provided that you also meet, for each linked independent module,
-the  terms and conditions of the license of that module.? An independent
-module is a module which is not derived from or based on this  library.?
-If  you  modify  this  library,  you  may  extend this exception to your
-version of the library, but you are not obligated to do so.? If  you  do
-not wish to do so, delete this exception statement from your version.
-
\ No newline at end of file
diff --git a/plugins/cloud-azure/licenses/jaxb-NOTICE.txt b/plugins/cloud-azure/licenses/jaxb-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/jaxb-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/jaxb-api-2.2.2.jar.sha1 b/plugins/cloud-azure/licenses/jaxb-api-2.2.2.jar.sha1
deleted file mode 100644
index a145d47..0000000
--- a/plugins/cloud-azure/licenses/jaxb-api-2.2.2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-aeb3021ca93dde265796d82015beecdcff95bf09
diff --git a/plugins/cloud-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1 b/plugins/cloud-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1
deleted file mode 100644
index 79fe55d..0000000
--- a/plugins/cloud-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-56baae106392040a45a06d4a41099173425da1e6
diff --git a/plugins/cloud-azure/licenses/jersey-LICENSE.txt b/plugins/cloud-azure/licenses/jersey-LICENSE.txt
deleted file mode 100644
index 833a843..0000000
--- a/plugins/cloud-azure/licenses/jersey-LICENSE.txt
+++ /dev/null
@@ -1,274 +0,0 @@
-COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)Version 1.1
-
-1. Definitions.
-
-     1.1. "Contributor" means each individual or entity that creates or contributes to the creation of Modifications.
-
-     1.2. "Contributor Version" means the combination of the Original Software, prior Modifications used by a Contributor (if any), and the Modifications made by that particular Contributor.
-
-     1.3. "Covered Software" means (a) the Original Software, or (b) Modifications, or (c) the combination of files containing Original Software with files containing Modifications, in each case including portions thereof.
-
-     1.4. "Executable" means the Covered Software in any form other than Source Code.
-
-     1.5. "Initial Developer" means the individual or entity that first makes Original Software available under this License.
-
-     1.6. "Larger Work" means a work which combines Covered Software or portions thereof with code not governed by the terms of this License.
-
-     1.7. "License" means this document.
-
-     1.8. "Licensable" means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently acquired, any and all of the rights conveyed herein.
-
-     1.9. "Modifications" means the Source Code and Executable form of any of the following:
-
-     A. Any file that results from an addition to, deletion from or modification of the contents of a file containing Original Software or previous Modifications;
-
-     B. Any new file that contains any part of the Original Software or previous Modification; or
-
-     C. Any new file that is contributed or otherwise made available under the terms of this License.
-
-     1.10. "Original Software" means the Source Code and Executable form of computer software code that is originally released under this License.
-
-     1.11. "Patent Claims" means any patent claim(s), now owned or hereafter acquired, including without limitation, method, process, and apparatus claims, in any patent Licensable by grantor.
-
-     1.12. "Source Code" means (a) the common form of computer software code in which modifications are made and (b) associated documentation included in or with such code.
-
-     1.13. "You" (or "Your") means an individual or a legal entity exercising rights under, and complying with all of the terms of, this License. For legal entities, "You" includes any entity which controls, is controlled by, or is under common control with You. For purposes of this definition, "control" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.
-
-2. License Grants.
-
-     2.1. The Initial Developer Grant.
-
-     Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, the Initial Developer hereby grants You a world-wide, royalty-free, non-exclusive license:
-
-     (a) under intellectual property rights (other than patent or trademark) Licensable by Initial Developer, to use, reproduce, modify, display, perform, sublicense and distribute the Original Software (or portions thereof), with or without Modifications, and/or as part of a Larger Work; and
-
-     (b) under Patent Claims infringed by the making, using or selling of Original Software, to make, have made, use, practice, sell, and offer for sale, and/or otherwise dispose of the Original Software (or portions thereof).
-
-     (c) The licenses granted in Sections 2.1(a) and (b) are effective on the date Initial Developer first distributes or otherwise makes the Original Software available to a third party under the terms of this License.
-
-     (d) Notwithstanding Section 2.1(b) above, no patent license is granted: (1) for code that You delete from the Original Software, or (2) for infringements caused by: (i) the modification of the Original Software, or (ii) the combination of the Original Software with other software or devices.
-
-     2.2. Contributor Grant.
-
-     Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:
-
-     (a) under intellectual property rights (other than patent or trademark) Licensable by Contributor to use, reproduce, modify, display, perform, sublicense and distribute the Modifications created by such Contributor (or portions thereof), either on an unmodified basis, with other Modifications, as Covered Software and/or as part of a Larger Work; and
-
-     (b) under Patent Claims infringed by the making, using, or selling of Modifications made by that Contributor either alone and/or in combination with its Contributor Version (or portions of such combination), to make, use, sell, offer for sale, have made, and/or otherwise dispose of: (1) Modifications made by that Contributor (or portions thereof); and (2) the combination of Modifications made by that Contributor with its Contributor Version (or portions of such combination).
-
-     (c) The licenses granted in Sections 2.2(a) and 2.2(b) are effective on the date Contributor first distributes or otherwise makes the Modifications available to a third party.
-
-     (d) Notwithstanding Section 2.2(b) above, no patent license is granted: (1) for any code that Contributor has deleted from the Contributor Version; (2) for infringements caused by: (i) third party modifications of Contributor Version, or (ii) the combination of Modifications made by that Contributor with other software (except as part of the Contributor Version) or other devices; or (3) under Patent Claims infringed by Covered Software in the absence of Modifications made by that Contributor.
-
-3. Distribution Obligations.
-
-     3.1. Availability of Source Code.
-
-     Any Covered Software that You distribute or otherwise make available in Executable form must also be made available in Source Code form and that Source Code form must be distributed only under the terms of this License. You must include a copy of this License with every copy of the Source Code form of the Covered Software You distribute or otherwise make available. You must inform recipients of any such Covered Software in Executable form as to how they can obtain such Covered Software in Source Code form in a reasonable manner on or through a medium customarily used for software exchange.
-
-     3.2. Modifications.
-
-     The Modifications that You create or to which You contribute are governed by the terms of this License. You represent that You believe Your Modifications are Your original creation(s) and/or You have sufficient rights to grant the rights conveyed by this License.
-
-     3.3. Required Notices.
-
-     You must include a notice in each of Your Modifications that identifies You as the Contributor of the Modification. You may not remove or alter any copyright, patent or trademark notices contained within the Covered Software, or any notices of licensing or any descriptive text giving attribution to any Contributor or the Initial Developer.
-
-     3.4. Application of Additional Terms.
-
-     You may not offer or impose any terms on any Covered Software in Source Code form that alters or restricts the applicable version of this License or the recipients' rights hereunder. You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, you may do so only on Your own behalf, and not on behalf of the Initial Developer or any Contributor. You must make it absolutely clear that any such warranty, support, indemnity or liability obligation is offered by You alone, and You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of warranty, support, indemnity or liability terms You offer.
-
-     3.5. Distribution of Executable Versions.
-
-     You may distribute the Executable form of the Covered Software under the terms of this License or under the terms of a license of Your choice, which may contain terms different from this License, provided that You are in compliance with the terms of this License and that the license for the Executable form does not attempt to limit or alter the recipient's rights in the Source Code form from the rights set forth in this License. If You distribute the Covered Software in Executable form under a different license, You must make it absolutely clear that any terms which differ from this License are offered by You alone, not by the Initial Developer or Contributor. You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of any such terms You offer.
-
-     3.6. Larger Works.
-
-     You may create a Larger Work by combining Covered Software with other code not governed by the terms of this License and distribute the Larger Work as a single product. In such a case, You must make sure the requirements of this License are fulfilled for the Covered Software.
-
-4. Versions of the License.
-
-     4.1. New Versions.
-
-     Oracle is the initial license steward and may publish revised and/or new versions of this License from time to time. Each version will be given a distinguishing version number. Except as provided in Section 4.3, no one other than the license steward has the right to modify this License.
-
-     4.2. Effect of New Versions.
-
-     You may always continue to use, distribute or otherwise make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. If the Initial Developer includes a notice in the Original Software prohibiting it from being distributed or otherwise made available under any subsequent version of the License, You must distribute and make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. Otherwise, You may also choose to use, distribute or otherwise make the Covered Software available under the terms of any subsequent version of the License published by the license steward.
-
-     4.3. Modified Versions.
-
-     When You are an Initial Developer and You want to create a new license for Your Original Software, You may create and use a modified version of this License if You: (a) rename the license and remove any references to the name of the license steward (except to note that the license differs from this License); and (b) otherwise make it clear that the license contains terms which differ from this License.
-
-5. DISCLAIMER OF WARRANTY.
-
-     COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN "AS IS" BASIS, WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, WITHOUT LIMITATION, WARRANTIES THAT THE COVERED SOFTWARE IS FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE COVERED SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME THE COST OF ANY NECESSARY SERVICING, REPAIR OR CORRECTION. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.
-
-6. TERMINATION.
-
-     6.1. This License and the rights granted hereunder will terminate automatically if You fail to comply with terms herein and fail to cure such breach within 30 days of becoming aware of the breach. Provisions which, by their nature, must remain in effect beyond the termination of this License shall survive.
-
-     6.2. If You assert a patent infringement claim (excluding declaratory judgment actions) against Initial Developer or a Contributor (the Initial Developer or Contributor against whom You assert such claim is referred to as "Participant") alleging that the Participant Software (meaning the Contributor Version where the Participant is a Contributor or the Original Software where the Participant is the Initial Developer) directly or indirectly infringes any patent, then any and all rights granted directly or indirectly to You by such Participant, the Initial Developer (if the Initial Developer is not the Participant) and all Contributors under Sections 2.1 and/or 2.2 of this License shall, upon 60 days notice from Participant terminate prospectively and automatically at the expiration of such 60 day notice period, unless if within such 60 day period You withdraw Your claim with respect to the Participant Software against such Participant either unilaterally or pursuant to a written agreement with Participant.
-
-     6.3. If You assert a patent infringement claim against Participant alleging that the Participant Software directly or indirectly infringes any patent where such claim is resolved (such as by license or settlement) prior to the initiation of patent infringement litigation, then the reasonable value of the licenses granted by such Participant under Sections 2.1 or 2.2 shall be taken into account in determining the amount or value of any payment or license.
-
-     6.4. In the event of termination under Sections 6.1 or 6.2 above, all end user licenses that have been validly granted by You or any distributor hereunder prior to termination (excluding licenses granted to You by any distributor) shall survive termination.
-
-7. LIMITATION OF LIABILITY.
-
-     UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE, OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTY'S NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
-
-8. U.S. GOVERNMENT END USERS.
-
-     The Covered Software is a "commercial item," as that term is defined in 48 C.F.R. 2.101 (Oct. 1995), consisting of "commercial computer software" (as that term is defined at 48 C.F.R. ? 252.227-7014(a)(1)) and "commercial computer software documentation" as such terms are used in 48 C.F.R. 12.212 (Sept. 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government End Users acquire Covered Software with only those rights set forth herein. This U.S. Government Rights clause is in lieu of, and supersedes, any other FAR, DFAR, or other clause or provision that addresses Government rights in computer software under this License.
-
-9. MISCELLANEOUS.
-
-     This License represents the complete agreement concerning subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. This License shall be governed by the law of the jurisdiction specified in a notice contained within the Original Software (except to the extent applicable law, if any, provides otherwise), excluding such jurisdiction's conflict-of-law provisions. Any litigation relating to this License shall be subject to the jurisdiction of the courts located in the jurisdiction and venue specified in a notice contained within the Original Software, with the losing party responsible for costs, including, without limitation, court costs and reasonable attorneys' fees and expenses. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not apply to this License. You agree that You alone are responsible for compliance with the United States export administration regulations (and the export control laws and regulation of any other countries) when You use, distribute or otherwise make available any Covered Software.
-
-10. RESPONSIBILITY FOR CLAIMS.
-
-     As between Initial Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.
-
-----------
-NOTICE PURSUANT TO SECTION 9 OF THE COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)
-The code released under the CDDL shall be governed by the laws of the State of California (excluding conflict-of-law provisions). Any litigation relating to this License shall be subject to the jurisdiction of the Federal Courts of the Northern District of California and the state courts of the State of California, with venue lying in Santa Clara County, California.
-
-
-
-
-The GNU General Public License (GPL) Version 2, June 1991
-
-
-Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
-
-Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.
-
-Preamble
-
-The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too.
-
-When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things.
-
-To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it.
-
-For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.
-
-We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software.
-
-Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations.
-
-Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all.
-
-The precise terms and conditions for copying, distribution and modification follow.
-
-
-TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
-
-0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The "Program", below, refers to any such program or work, and a "work based on the Program" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term "modification".) Each licensee is addressed as "you".
-
-Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does.
-
-1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program.
-
-You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.
-
-2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:
-
-   a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change.
-
-   b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License.
-
-   c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.)
-
-These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.
-
-Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program.
-
-In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.
-
-3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following:
-
-   a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,
-
-   b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,
-
-   c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.)
-
-The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.
-
-If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code.
-
-4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.
-
-5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it.
-
-6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License.
-
-7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program.
-
-If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances.
-
-It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.
-
-This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.
-
-8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License.
-
-9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
-
-Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and "any later version", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation.
-
-10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.
-
-NO WARRANTY
-
-11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
-
-END OF TERMS AND CONDITIONS
-
-
-How to Apply These Terms to Your New Programs
-
-If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.
-
-To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the "copyright" line and a pointer to where the full notice is found.
-
-   One line to give the program's name and a brief idea of what it does.
-
-   Copyright (C)
-
-   This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.
-
-   This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
-
-   You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
-
-Also add information on how to contact you by electronic and paper mail.
-
-If the program is interactive, make it output a short notice like this when it starts in an interactive mode:
-
-   Gnomovision version 69, Copyright (C) year name of author
-   Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details.
-
-The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than `show w' and `show c'; they could even be mouse-clicks or menu items--whatever suits your program.
-
-You should also get your employer (if you work as a programmer) or your school, if any, to sign a "copyright disclaimer" for the program, if necessary. Here is a sample; alter the names:
-
-   Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.
-
-   signature of Ty Coon, 1 April 1989
-   Ty Coon, President of Vice
-
-This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License.
-
-
-"CLASSPATH" EXCEPTION TO THE GPL VERSION 2
-
-Certain source files distributed by Oracle are subject to the following clarification and special exception to the GPL Version 2, but only where Oracle has expressly included in the particular source file's header the words "Oracle designates this particular file as subject to the "Classpath" exception as provided by Oracle in the License file that accompanied this code."
-
-Linking this library statically or dynamically with other modules is making a combined work based on this library.  Thus, the terms and conditions of the GNU General Public License Version 2 cover the whole combination.
-
-As a special exception, the copyright holders of this library give you permission to link this library with independent modules to produce an executable, regardless of the license terms of these independent modules, and to copy and distribute the resulting executable under terms of your choice, provided that you also meet, for each linked independent module, the terms and conditions of the license of that module.  An independent module is a module which is not derived from or based on this library.  If you modify this library, you may extend this exception to your version of the library, but you are not obligated to do so.  If you do not wish to do so, delete this exception statement from your version.
diff --git a/plugins/cloud-azure/licenses/jersey-NOTICE.txt b/plugins/cloud-azure/licenses/jersey-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/jersey-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/jersey-client-1.13.jar.sha1 b/plugins/cloud-azure/licenses/jersey-client-1.13.jar.sha1
deleted file mode 100644
index 6244c69..0000000
--- a/plugins/cloud-azure/licenses/jersey-client-1.13.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0ec38c57a78940bf5f8f5971307ca89406849647
diff --git a/plugins/cloud-azure/licenses/jersey-core-1.13.jar.sha1 b/plugins/cloud-azure/licenses/jersey-core-1.13.jar.sha1
deleted file mode 100644
index ee2aa99..0000000
--- a/plugins/cloud-azure/licenses/jersey-core-1.13.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-4326a56dc6b2d67b7313905c353e1af225bb164f
diff --git a/plugins/cloud-azure/licenses/jersey-json-1.13.jar.sha1 b/plugins/cloud-azure/licenses/jersey-json-1.13.jar.sha1
deleted file mode 100644
index 266f2fc..0000000
--- a/plugins/cloud-azure/licenses/jersey-json-1.13.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f7346cce2c0e73afd39e2783c173ee134f79a0f9
diff --git a/plugins/cloud-azure/licenses/jettison-1.1.jar.sha1 b/plugins/cloud-azure/licenses/jettison-1.1.jar.sha1
deleted file mode 100644
index 53133f3..0000000
--- a/plugins/cloud-azure/licenses/jettison-1.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-1a01a2a1218fcf9faa2cc2a6ced025bdea687262
diff --git a/plugins/cloud-azure/licenses/jettison-LICENSE.txt b/plugins/cloud-azure/licenses/jettison-LICENSE.txt
deleted file mode 100644
index 6884c4d..0000000
--- a/plugins/cloud-azure/licenses/jettison-LICENSE.txt
+++ /dev/null
@@ -1,192 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   Copyright 2006 Envoi Solutions LLC
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
diff --git a/plugins/cloud-azure/licenses/jettison-NOTICE.txt b/plugins/cloud-azure/licenses/jettison-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/jettison-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/mail-1.4.5.jar.sha1 b/plugins/cloud-azure/licenses/mail-1.4.5.jar.sha1
deleted file mode 100644
index b79503e..0000000
--- a/plugins/cloud-azure/licenses/mail-1.4.5.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-85319c87280f30e1afc54c355f91f44741beac49
diff --git a/plugins/cloud-azure/licenses/mail-LICENSE.txt b/plugins/cloud-azure/licenses/mail-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/mail-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/mail-NOTICE.txt b/plugins/cloud-azure/licenses/mail-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/mail-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/stax-LICENSE.txt b/plugins/cloud-azure/licenses/stax-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-azure/licenses/stax-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/cloud-azure/licenses/stax-NOTICE.txt b/plugins/cloud-azure/licenses/stax-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/plugins/cloud-azure/licenses/stax-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/plugins/cloud-azure/licenses/stax-api-1.0-2.jar.sha1 b/plugins/cloud-azure/licenses/stax-api-1.0-2.jar.sha1
deleted file mode 100644
index fb00ad8..0000000
--- a/plugins/cloud-azure/licenses/stax-api-1.0-2.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d6337b0de8b25e53e81b922352fbea9f9f57ba0b
diff --git a/plugins/cloud-azure/pom.xml b/plugins/cloud-azure/pom.xml
deleted file mode 100644
index 08dcbd4..0000000
--- a/plugins/cloud-azure/pom.xml
+++ /dev/null
@@ -1,78 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!-- Licensed to Elasticsearch under one or more contributor
-license agreements. See the NOTICE file distributed with this work for additional
-information regarding copyright ownership. ElasticSearch licenses this file to you
-under the Apache License, Version 2.0 (the "License"); you may not use this
-file except in compliance with the License. You may obtain a copy of the
-License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
-applicable law or agreed to in writing, software distributed under the License
-is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied. See the License for the specific language
-governing permissions and limitations under the License. -->
-
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-    <modelVersion>4.0.0</modelVersion>
-
-    <parent>
-        <groupId>org.elasticsearch.plugin</groupId>
-        <artifactId>plugins</artifactId>
-        <version>3.0.0-SNAPSHOT</version>
-    </parent>
-
-    <artifactId>cloud-azure</artifactId>
-    <name>Plugin: Cloud: Azure</name>
-    <description>The Azure Cloud plugin allows to use Azure API for the unicast discovery mechanism and add Azure storage repositories.</description>
-
-    <properties>
-        <elasticsearch.plugin.classname>org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin</elasticsearch.plugin.classname>
-        <tests.jvms>1</tests.jvms>
-        <tests.rest.suite>cloud_azure</tests.rest.suite>
-        <tests.rest.load_packaged>false</tests.rest.load_packaged>
-        <!-- need -path because there is no resources dir... -->
-        <xlint.options>-Xlint:-path,-serial,-static,-unchecked</xlint.options>
-    </properties>
-
-    <dependencies>
-        <!-- Azure API -->
-        <dependency>
-            <groupId>com.microsoft.azure</groupId>
-            <artifactId>azure-storage</artifactId>
-            <version>2.0.0</version>
-        </dependency>
-        <dependency>
-            <groupId>com.microsoft.azure</groupId>
-            <artifactId>azure-management-compute</artifactId>
-            <version>0.7.0</version>
-            <exclusions>
-                <exclusion>
-                    <groupId>stax</groupId>
-                    <artifactId>stax-api</artifactId>
-                </exclusion>
-            </exclusions>
-        </dependency>
-        <dependency>
-            <groupId>com.microsoft.azure</groupId>
-            <artifactId>azure-management</artifactId>
-            <version>0.7.0</version>
-        </dependency>
-        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
-        <!-- TODO: remove this dependency when we will have a REST Test module -->
-        <dependency>
-            <groupId>org.apache.httpcomponents</groupId>
-            <artifactId>httpclient</artifactId>
-            <scope>compile</scope>
-        </dependency>
-    </dependencies>
-
-    <build>
-        <plugins>
-            <plugin>
-                <groupId>org.apache.maven.plugins</groupId>
-                <artifactId>maven-assembly-plugin</artifactId>
-            </plugin>
-        </plugins>
-    </build>
-
-</project>
diff --git a/plugins/cloud-azure/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java b/plugins/cloud-azure/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java
deleted file mode 100644
index 1e783fd..0000000
--- a/plugins/cloud-azure/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.store;
-
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.nio.channels.Channels;
-import java.nio.file.Files;
-import java.nio.file.StandardOpenOption;
-
-/**
- * This class is used to wrap an existing {@link org.apache.lucene.store.FSDirectory} so that
- * the new shard segment files will be opened for Read and Write access.
- * <p>
- * When storing index files on an SMB share like Azure File Service, opening the file for Read
- * access can save a lot of roundtrips to the storage server and thus offering better performance.
- */
-public final class SmbDirectoryWrapper extends FilterDirectory {
-
-    private final FSDirectory fsDirectory;
-
-    public SmbDirectoryWrapper(FSDirectory in) {
-        super(in);
-        fsDirectory = in;
-    }
-
-    @Override
-    public IndexOutput createOutput(String name, IOContext context) throws IOException {
-        fsDirectory.ensureOpen();
-        fsDirectory.ensureCanWrite(name);
-        return new SmbFSIndexOutput(name);
-    }
-
-    /**
-     * Copied from final inner class {@link org.apache.lucene.store.FSDirectory.FSIndexOutput}
-     */
-    final class SmbFSIndexOutput extends OutputStreamIndexOutput {
-        /**
-         * The maximum chunk size is 8192 bytes, because {@link java.io.FileOutputStream} mallocs
-         * a native buffer outside of stack if the write buffer size is larger.
-         */
-        static final int CHUNK_SIZE = 8192;
-
-        private final String name;
-
-        public SmbFSIndexOutput(String name) throws IOException {
-            super("SmbFSIndexOutput(path=\"" + fsDirectory.getDirectory().resolve(name) + "\")", new FilterOutputStream(Channels.newOutputStream(Files.newByteChannel(fsDirectory.getDirectory().resolve(name), StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING, StandardOpenOption.READ, StandardOpenOption.WRITE))) {
-                // This implementation ensures, that we never write more than CHUNK_SIZE bytes:
-                @Override
-                public void write(byte[] b, int offset, int length) throws IOException {
-                    while (length > 0) {
-                        final int chunk = Math.min(length, CHUNK_SIZE);
-                        out.write(b, offset, chunk);
-                        length -= chunk;
-                        offset += chunk;
-                    }
-                }
-            }, CHUNK_SIZE);
-            this.name = name;
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureModule.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureModule.java
deleted file mode 100644
index 29d260f..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureModule.java
+++ /dev/null
@@ -1,168 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.cloud.azure.management.AzureComputeService;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl;
-import org.elasticsearch.cloud.azure.management.AzureComputeSettingsFilter;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
-import org.elasticsearch.cloud.azure.storage.AzureStorageSettingsFilter;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.azure.AzureDiscovery;
-
-/**
- * Azure Module
- *
- * <ul>
- * <li>If needed this module will bind azure discovery service by default
- * to AzureComputeServiceImpl.</li>
- * <li>If needed this module will bind azure repository service by default
- * to AzureStorageServiceImpl.</li>
- * </ul>
- *
- * @see org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl
- * @see org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl
- */
-public class AzureModule extends AbstractModule {
-    protected final ESLogger logger;
-    private Settings settings;
-
-    // pkg private so it is settable by tests
-    static Class<? extends AzureComputeService> computeServiceImpl = AzureComputeServiceImpl.class;
-    static Class<? extends AzureStorageService> storageServiceImpl = AzureStorageServiceImpl.class;
-
-    public static Class<? extends AzureComputeService> getComputeServiceImpl() {
-        return computeServiceImpl;
-    }
-
-    public static Class<? extends AzureStorageService> getStorageServiceImpl() {
-        return storageServiceImpl;
-    }
-
-    @Inject
-    public AzureModule(Settings settings) {
-        this.settings = settings;
-        this.logger = Loggers.getLogger(getClass(), settings);
-    }
-
-    @Override
-    protected void configure() {
-        logger.debug("starting azure services");
-        bind(AzureStorageSettingsFilter.class).asEagerSingleton();
-        bind(AzureComputeSettingsFilter.class).asEagerSingleton();
-
-        // If we have set discovery to azure, let's start the azure compute service
-        if (isDiscoveryReady(settings, logger)) {
-            logger.debug("starting azure discovery service");
-            bind(AzureComputeService.class).to(computeServiceImpl).asEagerSingleton();
-        }
-
-        // If we have settings for azure repository, let's start the azure storage service
-        if (isSnapshotReady(settings, logger)) {
-            logger.debug("starting azure repository service");
-            bind(AzureStorageService.class).to(storageServiceImpl).asEagerSingleton();
-        }
-    }
-
-    /**
-     * Check if discovery is meant to start
-     * @return true if we can start discovery features
-     */
-    public static boolean isCloudReady(Settings settings) {
-        return (settings.getAsBoolean("cloud.enabled", true));
-    }
-
-    /**
-     * Check if discovery is meant to start
-     * @return true if we can start discovery features
-     */
-    public static boolean isDiscoveryReady(Settings settings, ESLogger logger) {
-        // Cloud services are disabled
-        if (!isCloudReady(settings)) {
-            logger.trace("cloud settings are disabled");
-            return false;
-        }
-
-        // User set discovery.type: azure
-        if (!AzureDiscovery.AZURE.equalsIgnoreCase(settings.get("discovery.type"))) {
-            logger.trace("discovery.type not set to {}", AzureDiscovery.AZURE);
-            return false;
-        }
-
-        if (isPropertyMissing(settings, Management.SUBSCRIPTION_ID) ||
-                isPropertyMissing(settings, Management.SERVICE_NAME) ||
-                isPropertyMissing(settings, Management.KEYSTORE_PATH) ||
-                isPropertyMissing(settings, Management.KEYSTORE_PASSWORD)
-            ) {
-            logger.debug("one or more azure discovery settings are missing. " +
-                            "Check elasticsearch.yml file. Should have [{}], [{}], [{}] and [{}].",
-                    Management.SUBSCRIPTION_ID,
-                    Management.SERVICE_NAME,
-                    Management.KEYSTORE_PATH,
-                    Management.KEYSTORE_PASSWORD);
-            return false;
-        }
-
-        logger.trace("all required properties for azure discovery are set!");
-
-        return true;
-    }
-
-    /**
-     * Check if we have repository azure settings available
-     * @return true if we can use snapshot and restore
-     */
-    public static boolean isSnapshotReady(Settings settings, ESLogger logger) {
-        // Cloud services are disabled
-        if (!isCloudReady(settings)) {
-            logger.trace("cloud settings are disabled");
-            return false;
-        }
-
-        if (isPropertyMissing(settings, Storage.ACCOUNT) ||
-                isPropertyMissing(settings, Storage.KEY)) {
-            logger.debug("azure repository is not set using [{}] and [{}] properties",
-                    Storage.ACCOUNT,
-                    Storage.KEY);
-            return false;
-        }
-
-        logger.trace("all required properties for azure repository are set!");
-
-        return true;
-   }
-
-    public static boolean isPropertyMissing(Settings settings, String name) throws ElasticsearchException {
-        if (!Strings.hasText(settings.get(name))) {
-            return true;
-        }
-        return false;
-    }
-
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
deleted file mode 100644
index 487997d..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-public class AzureServiceDisableException extends IllegalStateException {
-    public AzureServiceDisableException(String msg) {
-        super(msg);
-    }
-
-    public AzureServiceDisableException(String msg, Throwable cause) {
-        super(msg, cause);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
deleted file mode 100644
index 4bd4f1d..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-public class AzureServiceRemoteException extends IllegalStateException {
-    public AzureServiceRemoteException(String msg) {
-        super(msg);
-    }
-
-    public AzureServiceRemoteException(String msg, Throwable cause) {
-        super(msg, cause);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
deleted file mode 100644
index a7c980c..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.blobstore;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.blobstore.BlobMetaData;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.support.AbstractLegacyBlobContainer;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.repositories.RepositoryException;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.net.HttpURLConnection;
-import java.net.URISyntaxException;
-import java.util.Map;
-
-/**
- *
- */
-public class AzureBlobContainer extends AbstractLegacyBlobContainer {
-
-    protected final ESLogger logger = Loggers.getLogger(AzureBlobContainer.class);
-    protected final AzureBlobStore blobStore;
-
-    protected final String keyPath;
-    protected final String repositoryName;
-
-    public AzureBlobContainer(String repositoryName, BlobPath path, AzureBlobStore blobStore) {
-        super(path);
-        this.blobStore = blobStore;
-        String keyPath = path.buildAsString("/");
-        if (!keyPath.isEmpty()) {
-            keyPath = keyPath + "/";
-        }
-        this.keyPath = keyPath;
-        this.repositoryName = repositoryName;
-    }
-
-    @Override
-    public boolean blobExists(String blobName) {
-        try {
-            return blobStore.client().blobExists(blobStore.container(), buildKey(blobName));
-        } catch (URISyntaxException | StorageException e) {
-            logger.warn("can not access [{}] in container {{}}: {}", blobName, blobStore.container(), e.getMessage());
-        }
-        return false;
-    }
-
-    @Override
-    public InputStream openInput(String blobName) throws IOException {
-        try {
-            return blobStore.client().getInputStream(blobStore.container(), buildKey(blobName));
-        } catch (StorageException e) {
-            if (e.getHttpStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {
-                throw new FileNotFoundException(e.getMessage());
-            }
-            throw new IOException(e);
-        } catch (URISyntaxException e) {
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public OutputStream createOutput(String blobName) throws IOException {
-        try {
-            return new AzureOutputStream(blobStore.client().getOutputStream(blobStore.container(), buildKey(blobName)));
-        } catch (StorageException e) {
-            if (e.getHttpStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {
-                throw new FileNotFoundException(e.getMessage());
-            }
-            throw new IOException(e);
-        } catch (URISyntaxException e) {
-            throw new IOException(e);
-        } catch (IllegalArgumentException e) {
-            throw new RepositoryException(repositoryName, e.getMessage());
-        }
-    }
-
-    @Override
-    public void deleteBlob(String blobName) throws IOException {
-        try {
-            blobStore.client().deleteBlob(blobStore.container(), buildKey(blobName));
-        } catch (URISyntaxException | StorageException e) {
-            logger.warn("can not access [{}] in container {{}}: {}", blobName, blobStore.container(), e.getMessage());
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(@Nullable String prefix) throws IOException {
-
-        try {
-            return blobStore.client().listBlobsByPrefix(blobStore.container(), keyPath, prefix);
-        } catch (URISyntaxException | StorageException e) {
-            logger.warn("can not access [{}] in container {{}}: {}", prefix, blobStore.container(), e.getMessage());
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public void move(String sourceBlobName, String targetBlobName) throws IOException {
-        try {
-            String source = keyPath + sourceBlobName;
-            String target = keyPath + targetBlobName;
-
-            logger.debug("moving blob [{}] to [{}] in container {{}}", source, target, blobStore.container());
-
-            blobStore.client().moveBlob(blobStore.container(), source, target);
-        } catch (URISyntaxException e) {
-            logger.warn("can not move blob [{}] to [{}] in container {{}}: {}", sourceBlobName, targetBlobName, blobStore.container(), e.getMessage());
-            throw new IOException(e);
-        } catch (StorageException e) {
-            logger.warn("can not move blob [{}] to [{}] in container {{}}: {}", sourceBlobName, targetBlobName, blobStore.container(), e.getMessage());
-            throw new IOException(e);
-        }
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobs() throws IOException {
-        return listBlobsByPrefix(null);
-    }
-
-    protected String buildKey(String blobName) {
-        return keyPath + (blobName == null ? "" : blobName);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java
deleted file mode 100644
index 6edcae7..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.blobstore;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.common.blobstore.BlobContainer;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.BlobStore;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.repositories.RepositoryName;
-import org.elasticsearch.repositories.RepositorySettings;
-
-import java.net.URISyntaxException;
-
-import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.CONTAINER;
-import static org.elasticsearch.repositories.azure.AzureRepository.CONTAINER_DEFAULT;
-
-/**
- *
- */
-public class AzureBlobStore extends AbstractComponent implements BlobStore {
-
-    private final AzureStorageService client;
-
-    private final String container;
-    private final String repositoryName;
-
-    @Inject
-    public AzureBlobStore(RepositoryName name, Settings settings, RepositorySettings repositorySettings,
-                          AzureStorageService client) throws URISyntaxException, StorageException {
-        super(settings);
-        this.client = client;
-        this.container = repositorySettings.settings().get("container", settings.get(CONTAINER, CONTAINER_DEFAULT));
-        this.repositoryName = name.getName();
-    }
-
-    @Override
-    public String toString() {
-        return container;
-    }
-
-    public AzureStorageService client() {
-        return client;
-    }
-
-    public String container() {
-        return container;
-    }
-
-    @Override
-    public BlobContainer blobContainer(BlobPath path) {
-        return new AzureBlobContainer(repositoryName, path, this);
-    }
-
-    @Override
-    public void delete(BlobPath path) {
-        String keyPath = path.buildAsString("/");
-        if (!keyPath.isEmpty()) {
-            keyPath = keyPath + "/";
-        }
-
-        try {
-            client.deleteFiles(container, keyPath);
-        } catch (URISyntaxException | StorageException e) {
-            logger.warn("can not remove [{}] in container {{}}: {}", keyPath, container, e.getMessage());
-        }
-    }
-
-    @Override
-    public void close() {
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java
deleted file mode 100644
index 6a95eeb..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.blobstore;
-
-import java.io.IOException;
-import java.io.OutputStream;
-
-public class AzureOutputStream extends OutputStream {
-
-    private final OutputStream blobOutputStream;
-
-    public AzureOutputStream(OutputStream blobOutputStream) {
-        this.blobOutputStream = blobOutputStream;
-    }
-
-    @Override
-    public void write(int b) throws IOException {
-        blobOutputStream.write(b);
-    }
-
-    @Override
-    public void close() throws IOException {
-        try {
-            blobOutputStream.close();
-        } catch (IOException e) {
-            // Azure is sending a "java.io.IOException: Stream is already closed."
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
deleted file mode 100644
index c79a745..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.management;
-
-import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
-
-/**
- *
- */
-public interface AzureComputeService {
-
-    static public final class Management {
-        public static final String API_IMPLEMENTATION = "cloud.azure.management.api.impl";
-
-        public static final String SUBSCRIPTION_ID = "cloud.azure.management.subscription.id";
-        public static final String SERVICE_NAME = "cloud.azure.management.cloud.service.name";
-
-        // Keystore settings
-        public static final String KEYSTORE_PATH = "cloud.azure.management.keystore.path";
-        public static final String KEYSTORE_PASSWORD = "cloud.azure.management.keystore.password";
-        public static final String KEYSTORE_TYPE = "cloud.azure.management.keystore.type";
-    }
-
-    static public final class Discovery {
-        public static final String REFRESH = "discovery.azure.refresh_interval";
-
-        public static final String HOST_TYPE = "discovery.azure.host.type";
-        public static final String ENDPOINT_NAME = "discovery.azure.endpoint.name";
-        public static final String DEPLOYMENT_NAME = "discovery.azure.deployment.name";
-        public static final String DEPLOYMENT_SLOT = "discovery.azure.deployment.slot";
-    }
-    public HostedServiceGetDetailedResponse getServiceDetails();
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java
deleted file mode 100644
index 26406e3..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.management;
-
-import com.microsoft.windowsazure.Configuration;
-import com.microsoft.windowsazure.core.utils.KeyStoreType;
-import com.microsoft.windowsazure.management.compute.ComputeManagementClient;
-import com.microsoft.windowsazure.management.compute.ComputeManagementService;
-import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
-import com.microsoft.windowsazure.management.configuration.ManagementConfiguration;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.cloud.azure.AzureServiceDisableException;
-import org.elasticsearch.cloud.azure.AzureServiceRemoteException;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
-
-import static org.elasticsearch.cloud.azure.management.AzureComputeService.Management.*;
-
-/**
- *
- */
-public class AzureComputeServiceImpl extends AbstractLifecycleComponent<AzureComputeServiceImpl>
-    implements AzureComputeService {
-
-    static final class Azure {
-        private static final String ENDPOINT = "https://management.core.windows.net/";
-    }
-
-    private final ComputeManagementClient computeManagementClient;
-    private final String serviceName;
-
-    @Inject
-    public AzureComputeServiceImpl(Settings settings) {
-        super(settings);
-        String subscriptionId = settings.get(SUBSCRIPTION_ID);
-
-        serviceName = settings.get(Management.SERVICE_NAME);
-        String keystorePath = settings.get(KEYSTORE_PATH);
-        String keystorePassword = settings.get(KEYSTORE_PASSWORD);
-        String strKeyStoreType = settings.get(KEYSTORE_TYPE, KeyStoreType.pkcs12.name());
-        KeyStoreType tmpKeyStoreType = KeyStoreType.pkcs12;
-        try {
-            tmpKeyStoreType = KeyStoreType.fromString(strKeyStoreType);
-        } catch (Exception e) {
-            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", KEYSTORE_TYPE,
-                    strKeyStoreType, KeyStoreType.pkcs12.name());
-        }
-        KeyStoreType keystoreType = tmpKeyStoreType;
-
-        // Check that we have all needed properties
-        Configuration configuration;
-        try {
-            configuration = ManagementConfiguration.configure(new URI(Azure.ENDPOINT),
-                    subscriptionId, keystorePath, keystorePassword, keystoreType);
-        } catch (IOException|URISyntaxException e) {
-            logger.error("can not start azure client: {}", e.getMessage());
-            computeManagementClient = null;
-            return;
-        }
-        logger.trace("creating new Azure client for [{}], [{}]", subscriptionId, serviceName);
-        computeManagementClient = ComputeManagementService.create(configuration);
-    }
-
-    @Override
-    public HostedServiceGetDetailedResponse getServiceDetails() {
-        if (computeManagementClient == null) {
-            // Azure plugin is disabled
-            throw new AzureServiceDisableException("azure plugin is disabled.");
-        }
-
-        try {
-            return computeManagementClient.getHostedServicesOperations().getDetailed(serviceName);
-        } catch (Exception e) {
-            throw new AzureServiceRemoteException("can not get list of azure nodes", e);
-        }
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-        if (computeManagementClient != null) {
-            try {
-                computeManagementClient.close();
-            } catch (IOException e) {
-                logger.error("error while closing Azure client", e);
-            }
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
deleted file mode 100644
index c4a1837..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.management;
-
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-
-import static org.elasticsearch.cloud.azure.management.AzureComputeService.Management.*;
-
-public class AzureComputeSettingsFilter extends AbstractComponent {
-
-    @Inject
-    public AzureComputeSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
-        super(settings);
-        // Cloud management API settings we need to hide
-        settingsFilter.addFilter(KEYSTORE_PATH);
-        settingsFilter.addFilter(KEYSTORE_PASSWORD);
-        settingsFilter.addFilter(KEYSTORE_TYPE);
-        settingsFilter.addFilter(SUBSCRIPTION_ID);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
deleted file mode 100644
index c9b48ae..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.storage;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.common.blobstore.BlobMetaData;
-
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.net.URISyntaxException;
-import java.util.Map;
-
-/**
- * Azure Storage Service interface
- * @see AzureStorageServiceImpl for Azure REST API implementation
- */
-public interface AzureStorageService {
-    static public final class Storage {
-        public static final String API_IMPLEMENTATION = "cloud.azure.storage.api.impl";
-        public static final String ACCOUNT = "cloud.azure.storage.account";
-        public static final String KEY = "cloud.azure.storage.key";
-        public static final String CONTAINER = "repositories.azure.container";
-        public static final String BASE_PATH = "repositories.azure.base_path";
-        public static final String CHUNK_SIZE = "repositories.azure.chunk_size";
-        public static final String COMPRESS = "repositories.azure.compress";
-    }
-
-    boolean doesContainerExist(String container);
-
-    void removeContainer(String container) throws URISyntaxException, StorageException;
-
-    void createContainer(String container) throws URISyntaxException, StorageException;
-
-    void deleteFiles(String container, String path) throws URISyntaxException, StorageException;
-
-    boolean blobExists(String container, String blob) throws URISyntaxException, StorageException;
-
-    void deleteBlob(String container, String blob) throws URISyntaxException, StorageException;
-
-    InputStream getInputStream(String container, String blob) throws URISyntaxException, StorageException;
-
-    OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException;
-
-    Map<String,BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) throws URISyntaxException, StorageException;
-
-    void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException;
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java
deleted file mode 100644
index f38b232..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.storage;
-
-import com.microsoft.azure.storage.CloudStorageAccount;
-import com.microsoft.azure.storage.StorageException;
-import com.microsoft.azure.storage.blob.*;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.blobstore.BlobMetaData;
-import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
-import org.elasticsearch.common.collect.MapBuilder;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.repositories.RepositoryException;
-
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.util.Map;
-
-import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.*;
-
-/**
- *
- */
-public class AzureStorageServiceImpl extends AbstractLifecycleComponent<AzureStorageServiceImpl>
-    implements AzureStorageService {
-
-    private final String account;
-    private final String key;
-    private final String blob;
-
-    private CloudBlobClient client;
-
-    @Inject
-    public AzureStorageServiceImpl(Settings settings) {
-        super(settings);
-        // We try to load storage API settings from `cloud.azure.`
-        account = settings.get(ACCOUNT);
-        key = settings.get(KEY);
-        blob = "https://" + account + ".blob.core.windows.net/";
-
-        try {
-            if (account != null) {
-                logger.trace("creating new Azure storage client using account [{}], key [{}], blob [{}]", account, key, blob);
-
-                String storageConnectionString =
-                        "DefaultEndpointsProtocol=https;"
-                                + "AccountName="+ account +";"
-                                + "AccountKey=" + key;
-
-                // Retrieve storage account from connection-string.
-                CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString);
-
-                // Create the blob client.
-                client = storageAccount.createCloudBlobClient();
-            }
-        } catch (Exception e) {
-            // Can not start Azure Storage Client
-            logger.error("can not start azure storage client: {}", e.getMessage());
-        }
-    }
-
-    @Override
-    public boolean doesContainerExist(String container) {
-        try {
-            CloudBlobContainer blob_container = client.getContainerReference(container);
-            return blob_container.exists();
-        } catch (Exception e) {
-            logger.error("can not access container [{}]", container);
-        }
-        return false;
-    }
-
-    @Override
-    public void removeContainer(String container) throws URISyntaxException, StorageException {
-        CloudBlobContainer blob_container = client.getContainerReference(container);
-        // TODO Should we set some timeout and retry options?
-        /*
-        BlobRequestOptions options = new BlobRequestOptions();
-        options.setTimeoutIntervalInMs(1000);
-        options.setRetryPolicyFactory(new RetryNoRetry());
-        blob_container.deleteIfExists(options, null);
-        */
-        logger.trace("removing container [{}]", container);
-        blob_container.deleteIfExists();
-    }
-
-    @Override
-    public void createContainer(String container) throws URISyntaxException, StorageException {
-        try {
-            CloudBlobContainer blob_container = client.getContainerReference(container);
-            logger.trace("creating container [{}]", container);
-            blob_container.createIfNotExists();
-        } catch (IllegalArgumentException e) {
-            logger.trace("fails creating container [{}]", container, e.getMessage());
-            throw new RepositoryException(container, e.getMessage());
-        }
-    }
-
-    @Override
-    public void deleteFiles(String container, String path) throws URISyntaxException, StorageException {
-        logger.trace("delete files container [{}], path [{}]", container, path);
-
-        // Container name must be lower case.
-        CloudBlobContainer blob_container = client.getContainerReference(container);
-        if (blob_container.exists()) {
-            for (ListBlobItem blobItem : blob_container.listBlobs(path)) {
-                logger.trace("removing blob [{}]", blobItem.getUri());
-                deleteBlob(container, blobItem.getUri().toString());
-            }
-        }
-    }
-
-    @Override
-    public boolean blobExists(String container, String blob) throws URISyntaxException, StorageException {
-        // Container name must be lower case.
-        CloudBlobContainer blob_container = client.getContainerReference(container);
-        if (blob_container.exists()) {
-            CloudBlockBlob azureBlob = blob_container.getBlockBlobReference(blob);
-            return azureBlob.exists();
-        }
-
-        return false;
-    }
-
-    @Override
-    public void deleteBlob(String container, String blob) throws URISyntaxException, StorageException {
-        logger.trace("delete blob for container [{}], blob [{}]", container, blob);
-
-        // Container name must be lower case.
-        CloudBlobContainer blob_container = client.getContainerReference(container);
-        if (blob_container.exists()) {
-            logger.trace("container [{}]: blob [{}] found. removing.", container, blob);
-            CloudBlockBlob azureBlob = blob_container.getBlockBlobReference(blob);
-            azureBlob.delete();
-        }
-    }
-
-    @Override
-    public InputStream getInputStream(String container, String blob) throws URISyntaxException, StorageException {
-        logger.trace("reading container [{}], blob [{}]", container, blob);
-        return client.getContainerReference(container).getBlockBlobReference(blob).openInputStream();
-    }
-
-    @Override
-    public OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException {
-        logger.trace("writing container [{}], blob [{}]", container, blob);
-        return client.getContainerReference(container).getBlockBlobReference(blob).openOutputStream();
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) throws URISyntaxException, StorageException {
-        logger.debug("listing container [{}], keyPath [{}], prefix [{}]", container, keyPath, prefix);
-        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
-
-        CloudBlobContainer blobContainer = client.getContainerReference(container);
-        if (blobContainer.exists()) {
-            for (ListBlobItem blobItem : blobContainer.listBlobs(keyPath + (prefix == null ? "" : prefix))) {
-                URI uri = blobItem.getUri();
-                logger.trace("blob url [{}]", uri);
-
-                // uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/
-                // this requires 1 + container.length() + 1, with each 1 corresponding to one of the /
-                String blobPath = uri.getPath().substring(1 + container.length() + 1);
-
-                CloudBlockBlob blob = blobContainer.getBlockBlobReference(blobPath);
-
-                // fetch the blob attributes from Azure (getBlockBlobReference does not do this)
-                // this is needed to retrieve the blob length (among other metadata) from Azure Storage
-                blob.downloadAttributes();
-
-                BlobProperties properties = blob.getProperties();
-                String name = blobPath.substring(keyPath.length());
-                logger.trace("blob url [{}], name [{}], size [{}]", uri, name, properties.getLength());
-                blobsBuilder.put(name, new PlainBlobMetaData(name, properties.getLength()));
-            }
-        }
-
-        return blobsBuilder.immutableMap();
-    }
-
-    @Override
-    public void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException {
-        logger.debug("moveBlob container [{}], sourceBlob [{}], targetBlob [{}]", container, sourceBlob, targetBlob);
-        CloudBlobContainer blob_container = client.getContainerReference(container);
-        CloudBlockBlob blobSource = blob_container.getBlockBlobReference(sourceBlob);
-        if (blobSource.exists()) {
-            CloudBlockBlob blobTarget = blob_container.getBlockBlobReference(targetBlob);
-            blobTarget.startCopyFromBlob(blobSource);
-            blobSource.delete();
-            logger.debug("moveBlob container [{}], sourceBlob [{}], targetBlob [{}] -> done", container, sourceBlob, targetBlob);
-        }
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-        logger.debug("starting azure storage client instance");
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-        logger.debug("stopping azure storage client instance");
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
deleted file mode 100644
index da3aa8c..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.storage;
-
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-
-import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.*;
-
-public class AzureStorageSettingsFilter extends AbstractComponent {
-
-    @Inject
-    public AzureStorageSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
-        super(settings);
-        // Cloud storage API settings needed to be hidden
-        settingsFilter.addFilter(ACCOUNT);
-        settingsFilter.addFilter(KEY);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
deleted file mode 100755
index ff8b43f..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
-import org.elasticsearch.cluster.settings.DynamicSettings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.discovery.zen.ping.ZenPingService;
-import org.elasticsearch.node.settings.NodeSettingsService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-/**
- *
- */
-public class AzureDiscovery extends ZenDiscovery {
-
-    public static final String AZURE = "azure";
-
-    @Inject
-    public AzureDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                          ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
-                          DiscoverySettings discoverySettings, ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
deleted file mode 100644
index 4b20dfe..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
+++ /dev/null
@@ -1,274 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import com.microsoft.windowsazure.management.compute.models.*;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cloud.azure.AzureServiceDisableException;
-import org.elasticsearch.cloud.azure.AzureServiceRemoteException;
-import org.elasticsearch.cloud.azure.management.AzureComputeService;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.TransportAddress;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
-import org.elasticsearch.transport.TransportService;
-
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-import java.util.ArrayList;
-import java.util.Locale;
-import java.util.List;
-
-/**
- *
- */
-public class AzureUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
-
-    public static enum HostType {
-        PRIVATE_IP("private_ip"),
-        PUBLIC_IP("public_ip");
-
-        private String type ;
-
-        private HostType(String type) {
-            this.type = type ;
-        }
-
-        public static HostType fromString(String type) {
-            for (HostType hostType : values()) {
-                if (hostType.type.equalsIgnoreCase(type)) {
-                    return hostType;
-                }
-            }
-            return null;
-        }
-    }
-
-    public static enum Deployment {
-        PRODUCTION("production", DeploymentSlot.Production),
-        STAGING("staging", DeploymentSlot.Staging);
-
-        private String deployment;
-        private DeploymentSlot slot;
-
-        private Deployment(String deployment, DeploymentSlot slot) {
-            this.deployment = deployment;
-            this.slot = slot;
-        }
-
-        public static Deployment fromString(String string) {
-            for (Deployment deployment : values()) {
-                if (deployment.deployment.equalsIgnoreCase(string)) {
-                    return deployment;
-                }
-            }
-            return null;
-        }
-    }
-
-    private final AzureComputeService azureComputeService;
-    private TransportService transportService;
-    private NetworkService networkService;
-    private final Version version;
-
-    private final TimeValue refreshInterval;
-    private long lastRefresh;
-    private List<DiscoveryNode> cachedDiscoNodes;
-    private final HostType hostType;
-    private final String publicEndpointName;
-    private final String deploymentName;
-    private final DeploymentSlot deploymentSlot;
-
-    @Inject
-    public AzureUnicastHostsProvider(Settings settings, AzureComputeService azureComputeService,
-                                   TransportService transportService,
-                                   NetworkService networkService,
-                                   Version version) {
-        super(settings);
-        this.azureComputeService = azureComputeService;
-        this.transportService = transportService;
-        this.networkService = networkService;
-        this.version = version;
-
-        this.refreshInterval = settings.getAsTime(Discovery.REFRESH, TimeValue.timeValueSeconds(0));
-
-        String strHostType = settings.get(Discovery.HOST_TYPE, HostType.PRIVATE_IP.name()).toUpperCase(Locale.ROOT);
-        HostType tmpHostType = HostType.fromString(strHostType);
-        if (tmpHostType == null) {
-            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", Discovery.HOST_TYPE,
-                    strHostType, HostType.PRIVATE_IP.name().toLowerCase(Locale.ROOT));
-            tmpHostType = HostType.PRIVATE_IP;
-        }
-        this.hostType = tmpHostType;
-        this.publicEndpointName = settings.get(Discovery.ENDPOINT_NAME, "elasticsearch");
-
-        // Deployment name could be set with discovery.azure.deployment.name
-        // Default to cloud.azure.management.cloud.service.name
-        this.deploymentName = settings.get(Discovery.DEPLOYMENT_NAME);
-
-        // Reading deployment_slot
-        String strDeployment = settings.get(Discovery.DEPLOYMENT_SLOT, Deployment.PRODUCTION.deployment);
-        Deployment tmpDeployment = Deployment.fromString(strDeployment);
-        if (tmpDeployment == null) {
-            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", Discovery.DEPLOYMENT_SLOT, strDeployment,
-                    Deployment.PRODUCTION.deployment);
-            tmpDeployment = Deployment.PRODUCTION;
-        }
-        this.deploymentSlot = tmpDeployment.slot;
-    }
-
-    /**
-     * We build the list of Nodes from Azure Management API
-     * Information can be cached using `cloud.azure.refresh_interval` property if needed.
-     * Setting `cloud.azure.refresh_interval` to `-1` will cause infinite caching.
-     * Setting `cloud.azure.refresh_interval` to `0` will disable caching (default).
-     */
-    @Override
-    public List<DiscoveryNode> buildDynamicNodes() {
-        if (refreshInterval.millis() != 0) {
-            if (cachedDiscoNodes != null &&
-                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
-                logger.trace("using cache to retrieve node list");
-                return cachedDiscoNodes;
-            }
-            lastRefresh = System.currentTimeMillis();
-        }
-        logger.debug("start building nodes list using Azure API");
-
-        cachedDiscoNodes = new ArrayList<>();
-
-        HostedServiceGetDetailedResponse detailed;
-        try {
-            detailed = azureComputeService.getServiceDetails();
-        } catch (AzureServiceDisableException e) {
-            logger.debug("Azure discovery service has been disabled. Returning empty list of nodes.");
-            return cachedDiscoNodes;
-        } catch (AzureServiceRemoteException e) {
-            // We got a remote exception
-            logger.warn("can not get list of azure nodes: [{}]. Returning empty list of nodes.", e.getMessage());
-            logger.trace("AzureServiceRemoteException caught", e);
-            return cachedDiscoNodes;
-        }
-
-        InetAddress ipAddress = null;
-        try {
-            ipAddress = networkService.resolvePublishHostAddress(null);
-            logger.trace("ip of current node: [{}]", ipAddress);
-        } catch (IOException e) {
-            // We can't find the publish host address... Hmmm. Too bad :-(
-            logger.trace("exception while finding ip", e);
-        }
-
-        for (HostedServiceGetDetailedResponse.Deployment deployment : detailed.getDeployments()) {
-            // We check the deployment slot
-            if (deployment.getDeploymentSlot() != deploymentSlot) {
-                logger.debug("current deployment slot [{}] for [{}] is different from [{}]. skipping...",
-                        deployment.getDeploymentSlot(), deployment.getName(), deploymentSlot);
-                continue;
-            }
-
-            // If provided, we check the deployment name
-            if (deploymentName != null && !deploymentName.equals(deployment.getName())) {
-                logger.debug("current deployment name [{}] different from [{}]. skipping...",
-                        deployment.getName(), deploymentName);
-                continue;
-            }
-
-            // We check current deployment status
-            if (deployment.getStatus() != DeploymentStatus.Starting &&
-                    deployment.getStatus() != DeploymentStatus.Deploying &&
-                    deployment.getStatus() != DeploymentStatus.Running) {
-                logger.debug("[{}] status is [{}]. skipping...",
-                        deployment.getName(), deployment.getStatus());
-                continue;
-            }
-
-            // In other case, it should be the right deployment so we can add it to the list of instances
-
-            for (RoleInstance instance : deployment.getRoleInstances()) {
-                String networkAddress = null;
-                // Let's detect if we want to use public or private IP
-                switch (hostType) {
-                    case PRIVATE_IP:
-                        InetAddress privateIp = instance.getIPAddress();
-
-                        if (privateIp != null) {
-                            if (privateIp.equals(ipAddress)) {
-                                logger.trace("adding ourselves {}", NetworkAddress.format(ipAddress));
-                            }
-                            networkAddress = NetworkAddress.formatAddress(privateIp);
-                        } else {
-                            logger.trace("no private ip provided. ignoring [{}]...", instance.getInstanceName());
-                        }
-                        break;
-                    case PUBLIC_IP:
-                        for (InstanceEndpoint endpoint : instance.getInstanceEndpoints()) {
-                            if (!publicEndpointName.equals(endpoint.getName())) {
-                                logger.trace("ignoring endpoint [{}] as different than [{}]",
-                                        endpoint.getName(), publicEndpointName);
-                                continue;
-                            }
-
-                            networkAddress = NetworkAddress.formatAddress(new InetSocketAddress(endpoint.getVirtualIPAddress(), endpoint.getPort()));
-                        }
-
-                        if (networkAddress == null) {
-                            logger.trace("no public ip provided. ignoring [{}]...", instance.getInstanceName());
-                        }
-                        break;
-                    default:
-                        // This could never happen!
-                        logger.warn("undefined host_type [{}]. Please check your settings.", hostType);
-                        return cachedDiscoNodes;
-                }
-
-                if (networkAddress == null) {
-                    // We have a bad parameter here or not enough information from azure
-                    logger.warn("no network address found. ignoring [{}]...", instance.getInstanceName());
-                    continue;
-                }
-
-                try {
-                    // we only limit to 1 port per address, makes no sense to ping 100 ports
-                    TransportAddress[] addresses = transportService.addressesFromString(networkAddress, 1);
-                    for (TransportAddress address : addresses) {
-                        logger.trace("adding {}, transport_address {}", networkAddress, address);
-                        cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceName(), address,
-                            version.minimumCompatibilityVersion()));
-                    }
-                } catch (Exception e) {
-                    logger.warn("can not convert [{}] to transport address. skipping. [{}]", networkAddress, e.getMessage());
-                }
-            }
-        }
-
-        logger.debug("{} node(s) added", cachedDiscoNodes.size());
-
-        return cachedDiscoNodes;
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java
deleted file mode 100644
index 47efbd0..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store.smbmmapfs;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.MMapDirectory;
-import org.apache.lucene.store.SmbDirectoryWrapper;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.index.store.FsDirectoryService;
-import org.elasticsearch.index.store.IndexStore;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbMmapFsDirectoryService extends FsDirectoryService {
-
-    @Inject
-    public SmbMmapFsDirectoryService(@IndexSettings Settings indexSettings, IndexStore indexStore, ShardPath path) {
-        super(indexSettings, indexStore, path);
-    }
-
-    @Override
-    protected Directory newFSDirectory(Path location, LockFactory lockFactory) throws IOException {
-        logger.debug("wrapping MMapDirectory for SMB");
-        return new SmbDirectoryWrapper(new MMapDirectory(location, buildLockFactory(indexSettings)));
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java
deleted file mode 100644
index 0422975..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store.smbmmapfs;
-
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.index.store.DirectoryService;
-import org.elasticsearch.index.store.IndexStore;
-import org.elasticsearch.indices.store.IndicesStore;
-
-public class SmbMmapFsIndexStore extends IndexStore {
-
-    @Inject
-    public SmbMmapFsIndexStore(Index index, @IndexSettings Settings indexSettings,
-                               IndexSettingsService indexSettingsService, IndicesStore indicesStore) {
-        super(index, indexSettings, indexSettingsService, indicesStore);
-    }
-
-    @Override
-    public Class<? extends DirectoryService> shardDirectory() {
-        return SmbMmapFsDirectoryService.class;
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java
deleted file mode 100644
index 7866090..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store.smbsimplefs;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockFactory;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.store.SmbDirectoryWrapper;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.index.store.FsDirectoryService;
-import org.elasticsearch.index.store.IndexStore;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbSimpleFsDirectoryService extends FsDirectoryService {
-
-    @Inject
-    public SmbSimpleFsDirectoryService(@IndexSettings Settings indexSettings, IndexStore indexStore, ShardPath path) {
-        super(indexSettings, indexStore, path);
-    }
-
-    @Override
-    protected Directory newFSDirectory(Path location, LockFactory lockFactory) throws IOException {
-        logger.debug("wrapping SimpleFSDirectory for SMB");
-        return new SmbDirectoryWrapper(new SimpleFSDirectory(location, lockFactory));
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java
deleted file mode 100644
index 4813344..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store.smbsimplefs;
-
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.index.store.DirectoryService;
-import org.elasticsearch.index.store.IndexStore;
-import org.elasticsearch.indices.store.IndicesStore;
-
-public class SmbSimpleFsIndexStore extends IndexStore {
-
-    @Inject
-    public SmbSimpleFsIndexStore(Index index, @IndexSettings Settings indexSettings,
-                                 IndexSettingsService indexSettingsService, IndicesStore indicesStore) {
-        super(index, indexSettings, indexSettingsService, indicesStore);
-    }
-
-    @Override
-    public Class<? extends DirectoryService> shardDirectory() {
-        return SmbSimpleFsDirectoryService.class;
-    }
-}
-
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java
deleted file mode 100644
index fa896fd..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/plugin/cloud/azure/CloudAzurePlugin.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugin.cloud.azure;
-
-import org.elasticsearch.cloud.azure.AzureModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.DiscoveryModule;
-import org.elasticsearch.discovery.azure.AzureDiscovery;
-import org.elasticsearch.discovery.azure.AzureUnicastHostsProvider;
-import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
-import org.elasticsearch.index.store.IndexStoreModule;
-import org.elasticsearch.index.store.smbmmapfs.SmbMmapFsIndexStore;
-import org.elasticsearch.index.store.smbsimplefs.SmbSimpleFsIndexStore;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.repositories.RepositoriesModule;
-import org.elasticsearch.repositories.azure.AzureRepository;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import static org.elasticsearch.cloud.azure.AzureModule.isSnapshotReady;
-
-/**
- *
- */
-public class CloudAzurePlugin extends Plugin {
-
-    private final Settings settings;
-    protected final ESLogger logger = Loggers.getLogger(CloudAzurePlugin.class);
-
-    public CloudAzurePlugin(Settings settings) {
-        this.settings = settings;
-        logger.trace("starting azure plugin...");
-    }
-
-    @Override
-    public String name() {
-        return "cloud-azure";
-    }
-
-    @Override
-    public String description() {
-        return "Cloud Azure Plugin";
-    }
-
-    @Override
-    public Collection<Module> nodeModules() {
-        List<Module> modules = new ArrayList<>();
-        if (AzureModule.isCloudReady(settings)) {
-            modules.add(new AzureModule(settings));
-        }
-        return modules;
-    }
-
-    public void onModule(RepositoriesModule module) {
-        if (isSnapshotReady(settings, logger)) {
-            module.registerRepository(AzureRepository.TYPE, AzureRepository.class, BlobStoreIndexShardRepository.class);
-        }
-    }
-
-    public void onModule(DiscoveryModule discoveryModule) {
-        if (AzureModule.isDiscoveryReady(settings, logger)) {
-            discoveryModule.addDiscoveryType("azure", AzureDiscovery.class);
-            discoveryModule.addUnicastHostProvider(AzureUnicastHostsProvider.class);
-        }
-    }
-
-    public void onModule(IndexStoreModule storeModule) {
-        storeModule.addIndexStore("smb_mmap_fs", SmbMmapFsIndexStore.class);
-        storeModule.addIndexStore("smb_simple_fs", SmbSimpleFsIndexStore.class);
-    }
-}
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java
deleted file mode 100644
index 5d9cbd9..0000000
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java
+++ /dev/null
@@ -1,174 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.azure;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.cloud.azure.blobstore.AzureBlobStore;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.metadata.SnapshotId;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.blobstore.BlobPath;
-import org.elasticsearch.common.blobstore.BlobStore;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.repositories.RepositoryName;
-import org.elasticsearch.repositories.RepositorySettings;
-import org.elasticsearch.repositories.RepositoryVerificationException;
-import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
-import org.elasticsearch.snapshots.SnapshotCreationException;
-
-import java.io.IOException;
-import java.net.URISyntaxException;
-import java.util.List;
-
-/**
- * Azure file system implementation of the BlobStoreRepository
- * <p/>
- * Azure file system repository supports the following settings:
- * <dl>
- * <dt>{@code container}</dt><dd>Azure container name. Defaults to elasticsearch-snapshots</dd>
- * <dt>{@code base_path}</dt><dd>Specifies the path within bucket to repository data. Defaults to root directory.</dd>
- * <dt>{@code chunk_size}</dt><dd>Large file can be divided into chunks. This parameter specifies the chunk size. Defaults to 64mb.</dd>
- * <dt>{@code compress}</dt><dd>If set to true metadata files will be stored compressed. Defaults to false.</dd>
- * </dl>
- */
-public class AzureRepository extends BlobStoreRepository {
-
-    public final static String TYPE = "azure";
-    public final static String CONTAINER_DEFAULT = "elasticsearch-snapshots";
-
-    static public final class Repository {
-        public static final String CONTAINER = "container";
-        public static final String CHUNK_SIZE = "chunk_size";
-        public static final String COMPRESS = "compress";
-        public static final String BASE_PATH = "base_path";
-    }
-
-    private final AzureBlobStore blobStore;
-
-    private final BlobPath basePath;
-
-    private ByteSizeValue chunkSize;
-
-    private boolean compress;
-
-    @Inject
-    public AzureRepository(RepositoryName name, RepositorySettings repositorySettings,
-                           IndexShardRepository indexShardRepository,
-                           AzureBlobStore azureBlobStore) throws IOException, URISyntaxException, StorageException {
-        super(name.getName(), repositorySettings, indexShardRepository);
-
-        String container = repositorySettings.settings().get(Repository.CONTAINER,
-                settings.get(Storage.CONTAINER, CONTAINER_DEFAULT));
-
-        this.blobStore = azureBlobStore;
-        this.chunkSize = repositorySettings.settings().getAsBytesSize(Repository.CHUNK_SIZE,
-                settings.getAsBytesSize(Storage.CHUNK_SIZE, new ByteSizeValue(64, ByteSizeUnit.MB)));
-
-        if (this.chunkSize.getMb() > 64) {
-            logger.warn("azure repository does not support yet size > 64mb. Fall back to 64mb.");
-            this.chunkSize = new ByteSizeValue(64, ByteSizeUnit.MB);
-        }
-
-        this.compress = repositorySettings.settings().getAsBoolean(Repository.COMPRESS,
-                settings.getAsBoolean(Storage.COMPRESS, false));
-        String basePath = repositorySettings.settings().get(Repository.BASE_PATH, null);
-
-        if (Strings.hasLength(basePath)) {
-            // Remove starting / if any
-            basePath = Strings.trimLeadingCharacter(basePath, '/');
-            BlobPath path = new BlobPath();
-            for(String elem : Strings.splitStringToArray(basePath, '/')) {
-                path = path.add(elem);
-            }
-            this.basePath = path;
-        } else {
-            this.basePath = BlobPath.cleanPath();
-        }
-        logger.debug("using container [{}], chunk_size [{}], compress [{}], base_path [{}]",
-                container, chunkSize, compress, basePath);
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected BlobStore blobStore() {
-        return blobStore;
-    }
-
-    @Override
-    protected BlobPath basePath() {
-        return basePath;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected boolean isCompress() {
-        return compress;
-    }
-
-    /**
-     * {@inheritDoc}
-     */
-    @Override
-    protected ByteSizeValue chunkSize() {
-        return chunkSize;
-    }
-
-    @Override
-    public void initializeSnapshot(SnapshotId snapshotId, List<String> indices, MetaData metaData) {
-        try {
-            if (!blobStore.client().doesContainerExist(blobStore.container())) {
-                logger.debug("container [{}] does not exist. Creating...", blobStore.container());
-                blobStore.client().createContainer(blobStore.container());
-            }
-            super.initializeSnapshot(snapshotId, indices, metaData);
-        } catch (StorageException e) {
-            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
-            throw new SnapshotCreationException(snapshotId, e);
-        } catch (URISyntaxException e) {
-            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
-            throw new SnapshotCreationException(snapshotId, e);
-        }
-    }
-
-    @Override
-    public String startVerification() {
-        try {
-            if (!blobStore.client().doesContainerExist(blobStore.container())) {
-                logger.debug("container [{}] does not exist. Creating...", blobStore.container());
-                blobStore.client().createContainer(blobStore.container());
-            }
-            return super.startVerification();
-        } catch (StorageException e) {
-            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
-            throw new RepositoryVerificationException(repositoryName, "can not initialize container " + blobStore.container(), e);
-        } catch (URISyntaxException e) {
-            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
-            throw new RepositoryVerificationException(repositoryName, "can not initialize container " + blobStore.container(), e);
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java
deleted file mode 100644
index 4c6c230..0000000
--- a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.store;
-
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TimeUnits;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
-
-import com.carrotsearch.randomizedtesting.annotations.Listeners;
-import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
-
-/**
- * Extends Lucene's BaseDirectoryTestCase with ES test behavior.
- */
-@Listeners({
-  ReproduceInfoPrinter.class
-})
-@TimeoutSuite(millis = TimeUnits.HOUR)
-@LuceneTestCase.SuppressReproduceLine
-@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
-public abstract class ESBaseDirectoryTestCase extends BaseDirectoryTestCase {
-    static {
-        BootstrapForTesting.ensureInitialized();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
deleted file mode 100644
index 43c61d8..0000000
--- a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.store;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbMMapDirectoryTests extends ESBaseDirectoryTestCase {
-
-    @Override
-    protected Directory getDirectory(Path file) throws IOException {
-        return new SmbDirectoryWrapper(new MMapDirectory(file));
-    }
-}
\ No newline at end of file
diff --git a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java b/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
deleted file mode 100644
index 208eb6c..0000000
--- a/plugins/cloud-azure/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.store;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class SmbSimpleFSDirectoryTests extends ESBaseDirectoryTestCase {
-
-    @Override
-    protected Directory getDirectory(Path file) throws IOException {
-        return new SmbDirectoryWrapper(new SimpleFSDirectory(file));
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java
deleted file mode 100644
index 389f2a4..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/azure/itest/AzureSimpleTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.azure.itest;
-
-import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
-import org.elasticsearch.cloud.azure.AbstractAzureTestCase;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-/**
- * This test needs Azure to run and -Dtests.thirdparty=true to be set
- * and -Des.config=/path/to/elasticsearch.yml
- * @see AbstractAzureTestCase
- */
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 1,
-        numClientNodes = 0,
-        transportClientRatio = 0.0)
-public class AzureSimpleTests extends AbstractAzureTestCase {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                // For now we let the user who runs tests to define if he wants or not to run discovery tests
-                // by setting in elasticsearch.yml: discovery.type: azure
-                // .put("discovery.type", "azure")
-                .build();
-    }
-
-    @Test
-    public void one_node_should_run() {
-        // Do nothing... Just start :-)
-        // but let's check that we have at least 1 node (local node)
-        ClusterStateResponse clusterState = client().admin().cluster().prepareState().execute().actionGet();
-
-        assertThat(clusterState.getState().getNodes().getSize(), Matchers.greaterThanOrEqualTo(1));
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
deleted file mode 100644
index dd69e08..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-public abstract class AbstractAzureComputeServiceTestCase extends ESIntegTestCase {
-
-    private Class<? extends Plugin> mockPlugin;
-
-    public AbstractAzureComputeServiceTestCase(Class<? extends Plugin> mockPlugin) {
-        // We want to inject the Azure API Mock
-        this.mockPlugin = mockPlugin;
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-            .put(super.nodeSettings(nodeOrdinal))
-            .put("discovery.type", "azure")
-                // We need the network to make the mock working
-            .put("node.mode", "network");
-
-        // We add a fake subscription_id to start mock compute service
-        builder.put(Management.SUBSCRIPTION_ID, "fake")
-            .put(Discovery.REFRESH, "5s")
-            .put(Management.KEYSTORE_PATH, "dummy")
-            .put(Management.KEYSTORE_PASSWORD, "dummy")
-            .put(Management.SERVICE_NAME, "dummy");
-        return builder.build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class, mockPlugin);
-    }
-
-    protected void checkNumberOfNodes(int expected) {
-        NodesInfoResponse nodeInfos = client().admin().cluster().prepareNodesInfo().execute().actionGet();
-        assertNotNull(nodeInfos);
-        assertNotNull(nodeInfos.getNodes());
-        assertEquals(expected, nodeInfos.getNodes().length);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
deleted file mode 100644
index 96fe8eb..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.repositories.RepositoryMissingException;
-import org.elasticsearch.test.store.MockFSDirectoryService;
-import org.junit.After;
-import org.junit.Before;
-
-import java.net.URISyntaxException;
-import java.util.Collection;
-
-public abstract class AbstractAzureRepositoryServiceTestCase extends AbstractAzureTestCase {
-
-    public static class TestPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "mock-stoarge-service";
-        }
-        @Override
-        public String description() {
-            return "plugs in a mock storage service for testing";
-        }
-        public void onModule(AzureModule azureModule) {
-            azureModule.storageServiceImpl = AzureStorageServiceMock.class;
-        }
-    }
-
-    protected String basePath;
-    private Class<? extends AzureStorageService> mock;
-
-    public AbstractAzureRepositoryServiceTestCase(String basePath) {
-        this.basePath = basePath;
-    }
-
-    /**
-     * Deletes repositories, supports wildcard notation.
-     */
-    public static void wipeRepositories(String... repositories) {
-        // if nothing is provided, delete all
-        if (repositories.length == 0) {
-            repositories = new String[]{"*"};
-        }
-        for (String repository : repositories) {
-            try {
-                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
-            } catch (RepositoryMissingException ex) {
-                // ignore
-            }
-        }
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-                .put(Storage.API_IMPLEMENTATION, mock)
-                .put(Storage.CONTAINER, "snapshots");
-
-        // We use sometime deprecated settings in tests
-        builder.put(Storage.ACCOUNT, "mock_azure_account")
-                .put(Storage.KEY, "mock_azure_key");
-
-        return builder.build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class, TestPlugin.class);
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
-                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
-                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-    }
-
-    @Before @After
-    public final void wipe() throws StorageException, URISyntaxException {
-        wipeRepositories();
-        cleanRepositoryFiles(basePath);
-    }
-
-    /**
-     * Purge the test container
-     */
-    public void cleanRepositoryFiles(String path) throws StorageException, URISyntaxException {
-        String container = internalCluster().getInstance(Settings.class).get("repositories.azure.container");
-        logger.info("--> remove blobs in container [{}]", container);
-        AzureStorageService client = internalCluster().getInstance(AzureStorageService.class);
-        client.deleteFiles(container, path);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
deleted file mode 100644
index 39e6d1b..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsException;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
-
-import java.util.Collection;
-
-/**
- * Base class for Azure tests that require credentials.
- * <p>
- * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
- * in order to run these tests.
- */
-@ThirdParty
-public abstract class AbstractAzureTestCase extends ESIntegTestCase {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put(readSettingsFromFile())
-                .build();
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class);
-    }
-
-    protected Settings readSettingsFromFile() {
-        Settings.Builder settings = Settings.builder();
-        settings.put("path.home", createTempDir());
-
-        // if explicit, just load it and don't load from env
-        try {
-            if (Strings.hasText(System.getProperty("tests.config"))) {
-                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
-            } else {
-                throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
-            }
-        } catch (SettingsException exception) {
-          throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
-        }
-        return settings.build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
deleted file mode 100644
index e303315..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import com.microsoft.windowsazure.management.compute.models.DeploymentSlot;
-import com.microsoft.windowsazure.management.compute.models.DeploymentStatus;
-import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
-import com.microsoft.windowsazure.management.compute.models.InstanceEndpoint;
-import com.microsoft.windowsazure.management.compute.models.RoleInstance;
-import org.elasticsearch.cloud.azure.management.AzureComputeServiceAbstractMock;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.CollectionUtils;
-import org.elasticsearch.plugins.Plugin;
-
-import java.net.InetAddress;
-
-/**
- * Mock Azure API with a single started node
- */
-public class AzureComputeServiceSimpleMock extends AzureComputeServiceAbstractMock {
-
-    public static class TestPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "mock-compute-service";
-        }
-        @Override
-        public String description() {
-            return "plugs in a mock compute service for testing";
-        }
-        public void onModule(AzureModule azureModule) {
-            azureModule.computeServiceImpl = AzureComputeServiceSimpleMock.class;
-        }
-    }
-
-    @Inject
-    public AzureComputeServiceSimpleMock(Settings settings) {
-        super(settings);
-    }
-
-    @Override
-    public HostedServiceGetDetailedResponse getServiceDetails() {
-        HostedServiceGetDetailedResponse response = new HostedServiceGetDetailedResponse();
-        HostedServiceGetDetailedResponse.Deployment deployment = new HostedServiceGetDetailedResponse.Deployment();
-
-        // Fake the deployment
-        deployment.setName("dummy");
-        deployment.setDeploymentSlot(DeploymentSlot.Production);
-        deployment.setStatus(DeploymentStatus.Running);
-
-        // Fake an instance
-        RoleInstance instance = new RoleInstance();
-        instance.setInstanceName("dummy1");
-
-        // Fake the private IP
-        instance.setIPAddress(InetAddress.getLoopbackAddress());
-
-        // Fake the public IP
-        InstanceEndpoint endpoint = new InstanceEndpoint();
-        endpoint.setName("elasticsearch");
-        endpoint.setVirtualIPAddress(InetAddress.getLoopbackAddress());
-        endpoint.setPort(9400);
-        instance.setInstanceEndpoints(CollectionUtils.newSingletonArrayList(endpoint));
-
-        deployment.setRoleInstances(CollectionUtils.newSingletonArrayList(instance));
-        response.setDeployments(CollectionUtils.newSingletonArrayList(deployment));
-
-        return response;
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java
deleted file mode 100644
index ee8bf35..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import com.microsoft.windowsazure.management.compute.models.DeploymentSlot;
-import com.microsoft.windowsazure.management.compute.models.DeploymentStatus;
-import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
-import com.microsoft.windowsazure.management.compute.models.InstanceEndpoint;
-import com.microsoft.windowsazure.management.compute.models.RoleInstance;
-import org.elasticsearch.cloud.azure.management.AzureComputeServiceAbstractMock;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugins.Plugin;
-
-import java.net.InetAddress;
-import java.util.ArrayList;
-import java.util.Arrays;
-
-import static org.elasticsearch.common.util.CollectionUtils.newSingletonArrayList;
-
-
-/**
- * Mock Azure API with two started nodes
- */
-public class AzureComputeServiceTwoNodesMock extends AzureComputeServiceAbstractMock {
-    public static class TestPlugin extends Plugin {
-        @Override
-        public String name() {
-            return "mock-compute-service";
-        }
-        @Override
-        public String description() {
-            return "plugs in a mock compute service for testing";
-        }
-        public void onModule(AzureModule azureModule) {
-            azureModule.computeServiceImpl = AzureComputeServiceTwoNodesMock.class;
-        }
-    }
-
-    NetworkService networkService;
-
-    @Inject
-    protected AzureComputeServiceTwoNodesMock(Settings settings, NetworkService networkService) {
-        super(settings);
-        this.networkService = networkService;
-    }
-
-    @Override
-    public HostedServiceGetDetailedResponse getServiceDetails() {
-        HostedServiceGetDetailedResponse response = new HostedServiceGetDetailedResponse();
-        HostedServiceGetDetailedResponse.Deployment deployment = new HostedServiceGetDetailedResponse.Deployment();
-
-        // Fake the deployment
-        deployment.setName("dummy");
-        deployment.setDeploymentSlot(DeploymentSlot.Production);
-        deployment.setStatus(DeploymentStatus.Running);
-
-        // Fake a first instance
-        RoleInstance instance1 = new RoleInstance();
-        instance1.setInstanceName("dummy1");
-
-        // Fake the private IP
-        instance1.setIPAddress(InetAddress.getLoopbackAddress());
-
-        // Fake the public IP
-        InstanceEndpoint endpoint1 = new InstanceEndpoint();
-        endpoint1.setName("elasticsearch");
-        endpoint1.setVirtualIPAddress(InetAddress.getLoopbackAddress());
-        endpoint1.setPort(9400);
-        instance1.setInstanceEndpoints(newSingletonArrayList(endpoint1));
-
-        // Fake a first instance
-        RoleInstance instance2 = new RoleInstance();
-        instance2.setInstanceName("dummy1");
-
-        // Fake the private IP
-        instance2.setIPAddress(InetAddress.getLoopbackAddress());
-
-        // Fake the public IP
-        InstanceEndpoint endpoint2 = new InstanceEndpoint();
-        endpoint2.setName("elasticsearch");
-        endpoint2.setVirtualIPAddress(InetAddress.getLoopbackAddress());
-        endpoint2.setPort(9401);
-        instance2.setInstanceEndpoints(newSingletonArrayList(endpoint2));
-
-        deployment.setRoleInstances(new ArrayList<>(Arrays.asList(instance1, instance2)));
-
-        response.setDeployments(newSingletonArrayList(deployment));
-
-        return response;
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/CloudAzureRestIT.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/CloudAzureRestIT.java
deleted file mode 100644
index be74381..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/CloudAzureRestIT.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class CloudAzureRestIT extends ESRestTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class);
-    }
-
-    public CloudAzureRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java
deleted file mode 100644
index c11060a..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.management;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.settings.Settings;
-
-/**
- *
- */
-public abstract class AzureComputeServiceAbstractMock extends AbstractLifecycleComponent<AzureComputeServiceAbstractMock>
-    implements AzureComputeService {
-
-    protected AzureComputeServiceAbstractMock(Settings settings) {
-        super(settings);
-        logger.debug("starting Azure Mock [{}]", this.getClass().getSimpleName());
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-        logger.debug("starting Azure Api Mock");
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-        logger.debug("stopping Azure Api Mock");
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
deleted file mode 100644
index 8fe1923..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cloud.azure.storage;
-
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.blobstore.BlobMetaData;
-import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
-import org.elasticsearch.common.collect.MapBuilder;
-import org.elasticsearch.common.component.AbstractLifecycleComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.net.URISyntaxException;
-import java.util.Locale;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-
-/**
- * In memory storage for unit tests
- */
-public class AzureStorageServiceMock extends AbstractLifecycleComponent<AzureStorageServiceMock>
-        implements AzureStorageService {
-
-    protected Map<String, ByteArrayOutputStream> blobs = new ConcurrentHashMap<>();
-
-    @Inject
-    public AzureStorageServiceMock(Settings settings) {
-        super(settings);
-    }
-
-    @Override
-    public boolean doesContainerExist(String container) {
-        return true;
-    }
-
-    @Override
-    public void removeContainer(String container) {
-    }
-
-    @Override
-    public void createContainer(String container) {
-    }
-
-    @Override
-    public void deleteFiles(String container, String path) {
-    }
-
-    @Override
-    public boolean blobExists(String container, String blob) {
-        return blobs.containsKey(blob);
-    }
-
-    @Override
-    public void deleteBlob(String container, String blob) {
-        blobs.remove(blob);
-    }
-
-    @Override
-    public InputStream getInputStream(String container, String blob) {
-        return new ByteArrayInputStream(blobs.get(blob).toByteArray());
-    }
-
-    @Override
-    public OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException {
-        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
-        blobs.put(blob, outputStream);
-        return outputStream;
-    }
-
-    @Override
-    public Map<String, BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) {
-        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
-        for (String blobName : blobs.keySet()) {
-            if (startsWithIgnoreCase(blobName, prefix)) {
-                blobsBuilder.put(blobName, new PlainBlobMetaData(blobName, blobs.get(blobName).size()));
-            }
-        }
-        return blobsBuilder.immutableMap();
-    }
-
-    @Override
-    public void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException {
-        for (String blobName : blobs.keySet()) {
-            if (endsWithIgnoreCase(blobName, sourceBlob)) {
-                ByteArrayOutputStream outputStream = blobs.get(blobName);
-                blobs.put(blobName.replace(sourceBlob, targetBlob), outputStream);
-                blobs.remove(blobName);
-            }
-        }
-    }
-
-    @Override
-    protected void doStart() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doStop() throws ElasticsearchException {
-    }
-
-    @Override
-    protected void doClose() throws ElasticsearchException {
-    }
-
-    /**
-     * Test if the given String starts with the specified prefix,
-     * ignoring upper/lower case.
-     *
-     * @param str    the String to check
-     * @param prefix the prefix to look for
-     * @see java.lang.String#startsWith
-     */
-    public static boolean startsWithIgnoreCase(String str, String prefix) {
-        if (str == null || prefix == null) {
-            return false;
-        }
-        if (str.startsWith(prefix)) {
-            return true;
-        }
-        if (str.length() < prefix.length()) {
-            return false;
-        }
-        String lcStr = str.substring(0, prefix.length()).toLowerCase(Locale.ROOT);
-        String lcPrefix = prefix.toLowerCase(Locale.ROOT);
-        return lcStr.equals(lcPrefix);
-    }
-
-    /**
-     * Test if the given String ends with the specified suffix,
-     * ignoring upper/lower case.
-     *
-     * @param str    the String to check
-     * @param suffix the suffix to look for
-     * @see java.lang.String#startsWith
-     */
-    public static boolean endsWithIgnoreCase(String str, String suffix) {
-        if (str == null || suffix == null) {
-            return false;
-        }
-        if (str.endsWith(suffix)) {
-            return true;
-        }
-        if (str.length() < suffix.length()) {
-            return false;
-        }
-        String lcStr = str.substring(0, suffix.length()).toLowerCase(Locale.ROOT);
-        String lcPrefix = suffix.toLowerCase(Locale.ROOT);
-        return lcStr.equals(lcPrefix);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
deleted file mode 100644
index 8465136..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
-import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.discovery.MasterNotDiscoveredException;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- * Reported issue in #15
- * (https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/15)
- */
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-azure/issues/89")
-public class AzureMinimumMasterNodesTests extends AbstractAzureComputeServiceTestCase {
-
-    public AzureMinimumMasterNodesTests() {
-        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.settingsBuilder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("discovery.zen.minimum_master_nodes", 2)
-                // Make the test run faster
-                .put("discovery.zen.join.timeout", "50ms")
-                .put("discovery.zen.ping.timeout", "10ms")
-                .put("discovery.initial_state_timeout", "100ms");
-        return builder.build();
-    }
-
-    @Test
-    public void simpleOnlyMasterNodeElection() throws IOException {
-        logger.info("--> start data node / non master node");
-        internalCluster().startNode();
-        try {
-            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("100ms").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
-            fail("should not be able to find master");
-        } catch (MasterNotDiscoveredException e) {
-            // all is well, no master elected
-        }
-        logger.info("--> start another node");
-        internalCluster().startNode();
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> stop master node");
-        internalCluster().stopCurrentMasterNode();
-
-        try {
-            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
-            fail("should not be able to find master");
-        } catch (MasterNotDiscoveredException e) {
-            // all is well, no master elected
-        }
-
-        logger.info("--> start another node");
-        internalCluster().startNode();
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
deleted file mode 100644
index 74daf1a..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.cloud.azure.AzureComputeServiceSimpleMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
-
-    public AzureSimpleTests() {
-        super(AzureComputeServiceSimpleMock.TestPlugin.class);
-    }
-
-    @Test
-    public void one_node_should_run_using_private_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-
-    @Test
-    public void one_node_should_run_using_public_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-
-    @Test
-    public void one_node_should_run_using_wrong_settings() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "do_not_exist");
-
-        logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
deleted file mode 100644
index dbccc4a..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.discovery.azure;
-
-import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
-import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
-import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
-        numDataNodes = 0,
-        transportClientRatio = 0.0,
-        numClientNodes = 0)
-public class AzureTwoStartedNodesTests extends AbstractAzureComputeServiceTestCase {
-
-    public AzureTwoStartedNodesTests() {
-        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
-    }
-
-    @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
-    public void two_nodes_should_run_using_private_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
-
-        logger.info("--> start first node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> start another node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 2 nodes as part of the cluster, let's test that
-        checkNumberOfNodes(2);
-    }
-
-    @Test
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
-    public void two_nodes_should_run_using_public_ip() {
-        Settings.Builder settings = Settings.settingsBuilder()
-                .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
-
-        logger.info("--> start first node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        logger.info("--> start another node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 2 nodes as part of the cluster, let's test that
-        checkNumberOfNodes(2);
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
deleted file mode 100644
index 12eacc4..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.Collection;
-
-import static org.hamcrest.Matchers.is;
-
-abstract public class AbstractAzureFsTestCase extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(CloudAzurePlugin.class);
-    }
-
-    @Test
-    public void testAzureFs() {
-        // Create an index and index some documents
-        createIndex("test");
-        long nbDocs = randomIntBetween(10, 1000);
-        for (long i = 0; i < nbDocs; i++) {
-            index("test", "doc", "" + i, "foo", "bar");
-        }
-        refresh();
-        SearchResponse response = client().prepareSearch("test").get();
-        assertThat(response.getHits().totalHits(), is(nbDocs));
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
deleted file mode 100644
index 0f9e874..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.common.settings.Settings;
-
-
-public class SmbMMapFsTests extends AbstractAzureFsTestCase {
-
-    @Override
-    public Settings indexSettings() {
-        return Settings.builder()
-                .put(super.indexSettings())
-                .put("index.store.type", "smb_mmap_fs")
-                .build();
-    }
-
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
deleted file mode 100644
index ed15718..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.store;
-
-import org.elasticsearch.common.settings.Settings;
-
-
-public class SmbSimpleFsTests extends AbstractAzureFsTestCase {
-    @Override
-    public Settings indexSettings() {
-        return Settings.builder()
-                .put(super.indexSettings())
-                .put("index.store.type", "smb_simple_fs")
-                .build();
-    }
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
deleted file mode 100644
index 37a21a3..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.azure;
-
-
-import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cloud.azure.AbstractAzureRepositoryServiceTestCase;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.snapshots.SnapshotState;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 1,
-        numClientNodes = 0,
-        transportClientRatio = 0.0)
-public class AzureSnapshotRestoreServiceTests extends AbstractAzureRepositoryServiceTestCase {
-
-    public AzureSnapshotRestoreServiceTests() {
-        super("/snapshot-test/repo-" + randomInt());
-    }
-
-    @Test
-    public void testSimpleWorkflow() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", basePath);
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put("base_path", basePath)
-                        .put("chunk_size", randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-}
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
deleted file mode 100644
index 6f8d0d0..0000000
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
+++ /dev/null
@@ -1,535 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.repositories.azure;
-
-
-import com.carrotsearch.randomizedtesting.RandomizedTest;
-import com.microsoft.azure.storage.StorageException;
-import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.ClusterAdminClient;
-import org.elasticsearch.cloud.azure.AbstractAzureTestCase;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
-import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.repositories.RepositoryMissingException;
-import org.elasticsearch.repositories.RepositoryVerificationException;
-import org.elasticsearch.repositories.azure.AzureRepository.Repository;
-import org.elasticsearch.snapshots.SnapshotMissingException;
-import org.elasticsearch.snapshots.SnapshotState;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.store.MockFSDirectoryService;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.net.URISyntaxException;
-import java.util.Locale;
-import java.util.concurrent.TimeUnit;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-
-/**
- * This test needs Azure to run and -Dtests.thirdparty=true to be set
- * and -Dtests.config=/path/to/elasticsearch.yml
- * @see AbstractAzureTestCase
- */
-@ESIntegTestCase.ClusterScope(
-        scope = ESIntegTestCase.Scope.SUITE,
-        numDataNodes = 1,
-        transportClientRatio = 0.0)
-public class AzureSnapshotRestoreTests extends AbstractAzureTestCase {
-
-    private String getRepositoryPath() {
-        String testName = "it-".concat(Strings.toUnderscoreCase(getTestName()).replaceAll("_", "-"));
-        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
-    }
-
-    private static String getContainerName() {
-        String testName = "snapshot-itest-".concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT));
-        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
-    }
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
-                // In snapshot tests, we explicitly disable cloud discovery
-                .put("discovery.type", "local")
-                .build();
-    }
-
-    @Override
-    public Settings indexSettings() {
-        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
-        // checksum file to be written twice during restore operation
-        return Settings.builder().put(super.indexSettings())
-                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
-                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
-                .build();
-    }
-
-    @Before @After
-    public final void wipeAzureRepositories() throws StorageException, URISyntaxException {
-        wipeRepositories();
-        cleanRepositoryFiles(
-            getContainerName(),
-            getContainerName().concat("-1"),
-            getContainerName().concat("-2"));
-    }
-
-    @Test
-    public void testSimpleWorkflow() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("--> delete some data");
-        for (int i = 0; i < 50; i++) {
-            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 50; i < 100; i++) {
-            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
-        }
-        for (int i = 0; i < 100; i += 2) {
-            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        logger.info("--> close indices");
-        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
-
-        logger.info("--> restore all indices from the snapshot");
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion");
-        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-    }
-
-    /**
-     * For issue #51: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/51
-     */
-    @Test
-    public void testMultipleSnapshots() throws URISyntaxException, StorageException {
-        final String indexName = "test-idx-1";
-        final String typeName = "doc";
-        final String repositoryName = "test-repo";
-        final String snapshot1Name = "test-snap-1";
-        final String snapshot2Name = "test-snap-2";
-
-        Client client = client();
-
-        logger.info("creating index [{}]", indexName);
-        createIndex(indexName);
-        ensureGreen();
-
-        logger.info("indexing first document");
-        index(indexName, typeName, Integer.toString(1), "foo", "bar " + Integer.toString(1));
-        refresh();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
-
-        logger.info("creating Azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository(repositoryName)
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                                .put(Repository.CONTAINER, getContainerName())
-                                .put(Repository.BASE_PATH, getRepositoryPath())
-                                .put(Repository.BASE_PATH, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        logger.info("creating snapshot [{}]", snapshot1Name);
-        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).setIndices(indexName).get();
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot1Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("indexing second document");
-        index(indexName, typeName, Integer.toString(2), "foo", "bar " + Integer.toString(2));
-        refresh();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(2L));
-
-        logger.info("creating snapshot [{}]", snapshot2Name);
-        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot2Name).setWaitForCompletion(true).setIndices(indexName).get();
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot2Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        logger.info("closing index [{}]", indexName);
-        client.admin().indices().prepareClose(indexName).get();
-
-        logger.info("attempting restore from snapshot [{}]", snapshot1Name);
-        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).execute().actionGet();
-        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
-    }
-
-    @Test
-    public void testMultipleRepositories() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse1 = client.admin().cluster().preparePutRepository("test-repo1")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName().concat("-1"))
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse1.isAcknowledged(), equalTo(true));
-        PutRepositoryResponse putRepositoryResponse2 = client.admin().cluster().preparePutRepository("test-repo2")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName().concat("-2"))
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse2.isAcknowledged(), equalTo(true));
-
-        createIndex("test-idx-1", "test-idx-2");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-
-        logger.info("--> snapshot 1");
-        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").get();
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
-
-        logger.info("--> snapshot 2");
-        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").get();
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
-        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
-
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo1").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo2").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
-
-        // Test restore after index deletion
-        logger.info("--> delete indices");
-        cluster().wipeIndices("test-idx-1", "test-idx-2");
-        logger.info("--> restore one index after deletion from snapshot 1");
-        RestoreSnapshotResponse restoreSnapshotResponse1 = client.admin().cluster().prepareRestoreSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").execute().actionGet();
-        assertThat(restoreSnapshotResponse1.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
-        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
-
-        logger.info("--> restore other index after deletion from snapshot 2");
-        RestoreSnapshotResponse restoreSnapshotResponse2 = client.admin().cluster().prepareRestoreSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").execute().actionGet();
-        assertThat(restoreSnapshotResponse2.getRestoreInfo().totalShards(), greaterThan(0));
-        ensureGreen();
-        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
-        clusterState = client.admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
-        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(true));
-    }
-
-    /**
-     * For issue #26: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/26
-     */
-    @Test
-    public void testListBlobs_26() throws StorageException, URISyntaxException {
-        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
-        ensureGreen();
-
-        logger.info("--> indexing some data");
-        for (int i = 0; i < 100; i++) {
-            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
-            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
-            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
-        }
-        refresh();
-
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository without any path");
-        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        // Get all snapshots - should be empty
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
-
-        logger.info("--> snapshot");
-        CreateSnapshotResponse createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-
-        // Get all snapshots - should have one
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
-
-        // Clean the snapshot
-        client.prepareDeleteSnapshot("test-repo", "test-snap-26").get();
-        client.prepareDeleteRepository("test-repo").get();
-
-        logger.info("-->  creating azure repository path [{}]", getRepositoryPath());
-        putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-        ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        // Get all snapshots - should be empty
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
-
-        logger.info("--> snapshot");
-        createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
-        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
-
-        // Get all snapshots - should have one
-        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
-
-
-    }
-
-    /**
-     * For issue #28: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/28
-     */
-    @Test
-    public void testGetDeleteNonExistingSnapshot_28() throws StorageException, URISyntaxException {
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository without any path");
-        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
-                .setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        try {
-            client.prepareGetSnapshots("test-repo").addSnapshots("nonexistingsnapshotname").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-
-        try {
-            client.prepareDeleteSnapshot("test-repo", "nonexistingsnapshotname").get();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    /**
-     * For issue #21: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21
-     */
-    @Test
-    public void testForbiddenContainerName() throws Exception {
-        checkContainerName("", false);
-        checkContainerName("es", false);
-        checkContainerName("-elasticsearch", false);
-        checkContainerName("elasticsearch--integration", false);
-        checkContainerName("elasticsearch_integration", false);
-        checkContainerName("ElAsTicsearch_integration", false);
-        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-1234", false);
-        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-123", true);
-        checkContainerName("elasticsearch-integration", true);
-        checkContainerName("elasticsearch-integration-007", true);
-    }
-
-    /**
-     * Create repository with wrong or correct container name
-     * @param container Container name we want to create
-     * @param correct Is this container name correct
-     */
-    private void checkContainerName(final String container, final boolean correct) throws Exception {
-        logger.info("-->  creating azure repository with container name [{}]", container);
-        // It could happen that we just removed from a previous test the same container so
-        // we can not create it yet.
-        assertBusy(new Runnable() {
-
-            public void run() {
-                try {
-                    PutRepositoryResponse putRepositoryResponse = client().admin().cluster().preparePutRepository("test-repo")
-                            .setType("azure").setSettings(Settings.settingsBuilder()
-                                            .put(Repository.CONTAINER, container)
-                                            .put(Repository.BASE_PATH, getRepositoryPath())
-                                            .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                            ).get();
-                    client().admin().cluster().prepareDeleteRepository("test-repo").get();
-                    try {
-                        logger.info("--> remove container [{}]", container);
-                        cleanRepositoryFiles(container);
-                    } catch (StorageException | URISyntaxException e) {
-                        // We can ignore that as we just try to clean after the test
-                    }
-                    assertTrue(putRepositoryResponse.isAcknowledged() == correct);
-                } catch (RepositoryVerificationException e) {
-                    if (correct) {
-                        logger.debug(" -> container is being removed. Let's wait a bit...");
-                        fail();
-                    }
-                }
-            }
-        }, 5, TimeUnit.MINUTES);
-    }
-
-    /**
-     * Test case for issue #23: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/23
-     */
-    @Test
-    public void testNonExistingRepo_23() {
-        Client client = client();
-        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
-        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
-                .setType("azure").setSettings(Settings.settingsBuilder()
-                        .put(Repository.CONTAINER, getContainerName())
-                        .put(Repository.BASE_PATH, getRepositoryPath())
-                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
-                ).get();
-        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
-
-        logger.info("--> restore non existing snapshot");
-        try {
-            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
-            fail("Shouldn't be here");
-        } catch (SnapshotMissingException ex) {
-            // Expected
-        }
-    }
-
-    /**
-     * When a user remove a container you can not immediately create it again.
-     */
-    @Test
-    public void testRemoveAndCreateContainer() throws Exception {
-        final String container = getContainerName().concat("-testremove");
-        final AzureStorageService storageService = internalCluster().getInstance(AzureStorageService.class);
-
-        // It could happen that we run this test really close to a previous one
-        // so we might need some time to be able to create the container
-        assertBusy(new Runnable() {
-
-            public void run()  {
-                try {
-                    storageService.createContainer(container);
-                    logger.debug(" -> container created...");
-                } catch (URISyntaxException e) {
-                    // Incorrect URL. This should never happen.
-                    fail();
-                } catch (StorageException e) {
-                    // It could happen. Let's wait for a while.
-                    logger.debug(" -> container is being removed. Let's wait a bit...");
-                    fail();
-                }
-            }
-        }, 30, TimeUnit.SECONDS);
-        storageService.removeContainer(container);
-
-        ClusterAdminClient client = client().admin().cluster();
-        logger.info("-->  creating azure repository while container is being removed");
-        try {
-            client.preparePutRepository("test-repo").setType("azure")
-                    .setSettings(Settings.settingsBuilder()
-                            .put(Repository.CONTAINER, container)
-                    ).get();
-            fail("we should get a RepositoryVerificationException");
-        } catch (RepositoryVerificationException e) {
-            // Fine we expect that
-        }
-    }
-
-    /**
-     * Deletes repositories, supports wildcard notation.
-     */
-    public static void wipeRepositories(String... repositories) {
-        // if nothing is provided, delete all
-        if (repositories.length == 0) {
-            repositories = new String[]{"*"};
-        }
-        for (String repository : repositories) {
-            try {
-                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
-            } catch (RepositoryMissingException ex) {
-                // ignore
-            }
-        }
-    }
-
-    /**
-     * Purge the test containers
-     */
-    public void cleanRepositoryFiles(String... containers) throws StorageException, URISyntaxException {
-        Settings settings = readSettingsFromFile();
-        AzureStorageService client = new AzureStorageServiceImpl(settings);
-        for (String container : containers) {
-            client.removeContainer(container);
-        }
-    }
-}
diff --git a/plugins/cloud-azure/src/test/resources/rest-api-spec/test/cloud_azure/10_basic.yaml b/plugins/cloud-azure/src/test/resources/rest-api-spec/test/cloud_azure/10_basic.yaml
deleted file mode 100644
index 1abbf2a..0000000
--- a/plugins/cloud-azure/src/test/resources/rest-api-spec/test/cloud_azure/10_basic.yaml
+++ /dev/null
@@ -1,14 +0,0 @@
-# Integration tests for Cloud Azure components
-#
-"Cloud Azure loaded":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.plugins.0.name: cloud-azure  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/cloud-gce/LICENSE.txt b/plugins/cloud-gce/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/cloud-gce/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/delete-by-query/LICENSE.txt b/plugins/delete-by-query/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/delete-by-query/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/delete-by-query/NOTICE.txt b/plugins/delete-by-query/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/delete-by-query/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
index c54cdc5..4c29e7c 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
@@ -49,22 +49,22 @@ import static org.elasticsearch.search.Scroll.readScroll;
  * and is not part of elasticsearch core. In contrast to the previous, in-core, implementation delete-by-query now
  * uses scan/scroll and the returned IDs do delete all documents matching the query. This can have performance
  * as well as visibility implications. Delete-by-query now has the following semantics:
- * <li>
- *     <ul>it's <tt>non-actomic</tt>, a delete-by-query may fail at any time while some documents matching the query have already been deleted</ul>
- *     <ul>it's <tt>try-once</tt>, a delete-by-query may fail at any time and will not retry it's execution. All retry logic is left to the user</ul>
- *     <ul>it's <tt>syntactic sugar</tt>, a delete-by-query is equivalent to a scan/scroll search and corresponding bulk-deletes by ID</ul>
- *     <ul>it's executed on a <tt>point-in-time</tt> snapshot, a delete-by-query will only delete the documents that are visible at the point in time the delete-by-query was started, equivalent to the scan/scroll API</ul>
- *     <ul>it's <tt>consistent</tt>, a delete-by-query will yield consistent results across all replicas of a shard</ul>
- *     <ul>it's <tt>forward-compativle</tt>, a delete-by-query will only send IDs to the shards as deletes such that no queries are stored in the transaction logs that might not be supported in the future.</ul>
- *     <ul>it's results won't be visible until the user refreshes the index.</ul>
- * </li>
+ * <ul>
+ *     <li>it's <tt>non-actomic</tt>, a delete-by-query may fail at any time while some documents matching the query have already been deleted</li>
+ *     <li>it's <tt>try-once</tt>, a delete-by-query may fail at any time and will not retry it's execution. All retry logic is left to the user</li>
+ *     <li>it's <tt>syntactic sugar</tt>, a delete-by-query is equivalent to a scan/scroll search and corresponding bulk-deletes by ID</li>
+ *     <li>it's executed on a <tt>point-in-time</tt> snapshot, a delete-by-query will only delete the documents that are visible at the point in time the delete-by-query was started, equivalent to the scan/scroll API</li>
+ *     <li>it's <tt>consistent</tt>, a delete-by-query will yield consistent results across all replicas of a shard</li>
+ *     <li>it's <tt>forward-compativle</tt>, a delete-by-query will only send IDs to the shards as deletes such that no queries are stored in the transaction logs that might not be supported in the future.</li>
+ *     <li>it's results won't be visible until the user refreshes the index.</li>
+ * </ul>
  *
  * The main reason why delete-by-query is now extracted as a plugin are:
- *  <li>
- *     <ul><tt>forward-compatibility</tt>, the previous implementation was prone to store unsupported queries in the transaction logs which is equvalent to data-loss</ul>
- *     <ul><tt>consistency & correctness</tt>, the previous implementation was prone to produce different results on a shards replica which can essentially result in a corrupted index</ul>
- *     <ul><tt>resiliency</tt>, the previous implementation could cause OOM errors, merge-storms and dramatic slowdowns if used incorrectly</ul>
- * </li>
+ * <ul>
+ *     <li><tt>forward-compatibility</tt>, the previous implementation was prone to store unsupported queries in the transaction logs which is equvalent to data-loss</li>
+ *     <li><tt>consistency &amp; correctness</tt>, the previous implementation was prone to produce different results on a shards replica which can essentially result in a corrupted index</li>
+ *     <li><tt>resiliency</tt>, the previous implementation could cause OOM errors, merge-storms and dramatic slowdowns if used incorrectly</li>
+ * </ul>
  *
  * While delete-by-query is a very useful feature, it's implementation is very tricky in system that is based on per-document modifications. The move towards
  * a plugin based solution was mainly done to minimize the risk of cluster failures or corrupted indices which where easily possible wiht the previous implementation.
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
index 803060c..651dbbf 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
@@ -50,7 +50,7 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
 
     /**
      * Specifies what type of requested indices to ignore and wildcard indices expressions.
-     * <p/>
+     * <p>
      * For example indices that don't exist.
      */
     public DeleteByQueryRequestBuilder setIndicesOptions(IndicesOptions options) {
@@ -63,7 +63,7 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public DeleteByQueryRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
+    public DeleteByQueryRequestBuilder setQuery(QueryBuilder queryBuilder) {
         sourceBuilder().setQuery(queryBuilder);
         return this;
     }
diff --git a/plugins/discovery-azure/LICENSE.txt b/plugins/discovery-azure/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/activation-1.1.jar.sha1 b/plugins/discovery-azure/licenses/activation-1.1.jar.sha1
new file mode 100644
index 0000000..c4ee8fa
--- /dev/null
+++ b/plugins/discovery-azure/licenses/activation-1.1.jar.sha1
@@ -0,0 +1 @@
+e6cb541461c2834bdea3eb920f1884d1eb508b50
diff --git a/plugins/discovery-azure/licenses/activation-LICENSE.txt b/plugins/discovery-azure/licenses/activation-LICENSE.txt
new file mode 100644
index 0000000..1154e0a
--- /dev/null
+++ b/plugins/discovery-azure/licenses/activation-LICENSE.txt
@@ -0,0 +1,119 @@
+COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0
+
+1. Definitions.
+
+1.1. Contributor means each individual or entity that creates or contributes to the creation of Modifications.
+
+1.2. Contributor Version means the combination of the Original Software, prior Modifications used by a Contributor (if any), and the Modifications made by that particular Contributor.
+
+1.3. Covered Software means (a) the Original Software, or (b) Modifications, or (c) the combination of files containing Original Software with files containing Modifications, in each case including portions thereof.
+
+1.4. Executable means the Covered Software in any form other than Source Code.
+
+1.5. Initial Developer means the individual or entity that first makes Original Software available under this License.
+
+1.6. Larger Work means a work which combines Covered Software or portions thereof with code not governed by the terms of this License.
+
+1.7. License means this document.
+
+1.8. Licensable means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently acquired, any and all of the rights conveyed herein.
+
+1.9. Modifications means the Source Code and Executable form of any of the following:
+
+A. Any file that results from an addition to, deletion from or modification of the contents of a file containing Original Software or previous Modifications;
+
+B. Any new file that contains any part of the Original Software or previous Modification; or
+
+C. Any new file that is contributed or otherwise made available under the terms of this License.
+
+1.10. Original Software means the Source Code and Executable form of computer software code that is originally released under this License.
+
+1.11. Patent Claims means any patent claim(s), now owned or hereafter acquired, including without limitation, method, process, and apparatus claims, in any patent Licensable by grantor.
+
+1.12. Source Code means (a) the common form of computer software code in which modifications are made and (b) associated documentation included in or with such code.
+
+1.13. You (or Your) means an individual or a legal entity exercising rights under, and complying with all of the terms of, this License. For legal entities, You includes any entity which controls, is controlled by, or is under common control with You. For purposes of this definition, control means (a)the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b)ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.
+
+2. License Grants.
+
+2.1. The Initial Developer Grant.
+Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, the Initial Developer hereby grants You a world-wide, royalty-free, non-exclusive license:
+(a) under intellectual property rights (other than patent or trademark) Licensable by Initial Developer, to use, reproduce, modify, display, perform, sublicense and distribute the Original Software (or portions thereof), with or without Modifications, and/or as part of a Larger Work; and
+(b) under Patent Claims infringed by the making, using or selling of Original Software, to make, have made, use, practice, sell, and offer for sale, and/or otherwise dispose of the Original Software (or portions thereof).
+(c) The licenses granted in Sections2.1(a) and (b) are effective on the date Initial Developer first distributes or otherwise makes the Original Software available to a third party under the terms of this License.
+(d) Notwithstanding Section2.1(b) above, no patent license is granted: (1)for code that You delete from the Original Software, or (2)for infringements caused by: (i)the modification of the Original Software, or (ii)the combination of the Original Software with other software or devices.
+
+2.2. Contributor Grant.
+Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:
+(a) under intellectual property rights (other than patent or trademark) Licensable by Contributor to use, reproduce, modify, display, perform, sublicense and distribute the Modifications created by such Contributor (or portions thereof), either on an unmodified basis, with other Modifications, as Covered Software and/or as part of a Larger Work; and
+(b) under Patent Claims infringed by the making, using, or selling of Modifications made by that Contributor either alone and/or in combination with its Contributor Version (or portions of such combination), to make, use, sell, offer for sale, have made, and/or otherwise dispose of: (1)Modifications made by that Contributor (or portions thereof); and (2)the combination of Modifications made by that Contributor with its Contributor Version (or portions of such combination).
+(c) The licenses granted in Sections2.2(a) and 2.2(b) are effective on the date Contributor first distributes or otherwise makes the Modifications available to a third party.
+(d) Notwithstanding Section2.2(b) above, no patent license is granted: (1)for any code that Contributor has deleted from the Contributor Version; (2)for infringements caused by: (i)third party modifications of Contributor Version, or (ii)the combination of Modifications made by that Contributor with other software (except as part of the Contributor Version) or other devices; or (3)under Patent Claims infringed by Covered Software in the absence of Modifications made by that Contributor.
+
+3. Distribution Obligations.
+
+3.1. Availability of Source Code.
+
+Any Covered Software that You distribute or otherwise make available in Executable form must also be made available in Source Code form and that Source Code form must be distributed only under the terms of this License. You must include a copy of this License with every copy of the Source Code form of the Covered Software You distribute or otherwise make available. You must inform recipients of any such Covered Software in Executable form as to how they can obtain such Covered Software in Source Code form in a reasonable manner on or through a medium customarily used for software exchange.
+
+3.2. Modifications.
+
+The Modifications that You create or to which You contribute are governed by the terms of this License. You represent that You believe Your Modifications are Your original creation(s) and/or You have sufficient rights to grant the rights conveyed by this License.
+
+3.3. Required Notices.
+You must include a notice in each of Your Modifications that identifies You as the Contributor of the Modification. You may not remove or alter any copyright, patent or trademark notices contained within the Covered Software, or any notices of licensing or any descriptive text giving attribution to any Contributor or the Initial Developer.
+
+3.4. Application of Additional Terms.
+You may not offer or impose any terms on any Covered Software in Source Code form that alters or restricts the applicable version of this License or the recipients rights hereunder. You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, you may do so only on Your own behalf, and not on behalf of the Initial Developer or any Contributor. You must make it absolutely clear that any such warranty, support, indemnity or liability obligation is offered by You alone, and You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of warranty, support, indemnity or liability terms You offer.
+
+3.5. Distribution of Executable Versions.
+You may distribute the Executable form of the Covered Software under the terms of this License or under the terms of a license of Your choice, which may contain terms different from this License, provided that You are in compliance with the terms of this License and that the license for the Executable form does not attempt to limit or alter the recipients rights in the Source Code form from the rights set forth in this License. If You distribute the Covered Software in Executable form under a different license, You must make it absolutely clear that any terms which differ from this License are offered by You alone, not by the Initial Developer or Contributor. You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of any such terms You offer.
+
+3.6. Larger Works.
+You may create a Larger Work by combining Covered Software with other code not governed by the terms of this License and distribute the Larger Work as a single product. In such a case, You must make sure the requirements of this License are fulfilled for the Covered Software.
+
+4. Versions of the License.
+
+4.1. New Versions.
+Sun Microsystems, Inc. is the initial license steward and may publish revised and/or new versions of this License from time to time. Each version will be given a distinguishing version number. Except as provided in Section 4.3, no one other than the license steward has the right to modify this License.
+
+4.2. Effect of New Versions.
+
+You may always continue to use, distribute or otherwise make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. If the Initial Developer includes a notice in the Original Software prohibiting it from being distributed or otherwise made available under any subsequent version of the License, You must distribute and make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. Otherwise, You may also choose to use, distribute or otherwise make the Covered Software available under the terms of any subsequent version of the License published by the license steward.
+4.3. Modified Versions.
+
+When You are an Initial Developer and You want to create a new license for Your Original Software, You may create and use a modified version of this License if You: (a)rename the license and remove any references to the name of the license steward (except to note that the license differs from this License); and (b)otherwise make it clear that the license contains terms which differ from this License.
+
+5. DISCLAIMER OF WARRANTY.
+
+COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN AS IS BASIS, WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, WITHOUT LIMITATION, WARRANTIES THAT THE COVERED SOFTWARE IS FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE COVERED SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME THE COST OF ANY NECESSARY SERVICING, REPAIR OR CORRECTION. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.
+
+6. TERMINATION.
+
+6.1. This License and the rights granted hereunder will terminate automatically if You fail to comply with terms herein and fail to cure such breach within 30 days of becoming aware of the breach. Provisions which, by their nature, must remain in effect beyond the termination of this License shall survive.
+
+6.2. If You assert a patent infringement claim (excluding declaratory judgment actions) against Initial Developer or a Contributor (the Initial Developer or Contributor against whom You assert such claim is referred to as Participant) alleging that the Participant Software (meaning the Contributor Version where the Participant is a Contributor or the Original Software where the Participant is the Initial Developer) directly or indirectly infringes any patent, then any and all rights granted directly or indirectly to You by such Participant, the Initial Developer (if the Initial Developer is not the Participant) and all Contributors under Sections2.1 and/or 2.2 of this License shall, upon 60 days notice from Participant terminate prospectively and automatically at the expiration of such 60 day notice period, unless if within such 60 day period You withdraw Your claim with respect to the Participant Software against such Participant either unilaterally or pursuant to a written agreement with Participant.
+
+6.3. In the event of termination under Sections6.1 or 6.2 above, all end user licenses that have been validly granted by You or any distributor hereunder prior to termination (excluding licenses granted to You by any distributor) shall survive termination.
+
+7. LIMITATION OF LIABILITY.
+
+UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE, OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTYS NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
+
+8. U.S. GOVERNMENT END USERS.
+
+The Covered Software is a commercial item, as that term is defined in 48C.F.R.2.101 (Oct. 1995), consisting of commercial computer software (as that term is defined at 48 C.F.R. 252.227-7014(a)(1)) and commercial computer software documentation as such terms are used in 48C.F.R.12.212 (Sept. 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government End Users acquire Covered Software with only those rights set forth herein. This U.S. Government Rights clause is in lieu of, and supersedes, any other FAR, DFAR, or other clause or provision that addresses Government rights in computer software under this License.
+
+9. MISCELLANEOUS.
+
+This License represents the complete agreement concerning subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. This License shall be governed by the law of the jurisdiction specified in a notice contained within the Original Software (except to the extent applicable law, if any, provides otherwise), excluding such jurisdictions conflict-of-law provisions. Any litigation relating to this License shall be subject to the jurisdiction of the courts located in the jurisdiction and venue specified in a notice contained within the Original Software, with the losing party responsible for costs, including, without limitation, court costs and reasonable attorneys fees and expenses. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not apply to this License. You agree that You alone are responsible for compliance with the United States export administration regulations (and the export control laws and regulation of any other countries) when You use, distribute or otherwise make available any Covered Software.
+
+10. RESPONSIBILITY FOR CLAIMS.
+
+As between Initial Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.
+
+NOTICE PURSUANT TO SECTION 9 OF THE COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)
+The GlassFish code released under the CDDL shall be governed by the laws of the State of California (excluding conflict-of-law provisions). Any litigation relating to this License shall be subject to the jurisdiction of the Federal Courts of the Northern District of California and the state courts of the State of California, with venue lying in Santa Clara County, California. 
+
+
+
diff --git a/plugins/discovery-azure/licenses/activation-NOTICE.txt b/plugins/discovery-azure/licenses/activation-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/activation-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/azure-LICENSE.txt b/plugins/discovery-azure/licenses/azure-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/azure-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/azure-NOTICE.txt b/plugins/discovery-azure/licenses/azure-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/azure-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/azure-core-0.7.0.jar.sha1 b/plugins/discovery-azure/licenses/azure-core-0.7.0.jar.sha1
new file mode 100644
index 0000000..f7d0b7c
--- /dev/null
+++ b/plugins/discovery-azure/licenses/azure-core-0.7.0.jar.sha1
@@ -0,0 +1 @@
+feed802efe8a7a83d15962d11c6780c63997c528
diff --git a/plugins/discovery-azure/licenses/azure-management-0.7.0.jar.sha1 b/plugins/discovery-azure/licenses/azure-management-0.7.0.jar.sha1
new file mode 100644
index 0000000..f69856a
--- /dev/null
+++ b/plugins/discovery-azure/licenses/azure-management-0.7.0.jar.sha1
@@ -0,0 +1 @@
+0dfdd1c3a9bd783b087050e979f6ba34f06a68f3
diff --git a/plugins/discovery-azure/licenses/azure-management-compute-0.7.0.jar.sha1 b/plugins/discovery-azure/licenses/azure-management-compute-0.7.0.jar.sha1
new file mode 100644
index 0000000..bcab189
--- /dev/null
+++ b/plugins/discovery-azure/licenses/azure-management-compute-0.7.0.jar.sha1
@@ -0,0 +1 @@
+b945fc3968a4e5a64bbde419c14d92a4a53fa7a1
diff --git a/plugins/discovery-azure/licenses/commons-codec-1.6.jar.sha1 b/plugins/discovery-azure/licenses/commons-codec-1.6.jar.sha1
new file mode 100644
index 0000000..bf78aff
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-codec-1.6.jar.sha1
@@ -0,0 +1 @@
+b7f0fc8f61ecadeb3695f0b9464755eee44374d4
diff --git a/plugins/discovery-azure/licenses/commons-codec-LICENSE.txt b/plugins/discovery-azure/licenses/commons-codec-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-codec-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/commons-codec-NOTICE.txt b/plugins/discovery-azure/licenses/commons-codec-NOTICE.txt
new file mode 100644
index 0000000..5691644
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-codec-NOTICE.txt
@@ -0,0 +1,17 @@
+Apache Commons Codec
+Copyright 2002-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+src/test/org/apache/commons/codec/language/DoubleMetaphoneTest.java
+contains test data from http://aspell.net/test/orig/batch0.tab.
+Copyright (C) 2002 Kevin Atkinson (kevina@gnu.org)
+
+===============================================================================
+
+The content of package org.apache.commons.codec.language.bm has been translated
+from the original php source code available at http://stevemorse.org/phoneticinfo.htm
+with permission from the original authors.
+Original source copyright:
+Copyright (c) 2008 Alexander Beider & Stephen P. Morse.
diff --git a/plugins/discovery-azure/licenses/commons-logging-1.1.3.jar.sha1 b/plugins/discovery-azure/licenses/commons-logging-1.1.3.jar.sha1
new file mode 100644
index 0000000..c8756c4
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-logging-1.1.3.jar.sha1
@@ -0,0 +1 @@
+f6f66e966c70a83ffbdb6f17a0919eaf7c8aca7f
diff --git a/plugins/discovery-azure/licenses/commons-logging-LICENSE.txt b/plugins/discovery-azure/licenses/commons-logging-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-logging-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/commons-logging-NOTICE.txt b/plugins/discovery-azure/licenses/commons-logging-NOTICE.txt
new file mode 100644
index 0000000..d3d6e14
--- /dev/null
+++ b/plugins/discovery-azure/licenses/commons-logging-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache Commons Logging
+Copyright 2003-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-azure/licenses/httpclient-4.3.6.jar.sha1 b/plugins/discovery-azure/licenses/httpclient-4.3.6.jar.sha1
new file mode 100644
index 0000000..3d35ee9
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpclient-4.3.6.jar.sha1
@@ -0,0 +1 @@
+4c47155e3e6c9a41a28db36680b828ced53b8af4
diff --git a/plugins/discovery-azure/licenses/httpclient-LICENSE.txt b/plugins/discovery-azure/licenses/httpclient-LICENSE.txt
new file mode 100644
index 0000000..32f01ed
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpclient-LICENSE.txt
@@ -0,0 +1,558 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project includes Public Suffix List copied from
+<https://publicsuffix.org/list/effective_tld_names.dat>
+licensed under the terms of the Mozilla Public License, v. 2.0
+
+Full license text: <http://mozilla.org/MPL/2.0/>
+
+Mozilla Public License Version 2.0
+==================================
+
+1. Definitions
+--------------
+
+1.1. "Contributor"
+    means each individual or legal entity that creates, contributes to
+    the creation of, or owns Covered Software.
+
+1.2. "Contributor Version"
+    means the combination of the Contributions of others (if any) used
+    by a Contributor and that particular Contributor's Contribution.
+
+1.3. "Contribution"
+    means Covered Software of a particular Contributor.
+
+1.4. "Covered Software"
+    means Source Code Form to which the initial Contributor has attached
+    the notice in Exhibit A, the Executable Form of such Source Code
+    Form, and Modifications of such Source Code Form, in each case
+    including portions thereof.
+
+1.5. "Incompatible With Secondary Licenses"
+    means
+
+    (a) that the initial Contributor has attached the notice described
+        in Exhibit B to the Covered Software; or
+
+    (b) that the Covered Software was made available under the terms of
+        version 1.1 or earlier of the License, but not also under the
+        terms of a Secondary License.
+
+1.6. "Executable Form"
+    means any form of the work other than Source Code Form.
+
+1.7. "Larger Work"
+    means a work that combines Covered Software with other material, in
+    a separate file or files, that is not Covered Software.
+
+1.8. "License"
+    means this document.
+
+1.9. "Licensable"
+    means having the right to grant, to the maximum extent possible,
+    whether at the time of the initial grant or subsequently, any and
+    all of the rights conveyed by this License.
+
+1.10. "Modifications"
+    means any of the following:
+
+    (a) any file in Source Code Form that results from an addition to,
+        deletion from, or modification of the contents of Covered
+        Software; or
+
+    (b) any new file in Source Code Form that contains any Covered
+        Software.
+
+1.11. "Patent Claims" of a Contributor
+    means any patent claim(s), including without limitation, method,
+    process, and apparatus claims, in any patent Licensable by such
+    Contributor that would be infringed, but for the grant of the
+    License, by the making, using, selling, offering for sale, having
+    made, import, or transfer of either its Contributions or its
+    Contributor Version.
+
+1.12. "Secondary License"
+    means either the GNU General Public License, Version 2.0, the GNU
+    Lesser General Public License, Version 2.1, the GNU Affero General
+    Public License, Version 3.0, or any later versions of those
+    licenses.
+
+1.13. "Source Code Form"
+    means the form of the work preferred for making modifications.
+
+1.14. "You" (or "Your")
+    means an individual or a legal entity exercising rights under this
+    License. For legal entities, "You" includes any entity that
+    controls, is controlled by, or is under common control with You. For
+    purposes of this definition, "control" means (a) the power, direct
+    or indirect, to cause the direction or management of such entity,
+    whether by contract or otherwise, or (b) ownership of more than
+    fifty percent (50%) of the outstanding shares or beneficial
+    ownership of such entity.
+
+2. License Grants and Conditions
+--------------------------------
+
+2.1. Grants
+
+Each Contributor hereby grants You a world-wide, royalty-free,
+non-exclusive license:
+
+(a) under intellectual property rights (other than patent or trademark)
+    Licensable by such Contributor to use, reproduce, make available,
+    modify, display, perform, distribute, and otherwise exploit its
+    Contributions, either on an unmodified basis, with Modifications, or
+    as part of a Larger Work; and
+
+(b) under Patent Claims of such Contributor to make, use, sell, offer
+    for sale, have made, import, and otherwise transfer either its
+    Contributions or its Contributor Version.
+
+2.2. Effective Date
+
+The licenses granted in Section 2.1 with respect to any Contribution
+become effective for each Contribution on the date the Contributor first
+distributes such Contribution.
+
+2.3. Limitations on Grant Scope
+
+The licenses granted in this Section 2 are the only rights granted under
+this License. No additional rights or licenses will be implied from the
+distribution or licensing of Covered Software under this License.
+Notwithstanding Section 2.1(b) above, no patent license is granted by a
+Contributor:
+
+(a) for any code that a Contributor has removed from Covered Software;
+    or
+
+(b) for infringements caused by: (i) Your and any other third party's
+    modifications of Covered Software, or (ii) the combination of its
+    Contributions with other software (except as part of its Contributor
+    Version); or
+
+(c) under Patent Claims infringed by Covered Software in the absence of
+    its Contributions.
+
+This License does not grant any rights in the trademarks, service marks,
+or logos of any Contributor (except as may be necessary to comply with
+the notice requirements in Section 3.4).
+
+2.4. Subsequent Licenses
+
+No Contributor makes additional grants as a result of Your choice to
+distribute the Covered Software under a subsequent version of this
+License (see Section 10.2) or under the terms of a Secondary License (if
+permitted under the terms of Section 3.3).
+
+2.5. Representation
+
+Each Contributor represents that the Contributor believes its
+Contributions are its original creation(s) or it has sufficient rights
+to grant the rights to its Contributions conveyed by this License.
+
+2.6. Fair Use
+
+This License is not intended to limit any rights You have under
+applicable copyright doctrines of fair use, fair dealing, or other
+equivalents.
+
+2.7. Conditions
+
+Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
+in Section 2.1.
+
+3. Responsibilities
+-------------------
+
+3.1. Distribution of Source Form
+
+All distribution of Covered Software in Source Code Form, including any
+Modifications that You create or to which You contribute, must be under
+the terms of this License. You must inform recipients that the Source
+Code Form of the Covered Software is governed by the terms of this
+License, and how they can obtain a copy of this License. You may not
+attempt to alter or restrict the recipients' rights in the Source Code
+Form.
+
+3.2. Distribution of Executable Form
+
+If You distribute Covered Software in Executable Form then:
+
+(a) such Covered Software must also be made available in Source Code
+    Form, as described in Section 3.1, and You must inform recipients of
+    the Executable Form how they can obtain a copy of such Source Code
+    Form by reasonable means in a timely manner, at a charge no more
+    than the cost of distribution to the recipient; and
+
+(b) You may distribute such Executable Form under the terms of this
+    License, or sublicense it under different terms, provided that the
+    license for the Executable Form does not attempt to limit or alter
+    the recipients' rights in the Source Code Form under this License.
+
+3.3. Distribution of a Larger Work
+
+You may create and distribute a Larger Work under terms of Your choice,
+provided that You also comply with the requirements of this License for
+the Covered Software. If the Larger Work is a combination of Covered
+Software with a work governed by one or more Secondary Licenses, and the
+Covered Software is not Incompatible With Secondary Licenses, this
+License permits You to additionally distribute such Covered Software
+under the terms of such Secondary License(s), so that the recipient of
+the Larger Work may, at their option, further distribute the Covered
+Software under the terms of either this License or such Secondary
+License(s).
+
+3.4. Notices
+
+You may not remove or alter the substance of any license notices
+(including copyright notices, patent notices, disclaimers of warranty,
+or limitations of liability) contained within the Source Code Form of
+the Covered Software, except that You may alter any license notices to
+the extent required to remedy known factual inaccuracies.
+
+3.5. Application of Additional Terms
+
+You may choose to offer, and to charge a fee for, warranty, support,
+indemnity or liability obligations to one or more recipients of Covered
+Software. However, You may do so only on Your own behalf, and not on
+behalf of any Contributor. You must make it absolutely clear that any
+such warranty, support, indemnity, or liability obligation is offered by
+You alone, and You hereby agree to indemnify every Contributor for any
+liability incurred by such Contributor as a result of warranty, support,
+indemnity or liability terms You offer. You may include additional
+disclaimers of warranty and limitations of liability specific to any
+jurisdiction.
+
+4. Inability to Comply Due to Statute or Regulation
+---------------------------------------------------
+
+If it is impossible for You to comply with any of the terms of this
+License with respect to some or all of the Covered Software due to
+statute, judicial order, or regulation then You must: (a) comply with
+the terms of this License to the maximum extent possible; and (b)
+describe the limitations and the code they affect. Such description must
+be placed in a text file included with all distributions of the Covered
+Software under this License. Except to the extent prohibited by statute
+or regulation, such description must be sufficiently detailed for a
+recipient of ordinary skill to be able to understand it.
+
+5. Termination
+--------------
+
+5.1. The rights granted under this License will terminate automatically
+if You fail to comply with any of its terms. However, if You become
+compliant, then the rights granted under this License from a particular
+Contributor are reinstated (a) provisionally, unless and until such
+Contributor explicitly and finally terminates Your grants, and (b) on an
+ongoing basis, if such Contributor fails to notify You of the
+non-compliance by some reasonable means prior to 60 days after You have
+come back into compliance. Moreover, Your grants from a particular
+Contributor are reinstated on an ongoing basis if such Contributor
+notifies You of the non-compliance by some reasonable means, this is the
+first time You have received notice of non-compliance with this License
+from such Contributor, and You become compliant prior to 30 days after
+Your receipt of the notice.
+
+5.2. If You initiate litigation against any entity by asserting a patent
+infringement claim (excluding declaratory judgment actions,
+counter-claims, and cross-claims) alleging that a Contributor Version
+directly or indirectly infringes any patent, then the rights granted to
+You by any and all Contributors for the Covered Software under Section
+2.1 of this License shall terminate.
+
+5.3. In the event of termination under Sections 5.1 or 5.2 above, all
+end user license agreements (excluding distributors and resellers) which
+have been validly granted by You or Your distributors under this License
+prior to termination shall survive termination.
+
+************************************************************************
+*                                                                      *
+*  6. Disclaimer of Warranty                                           *
+*  -------------------------                                           *
+*                                                                      *
+*  Covered Software is provided under this License on an "as is"       *
+*  basis, without warranty of any kind, either expressed, implied, or  *
+*  statutory, including, without limitation, warranties that the       *
+*  Covered Software is free of defects, merchantable, fit for a        *
+*  particular purpose or non-infringing. The entire risk as to the     *
+*  quality and performance of the Covered Software is with You.        *
+*  Should any Covered Software prove defective in any respect, You     *
+*  (not any Contributor) assume the cost of any necessary servicing,   *
+*  repair, or correction. This disclaimer of warranty constitutes an   *
+*  essential part of this License. No use of any Covered Software is   *
+*  authorized under this License except under this disclaimer.         *
+*                                                                      *
+************************************************************************
+
+************************************************************************
+*                                                                      *
+*  7. Limitation of Liability                                          *
+*  --------------------------                                          *
+*                                                                      *
+*  Under no circumstances and under no legal theory, whether tort      *
+*  (including negligence), contract, or otherwise, shall any           *
+*  Contributor, or anyone who distributes Covered Software as          *
+*  permitted above, be liable to You for any direct, indirect,         *
+*  special, incidental, or consequential damages of any character      *
+*  including, without limitation, damages for lost profits, loss of    *
+*  goodwill, work stoppage, computer failure or malfunction, or any    *
+*  and all other commercial damages or losses, even if such party      *
+*  shall have been informed of the possibility of such damages. This   *
+*  limitation of liability shall not apply to liability for death or   *
+*  personal injury resulting from such party's negligence to the       *
+*  extent applicable law prohibits such limitation. Some               *
+*  jurisdictions do not allow the exclusion or limitation of           *
+*  incidental or consequential damages, so this exclusion and          *
+*  limitation may not apply to You.                                    *
+*                                                                      *
+************************************************************************
+
+8. Litigation
+-------------
+
+Any litigation relating to this License may be brought only in the
+courts of a jurisdiction where the defendant maintains its principal
+place of business and such litigation shall be governed by laws of that
+jurisdiction, without reference to its conflict-of-law provisions.
+Nothing in this Section shall prevent a party's ability to bring
+cross-claims or counter-claims.
+
+9. Miscellaneous
+----------------
+
+This License represents the complete agreement concerning the subject
+matter hereof. If any provision of this License is held to be
+unenforceable, such provision shall be reformed only to the extent
+necessary to make it enforceable. Any law or regulation which provides
+that the language of a contract shall be construed against the drafter
+shall not be used to construe this License against a Contributor.
+
+10. Versions of the License
+---------------------------
+
+10.1. New Versions
+
+Mozilla Foundation is the license steward. Except as provided in Section
+10.3, no one other than the license steward has the right to modify or
+publish new versions of this License. Each version will be given a
+distinguishing version number.
+
+10.2. Effect of New Versions
+
+You may distribute the Covered Software under the terms of the version
+of the License under which You originally received the Covered Software,
+or under the terms of any subsequent version published by the license
+steward.
+
+10.3. Modified Versions
+
+If you create software not governed by this License, and you want to
+create a new license for such software, you may create and use a
+modified version of this License if you rename the license and remove
+any references to the name of the license steward (except to note that
+such modified license differs from this License).
+
+10.4. Distributing Source Code Form that is Incompatible With Secondary
+Licenses
+
+If You choose to distribute Source Code Form that is Incompatible With
+Secondary Licenses under the terms of this version of the License, the
+notice described in Exhibit B of this License must be attached.
+
+Exhibit A - Source Code Form License Notice
+-------------------------------------------
+
+  This Source Code Form is subject to the terms of the Mozilla Public
+  License, v. 2.0. If a copy of the MPL was not distributed with this
+  file, You can obtain one at http://mozilla.org/MPL/2.0/.
+
+If it is not possible or desirable to put the notice in a particular
+file, then You may include the notice in a location (such as a LICENSE
+file in a relevant directory) where a recipient would be likely to look
+for such a notice.
+
+You may add additional accurate notices of copyright ownership.
+
+Exhibit B - "Incompatible With Secondary Licenses" Notice
+---------------------------------------------------------
+
+  This Source Code Form is "Incompatible With Secondary Licenses", as
+  defined by the Mozilla Public License, v. 2.0.
diff --git a/plugins/discovery-azure/licenses/httpclient-NOTICE.txt b/plugins/discovery-azure/licenses/httpclient-NOTICE.txt
new file mode 100644
index 0000000..4f60581
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpclient-NOTICE.txt
@@ -0,0 +1,5 @@
+Apache HttpComponents Client
+Copyright 1999-2015 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
diff --git a/plugins/discovery-azure/licenses/httpcore-4.3.3.jar.sha1 b/plugins/discovery-azure/licenses/httpcore-4.3.3.jar.sha1
new file mode 100644
index 0000000..5d9c0e2
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpcore-4.3.3.jar.sha1
@@ -0,0 +1 @@
+f91b7a4aadc5cf486df6e4634748d7dd7a73f06d
diff --git a/plugins/discovery-azure/licenses/httpcore-LICENSE.txt b/plugins/discovery-azure/licenses/httpcore-LICENSE.txt
new file mode 100644
index 0000000..72819a9
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpcore-LICENSE.txt
@@ -0,0 +1,241 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+=========================================================================
+
+This project contains annotations in the package org.apache.http.annotation
+which are derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls.
+See http://www.jcip.net and the Creative Commons Attribution License
+(http://creativecommons.org/licenses/by/2.5)
+Full text: http://creativecommons.org/licenses/by/2.5/legalcode
+
+License
+
+THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE COMMONS PUBLIC LICENSE ("CCPL" OR "LICENSE"). THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.
+
+BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.
+
+1. Definitions
+
+    "Collective Work" means a work, such as a periodical issue, anthology or encyclopedia, in which the Work in its entirety in unmodified form, along with a number of other contributions, constituting separate and independent works in themselves, are assembled into a collective whole. A work that constitutes a Collective Work will not be considered a Derivative Work (as defined below) for the purposes of this License.
+    "Derivative Work" means a work based upon the Work or upon the Work and other pre-existing works, such as a translation, musical arrangement, dramatization, fictionalization, motion picture version, sound recording, art reproduction, abridgment, condensation, or any other form in which the Work may be recast, transformed, or adapted, except that a work that constitutes a Collective Work will not be considered a Derivative Work for the purpose of this License. For the avoidance of doubt, where the Work is a musical composition or sound recording, the synchronization of the Work in timed-relation with a moving image ("synching") will be considered a Derivative Work for the purpose of this License.
+    "Licensor" means the individual or entity that offers the Work under the terms of this License.
+    "Original Author" means the individual or entity who created the Work.
+    "Work" means the copyrightable work of authorship offered under the terms of this License.
+    "You" means an individual or entity exercising rights under this License who has not previously violated the terms of this License with respect to the Work, or who has received express permission from the Licensor to exercise rights under this License despite a previous violation.
+
+2. Fair Use Rights. Nothing in this license is intended to reduce, limit, or restrict any rights arising from fair use, first sale or other limitations on the exclusive rights of the copyright owner under copyright law or other applicable laws.
+
+3. License Grant. Subject to the terms and conditions of this License, Licensor hereby grants You a worldwide, royalty-free, non-exclusive, perpetual (for the duration of the applicable copyright) license to exercise the rights in the Work as stated below:
+
+    to reproduce the Work, to incorporate the Work into one or more Collective Works, and to reproduce the Work as incorporated in the Collective Works;
+    to create and reproduce Derivative Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission the Work including as incorporated in Collective Works;
+    to distribute copies or phonorecords of, display publicly, perform publicly, and perform publicly by means of a digital audio transmission Derivative Works.
+
+    For the avoidance of doubt, where the work is a musical composition:
+        Performance Royalties Under Blanket Licenses. Licensor waives the exclusive right to collect, whether individually or via a performance rights society (e.g. ASCAP, BMI, SESAC), royalties for the public performance or public digital performance (e.g. webcast) of the Work.
+        Mechanical Rights and Statutory Royalties. Licensor waives the exclusive right to collect, whether individually or via a music rights agency or designated agent (e.g. Harry Fox Agency), royalties for any phonorecord You create from the Work ("cover version") and distribute, subject to the compulsory license created by 17 USC Section 115 of the US Copyright Act (or the equivalent in other jurisdictions).
+    Webcasting Rights and Statutory Royalties. For the avoidance of doubt, where the Work is a sound recording, Licensor waives the exclusive right to collect, whether individually or via a performance-rights society (e.g. SoundExchange), royalties for the public digital performance (e.g. webcast) of the Work, subject to the compulsory license created by 17 USC Section 114 of the US Copyright Act (or the equivalent in other jurisdictions).
+
+The above rights may be exercised in all media and formats whether now known or hereafter devised. The above rights include the right to make such modifications as are technically necessary to exercise the rights in other media and formats. All rights not expressly granted by Licensor are hereby reserved.
+
+4. Restrictions.The license granted in Section 3 above is expressly made subject to and limited by the following restrictions:
+
+    You may distribute, publicly display, publicly perform, or publicly digitally perform the Work only under the terms of this License, and You must include a copy of, or the Uniform Resource Identifier for, this License with every copy or phonorecord of the Work You distribute, publicly display, publicly perform, or publicly digitally perform. You may not offer or impose any terms on the Work that alter or restrict the terms of this License or the recipients' exercise of the rights granted hereunder. You may not sublicense the Work. You must keep intact all notices that refer to this License and to the disclaimer of warranties. You may not distribute, publicly display, publicly perform, or publicly digitally perform the Work with any technological measures that control access or use of the Work in a manner inconsistent with the terms of this License Agreement. The above applies to the Work as incorporated in a Collective Work, but this does not require the Collective Work apart from the Work itself to be made subject to the terms of this License. If You create a Collective Work, upon notice from any Licensor You must, to the extent practicable, remove from the Collective Work any credit as required by clause 4(b), as requested. If You create a Derivative Work, upon notice from any Licensor You must, to the extent practicable, remove from the Derivative Work any credit as required by clause 4(b), as requested.
+    If you distribute, publicly display, publicly perform, or publicly digitally perform the Work or any Derivative Works or Collective Works, You must keep intact all copyright notices for the Work and provide, reasonable to the medium or means You are utilizing: (i) the name of the Original Author (or pseudonym, if applicable) if supplied, and/or (ii) if the Original Author and/or Licensor designate another party or parties (e.g. a sponsor institute, publishing entity, journal) for attribution in Licensor's copyright notice, terms of service or by other reasonable means, the name of such party or parties; the title of the Work if supplied; to the extent reasonably practicable, the Uniform Resource Identifier, if any, that Licensor specifies to be associated with the Work, unless such URI does not refer to the copyright notice or licensing information for the Work; and in the case of a Derivative Work, a credit identifying the use of the Work in the Derivative Work (e.g., "French translation of the Work by Original Author," or "Screenplay based on original Work by Original Author"). Such credit may be implemented in any reasonable manner; provided, however, that in the case of a Derivative Work or Collective Work, at a minimum such credit will appear where any other comparable authorship credit appears and in a manner at least as prominent as such other comparable authorship credit.
+
+5. Representations, Warranties and Disclaimer
+
+UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING, LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.
+
+6. Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+7. Termination
+
+    This License and the rights granted hereunder will terminate automatically upon any breach by You of the terms of this License. Individuals or entities who have received Derivative Works or Collective Works from You under this License, however, will not have their licenses terminated provided such individuals or entities remain in full compliance with those licenses. Sections 1, 2, 5, 6, 7, and 8 will survive any termination of this License.
+    Subject to the above terms and conditions, the license granted here is perpetual (for the duration of the applicable copyright in the Work). Notwithstanding the above, Licensor reserves the right to release the Work under different license terms or to stop distributing the Work at any time; provided, however that any such election will not serve to withdraw this License (or any other license that has been, or is required to be, granted under the terms of this License), and this License will continue in full force and effect unless terminated as stated above.
+
+8. Miscellaneous
+
+    Each time You distribute or publicly digitally perform the Work or a Collective Work, the Licensor offers to the recipient a license to the Work on the same terms and conditions as the license granted to You under this License.
+    Each time You distribute or publicly digitally perform a Derivative Work, Licensor offers to the recipient a license to the original Work on the same terms and conditions as the license granted to You under this License.
+    If any provision of this License is invalid or unenforceable under applicable law, it shall not affect the validity or enforceability of the remainder of the terms of this License, and without further action by the parties to this agreement, such provision shall be reformed to the minimum extent necessary to make such provision valid and enforceable.
+    No term or provision of this License shall be deemed waived and no breach consented to unless such waiver or consent shall be in writing and signed by the party to be charged with such waiver or consent.
+    This License constitutes the entire agreement between the parties with respect to the Work licensed here. There are no understandings, agreements or representations with respect to the Work not specified here. Licensor shall not be bound by any additional provisions that may appear in any communication from You. This License may not be modified without the mutual written agreement of the Licensor and You.
diff --git a/plugins/discovery-azure/licenses/httpcore-NOTICE.txt b/plugins/discovery-azure/licenses/httpcore-NOTICE.txt
new file mode 100644
index 0000000..c0be50a
--- /dev/null
+++ b/plugins/discovery-azure/licenses/httpcore-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache HttpComponents Core
+Copyright 2005-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This project contains annotations derived from JCIP-ANNOTATIONS
+Copyright (c) 2005 Brian Goetz and Tim Peierls. See http://www.jcip.net
diff --git a/plugins/discovery-azure/licenses/jackson-LICENSE b/plugins/discovery-azure/licenses/jackson-LICENSE
new file mode 100644
index 0000000..f5f45d2
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-LICENSE
@@ -0,0 +1,8 @@
+This copy of Jackson JSON processor streaming parser/generator is licensed under the
+Apache (Software) License, version 2.0 ("the License").
+See the License for details about distribution rights, and the
+specific rights regarding derivate works.
+
+You may obtain a copy of the License at:
+
+http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/discovery-azure/licenses/jackson-NOTICE b/plugins/discovery-azure/licenses/jackson-NOTICE
new file mode 100644
index 0000000..4c976b7
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-NOTICE
@@ -0,0 +1,20 @@
+# Jackson JSON processor
+
+Jackson is a high-performance, Free/Open Source JSON processing library.
+It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
+been in development since 2007.
+It is currently developed by a community of developers, as well as supported
+commercially by FasterXML.com.
+
+## Licensing
+
+Jackson core and extension components may licensed under different licenses.
+To find the details that apply to this artifact see the accompanying LICENSE file.
+For more information, including possible other licensing options, contact
+FasterXML.com (http://fasterxml.com).
+
+## Credits
+
+A list of contributors may be found from CREDITS file, which is included
+in some artifacts (usually source distributions); but is always available
+from the source code management (SCM) system project uses.
diff --git a/plugins/discovery-azure/licenses/jackson-core-asl-1.9.2.jar.sha1 b/plugins/discovery-azure/licenses/jackson-core-asl-1.9.2.jar.sha1
new file mode 100644
index 0000000..a608bd1
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-core-asl-1.9.2.jar.sha1
@@ -0,0 +1 @@
+8493982bba1727106d767034bd0d8e77bc1931a9
diff --git a/plugins/discovery-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1 b/plugins/discovery-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1
new file mode 100644
index 0000000..a3dc0aa
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-jaxrs-1.9.2.jar.sha1
@@ -0,0 +1 @@
+aedf43f1d5005561e531b6bf0d067e4d20f58aba
diff --git a/plugins/discovery-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1 b/plugins/discovery-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1
new file mode 100644
index 0000000..fd88504
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-mapper-asl-1.9.2.jar.sha1
@@ -0,0 +1 @@
+95400a7922ce75383866eb72f6ef4a7897923945
diff --git a/plugins/discovery-azure/licenses/jackson-xc-1.9.2.jar.sha1 b/plugins/discovery-azure/licenses/jackson-xc-1.9.2.jar.sha1
new file mode 100644
index 0000000..f823e61
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jackson-xc-1.9.2.jar.sha1
@@ -0,0 +1 @@
+437c991a8eb2c8b69ef1dba2eba27fccb9b98448
diff --git a/plugins/discovery-azure/licenses/javax.inject-1.jar.sha1 b/plugins/discovery-azure/licenses/javax.inject-1.jar.sha1
new file mode 100644
index 0000000..7ef3c70
--- /dev/null
+++ b/plugins/discovery-azure/licenses/javax.inject-1.jar.sha1
@@ -0,0 +1 @@
+6975da39a7040257bd51d21a231b76c915872d38
diff --git a/plugins/discovery-azure/licenses/javax.inject-LICENSE.txt b/plugins/discovery-azure/licenses/javax.inject-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/javax.inject-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/javax.inject-NOTICE.txt b/plugins/discovery-azure/licenses/javax.inject-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/javax.inject-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/jaxb-LICENSE.txt b/plugins/discovery-azure/licenses/jaxb-LICENSE.txt
new file mode 100644
index 0000000..a3e62b0
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jaxb-LICENSE.txt
@@ -0,0 +1,705 @@
+COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL) Version 1.0
+
+1. Definitions.
+
+   1.1. Contributor. means each individual or  entity  that  creates  or
+contributes to the creation of Modifications.
+
+   1.2. Contributor Version.  means  the  combination  of  the  Original
+Software,  prior  Modifications  used by a Contributor (if any), and the
+Modifications made by that particular Contributor.
+
+   1.3. Covered Software.  means  (a)  the  Original  Software,  or  (b)
+Modifications,  or  (c)  the  combination  of  files containing Original
+Software with files containing Modifications,  in  each  case  including
+portions thereof.
+
+   1.4. Executable. means the Covered Software in any  form  other  than
+Source Code.
+
+   1.5. Initial Developer. means the individual  or  entity  that  first
+makes Original Software available under this License.
+
+   1.6. Larger Work. means a work which  combines  Covered  Software  or
+portions thereof with code not governed by the terms of this License.
+
+   1.7. License. means this document.
+
+   1.8. Licensable. means having the right  to  grant,  to  the  maximum
+extent   possible,   whether  at  the  time  of  the  initial  grant  or
+subsequently acquired, any and all of the rights conveyed herein.
+
+   1.9. Modifications. means the Source Code and Executable form of  any
+of the following:
+
+        A. Any file that results from an addition to, deletion  from  or
+modification  of  the contents of a file containing Original Software or
+previous Modifications;
+
+        B. Any new file that contains any part of the Original  Software
+or previous Modification; or
+
+        C. Any new file that is contributed or otherwise made  available
+under the terms of this License.
+
+   1.10. Original Software. means the Source Code and Executable form of
+computer software code that is originally released under this License.
+
+   1.11.  Patent  Claims.  means  any  patent  claim(s),  now  owned  or
+hereafter  acquired,  including without limitation, method, process, and
+apparatus claims, in any patent Licensable by grantor.
+
+   1.12. Source Code. means (a) the common  form  of  computer  software
+code  in  which  modifications are made and (b) associated documentation
+included in or with such code.
+
+   1.13. You.  (or  .Your.)  means  an  individual  or  a  legal  entity
+exercising  rights  under,  and complying with all of the terms of, this
+License. For legal entities, .You. includes any entity  which  controls,
+is  controlled  by, or is under common control with You. For purposes of
+this definition, .control. means (a) the power, direct or  indirect,  to
+cause the direction or management of such entity, whether by contract or
+otherwise,  or  (b)  ownership  of  more than fifty percent (50%) of the
+outstanding shares or beneficial ownership of such entity.
+
+2. License Grants.
+
+      2.1. The Initial Developer Grant.
+
+      Conditioned upon  Your  compliance  with  Section  3.1  below  and
+subject  to  third  party  intellectual  property  claims,  the  Initial
+Developer hereby grants You a  world-wide,  royalty-free,  non-exclusive
+license:
+
+         (a) under intellectual property rights (other  than  patent  or
+trademark)  Licensable  by Initial Developer, to use, reproduce, modify,
+display, perform, sublicense and distribute the  Original  Software  (or
+portions  thereof),  with  or without Modifications, and/or as part of a
+Larger Work; and
+
+         (b) under Patent Claims  infringed  by  the  making,  using  or
+selling  of  Original Software, to make, have made, use, practice, sell,
+and offer for sale, and/or otherwise dispose of  the  Original  Software
+(or portions thereof).
+
+        (c)  The  licenses  granted  in  Sections  2.1(a)  and  (b)  are
+effective  on  the date Initial Developer first distributes or otherwise
+makes the Original Software available to a third party under  the  terms
+of this License.
+
+        (d) Notwithstanding Section 2.1(b) above, no patent  license  is
+granted: (1) for code that You delete from the Original Software, or (2)
+for  infringements  caused  by:  (i)  the  modification  of the Original
+Software, or (ii) the combination of the Original  Software  with  other
+software or devices.
+
+    2.2. Contributor Grant.
+
+    Conditioned upon Your compliance with Section 3.1 below and  subject
+to  third  party  intellectual  property claims, each Contributor hereby
+grants You a world-wide, royalty-free, non-exclusive license:
+
+        (a) under intellectual property rights  (other  than  patent  or
+trademark) Licensable by Contributor to use, reproduce, modify, display,
+perform,  sublicense  and  distribute  the Modifications created by such
+Contributor (or portions thereof), either on an unmodified  basis,  with
+other Modifications, as Covered Software and/or as part of a Larger Work;
+and
+
+        (b) under Patent Claims  infringed  by  the  making,  using,  or
+selling of Modifications made by that Contributor either alone and/or in
+combination   with   its   Contributor  Version  (or  portions  of  such
+combination), to make, use, sell, offer  for  sale,  have  made,  and/or
+otherwise  dispose  of:  (1)  Modifications made by that Contributor (or
+portions thereof); and (2) the combination of Modifications made by that
+Contributor  with  its  Contributor  Version  (or   portions   of   such
+combination).
+
+        (c) The licenses granted  in  Sections  2.2(a)  and  2.2(b)  are
+effective  on  the date Contributor first distributes or otherwise makes
+the Modifications available to a third party.
+
+        (d) Notwithstanding Section 2.2(b) above, no patent  license  is
+granted:  (1)  for  any  code  that  Contributor  has  deleted  from the
+Contributor Version; (2) for infringements caused by:  (i)  third  party
+modifications  of  Contributor  Version,  or  (ii)  the  combination  of
+Modifications made by that Contributor with other  software  (except  as
+part  of  the Contributor Version) or other devices; or (3) under Patent
+Claims infringed by Covered Software in  the  absence  of  Modifications
+made by that Contributor.
+
+3. Distribution Obligations.
+
+      3.1. Availability of Source Code.
+      Any  Covered  Software  that  You  distribute  or  otherwise  make
+available  in Executable form must also be made available in Source Code
+form and that Source Code form must be distributed only under the  terms
+of this License. You must include a copy of this License with every copy
+of  the  Source  Code  form  of  the  Covered Software You distribute or
+otherwise make available. You must inform recipients of any such Covered
+Software in Executable form as to  how  they  can  obtain  such  Covered
+Software  in  Source  Code  form  in a reasonable manner on or through a
+medium customarily used for software exchange.
+
+      3.2. Modifications.
+      The Modifications that You create or to which You  contribute  are
+governed  by  the  terms of this License. You represent that You believe
+Your  Modifications  are  Your  original  creation(s)  and/or  You  have
+sufficient rights to grant the rights conveyed by this License.
+
+      3.3. Required Notices.
+      You must include a notice  in  each  of  Your  Modifications  that
+identifies  You  as  the  Contributor  of  the Modification. You may not
+remove or alter any copyright, patent  or  trademark  notices  contained
+within  the  Covered  Software,  or  any  notices  of  licensing  or any
+descriptive text giving attribution to any Contributor  or  the  Initial
+Developer.
+
+      3.4. Application of Additional Terms.
+      You may not offer or impose any terms on any Covered  Software  in
+Source Code form that alters or restricts the applicable version of this
+License  or  the  recipients. rights hereunder. You may choose to offer,
+and to charge a fee  for,  warranty,  support,  indemnity  or  liability
+obligations  to one or more recipients of Covered Software. However, you
+may do so only on Your own behalf, and not  on  behalf  of  the  Initial
+Developer or any Contributor. You must make it absolutely clear that any
+such  warranty, support, indemnity or liability obligation is offered by
+You alone, and You hereby agree to indemnify the Initial  Developer  and
+every Contributor for any liability incurred by the Initial Developer or
+such  Contributor  as  a  result  of  warranty,  support,  indemnity  or
+liability terms You offer.
+
+      3.5. Distribution of Executable Versions.
+      You may distribute the Executable form  of  the  Covered  Software
+under  the terms of this License or under the terms of a license of Your
+choice, which may contain terms different from  this  License,  provided
+that  You  are in compliance with the terms of this License and that the
+license for the Executable form does not attempt to limit or  alter  the
+recipient.s  rights in the Source Code form from the rights set forth in
+this License. If You distribute the Covered Software in Executable  form
+under  a  different  license, You must make it absolutely clear that any
+terms which differ from this License are offered by You  alone,  not  by
+the  Initial Developer or Contributor. You hereby agree to indemnify the
+Initial Developer and every Contributor for any  liability  incurred  by
+the  Initial Developer or such Contributor as a result of any such terms
+You offer.
+
+      3.6. Larger Works.
+      You may create a Larger Work by combining  Covered  Software  with
+other  code not governed by the terms of this License and distribute the
+Larger Work as a single product. In such a case, You must make sure  the
+requirements of this License are fulfilled for the Covered Software.
+
+4. Versions of the License.
+
+      4.1. New Versions.
+      Sun Microsystems, Inc. is the  initial  license  steward  and  may
+publish  revised  and/or new versions of this License from time to time.
+Each version will be given a distinguishing version  number.  Except  as
+provided  in  Section 4.3, no one other than the license steward has the
+right to modify this License.
+
+      4.2. Effect of New Versions.
+      You may always continue to use, distribute or otherwise  make  the
+Covered Software available under the terms of the version of the License
+under which You originally received the Covered Software. If the Initial
+Developer includes a notice in the Original Software prohibiting it from
+being  distributed  or  otherwise  made  available  under any subsequent
+version of the  License,  You  must  distribute  and  make  the  Covered
+Software  available  under the terms of the version of the License under
+which You originally received the Covered Software. Otherwise,  You  may
+also  choose  to  use, distribute or otherwise make the Covered Software
+available under the terms of  any  subsequent  version  of  the  License
+published by the license steward.
+
+      4.3. Modified Versions.
+      When You are an Initial Developer and You want  to  create  a  new
+license  for  Your  Original Software, You may create and use a modified
+version of this License if You: (a) rename the license  and  remove  any
+references  to  the name of the license steward (except to note that the
+license differs from this License); and (b) otherwise make it clear that
+the license contains terms which differ from this License.
+
+5. DISCLAIMER OF WARRANTY.
+
+   COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN .AS IS.  BASIS,
+WITHOUT  WARRANTY  OF  ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING,
+WITHOUT LIMITATION, WARRANTIES THAT THE  COVERED  SOFTWARE  IS  FREE  OF
+DEFECTS,  MERCHANTABLE,  FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING.
+THE ENTIRE RISK AS  TO  THE  QUALITY  AND  PERFORMANCE  OF  THE  COVERED
+SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY
+RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME
+THE  COST  OF  ANY  NECESSARY  SERVICING,  REPAIR  OR  CORRECTION.  THIS
+DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO
+USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER  EXCEPT  UNDER  THIS
+DISCLAIMER.
+
+6. TERMINATION.
+
+      6.1. This License and the rights granted hereunder will  terminate
+automatically  if  You fail to comply with terms herein and fail to cure
+such breach within 30 days of becoming aware of the  breach.  Provisions
+which,  by their nature, must remain in effect beyond the termination of
+this License shall survive.
+
+      6.2.  If  You  assert  a  patent  infringement  claim   (excluding
+declaratory judgment actions) against Initial Developer or a Contributor
+(the Initial Developer or Contributor against whom You assert such claim
+is  referred to as .Participant.) alleging that the Participant Software
+(meaning the Contributor Version where the Participant is a  Contributor
+or the Original Software where the Participant is the Initial Developer)
+directly  or  indirectly  infringes  any patent, then any and all rights
+granted directly or indirectly to You by such Participant,  the  Initial
+Developer  (if  the  Initial  Developer  is not the Participant) and all
+Contributors under Sections 2.1 and/or 2.2 of this License  shall,  upon
+60   days   notice   from   Participant   terminate   prospectively  and
+automatically at the expiration of such 60 day notice period, unless  if
+within  such  60  day period You withdraw Your claim with respect to the
+Participant Software against such  Participant  either  unilaterally  or
+pursuant to a written agreement with Participant.
+
+      6.3. In the event of termination under Sections 6.1 or 6.2  above,
+all  end  user  licenses  that  have  been validly granted by You or any
+distributor hereunder prior to termination (excluding  licenses  granted
+to You by any distributor) shall survive termination.
+
+7. LIMITATION OF LIABILITY.
+
+   UNDER NO CIRCUMSTANCES  AND  UNDER  NO  LEGAL  THEORY,  WHETHER  TORT
+(INCLUDING  NEGLIGENCE),  CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL
+DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE,
+OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR  ANY
+INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER
+INCLUDING,  WITHOUT  LIMITATION,  DAMAGES  FOR  LOST  PROFITS,  LOSS  OF
+GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL
+OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL  HAVE  BEEN
+INFORMED  OF  THE  POSSIBILITY  OF  SUCH  DAMAGES.  THIS  LIMITATION  OF
+LIABILITY SHALL NOT APPLY TO LIABILITY  FOR  DEATH  OR  PERSONAL  INJURY
+RESULTING  FROM  SUCH  PARTY.S  NEGLIGENCE  TO THE EXTENT APPLICABLE LAW
+PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION
+OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS  EXCLUSION
+AND LIMITATION MAY NOT APPLY TO YOU.
+
+8. U.S. GOVERNMENT END USERS.
+
+   The Covered Software is a .commercial item,. as that term is  defined
+in  48  C.F.R.  2.101  (Oct.  1995),  consisting of .commercial computer
+software. (as that term is defined at 48  C.F.R.    252.227-7014(a)(1))
+and  .commercial computer software documentation. as such terms are used
+in 48 C.F.R. 12.212 (Sept. 1995). Consistent with 48 C.F.R.  12.212  and
+48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government
+End  Users  acquire  Covered  Software  with only those rights set forth
+herein. This U.S. Government Rights clause is in lieu of, and supersedes,
+any other FAR,  DFAR,  or  other  clause  or  provision  that  addresses
+Government rights in computer software under this License.
+
+9. MISCELLANEOUS.
+
+   This License represents the  complete  agreement  concerning  subject
+matter  hereof.  If  any  provision  of  this  License  is  held  to  be
+unenforceable, such provision shall  be  reformed  only  to  the  extent
+necessary  to make it enforceable. This License shall be governed by the
+law of the jurisdiction specified  in  a  notice  contained  within  the
+Original Software (except to the extent applicable law, if any, provides
+otherwise),  excluding  such  jurisdiction.s conflict-of-law provisions.
+Any litigation  relating  to  this  License  shall  be  subject  to  the
+jurisdiction  of  the  courts  located  in  the  jurisdiction  and venue
+specified in a notice contained within the Original Software,  with  the
+losing party responsible for costs, including, without limitation, court
+costs  and  reasonable  attorneys. fees and expenses. The application of
+the United Nations Convention on Contracts for the International Sale of
+Goods is expressly excluded. Any law or regulation which  provides  that
+the  language of a contract shall be construed against the drafter shall
+not apply to this License. You agree that You alone are responsible  for
+compliance with the United States export administration regulations (and
+the  export control laws and regulation of any other countries) when You
+use, distribute or otherwise make available any Covered Software.
+
+10. RESPONSIBILITY FOR CLAIMS.
+
+   As between Initial Developer and  the  Contributors,  each  party  is
+responsible  for claims and damages arising, directly or indirectly, out
+of its utilization of rights under this License and You  agree  to  work
+with   Initial   Developer   and   Contributors   to   distribute   such
+responsibility on an equitable basis.  Nothing  herein  is  intended  or
+shall be deemed to constitute any admission of liability.
+
+   NOTICE  PURSUANT  TO  SECTION  9  OF  THE  COMMON   DEVELOPMENT   AND
+DISTRIBUTION LICENSE (CDDL)
+
+   The code released under the CDDL shall be governed by the laws of the
+State  of  California  (excluding   conflict-of-law   provisions).   Any
+litigation relating to this License shall be subject to the jurisdiction
+of  the  Federal  Courts  of the Northern District of California and the
+state courts of the State of California, with venue lying in Santa Clara
+County, California.
+
+
+The GNU General Public License (GPL) Version 2, June 1991
+
+
+Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place,
+Suite 330, Boston, MA 02111-1307 USA
+
+Everyone is permitted to copy and distribute  verbatim  copies  of  this
+license document, but changing it is not allowed.
+
+Preamble
+
+The licenses for most software are designed to take away your freedom to
+share and change it. By contrast, the  GNU  General  Public  License  is
+intended to guarantee your freedom to share and change free software--to
+make  sure  the  software is free for all its users. This General Public
+License applies to most of the Free Software Foundation's  software  and
+to  any other program whose authors commit to using it. (Some other Free
+Software Foundation software is  covered  by  the  GNU  Library  General
+Public License instead.) You can apply it to your programs, too.
+
+When we speak of free software, we are referring to freedom, not  price.
+Our  General Public Licenses are designed to make sure that you have the
+freedom to distribute copies of  free  software  (and  charge  for  this
+service  if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces  of  it  in  new
+free programs; and that you know you can do these things.
+
+To protect your rights, we need to make restrictions that forbid  anyone
+to  deny  you  these rights or to ask you to surrender the rights. These
+restrictions translate  to  certain  responsibilities  for  you  if  you
+distribute copies of the software, or if you modify it.
+
+For example, if you distribute copies of such a program, whether  gratis
+or for a fee, you must give the recipients all the rights that you have.
+You  must  make sure that they, too, receive or can get the source code.
+And you must show them these terms so they know their rights.
+
+We protect your rights with two steps: (1) copyright the  software,  and
+(2)  offer  you  this  license which gives you legal permission to copy,
+distribute and/or modify the software.
+
+Also, for each author's protection and ours, we  want  to  make  certain
+that  everyone  understands  that  there  is  no  warranty for this free
+software. If the software is modified by someone else and passed on,  we
+want  its recipients to know that what they have is not the original, so
+that any problems introduced by others will not reflect on the  original
+authors' reputations.
+
+Finally, any free program is threatened constantly by software  patents.
+We  wish  to avoid the danger that redistributors of a free program will
+individually obtain  patent  licenses,  in  effect  making  the  program
+proprietary. To prevent this, we have made it clear that any patent must
+be licensed for everyone's free use or not licensed at all.
+
+The  precise  terms  and  conditions  for  copying,   distribution   and
+modification follow.
+
+
+TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+0. This License applies to any program or other work  which  contains  a
+notice placed by the copyright holder saying it may be distributed under
+the  terms  of this General Public License. The "Program", below, refers
+to any such program or work, and a "work based  on  the  Program"  means
+either  the  Program or any derivative work under copyright law: that is
+to say, a work containing  the  Program  or  a  portion  of  it,  either
+verbatim  or with modifications and/or translated into another language.
+(Hereinafter, translation is included without  limitation  in  the  term
+"modification".) Each licensee is addressed as "you".
+
+Activities other than copying, distribution  and  modification  are  not
+covered  by this License; they are outside its scope. The act of running
+the Program is not restricted,  and  the  output  from  the  Program  is
+covered  only  if  its  contents  constitute a work based on the Program
+(independent of having been made by running the Program).  Whether  that
+is true depends on what the Program does.
+
+1. You may copy and distribute verbatim copies of the  Program's  source
+code  as  you receive it, in any medium, provided that you conspicuously
+and appropriately publish on each copy an appropriate  copyright  notice
+and  disclaimer  of  warranty; keep intact all the notices that refer to
+this License and to the absence of any  warranty;  and  give  any  other
+recipients of the Program a copy of this License along with the Program.
+
+You may charge a fee for the physical act of transferring  a  copy,  and
+you may at your option offer warranty protection in exchange for a fee.
+
+2. You may modify your copy or copies of the Program or any  portion  of
+it,  thus  forming  a work based on the Program, and copy and distribute
+such modifications or work under the terms of Section 1 above,  provided
+that you also meet all of these conditions:
+
+   a) You must cause the  modified  files  to  carry  prominent  notices
+stating that you changed the files and the date of any change.
+
+   b) You must cause any work that you distribute or  publish,  that  in
+whole  or  in  part  contains or is derived from the Program or any part
+thereof, to be licensed as a whole at no charge  to  all  third  parties
+under the terms of this License.
+
+   c) If the modified program normally reads commands interactively when
+run, you must cause it, when started running for such interactive use in
+the most ordinary way, to print or display an announcement including  an
+appropriate  copyright notice and a notice that there is no warranty (or
+else,  saying  that  you  provide  a  warranty)  and  that   users   may
+redistribute  the  program  under these conditions, and telling the user
+how to view a copy of this License. (Exception: if the Program itself is
+interactive but does not normally print such an announcement, your  work
+based on the Program is not required to print an announcement.)
+
+These  requirements  apply  to  the  modified  work  as  a   whole.   If
+identifiable sections of that work are not derived from the Program, and
+can   be   reasonably  considered  independent  and  separate  works  in
+themselves, then this License, and its terms,  do  not  apply  to  those
+sections  when  you  distribute  them  as  separate  works. But when you
+distribute the same sections as part of a whole which is a work based on
+the Program, the distribution of the whole must be on the terms of  this
+License,  whose  permissions  for  other  licensees extend to the entire
+whole, and thus to each and every part regardless of who wrote it.
+
+Thus, it is not the intent of this section to claim  rights  or  contest
+your  rights  to  work written entirely by you; rather, the intent is to
+exercise  the  right  to  control  the  distribution  of  derivative  or
+collective works based on the Program.
+
+In addition, mere aggregation of another work not based on  the  Program
+with  the Program (or with a work based on the Program) on a volume of a
+storage or distribution medium does not bring the other work  under  the
+scope of this License.
+
+3. You may copy and distribute the Program (or a work based on it, under
+Section 2) in object code or executable form under the terms of Sections
+1 and 2 above provided that you also do one of the following:
+
+   a) Accompany it  with  the  complete  corresponding  machine-readable
+source code, which must be distributed under the terms of Sections 1 and
+2 above on a medium customarily used for software interchange; or,
+
+   b) Accompany it with a written offer, valid for at least three years,
+to give any third party,  for  a  charge  no  more  than  your  cost  of
+physically  performing  source distribution, a complete machine-readable
+copy of the corresponding source code, to be distributed under the terms
+of Sections 1 and 2 above on a  medium  customarily  used  for  software
+interchange; or,
+
+   c) Accompany it with the information you received as to the offer  to
+distribute  corresponding source code. (This alternative is allowed only
+for noncommercial distribution and only if you received the  program  in
+object  code  or  executable  form  with  such  an offer, in accord with
+Subsection b above.)
+
+The source code for a work means the preferred  form  of  the  work  for
+making modifications to it. For an executable work, complete source code
+means  all  the  source  code  for  all  modules  it  contains, plus any
+associated interface definition files, plus the scripts used to  control
+compilation  and  installation  of the executable. However, as a special
+exception, the source code distributed need not include anything that is
+normally distributed (in either source or binary form)  with  the  major
+components  (compiler,  kernel,  and  so  on) of the operating system on
+which the executable runs, unless that component itself accompanies  the
+executable.
+
+If distribution of executable or object code is made by offering  access
+to copy from a designated place, then offering equivalent access to copy
+the source code from the same place counts as distribution of the source
+code,  even  though  third  parties are not compelled to copy the source
+along with the object code.
+
+4. You may not copy,  modify,  sublicense,  or  distribute  the  Program
+except  as  expressly provided under this License. Any attempt otherwise
+to copy, modify, sublicense or distribute the Program is void, and  will
+automatically terminate your rights under this License. However, parties
+who  have  received  copies, or rights, from you under this License will
+not have their licenses terminated so long as  such  parties  remain  in
+full compliance.
+
+5. You are not required to accept  this  License,  since  you  have  not
+signed  it.  However,  nothing  else  grants you permission to modify or
+distribute the Program  or  its  derivative  works.  These  actions  are
+prohibited  by  law  if  you  do  not accept this License. Therefore, by
+modifying or distributing the Program (or any work based on the Program),
+you indicate your acceptance of this License to do so, and all its terms
+and conditions for copying, distributing or  modifying  the  Program  or
+works based on it.
+
+6. Each time you redistribute the Program (or  any  work  based  on  the
+Program),  the  recipient  automatically  receives  a  license  from the
+original licensor to copy, distribute or modify the Program  subject  to
+these  terms and conditions. You may not impose any further restrictions
+on the recipients' exercise of the rights granted herein.  You  are  not
+responsible for enforcing compliance by third parties to this License.
+
+7. If, as a consequence of a court  judgment  or  allegation  of  patent
+infringement  or  for  any  other reason (not limited to patent issues),
+conditions are imposed on you (whether  by  court  order,  agreement  or
+otherwise)  that  contradict the conditions of this License, they do not
+excuse you from the conditions of this License. If you cannot distribute
+so as to satisfy simultaneously your obligations under this License  and
+any  other  pertinent  obligations,  then  as  a consequence you may not
+distribute the Program at all. For example, if a  patent  license  would
+not  permit  royalty-free redistribution of the Program by all those who
+receive copies directly or indirectly through you, then the only way you
+could satisfy both it and this License would be to refrain entirely from
+distribution of the Program.
+
+If any portion of this section is held invalid  or  unenforceable  under
+any  particular  circumstance, the balance of the section is intended to
+apply and the  section  as  a  whole  is  intended  to  apply  in  other
+circumstances.
+
+It is not the purpose of this section to  induce  you  to  infringe  any
+patents  or  other  property  right claims or to contest validity of any
+such claims; this  section  has  the  sole  purpose  of  protecting  the
+integrity of the free software distribution system, which is implemented
+by   public   license   practices.   Many   people  have  made  generous
+contributions to the wide range of  software  distributed  through  that
+system in reliance on consistent application of that system; it is up to
+the  author/donor  to  decide  if  he  or  she  is willing to distribute
+software through any other system and  a  licensee  cannot  impose  that
+choice.
+
+This section is intended to make thoroughly clear what is believed to be
+a consequence of the rest of this License.
+
+8. If the distribution and/or  use  of  the  Program  is  restricted  in
+certain  countries  either  by patents or by copyrighted interfaces, the
+original copyright holder who places the Program under this License  may
+add  an  explicit  geographical  distribution limitation excluding those
+countries, so that distribution is permitted only in or among  countries
+not   thus  excluded.  In  such  case,  this  License  incorporates  the
+limitation as if written in the body of this License.
+
+9. The Free Software Foundation may publish revised and/or new  versions
+of  the General Public License from time to time. Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+Each version is given a distinguishing version number.  If  the  Program
+specifies  a version number of this License which applies to it and "any
+later  version",  you  have  the  option  of  following  the  terms  and
+conditions  either  of that version or of any later version published by
+the Free Software Foundation. If the Program does not specify a  version
+number of this License, you may choose any version ever published by the
+Free Software Foundation.
+
+10. If you wish to incorporate parts of  the  Program  into  other  free
+programs  whose  distribution  conditions  are  different,  write to the
+author to ask for permission. For software which is copyrighted  by  the
+Free  Software  Foundation,  write  to  the Free Software Foundation; we
+sometimes make exceptions for this. Our decision will be guided  by  the
+two  goals  of preserving the free status of all derivatives of our free
+software and of promoting the sharing and reuse of software generally.
+
+NO WARRANTY
+
+11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
+FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT  WHEN
+OTHERWISE  STATED  IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
+PROVIDE THE PROGRAM  "AS  IS"  WITHOUT  WARRANTY  OF  ANY  KIND,  EITHER
+EXPRESSED  OR  IMPLIED,  INCLUDING,  BUT  NOT  LIMITED  TO,  THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE
+ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.
+SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY
+SERVICING, REPAIR OR CORRECTION.
+
+12. IN NO EVENT UNLESS REQUIRED  BY  APPLICABLE  LAW  OR  AGREED  TO  IN
+WRITING  WILL  ANY  COPYRIGHT  HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY
+AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
+DAMAGES, INCLUDING ANY GENERAL,  SPECIAL,  INCIDENTAL  OR  CONSEQUENTIAL
+DAMAGES  ARISING  OUT  OF  THE  USE  OR  INABILITY  TO  USE  THE PROGRAM
+(INCLUDING BUT NOT LIMITED TO  LOSS  OF  DATA  OR  DATA  BEING  RENDERED
+INACCURATE  OR  LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF
+THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER  OR
+OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+END OF TERMS AND CONDITIONS
+
+
+How to Apply These Terms to Your New Programs
+
+If you develop a new program, and you want it  to  be  of  the  greatest
+possible  use  to the public, the best way to achieve this is to make it
+free software which everyone can redistribute  and  change  under  these
+terms.
+
+To do so, attach the following notices to the program. It is  safest  to
+attach  them to the start of each source file to most effectively convey
+the exclusion of warranty; and  each  file  should  have  at  least  the
+"copyright" line and a pointer to where the full notice is found.
+
+   One line to give the program's name and a brief idea of what it does.
+
+   Copyright (C)
+
+   This program is free software; you can redistribute it and/or  modify
+it under the terms of the GNU General Public License as published by the
+Free  Software  Foundation; either version 2 of the License, or (at your
+option) any later version.
+
+   This program is distributed in the hope that it will be  useful,  but
+WITHOUT   ANY   WARRANTY;   without   even   the   implied  warranty  of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
+Public License for more details.
+
+   You should have received a copy of the  GNU  General  Public  License
+along  with this program; if not, write to the Free Software Foundation,
+Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+
+Also add information on how to contact you by electronic and paper mail.
+
+If the program is interactive, make it output a short notice  like  this
+when it starts in an interactive mode:
+
+   Gnomovision version 69, Copyright (C) year name of author
+   Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show
+w'. This is free software, and you are welcome to redistribute it  under
+certain conditions; type `show c' for details.
+
+The hypothetical  commands  `show  w'  and  `show  c'  should  show  the
+appropriate parts of the General Public License. Of course, the commands
+you  use  may be called something other than `show w' and `show c'; they
+could even be mouse-clicks or menu items--whatever suits your program.
+
+You should also get your employer (if you work as a programmer) or  your
+school,  if  any,  to  sign a "copyright disclaimer" for the program, if
+necessary. Here is a sample; alter the names:
+
+   Yoyodyne, Inc.,  hereby  disclaims  all  copyright  interest  in  the
+program `Gnomovision' (which makes passes at compilers) written by James
+Hacker.
+
+   signature of Ty Coon, 1 April 1989
+   Ty Coon, President of Vice
+
+This General Public License does not permit incorporating  your  program
+into  proprietary programs. If your program is a subroutine library, you
+may consider it more useful to permit linking  proprietary  applications
+with  the  library.  If this is what you want to do, use the GNU Library
+General Public License instead of this License.
+
+
+"CLASSPATH" EXCEPTION TO THE GPL VERSION 2
+
+Certain source files distributed by Sun Microsystems, Inc.  are  subject
+to  the following clarification and special exception to the GPL Version
+2, but only where Sun has expressly included in  the  particular  source
+file's header the words
+
+"Sun designates this particular  file  as  subject  to  the  "Classpath"
+exception  as  provided by Sun in the License file that accompanied this
+code."
+
+Linking this library statically or dynamically  with  other  modules  is
+making  a  combined  work  based  on  this  library. Thus, the terms and
+conditions of the GNU General Public License Version 2 cover  the  whole
+combination.
+
+As a special exception, the copyright holders of this library  give  you
+permission  to  link this library with independent modules to produce an
+executable, regardless of the license terms of these independent modules,
+and to copy and distribute the resulting executable under terms of  your
+choice, provided that you also meet, for each linked independent module,
+the  terms and conditions of the license of that module.? An independent
+module is a module which is not derived from or based on this  library.?
+If  you  modify  this  library,  you  may  extend this exception to your
+version of the library, but you are not obligated to do so.? If  you  do
+not wish to do so, delete this exception statement from your version.
+
\ No newline at end of file
diff --git a/plugins/discovery-azure/licenses/jaxb-NOTICE.txt b/plugins/discovery-azure/licenses/jaxb-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jaxb-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/jaxb-api-2.2.2.jar.sha1 b/plugins/discovery-azure/licenses/jaxb-api-2.2.2.jar.sha1
new file mode 100644
index 0000000..a145d47
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jaxb-api-2.2.2.jar.sha1
@@ -0,0 +1 @@
+aeb3021ca93dde265796d82015beecdcff95bf09
diff --git a/plugins/discovery-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1 b/plugins/discovery-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1
new file mode 100644
index 0000000..79fe55d
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jaxb-impl-2.2.3-1.jar.sha1
@@ -0,0 +1 @@
+56baae106392040a45a06d4a41099173425da1e6
diff --git a/plugins/discovery-azure/licenses/jersey-LICENSE.txt b/plugins/discovery-azure/licenses/jersey-LICENSE.txt
new file mode 100644
index 0000000..833a843
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jersey-LICENSE.txt
@@ -0,0 +1,274 @@
+COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)Version 1.1
+
+1. Definitions.
+
+     1.1. "Contributor" means each individual or entity that creates or contributes to the creation of Modifications.
+
+     1.2. "Contributor Version" means the combination of the Original Software, prior Modifications used by a Contributor (if any), and the Modifications made by that particular Contributor.
+
+     1.3. "Covered Software" means (a) the Original Software, or (b) Modifications, or (c) the combination of files containing Original Software with files containing Modifications, in each case including portions thereof.
+
+     1.4. "Executable" means the Covered Software in any form other than Source Code.
+
+     1.5. "Initial Developer" means the individual or entity that first makes Original Software available under this License.
+
+     1.6. "Larger Work" means a work which combines Covered Software or portions thereof with code not governed by the terms of this License.
+
+     1.7. "License" means this document.
+
+     1.8. "Licensable" means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently acquired, any and all of the rights conveyed herein.
+
+     1.9. "Modifications" means the Source Code and Executable form of any of the following:
+
+     A. Any file that results from an addition to, deletion from or modification of the contents of a file containing Original Software or previous Modifications;
+
+     B. Any new file that contains any part of the Original Software or previous Modification; or
+
+     C. Any new file that is contributed or otherwise made available under the terms of this License.
+
+     1.10. "Original Software" means the Source Code and Executable form of computer software code that is originally released under this License.
+
+     1.11. "Patent Claims" means any patent claim(s), now owned or hereafter acquired, including without limitation, method, process, and apparatus claims, in any patent Licensable by grantor.
+
+     1.12. "Source Code" means (a) the common form of computer software code in which modifications are made and (b) associated documentation included in or with such code.
+
+     1.13. "You" (or "Your") means an individual or a legal entity exercising rights under, and complying with all of the terms of, this License. For legal entities, "You" includes any entity which controls, is controlled by, or is under common control with You. For purposes of this definition, "control" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.
+
+2. License Grants.
+
+     2.1. The Initial Developer Grant.
+
+     Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, the Initial Developer hereby grants You a world-wide, royalty-free, non-exclusive license:
+
+     (a) under intellectual property rights (other than patent or trademark) Licensable by Initial Developer, to use, reproduce, modify, display, perform, sublicense and distribute the Original Software (or portions thereof), with or without Modifications, and/or as part of a Larger Work; and
+
+     (b) under Patent Claims infringed by the making, using or selling of Original Software, to make, have made, use, practice, sell, and offer for sale, and/or otherwise dispose of the Original Software (or portions thereof).
+
+     (c) The licenses granted in Sections 2.1(a) and (b) are effective on the date Initial Developer first distributes or otherwise makes the Original Software available to a third party under the terms of this License.
+
+     (d) Notwithstanding Section 2.1(b) above, no patent license is granted: (1) for code that You delete from the Original Software, or (2) for infringements caused by: (i) the modification of the Original Software, or (ii) the combination of the Original Software with other software or devices.
+
+     2.2. Contributor Grant.
+
+     Conditioned upon Your compliance with Section 3.1 below and subject to third party intellectual property claims, each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:
+
+     (a) under intellectual property rights (other than patent or trademark) Licensable by Contributor to use, reproduce, modify, display, perform, sublicense and distribute the Modifications created by such Contributor (or portions thereof), either on an unmodified basis, with other Modifications, as Covered Software and/or as part of a Larger Work; and
+
+     (b) under Patent Claims infringed by the making, using, or selling of Modifications made by that Contributor either alone and/or in combination with its Contributor Version (or portions of such combination), to make, use, sell, offer for sale, have made, and/or otherwise dispose of: (1) Modifications made by that Contributor (or portions thereof); and (2) the combination of Modifications made by that Contributor with its Contributor Version (or portions of such combination).
+
+     (c) The licenses granted in Sections 2.2(a) and 2.2(b) are effective on the date Contributor first distributes or otherwise makes the Modifications available to a third party.
+
+     (d) Notwithstanding Section 2.2(b) above, no patent license is granted: (1) for any code that Contributor has deleted from the Contributor Version; (2) for infringements caused by: (i) third party modifications of Contributor Version, or (ii) the combination of Modifications made by that Contributor with other software (except as part of the Contributor Version) or other devices; or (3) under Patent Claims infringed by Covered Software in the absence of Modifications made by that Contributor.
+
+3. Distribution Obligations.
+
+     3.1. Availability of Source Code.
+
+     Any Covered Software that You distribute or otherwise make available in Executable form must also be made available in Source Code form and that Source Code form must be distributed only under the terms of this License. You must include a copy of this License with every copy of the Source Code form of the Covered Software You distribute or otherwise make available. You must inform recipients of any such Covered Software in Executable form as to how they can obtain such Covered Software in Source Code form in a reasonable manner on or through a medium customarily used for software exchange.
+
+     3.2. Modifications.
+
+     The Modifications that You create or to which You contribute are governed by the terms of this License. You represent that You believe Your Modifications are Your original creation(s) and/or You have sufficient rights to grant the rights conveyed by this License.
+
+     3.3. Required Notices.
+
+     You must include a notice in each of Your Modifications that identifies You as the Contributor of the Modification. You may not remove or alter any copyright, patent or trademark notices contained within the Covered Software, or any notices of licensing or any descriptive text giving attribution to any Contributor or the Initial Developer.
+
+     3.4. Application of Additional Terms.
+
+     You may not offer or impose any terms on any Covered Software in Source Code form that alters or restricts the applicable version of this License or the recipients' rights hereunder. You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, you may do so only on Your own behalf, and not on behalf of the Initial Developer or any Contributor. You must make it absolutely clear that any such warranty, support, indemnity or liability obligation is offered by You alone, and You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of warranty, support, indemnity or liability terms You offer.
+
+     3.5. Distribution of Executable Versions.
+
+     You may distribute the Executable form of the Covered Software under the terms of this License or under the terms of a license of Your choice, which may contain terms different from this License, provided that You are in compliance with the terms of this License and that the license for the Executable form does not attempt to limit or alter the recipient's rights in the Source Code form from the rights set forth in this License. If You distribute the Covered Software in Executable form under a different license, You must make it absolutely clear that any terms which differ from this License are offered by You alone, not by the Initial Developer or Contributor. You hereby agree to indemnify the Initial Developer and every Contributor for any liability incurred by the Initial Developer or such Contributor as a result of any such terms You offer.
+
+     3.6. Larger Works.
+
+     You may create a Larger Work by combining Covered Software with other code not governed by the terms of this License and distribute the Larger Work as a single product. In such a case, You must make sure the requirements of this License are fulfilled for the Covered Software.
+
+4. Versions of the License.
+
+     4.1. New Versions.
+
+     Oracle is the initial license steward and may publish revised and/or new versions of this License from time to time. Each version will be given a distinguishing version number. Except as provided in Section 4.3, no one other than the license steward has the right to modify this License.
+
+     4.2. Effect of New Versions.
+
+     You may always continue to use, distribute or otherwise make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. If the Initial Developer includes a notice in the Original Software prohibiting it from being distributed or otherwise made available under any subsequent version of the License, You must distribute and make the Covered Software available under the terms of the version of the License under which You originally received the Covered Software. Otherwise, You may also choose to use, distribute or otherwise make the Covered Software available under the terms of any subsequent version of the License published by the license steward.
+
+     4.3. Modified Versions.
+
+     When You are an Initial Developer and You want to create a new license for Your Original Software, You may create and use a modified version of this License if You: (a) rename the license and remove any references to the name of the license steward (except to note that the license differs from this License); and (b) otherwise make it clear that the license contains terms which differ from this License.
+
+5. DISCLAIMER OF WARRANTY.
+
+     COVERED SOFTWARE IS PROVIDED UNDER THIS LICENSE ON AN "AS IS" BASIS, WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, WITHOUT LIMITATION, WARRANTIES THAT THE COVERED SOFTWARE IS FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE OR NON-INFRINGING. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE COVERED SOFTWARE IS WITH YOU. SHOULD ANY COVERED SOFTWARE PROVE DEFECTIVE IN ANY RESPECT, YOU (NOT THE INITIAL DEVELOPER OR ANY OTHER CONTRIBUTOR) ASSUME THE COST OF ANY NECESSARY SERVICING, REPAIR OR CORRECTION. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO USE OF ANY COVERED SOFTWARE IS AUTHORIZED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.
+
+6. TERMINATION.
+
+     6.1. This License and the rights granted hereunder will terminate automatically if You fail to comply with terms herein and fail to cure such breach within 30 days of becoming aware of the breach. Provisions which, by their nature, must remain in effect beyond the termination of this License shall survive.
+
+     6.2. If You assert a patent infringement claim (excluding declaratory judgment actions) against Initial Developer or a Contributor (the Initial Developer or Contributor against whom You assert such claim is referred to as "Participant") alleging that the Participant Software (meaning the Contributor Version where the Participant is a Contributor or the Original Software where the Participant is the Initial Developer) directly or indirectly infringes any patent, then any and all rights granted directly or indirectly to You by such Participant, the Initial Developer (if the Initial Developer is not the Participant) and all Contributors under Sections 2.1 and/or 2.2 of this License shall, upon 60 days notice from Participant terminate prospectively and automatically at the expiration of such 60 day notice period, unless if within such 60 day period You withdraw Your claim with respect to the Participant Software against such Participant either unilaterally or pursuant to a written agreement with Participant.
+
+     6.3. If You assert a patent infringement claim against Participant alleging that the Participant Software directly or indirectly infringes any patent where such claim is resolved (such as by license or settlement) prior to the initiation of patent infringement litigation, then the reasonable value of the licenses granted by such Participant under Sections 2.1 or 2.2 shall be taken into account in determining the amount or value of any payment or license.
+
+     6.4. In the event of termination under Sections 6.1 or 6.2 above, all end user licenses that have been validly granted by You or any distributor hereunder prior to termination (excluding licenses granted to You by any distributor) shall survive termination.
+
+7. LIMITATION OF LIABILITY.
+
+     UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL YOU, THE INITIAL DEVELOPER, ANY OTHER CONTRIBUTOR, OR ANY DISTRIBUTOR OF COVERED SOFTWARE, OR ANY SUPPLIER OF ANY OF SUCH PARTIES, BE LIABLE TO ANY PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PARTY SHALL HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTY'S NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
+
+8. U.S. GOVERNMENT END USERS.
+
+     The Covered Software is a "commercial item," as that term is defined in 48 C.F.R. 2.101 (Oct. 1995), consisting of "commercial computer software" (as that term is defined at 48 C.F.R. ? 252.227-7014(a)(1)) and "commercial computer software documentation" as such terms are used in 48 C.F.R. 12.212 (Sept. 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (June 1995), all U.S. Government End Users acquire Covered Software with only those rights set forth herein. This U.S. Government Rights clause is in lieu of, and supersedes, any other FAR, DFAR, or other clause or provision that addresses Government rights in computer software under this License.
+
+9. MISCELLANEOUS.
+
+     This License represents the complete agreement concerning subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. This License shall be governed by the law of the jurisdiction specified in a notice contained within the Original Software (except to the extent applicable law, if any, provides otherwise), excluding such jurisdiction's conflict-of-law provisions. Any litigation relating to this License shall be subject to the jurisdiction of the courts located in the jurisdiction and venue specified in a notice contained within the Original Software, with the losing party responsible for costs, including, without limitation, court costs and reasonable attorneys' fees and expenses. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not apply to this License. You agree that You alone are responsible for compliance with the United States export administration regulations (and the export control laws and regulation of any other countries) when You use, distribute or otherwise make available any Covered Software.
+
+10. RESPONSIBILITY FOR CLAIMS.
+
+     As between Initial Developer and the Contributors, each party is responsible for claims and damages arising, directly or indirectly, out of its utilization of rights under this License and You agree to work with Initial Developer and Contributors to distribute such responsibility on an equitable basis. Nothing herein is intended or shall be deemed to constitute any admission of liability.
+
+----------
+NOTICE PURSUANT TO SECTION 9 OF THE COMMON DEVELOPMENT AND DISTRIBUTION LICENSE (CDDL)
+The code released under the CDDL shall be governed by the laws of the State of California (excluding conflict-of-law provisions). Any litigation relating to this License shall be subject to the jurisdiction of the Federal Courts of the Northern District of California and the state courts of the State of California, with venue lying in Santa Clara County, California.
+
+
+
+
+The GNU General Public License (GPL) Version 2, June 1991
+
+
+Copyright (C) 1989, 1991 Free Software Foundation, Inc. 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+
+Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.
+
+Preamble
+
+The licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users. This General Public License applies to most of the Free Software Foundation's software and to any other program whose authors commit to using it. (Some other Free Software Foundation software is covered by the GNU Library General Public License instead.) You can apply it to your programs, too.
+
+When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs; and that you know you can do these things.
+
+To protect your rights, we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights. These restrictions translate to certain responsibilities for you if you distribute copies of the software, or if you modify it.
+
+For example, if you distribute copies of such a program, whether gratis or for a fee, you must give the recipients all the rights that you have. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.
+
+We protect your rights with two steps: (1) copyright the software, and (2) offer you this license which gives you legal permission to copy, distribute and/or modify the software.
+
+Also, for each author's protection and ours, we want to make certain that everyone understands that there is no warranty for this free software. If the software is modified by someone else and passed on, we want its recipients to know that what they have is not the original, so that any problems introduced by others will not reflect on the original authors' reputations.
+
+Finally, any free program is threatened constantly by software patents. We wish to avoid the danger that redistributors of a free program will individually obtain patent licenses, in effect making the program proprietary. To prevent this, we have made it clear that any patent must be licensed for everyone's free use or not licensed at all.
+
+The precise terms and conditions for copying, distribution and modification follow.
+
+
+TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+0. This License applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this General Public License. The "Program", below, refers to any such program or work, and a "work based on the Program" means either the Program or any derivative work under copyright law: that is to say, a work containing the Program or a portion of it, either verbatim or with modifications and/or translated into another language. (Hereinafter, translation is included without limitation in the term "modification".) Each licensee is addressed as "you".
+
+Activities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running the Program is not restricted, and the output from the Program is covered only if its contents constitute a work based on the Program (independent of having been made by running the Program). Whether that is true depends on what the Program does.
+
+1. You may copy and distribute verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and give any other recipients of the Program a copy of this License along with the Program.
+
+You may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.
+
+2. You may modify your copy or copies of the Program or any portion of it, thus forming a work based on the Program, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:
+
+   a) You must cause the modified files to carry prominent notices stating that you changed the files and the date of any change.
+
+   b) You must cause any work that you distribute or publish, that in whole or in part contains or is derived from the Program or any part thereof, to be licensed as a whole at no charge to all third parties under the terms of this License.
+
+   c) If the modified program normally reads commands interactively when run, you must cause it, when started running for such interactive use in the most ordinary way, to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty (or else, saying that you provide a warranty) and that users may redistribute the program under these conditions, and telling the user how to view a copy of this License. (Exception: if the Program itself is interactive but does not normally print such an announcement, your work based on the Program is not required to print an announcement.)
+
+These requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Program, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Program, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.
+
+Thus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Program.
+
+In addition, mere aggregation of another work not based on the Program with the Program (or with a work based on the Program) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.
+
+3. You may copy and distribute the Program (or a work based on it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you also do one of the following:
+
+   a) Accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,
+
+   b) Accompany it with a written offer, valid for at least three years, to give any third party, for a charge no more than your cost of physically performing source distribution, a complete machine-readable copy of the corresponding source code, to be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange; or,
+
+   c) Accompany it with the information you received as to the offer to distribute corresponding source code. (This alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer, in accord with Subsection b above.)
+
+The source code for a work means the preferred form of the work for making modifications to it. For an executable work, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the executable. However, as a special exception, the source code distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.
+
+If distribution of executable or object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place counts as distribution of the source code, even though third parties are not compelled to copy the source along with the object code.
+
+4. You may not copy, modify, sublicense, or distribute the Program except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense or distribute the Program is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.
+
+5. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Program or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Program (or any work based on the Program), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Program or works based on it.
+
+6. Each time you redistribute the Program (or any work based on the Program), the recipient automatically receives a license from the original licensor to copy, distribute or modify the Program subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties to this License.
+
+7. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Program at all. For example, if a patent license would not permit royalty-free redistribution of the Program by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Program.
+
+If any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances.
+
+It is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system, which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.
+
+This section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.
+
+8. If the distribution and/or use of the Program is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Program under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License.
+
+9. The Free Software Foundation may publish revised and/or new versions of the General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
+
+Each version is given a distinguishing version number. If the Program specifies a version number of this License which applies to it and "any later version", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of this License, you may choose any version ever published by the Free Software Foundation.
+
+10. If you wish to incorporate parts of the Program into other free programs whose distribution conditions are different, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.
+
+NO WARRANTY
+
+11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+
+END OF TERMS AND CONDITIONS
+
+
+How to Apply These Terms to Your New Programs
+
+If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.
+
+To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the "copyright" line and a pointer to where the full notice is found.
+
+   One line to give the program's name and a brief idea of what it does.
+
+   Copyright (C)
+
+   This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+
+Also add information on how to contact you by electronic and paper mail.
+
+If the program is interactive, make it output a short notice like this when it starts in an interactive mode:
+
+   Gnomovision version 69, Copyright (C) year name of author
+   Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, the commands you use may be called something other than `show w' and `show c'; they could even be mouse-clicks or menu items--whatever suits your program.
+
+You should also get your employer (if you work as a programmer) or your school, if any, to sign a "copyright disclaimer" for the program, if necessary. Here is a sample; alter the names:
+
+   Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.
+
+   signature of Ty Coon, 1 April 1989
+   Ty Coon, President of Vice
+
+This General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Library General Public License instead of this License.
+
+
+"CLASSPATH" EXCEPTION TO THE GPL VERSION 2
+
+Certain source files distributed by Oracle are subject to the following clarification and special exception to the GPL Version 2, but only where Oracle has expressly included in the particular source file's header the words "Oracle designates this particular file as subject to the "Classpath" exception as provided by Oracle in the License file that accompanied this code."
+
+Linking this library statically or dynamically with other modules is making a combined work based on this library.  Thus, the terms and conditions of the GNU General Public License Version 2 cover the whole combination.
+
+As a special exception, the copyright holders of this library give you permission to link this library with independent modules to produce an executable, regardless of the license terms of these independent modules, and to copy and distribute the resulting executable under terms of your choice, provided that you also meet, for each linked independent module, the terms and conditions of the license of that module.  An independent module is a module which is not derived from or based on this library.  If you modify this library, you may extend this exception to your version of the library, but you are not obligated to do so.  If you do not wish to do so, delete this exception statement from your version.
diff --git a/plugins/discovery-azure/licenses/jersey-NOTICE.txt b/plugins/discovery-azure/licenses/jersey-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jersey-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/jersey-client-1.13.jar.sha1 b/plugins/discovery-azure/licenses/jersey-client-1.13.jar.sha1
new file mode 100644
index 0000000..6244c69
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jersey-client-1.13.jar.sha1
@@ -0,0 +1 @@
+0ec38c57a78940bf5f8f5971307ca89406849647
diff --git a/plugins/discovery-azure/licenses/jersey-core-1.13.jar.sha1 b/plugins/discovery-azure/licenses/jersey-core-1.13.jar.sha1
new file mode 100644
index 0000000..ee2aa99
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jersey-core-1.13.jar.sha1
@@ -0,0 +1 @@
+4326a56dc6b2d67b7313905c353e1af225bb164f
diff --git a/plugins/discovery-azure/licenses/jersey-json-1.13.jar.sha1 b/plugins/discovery-azure/licenses/jersey-json-1.13.jar.sha1
new file mode 100644
index 0000000..266f2fc
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jersey-json-1.13.jar.sha1
@@ -0,0 +1 @@
+f7346cce2c0e73afd39e2783c173ee134f79a0f9
diff --git a/plugins/discovery-azure/licenses/jettison-1.1.jar.sha1 b/plugins/discovery-azure/licenses/jettison-1.1.jar.sha1
new file mode 100644
index 0000000..53133f3
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jettison-1.1.jar.sha1
@@ -0,0 +1 @@
+1a01a2a1218fcf9faa2cc2a6ced025bdea687262
diff --git a/plugins/discovery-azure/licenses/jettison-LICENSE.txt b/plugins/discovery-azure/licenses/jettison-LICENSE.txt
new file mode 100644
index 0000000..6884c4d
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jettison-LICENSE.txt
@@ -0,0 +1,192 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   Copyright 2006 Envoi Solutions LLC
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
diff --git a/plugins/discovery-azure/licenses/jettison-NOTICE.txt b/plugins/discovery-azure/licenses/jettison-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/jettison-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/mail-1.4.5.jar.sha1 b/plugins/discovery-azure/licenses/mail-1.4.5.jar.sha1
new file mode 100644
index 0000000..b79503e
--- /dev/null
+++ b/plugins/discovery-azure/licenses/mail-1.4.5.jar.sha1
@@ -0,0 +1 @@
+85319c87280f30e1afc54c355f91f44741beac49
diff --git a/plugins/discovery-azure/licenses/mail-LICENSE.txt b/plugins/discovery-azure/licenses/mail-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/mail-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/mail-NOTICE.txt b/plugins/discovery-azure/licenses/mail-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/mail-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/stax-LICENSE.txt b/plugins/discovery-azure/licenses/stax-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/discovery-azure/licenses/stax-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/discovery-azure/licenses/stax-NOTICE.txt b/plugins/discovery-azure/licenses/stax-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/discovery-azure/licenses/stax-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/discovery-azure/licenses/stax-api-1.0-2.jar.sha1 b/plugins/discovery-azure/licenses/stax-api-1.0-2.jar.sha1
new file mode 100644
index 0000000..fb00ad8
--- /dev/null
+++ b/plugins/discovery-azure/licenses/stax-api-1.0-2.jar.sha1
@@ -0,0 +1 @@
+d6337b0de8b25e53e81b922352fbea9f9f57ba0b
diff --git a/plugins/discovery-azure/pom.xml b/plugins/discovery-azure/pom.xml
new file mode 100644
index 0000000..1ce0dc4
--- /dev/null
+++ b/plugins/discovery-azure/pom.xml
@@ -0,0 +1,73 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. ElasticSearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>discovery-azure</artifactId>
+    <name>Plugin: Discovery: Azure</name>
+    <description>The Azure Discovery plugin allows to use Azure API for the unicast discovery mechanism.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin</elasticsearch.plugin.classname>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>discovery_azure</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+        <!-- need -path because there is no resources dir... -->
+        <xlint.options>-Xlint:-path,-serial,-static,-unchecked</xlint.options>
+    </properties>
+
+    <dependencies>
+        <!-- Azure Management API -->
+        <dependency>
+            <groupId>com.microsoft.azure</groupId>
+            <artifactId>azure-management-compute</artifactId>
+            <version>0.7.0</version>
+            <exclusions>
+                <exclusion>
+                    <groupId>stax</groupId>
+                    <artifactId>stax-api</artifactId>
+                </exclusion>
+            </exclusions>
+        </dependency>
+        <dependency>
+            <groupId>com.microsoft.azure</groupId>
+            <artifactId>azure-management</artifactId>
+            <version>0.7.0</version>
+        </dependency>
+        <!-- We need to force here the compile scope as it was defined as test scope in plugins/pom.xml -->
+        <!-- TODO: remove this dependency when we will have a REST Test module -->
+        <dependency>
+            <groupId>org.apache.httpcomponents</groupId>
+            <artifactId>httpclient</artifactId>
+            <scope>compile</scope>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java
new file mode 100644
index 0000000..c10e86b
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.azure.management.AzureComputeService;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl;
+import org.elasticsearch.cloud.azure.management.AzureComputeSettingsFilter;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.azure.AzureDiscovery;
+
+/**
+ * Azure Module
+ *
+ * <ul>
+ * <li>If needed this module will bind azure discovery service by default
+ * to AzureComputeServiceImpl.</li>
+ * </ul>
+ *
+ * @see org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl
+ */
+public class AzureDiscoveryModule extends AbstractModule {
+    protected final ESLogger logger;
+    private Settings settings;
+
+    // pkg private so it is settable by tests
+    static Class<? extends AzureComputeService> computeServiceImpl = AzureComputeServiceImpl.class;
+
+    public static Class<? extends AzureComputeService> getComputeServiceImpl() {
+        return computeServiceImpl;
+    }
+
+    @Inject
+    public AzureDiscoveryModule(Settings settings) {
+        this.settings = settings;
+        this.logger = Loggers.getLogger(getClass(), settings);
+    }
+
+    @Override
+    protected void configure() {
+        logger.debug("starting azure services");
+        bind(AzureComputeSettingsFilter.class).asEagerSingleton();
+
+        // If we have set discovery to azure, let's start the azure compute service
+        if (isDiscoveryReady(settings, logger)) {
+            logger.debug("starting azure discovery service");
+            bind(AzureComputeService.class).to(computeServiceImpl).asEagerSingleton();
+        }
+    }
+
+    /**
+     * Check if discovery is meant to start
+     * @return true if we can start discovery features
+     */
+    public static boolean isDiscoveryReady(Settings settings, ESLogger logger) {
+        // User set discovery.type: azure
+        if (!AzureDiscovery.AZURE.equalsIgnoreCase(settings.get("discovery.type"))) {
+            logger.trace("discovery.type not set to {}", AzureDiscovery.AZURE);
+            return false;
+        }
+
+        if (isPropertyMissing(settings, Management.SUBSCRIPTION_ID) ||
+                isPropertyMissing(settings, Management.SERVICE_NAME) ||
+                isPropertyMissing(settings, Management.KEYSTORE_PATH) ||
+                isPropertyMissing(settings, Management.KEYSTORE_PASSWORD)
+            ) {
+            logger.debug("one or more azure discovery settings are missing. " +
+                            "Check elasticsearch.yml file. Should have [{}], [{}], [{}] and [{}].",
+                    Management.SUBSCRIPTION_ID,
+                    Management.SERVICE_NAME,
+                    Management.KEYSTORE_PATH,
+                    Management.KEYSTORE_PASSWORD);
+            return false;
+        }
+
+        logger.trace("all required properties for azure discovery are set!");
+
+        return true;
+    }
+
+    public static boolean isPropertyMissing(Settings settings, String name) throws ElasticsearchException {
+        if (!Strings.hasText(settings.get(name))) {
+            return true;
+        }
+        return false;
+    }
+
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
new file mode 100644
index 0000000..487997d
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+public class AzureServiceDisableException extends IllegalStateException {
+    public AzureServiceDisableException(String msg) {
+        super(msg);
+    }
+
+    public AzureServiceDisableException(String msg, Throwable cause) {
+        super(msg, cause);
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
new file mode 100644
index 0000000..4bd4f1d
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+public class AzureServiceRemoteException extends IllegalStateException {
+    public AzureServiceRemoteException(String msg) {
+        super(msg);
+    }
+
+    public AzureServiceRemoteException(String msg, Throwable cause) {
+        super(msg, cause);
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
new file mode 100644
index 0000000..c79a745
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.management;
+
+import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
+
+/**
+ *
+ */
+public interface AzureComputeService {
+
+    static public final class Management {
+        public static final String API_IMPLEMENTATION = "cloud.azure.management.api.impl";
+
+        public static final String SUBSCRIPTION_ID = "cloud.azure.management.subscription.id";
+        public static final String SERVICE_NAME = "cloud.azure.management.cloud.service.name";
+
+        // Keystore settings
+        public static final String KEYSTORE_PATH = "cloud.azure.management.keystore.path";
+        public static final String KEYSTORE_PASSWORD = "cloud.azure.management.keystore.password";
+        public static final String KEYSTORE_TYPE = "cloud.azure.management.keystore.type";
+    }
+
+    static public final class Discovery {
+        public static final String REFRESH = "discovery.azure.refresh_interval";
+
+        public static final String HOST_TYPE = "discovery.azure.host.type";
+        public static final String ENDPOINT_NAME = "discovery.azure.endpoint.name";
+        public static final String DEPLOYMENT_NAME = "discovery.azure.deployment.name";
+        public static final String DEPLOYMENT_SLOT = "discovery.azure.deployment.slot";
+    }
+    public HostedServiceGetDetailedResponse getServiceDetails();
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java
new file mode 100644
index 0000000..26406e3
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceImpl.java
@@ -0,0 +1,118 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.management;
+
+import com.microsoft.windowsazure.Configuration;
+import com.microsoft.windowsazure.core.utils.KeyStoreType;
+import com.microsoft.windowsazure.management.compute.ComputeManagementClient;
+import com.microsoft.windowsazure.management.compute.ComputeManagementService;
+import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
+import com.microsoft.windowsazure.management.configuration.ManagementConfiguration;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.azure.AzureServiceDisableException;
+import org.elasticsearch.cloud.azure.AzureServiceRemoteException;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+
+import static org.elasticsearch.cloud.azure.management.AzureComputeService.Management.*;
+
+/**
+ *
+ */
+public class AzureComputeServiceImpl extends AbstractLifecycleComponent<AzureComputeServiceImpl>
+    implements AzureComputeService {
+
+    static final class Azure {
+        private static final String ENDPOINT = "https://management.core.windows.net/";
+    }
+
+    private final ComputeManagementClient computeManagementClient;
+    private final String serviceName;
+
+    @Inject
+    public AzureComputeServiceImpl(Settings settings) {
+        super(settings);
+        String subscriptionId = settings.get(SUBSCRIPTION_ID);
+
+        serviceName = settings.get(Management.SERVICE_NAME);
+        String keystorePath = settings.get(KEYSTORE_PATH);
+        String keystorePassword = settings.get(KEYSTORE_PASSWORD);
+        String strKeyStoreType = settings.get(KEYSTORE_TYPE, KeyStoreType.pkcs12.name());
+        KeyStoreType tmpKeyStoreType = KeyStoreType.pkcs12;
+        try {
+            tmpKeyStoreType = KeyStoreType.fromString(strKeyStoreType);
+        } catch (Exception e) {
+            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", KEYSTORE_TYPE,
+                    strKeyStoreType, KeyStoreType.pkcs12.name());
+        }
+        KeyStoreType keystoreType = tmpKeyStoreType;
+
+        // Check that we have all needed properties
+        Configuration configuration;
+        try {
+            configuration = ManagementConfiguration.configure(new URI(Azure.ENDPOINT),
+                    subscriptionId, keystorePath, keystorePassword, keystoreType);
+        } catch (IOException|URISyntaxException e) {
+            logger.error("can not start azure client: {}", e.getMessage());
+            computeManagementClient = null;
+            return;
+        }
+        logger.trace("creating new Azure client for [{}], [{}]", subscriptionId, serviceName);
+        computeManagementClient = ComputeManagementService.create(configuration);
+    }
+
+    @Override
+    public HostedServiceGetDetailedResponse getServiceDetails() {
+        if (computeManagementClient == null) {
+            // Azure plugin is disabled
+            throw new AzureServiceDisableException("azure plugin is disabled.");
+        }
+
+        try {
+            return computeManagementClient.getHostedServicesOperations().getDetailed(serviceName);
+        } catch (Exception e) {
+            throw new AzureServiceRemoteException("can not get list of azure nodes", e);
+        }
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+        if (computeManagementClient != null) {
+            try {
+                computeManagementClient.close();
+            } catch (IOException e) {
+                logger.error("error while closing Azure client", e);
+            }
+        }
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
new file mode 100644
index 0000000..c4a1837
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeSettingsFilter.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.management;
+
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+
+import static org.elasticsearch.cloud.azure.management.AzureComputeService.Management.*;
+
+public class AzureComputeSettingsFilter extends AbstractComponent {
+
+    @Inject
+    public AzureComputeSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
+        super(settings);
+        // Cloud management API settings we need to hide
+        settingsFilter.addFilter(KEYSTORE_PATH);
+        settingsFilter.addFilter(KEYSTORE_PASSWORD);
+        settingsFilter.addFilter(KEYSTORE_TYPE);
+        settingsFilter.addFilter(SUBSCRIPTION_ID);
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
new file mode 100755
index 0000000..36b20b0
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+/**
+ *
+ */
+public class AzureDiscovery extends ZenDiscovery {
+
+    public static final String AZURE = "azure";
+
+    @Inject
+    public AzureDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
+                          ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
+                          DiscoverySettings discoverySettings, ElectMasterService electMasterService) {
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
+                pingService, electMasterService, discoverySettings);
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
new file mode 100644
index 0000000..b2e6821
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
@@ -0,0 +1,274 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import com.microsoft.windowsazure.management.compute.models.*;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cloud.azure.AzureServiceDisableException;
+import org.elasticsearch.cloud.azure.AzureServiceRemoteException;
+import org.elasticsearch.cloud.azure.management.AzureComputeService;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkAddress;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastHostsProvider;
+import org.elasticsearch.transport.TransportService;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.util.ArrayList;
+import java.util.Locale;
+import java.util.List;
+
+/**
+ *
+ */
+public class AzureUnicastHostsProvider extends AbstractComponent implements UnicastHostsProvider {
+
+    public enum HostType {
+        PRIVATE_IP("private_ip"),
+        PUBLIC_IP("public_ip");
+
+        private String type ;
+
+        HostType(String type) {
+            this.type = type ;
+        }
+
+        public static HostType fromString(String type) {
+            for (HostType hostType : values()) {
+                if (hostType.type.equalsIgnoreCase(type)) {
+                    return hostType;
+                }
+            }
+            return null;
+        }
+    }
+
+    public enum Deployment {
+        PRODUCTION("production", DeploymentSlot.Production),
+        STAGING("staging", DeploymentSlot.Staging);
+
+        private String deployment;
+        private DeploymentSlot slot;
+
+        Deployment(String deployment, DeploymentSlot slot) {
+            this.deployment = deployment;
+            this.slot = slot;
+        }
+
+        public static Deployment fromString(String string) {
+            for (Deployment deployment : values()) {
+                if (deployment.deployment.equalsIgnoreCase(string)) {
+                    return deployment;
+                }
+            }
+            return null;
+        }
+    }
+
+    private final AzureComputeService azureComputeService;
+    private TransportService transportService;
+    private NetworkService networkService;
+    private final Version version;
+
+    private final TimeValue refreshInterval;
+    private long lastRefresh;
+    private List<DiscoveryNode> cachedDiscoNodes;
+    private final HostType hostType;
+    private final String publicEndpointName;
+    private final String deploymentName;
+    private final DeploymentSlot deploymentSlot;
+
+    @Inject
+    public AzureUnicastHostsProvider(Settings settings, AzureComputeService azureComputeService,
+                                   TransportService transportService,
+                                   NetworkService networkService,
+                                   Version version) {
+        super(settings);
+        this.azureComputeService = azureComputeService;
+        this.transportService = transportService;
+        this.networkService = networkService;
+        this.version = version;
+
+        this.refreshInterval = settings.getAsTime(Discovery.REFRESH, TimeValue.timeValueSeconds(0));
+
+        String strHostType = settings.get(Discovery.HOST_TYPE, HostType.PRIVATE_IP.name()).toUpperCase(Locale.ROOT);
+        HostType tmpHostType = HostType.fromString(strHostType);
+        if (tmpHostType == null) {
+            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", Discovery.HOST_TYPE,
+                    strHostType, HostType.PRIVATE_IP.name().toLowerCase(Locale.ROOT));
+            tmpHostType = HostType.PRIVATE_IP;
+        }
+        this.hostType = tmpHostType;
+        this.publicEndpointName = settings.get(Discovery.ENDPOINT_NAME, "elasticsearch");
+
+        // Deployment name could be set with discovery.azure.deployment.name
+        // Default to cloud.azure.management.cloud.service.name
+        this.deploymentName = settings.get(Discovery.DEPLOYMENT_NAME);
+
+        // Reading deployment_slot
+        String strDeployment = settings.get(Discovery.DEPLOYMENT_SLOT, Deployment.PRODUCTION.deployment);
+        Deployment tmpDeployment = Deployment.fromString(strDeployment);
+        if (tmpDeployment == null) {
+            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", Discovery.DEPLOYMENT_SLOT, strDeployment,
+                    Deployment.PRODUCTION.deployment);
+            tmpDeployment = Deployment.PRODUCTION;
+        }
+        this.deploymentSlot = tmpDeployment.slot;
+    }
+
+    /**
+     * We build the list of Nodes from Azure Management API
+     * Information can be cached using `cloud.azure.refresh_interval` property if needed.
+     * Setting `cloud.azure.refresh_interval` to `-1` will cause infinite caching.
+     * Setting `cloud.azure.refresh_interval` to `0` will disable caching (default).
+     */
+    @Override
+    public List<DiscoveryNode> buildDynamicNodes() {
+        if (refreshInterval.millis() != 0) {
+            if (cachedDiscoNodes != null &&
+                    (refreshInterval.millis() < 0 || (System.currentTimeMillis() - lastRefresh) < refreshInterval.millis())) {
+                logger.trace("using cache to retrieve node list");
+                return cachedDiscoNodes;
+            }
+            lastRefresh = System.currentTimeMillis();
+        }
+        logger.debug("start building nodes list using Azure API");
+
+        cachedDiscoNodes = new ArrayList<>();
+
+        HostedServiceGetDetailedResponse detailed;
+        try {
+            detailed = azureComputeService.getServiceDetails();
+        } catch (AzureServiceDisableException e) {
+            logger.debug("Azure discovery service has been disabled. Returning empty list of nodes.");
+            return cachedDiscoNodes;
+        } catch (AzureServiceRemoteException e) {
+            // We got a remote exception
+            logger.warn("can not get list of azure nodes: [{}]. Returning empty list of nodes.", e.getMessage());
+            logger.trace("AzureServiceRemoteException caught", e);
+            return cachedDiscoNodes;
+        }
+
+        InetAddress ipAddress = null;
+        try {
+            ipAddress = networkService.resolvePublishHostAddress(null);
+            logger.trace("ip of current node: [{}]", ipAddress);
+        } catch (IOException e) {
+            // We can't find the publish host address... Hmmm. Too bad :-(
+            logger.trace("exception while finding ip", e);
+        }
+
+        for (HostedServiceGetDetailedResponse.Deployment deployment : detailed.getDeployments()) {
+            // We check the deployment slot
+            if (deployment.getDeploymentSlot() != deploymentSlot) {
+                logger.debug("current deployment slot [{}] for [{}] is different from [{}]. skipping...",
+                        deployment.getDeploymentSlot(), deployment.getName(), deploymentSlot);
+                continue;
+            }
+
+            // If provided, we check the deployment name
+            if (deploymentName != null && !deploymentName.equals(deployment.getName())) {
+                logger.debug("current deployment name [{}] different from [{}]. skipping...",
+                        deployment.getName(), deploymentName);
+                continue;
+            }
+
+            // We check current deployment status
+            if (deployment.getStatus() != DeploymentStatus.Starting &&
+                    deployment.getStatus() != DeploymentStatus.Deploying &&
+                    deployment.getStatus() != DeploymentStatus.Running) {
+                logger.debug("[{}] status is [{}]. skipping...",
+                        deployment.getName(), deployment.getStatus());
+                continue;
+            }
+
+            // In other case, it should be the right deployment so we can add it to the list of instances
+
+            for (RoleInstance instance : deployment.getRoleInstances()) {
+                String networkAddress = null;
+                // Let's detect if we want to use public or private IP
+                switch (hostType) {
+                    case PRIVATE_IP:
+                        InetAddress privateIp = instance.getIPAddress();
+
+                        if (privateIp != null) {
+                            if (privateIp.equals(ipAddress)) {
+                                logger.trace("adding ourselves {}", NetworkAddress.format(ipAddress));
+                            }
+                            networkAddress = NetworkAddress.formatAddress(privateIp);
+                        } else {
+                            logger.trace("no private ip provided. ignoring [{}]...", instance.getInstanceName());
+                        }
+                        break;
+                    case PUBLIC_IP:
+                        for (InstanceEndpoint endpoint : instance.getInstanceEndpoints()) {
+                            if (!publicEndpointName.equals(endpoint.getName())) {
+                                logger.trace("ignoring endpoint [{}] as different than [{}]",
+                                        endpoint.getName(), publicEndpointName);
+                                continue;
+                            }
+
+                            networkAddress = NetworkAddress.formatAddress(new InetSocketAddress(endpoint.getVirtualIPAddress(), endpoint.getPort()));
+                        }
+
+                        if (networkAddress == null) {
+                            logger.trace("no public ip provided. ignoring [{}]...", instance.getInstanceName());
+                        }
+                        break;
+                    default:
+                        // This could never happen!
+                        logger.warn("undefined host_type [{}]. Please check your settings.", hostType);
+                        return cachedDiscoNodes;
+                }
+
+                if (networkAddress == null) {
+                    // We have a bad parameter here or not enough information from azure
+                    logger.warn("no network address found. ignoring [{}]...", instance.getInstanceName());
+                    continue;
+                }
+
+                try {
+                    // we only limit to 1 port per address, makes no sense to ping 100 ports
+                    TransportAddress[] addresses = transportService.addressesFromString(networkAddress, 1);
+                    for (TransportAddress address : addresses) {
+                        logger.trace("adding {}, transport_address {}", networkAddress, address);
+                        cachedDiscoNodes.add(new DiscoveryNode("#cloud-" + instance.getInstanceName(), address,
+                            version.minimumCompatibilityVersion()));
+                    }
+                } catch (Exception e) {
+                    logger.warn("can not convert [{}] to transport address. skipping. [{}]", networkAddress, e.getMessage());
+                }
+            }
+        }
+
+        logger.debug("{} node(s) added", cachedDiscoNodes.size());
+
+        return cachedDiscoNodes;
+    }
+}
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
new file mode 100644
index 0000000..e61a82a
--- /dev/null
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AzureDiscoveryModule;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.azure.AzureDiscovery;
+import org.elasticsearch.discovery.azure.AzureUnicastHostsProvider;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.Collection;
+import java.util.Collections;
+
+/**
+ *
+ */
+public class AzureDiscoveryPlugin extends Plugin {
+
+    private final Settings settings;
+    protected final ESLogger logger = Loggers.getLogger(AzureDiscoveryPlugin.class);
+
+    public AzureDiscoveryPlugin(Settings settings) {
+        this.settings = settings;
+        logger.trace("starting azure discovery plugin...");
+    }
+
+    @Override
+    public String name() {
+        return "discovery-azure";
+    }
+
+    @Override
+    public String description() {
+        return "Azure Discovery Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        return Collections.singletonList((Module) new AzureDiscoveryModule(settings));
+    }
+
+    public void onModule(DiscoveryModule discoveryModule) {
+        if (AzureDiscoveryModule.isDiscoveryReady(settings, logger)) {
+            discoveryModule.addDiscoveryType("azure", AzureDiscovery.class);
+            discoveryModule.addUnicastHostProvider(AzureUnicastHostsProvider.class);
+        }
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
new file mode 100644
index 0000000..bf1ba94
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.Collection;
+
+public abstract class AbstractAzureComputeServiceTestCase extends ESIntegTestCase {
+
+    private Class<? extends Plugin> mockPlugin;
+
+    public AbstractAzureComputeServiceTestCase(Class<? extends Plugin> mockPlugin) {
+        // We want to inject the Azure API Mock
+        this.mockPlugin = mockPlugin;
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+            .put(super.nodeSettings(nodeOrdinal))
+            .put("discovery.type", "azure")
+                // We need the network to make the mock working
+            .put("node.mode", "network");
+
+        // We add a fake subscription_id to start mock compute service
+        builder.put(Management.SUBSCRIPTION_ID, "fake")
+            .put(Discovery.REFRESH, "5s")
+            .put(Management.KEYSTORE_PATH, "dummy")
+            .put(Management.KEYSTORE_PASSWORD, "dummy")
+            .put(Management.SERVICE_NAME, "dummy");
+        return builder.build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureDiscoveryPlugin.class, mockPlugin);
+    }
+
+    protected void checkNumberOfNodes(int expected) {
+        NodesInfoResponse nodeInfos = client().admin().cluster().prepareNodesInfo().execute().actionGet();
+        assertNotNull(nodeInfos);
+        assertNotNull(nodeInfos.getNodes());
+        assertEquals(expected, nodeInfos.getNodes().length);
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
new file mode 100644
index 0000000..9747543
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
+
+import java.util.Collection;
+
+/**
+ * Base class for Azure tests that require credentials.
+ * <p>
+ * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
+ * in order to run these tests.
+ */
+@ThirdParty
+public abstract class AbstractAzureTestCase extends ESIntegTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put(readSettingsFromFile())
+                .build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureDiscoveryPlugin.class);
+    }
+
+    protected Settings readSettingsFromFile() {
+        Settings.Builder settings = Settings.builder();
+        settings.put("path.home", createTempDir());
+
+        // if explicit, just load it and don't load from env
+        try {
+            if (Strings.hasText(System.getProperty("tests.config"))) {
+                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
+            } else {
+                throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
+            }
+        } catch (SettingsException exception) {
+          throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
+        }
+        return settings.build();
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
new file mode 100644
index 0000000..5f43de3
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import com.microsoft.windowsazure.management.compute.models.DeploymentSlot;
+import com.microsoft.windowsazure.management.compute.models.DeploymentStatus;
+import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
+import com.microsoft.windowsazure.management.compute.models.InstanceEndpoint;
+import com.microsoft.windowsazure.management.compute.models.RoleInstance;
+import org.elasticsearch.cloud.azure.management.AzureComputeServiceAbstractMock;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.plugins.Plugin;
+
+import java.net.InetAddress;
+
+/**
+ * Mock Azure API with a single started node
+ */
+public class AzureComputeServiceSimpleMock extends AzureComputeServiceAbstractMock {
+
+    public static class TestPlugin extends Plugin {
+        @Override
+        public String name() {
+            return "mock-compute-service";
+        }
+        @Override
+        public String description() {
+            return "plugs in a mock compute service for testing";
+        }
+        public void onModule(AzureDiscoveryModule azureDiscoveryModule) {
+            azureDiscoveryModule.computeServiceImpl = AzureComputeServiceSimpleMock.class;
+        }
+    }
+
+    @Inject
+    public AzureComputeServiceSimpleMock(Settings settings) {
+        super(settings);
+    }
+
+    @Override
+    public HostedServiceGetDetailedResponse getServiceDetails() {
+        HostedServiceGetDetailedResponse response = new HostedServiceGetDetailedResponse();
+        HostedServiceGetDetailedResponse.Deployment deployment = new HostedServiceGetDetailedResponse.Deployment();
+
+        // Fake the deployment
+        deployment.setName("dummy");
+        deployment.setDeploymentSlot(DeploymentSlot.Production);
+        deployment.setStatus(DeploymentStatus.Running);
+
+        // Fake an instance
+        RoleInstance instance = new RoleInstance();
+        instance.setInstanceName("dummy1");
+
+        // Fake the private IP
+        instance.setIPAddress(InetAddress.getLoopbackAddress());
+
+        // Fake the public IP
+        InstanceEndpoint endpoint = new InstanceEndpoint();
+        endpoint.setName("elasticsearch");
+        endpoint.setVirtualIPAddress(InetAddress.getLoopbackAddress());
+        endpoint.setPort(9400);
+        instance.setInstanceEndpoints(CollectionUtils.newSingletonArrayList(endpoint));
+
+        deployment.setRoleInstances(CollectionUtils.newSingletonArrayList(instance));
+        response.setDeployments(CollectionUtils.newSingletonArrayList(deployment));
+
+        return response;
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java
new file mode 100644
index 0000000..829c1dc
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import com.microsoft.windowsazure.management.compute.models.DeploymentSlot;
+import com.microsoft.windowsazure.management.compute.models.DeploymentStatus;
+import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
+import com.microsoft.windowsazure.management.compute.models.InstanceEndpoint;
+import com.microsoft.windowsazure.management.compute.models.RoleInstance;
+import org.elasticsearch.cloud.azure.management.AzureComputeServiceAbstractMock;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.network.NetworkService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugins.Plugin;
+
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import static org.elasticsearch.common.util.CollectionUtils.newSingletonArrayList;
+
+
+/**
+ * Mock Azure API with two started nodes
+ */
+public class AzureComputeServiceTwoNodesMock extends AzureComputeServiceAbstractMock {
+    public static class TestPlugin extends Plugin {
+        @Override
+        public String name() {
+            return "mock-compute-service";
+        }
+        @Override
+        public String description() {
+            return "plugs in a mock compute service for testing";
+        }
+        public void onModule(AzureDiscoveryModule azureDiscoveryModule) {
+            azureDiscoveryModule.computeServiceImpl = AzureComputeServiceTwoNodesMock.class;
+        }
+    }
+
+    NetworkService networkService;
+
+    @Inject
+    protected AzureComputeServiceTwoNodesMock(Settings settings, NetworkService networkService) {
+        super(settings);
+        this.networkService = networkService;
+    }
+
+    @Override
+    public HostedServiceGetDetailedResponse getServiceDetails() {
+        HostedServiceGetDetailedResponse response = new HostedServiceGetDetailedResponse();
+        HostedServiceGetDetailedResponse.Deployment deployment = new HostedServiceGetDetailedResponse.Deployment();
+
+        // Fake the deployment
+        deployment.setName("dummy");
+        deployment.setDeploymentSlot(DeploymentSlot.Production);
+        deployment.setStatus(DeploymentStatus.Running);
+
+        // Fake a first instance
+        RoleInstance instance1 = new RoleInstance();
+        instance1.setInstanceName("dummy1");
+
+        // Fake the private IP
+        instance1.setIPAddress(InetAddress.getLoopbackAddress());
+
+        // Fake the public IP
+        InstanceEndpoint endpoint1 = new InstanceEndpoint();
+        endpoint1.setName("elasticsearch");
+        endpoint1.setVirtualIPAddress(InetAddress.getLoopbackAddress());
+        endpoint1.setPort(9400);
+        instance1.setInstanceEndpoints(newSingletonArrayList(endpoint1));
+
+        // Fake a first instance
+        RoleInstance instance2 = new RoleInstance();
+        instance2.setInstanceName("dummy1");
+
+        // Fake the private IP
+        instance2.setIPAddress(InetAddress.getLoopbackAddress());
+
+        // Fake the public IP
+        InstanceEndpoint endpoint2 = new InstanceEndpoint();
+        endpoint2.setName("elasticsearch");
+        endpoint2.setVirtualIPAddress(InetAddress.getLoopbackAddress());
+        endpoint2.setPort(9401);
+        instance2.setInstanceEndpoints(newSingletonArrayList(endpoint2));
+
+        deployment.setRoleInstances(new ArrayList<>(Arrays.asList(instance1, instance2)));
+
+        response.setDeployments(newSingletonArrayList(deployment));
+
+        return response;
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java
new file mode 100644
index 0000000..c11060a
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/management/AzureComputeServiceAbstractMock.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.management;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.settings.Settings;
+
+/**
+ *
+ */
+public abstract class AzureComputeServiceAbstractMock extends AbstractLifecycleComponent<AzureComputeServiceAbstractMock>
+    implements AzureComputeService {
+
+    protected AzureComputeServiceAbstractMock(Settings settings) {
+        super(settings);
+        logger.debug("starting Azure Mock [{}]", this.getClass().getSimpleName());
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+        logger.debug("starting Azure Api Mock");
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+        logger.debug("stopping Azure Api Mock");
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureDiscoveryRestIT.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureDiscoveryRestIT.java
new file mode 100644
index 0000000..6388883
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureDiscoveryRestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class AzureDiscoveryRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureDiscoveryPlugin.class);
+    }
+
+    public AzureDiscoveryRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
new file mode 100644
index 0000000..8465136
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.MasterNotDiscoveredException;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+
+/**
+ * Reported issue in #15
+ * (https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/15)
+ */
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+@AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch-cloud-azure/issues/89")
+public class AzureMinimumMasterNodesTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureMinimumMasterNodesTests() {
+        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("discovery.zen.minimum_master_nodes", 2)
+                // Make the test run faster
+                .put("discovery.zen.join.timeout", "50ms")
+                .put("discovery.zen.ping.timeout", "10ms")
+                .put("discovery.initial_state_timeout", "100ms");
+        return builder.build();
+    }
+
+    @Test
+    public void simpleOnlyMasterNodeElection() throws IOException {
+        logger.info("--> start data node / non master node");
+        internalCluster().startNode();
+        try {
+            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("100ms").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
+            fail("should not be able to find master");
+        } catch (MasterNotDiscoveredException e) {
+            // all is well, no master elected
+        }
+        logger.info("--> start another node");
+        internalCluster().startNode();
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> stop master node");
+        internalCluster().stopCurrentMasterNode();
+
+        try {
+            assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), nullValue());
+            fail("should not be able to find master");
+        } catch (MasterNotDiscoveredException e) {
+            // all is well, no master elected
+        }
+
+        logger.info("--> start another node");
+        internalCluster().startNode();
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
new file mode 100644
index 0000000..74daf1a
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.cloud.azure.AzureComputeServiceSimpleMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureSimpleTests() {
+        super(AzureComputeServiceSimpleMock.TestPlugin.class);
+    }
+
+    @Test
+    public void one_node_should_run_using_private_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "private_ip");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+
+    @Test
+    public void one_node_should_run_using_public_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "public_ip");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+
+    @Test
+    public void one_node_should_run_using_wrong_settings() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "do_not_exist");
+
+        logger.info("--> start one node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 1 node as part of the cluster, let's test that
+        checkNumberOfNodes(1);
+    }
+}
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
new file mode 100644
index 0000000..dbccc4a
--- /dev/null
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.discovery.azure;
+
+import org.elasticsearch.cloud.azure.AbstractAzureComputeServiceTestCase;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Discovery;
+import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
+import org.elasticsearch.cloud.azure.AzureComputeServiceTwoNodesMock;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
+        numDataNodes = 0,
+        transportClientRatio = 0.0,
+        numClientNodes = 0)
+public class AzureTwoStartedNodesTests extends AbstractAzureComputeServiceTestCase {
+
+    public AzureTwoStartedNodesTests() {
+        super(AzureComputeServiceTwoNodesMock.TestPlugin.class);
+    }
+
+    @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
+    public void two_nodes_should_run_using_private_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "private_ip");
+
+        logger.info("--> start first node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> start another node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 2 nodes as part of the cluster, let's test that
+        checkNumberOfNodes(2);
+    }
+
+    @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/11533")
+    public void two_nodes_should_run_using_public_ip() {
+        Settings.Builder settings = Settings.settingsBuilder()
+                .put(Management.SERVICE_NAME, "dummy")
+                .put(Discovery.HOST_TYPE, "public_ip");
+
+        logger.info("--> start first node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        logger.info("--> start another node");
+        internalCluster().startNode(settings);
+        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
+
+        // We expect having 2 nodes as part of the cluster, let's test that
+        checkNumberOfNodes(2);
+    }
+}
diff --git a/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml b/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml
new file mode 100644
index 0000000..51ba41e
--- /dev/null
+++ b/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Azure Discovery component
+#
+"Discovery Azure loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: discovery-azure  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-ec2/LICENSE.txt b/plugins/discovery-ec2/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/discovery-ec2/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/discovery-ec2/NOTICE.txt b/plugins/discovery-ec2/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/discovery-ec2/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
index 4acbca4..f04d3ec 100755
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java
@@ -37,7 +37,7 @@ import java.nio.charset.StandardCharsets;
 /**
  * Resolves certain ec2 related 'meta' hostnames into an actual hostname
  * obtained from ec2 meta-data.
- * <p/>
+ * <p>
  * Valid config values for {@link Ec2HostnameType}s are -
  * <ul>
  * <li>_ec2_ - maps to privateIpv4</li>
@@ -88,8 +88,7 @@ public class Ec2NameResolver extends AbstractComponent implements CustomNameReso
 
     /**
      * @param type the ec2 hostname type to discover.
-     * @return the appropriate host resolved from ec2 meta-data.
-     * @throws IOException if ec2 meta-data cannot be obtained.
+     * @return the appropriate host resolved from ec2 meta-data, or null if it cannot be obtained.
      * @see CustomNameResolver#resolveIfPossible(String)
      */
     public InetAddress[] resolve(Ec2HostnameType type, boolean warnOnFailure) {
diff --git a/plugins/discovery-multicast/LICENSE.txt b/plugins/discovery-multicast/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/discovery-multicast/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/discovery-multicast/NOTICE.txt b/plugins/discovery-multicast/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/discovery-multicast/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java
index 3ec70a4..ba620dd 100644
--- a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastChannel.java
@@ -163,7 +163,7 @@ public abstract class MulticastChannel implements Closeable {
 
     public static final String SHARED_CHANNEL_NAME = "#shared#";
     /**
-     * A shared channel that keeps a static map of Config -> Shared channels, and closes shared
+     * A shared channel that keeps a static map of Config -&gt; Shared channels, and closes shared
      * channel once their reference count has reached 0. It also handles de-registering relevant
      * listener from the shared list of listeners.
      */
diff --git a/plugins/jvm-example/LICENSE.txt b/plugins/jvm-example/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/jvm-example/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/jvm-example/NOTICE.txt b/plugins/jvm-example/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/jvm-example/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/lang-javascript/LICENSE.txt b/plugins/lang-javascript/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/lang-javascript/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/lang-javascript/NOTICE.txt b/plugins/lang-javascript/NOTICE.txt
deleted file mode 100644
index a961b1d..0000000
--- a/plugins/lang-javascript/NOTICE.txt
+++ /dev/null
@@ -1,11 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-rhino-*.jar is licensed under the MPL 2.0.  The original source code for this
-can be found at https://github.com/mozilla/rhino.
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeMap.java b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeMap.java
index a43ecec..1dbf345 100644
--- a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeMap.java
+++ b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeMap.java
@@ -41,8 +41,6 @@ public class NativeMap implements Scriptable, Wrapper {
     /**
      * Construct
      *
-     * @param scope
-     * @param map
      * @return native map
      */
     public static NativeMap wrap(Scriptable scope, Map<Object, Object> map) {
@@ -51,9 +49,6 @@ public class NativeMap implements Scriptable, Wrapper {
 
     /**
      * Construct
-     *
-     * @param scope
-     * @param map
      */
     private NativeMap(Scriptable scope, Map<Object, Object> map) {
         this.parentScope = scope;
diff --git a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptValueConverter.java b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptValueConverter.java
index 7f9c390..f3a3989 100644
--- a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptValueConverter.java
+++ b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptValueConverter.java
@@ -41,7 +41,7 @@ public final class ScriptValueConverter {
     /**
      * Convert an object from a script wrapper value to a serializable value valid outside
      * of the Rhino script processor context.
-     * <p/>
+     * <p>
      * This includes converting JavaScript Array objects to Lists of valid objects.
      *
      * @param value Value to convert from script wrapper object to external object value.
diff --git a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableWrappedMap.java b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableWrappedMap.java
index a7c2f4f..9ff1f61 100644
--- a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableWrappedMap.java
+++ b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableWrappedMap.java
@@ -32,7 +32,6 @@ import java.util.Set;
  * persisted directly to an underlying map supplied on construction. The class automatically
  * wraps/unwraps JS objects as they enter/leave the underlying map via the Scriptable interface
  * methods - objects are untouched if accessed via the usual Map interface methods.
- * <p/>
  * <p>Access should be by string key only - not integer index - unless you are sure the wrapped
  * map will maintain insertion order of the elements.
  *
@@ -46,9 +45,6 @@ public class ScriptableWrappedMap implements ScriptableMap, Wrapper {
 
     /**
      * Construction
-     *
-     * @param scope
-     * @param map
      * @return scriptable wrapped map
      */
     public static ScriptableWrappedMap wrap(Scriptable scope, Map<Object, Object> map) {
@@ -57,8 +53,6 @@ public class ScriptableWrappedMap implements ScriptableMap, Wrapper {
 
     /**
      * Construct
-     *
-     * @param map
      */
     public ScriptableWrappedMap(Map map) {
         this.map = map;
@@ -66,9 +60,6 @@ public class ScriptableWrappedMap implements ScriptableMap, Wrapper {
 
     /**
      * Construct
-     *
-     * @param scope
-     * @param map
      */
     public ScriptableWrappedMap(Scriptable scope, Map map) {
         this.parentScope = scope;
diff --git a/plugins/lang-python/LICENSE.txt b/plugins/lang-python/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/lang-python/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/lang-python/NOTICE.txt b/plugins/lang-python/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/lang-python/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
index 4bdf985..199edc3 100644
--- a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
+++ b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.script.python;
 
 import java.io.IOException;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
 import java.util.Map;
 
 import org.apache.lucene.index.LeafReaderContext;
@@ -54,7 +56,13 @@ public class PythonScriptEngineService extends AbstractComponent implements Scri
     public PythonScriptEngineService(Settings settings) {
         super(settings);
 
-        this.interp = PythonInterpreter.threadLocalStateInterpreter(null);
+        // classloader created here
+        this.interp = AccessController.doPrivileged(new PrivilegedAction<PythonInterpreter> () {
+            @Override
+            public PythonInterpreter run() {
+                return PythonInterpreter.threadLocalStateInterpreter(null);
+            }
+        });
     }
 
     @Override
@@ -74,7 +82,13 @@ public class PythonScriptEngineService extends AbstractComponent implements Scri
 
     @Override
     public Object compile(String script) {
-        return interp.compile(script);
+        // classloader created here
+        return AccessController.doPrivileged(new PrivilegedAction<PyCode>() {
+            @Override
+            public PyCode run() {
+                return interp.compile(script);
+            }
+        });
     }
 
     @Override
diff --git a/plugins/pom.xml b/plugins/pom.xml
index 35a455a..3462485 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -396,16 +396,18 @@
         <module>analysis-phonetic</module>
         <module>analysis-smartcn</module>
         <module>analysis-stempel</module>
-        <module>cloud-azure</module>
         <module>cloud-gce</module>
         <module>delete-by-query</module>
+        <module>discovery-azure</module>
         <module>discovery-ec2</module>
         <module>discovery-multicast</module>
         <module>lang-javascript</module>
         <module>lang-python</module>
         <module>mapper-murmur3</module>
         <module>mapper-size</module>
+        <module>repository-azure</module>
         <module>repository-s3</module>
+        <module>store-smb</module>
 
         <!-- Internal plugins for test only -->
         <module>jvm-example</module>
diff --git a/plugins/repository-azure/LICENSE.txt b/plugins/repository-azure/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/repository-azure/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/repository-azure/licenses/azure-LICENSE.txt b/plugins/repository-azure/licenses/azure-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/repository-azure/licenses/azure-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/repository-azure/licenses/azure-NOTICE.txt b/plugins/repository-azure/licenses/azure-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/repository-azure/licenses/azure-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/repository-azure/licenses/azure-storage-2.0.0.jar.sha1 b/plugins/repository-azure/licenses/azure-storage-2.0.0.jar.sha1
new file mode 100644
index 0000000..9e72b7b
--- /dev/null
+++ b/plugins/repository-azure/licenses/azure-storage-2.0.0.jar.sha1
@@ -0,0 +1 @@
+b970c65a38da0569013e0c76de7c404f842496c2
diff --git a/plugins/repository-azure/licenses/commons-lang3-3.3.2.jar.sha1 b/plugins/repository-azure/licenses/commons-lang3-3.3.2.jar.sha1
new file mode 100644
index 0000000..bdd913c
--- /dev/null
+++ b/plugins/repository-azure/licenses/commons-lang3-3.3.2.jar.sha1
@@ -0,0 +1 @@
+90a3822c38ec8c996e84c16a3477ef632cbc87a3
diff --git a/plugins/repository-azure/licenses/commons-lang3-LICENSE.txt b/plugins/repository-azure/licenses/commons-lang3-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/repository-azure/licenses/commons-lang3-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/repository-azure/licenses/commons-lang3-NOTICE.txt b/plugins/repository-azure/licenses/commons-lang3-NOTICE.txt
new file mode 100644
index 0000000..0782824
--- /dev/null
+++ b/plugins/repository-azure/licenses/commons-lang3-NOTICE.txt
@@ -0,0 +1,8 @@
+Apache Commons Lang
+Copyright 2001-2014 The Apache Software Foundation
+
+This product includes software developed at
+The Apache Software Foundation (http://www.apache.org/).
+
+This product includes software from the Spring Framework,
+under the Apache License 2.0 (see: StringUtils.containsWhitespace())
diff --git a/plugins/repository-azure/pom.xml b/plugins/repository-azure/pom.xml
new file mode 100644
index 0000000..a6aff7f
--- /dev/null
+++ b/plugins/repository-azure/pom.xml
@@ -0,0 +1,54 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. ElasticSearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>repository-azure</artifactId>
+    <name>Plugin: Repository: Azure</name>
+    <description>The Azure Repository plugin adds support for Azure storage repositories.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.repository.azure.AzureRepositoryPlugin</elasticsearch.plugin.classname>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>repository_azure</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+        <xlint.options>-Xlint:-deprecation,-serial</xlint.options>
+    </properties>
+
+    <dependencies>
+        <!-- Azure Storage API -->
+        <dependency>
+            <groupId>com.microsoft.azure</groupId>
+            <artifactId>azure-storage</artifactId>
+            <version>2.0.0</version>
+        </dependency>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureRepositoryModule.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureRepositoryModule.java
new file mode 100644
index 0000000..e89f523
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureRepositoryModule.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
+import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
+import org.elasticsearch.cloud.azure.storage.AzureStorageSettingsFilter;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+
+/**
+ * Azure Module
+ *
+ * <ul>
+ * <li>If needed this module will bind azure repository service by default
+ * to AzureStorageServiceImpl.</li>
+ * </ul>
+ *
+ * @see org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl
+ */
+public class AzureRepositoryModule extends AbstractModule {
+    protected final ESLogger logger;
+    private Settings settings;
+
+    // pkg private so it is settable by tests
+    static Class<? extends AzureStorageService> storageServiceImpl = AzureStorageServiceImpl.class;
+
+    public static Class<? extends AzureStorageService> getStorageServiceImpl() {
+        return storageServiceImpl;
+    }
+
+    @Inject
+    public AzureRepositoryModule(Settings settings) {
+        this.settings = settings;
+        this.logger = Loggers.getLogger(getClass(), settings);
+    }
+
+    @Override
+    protected void configure() {
+        logger.debug("starting azure services");
+        bind(AzureStorageSettingsFilter.class).asEagerSingleton();
+
+        // If we have settings for azure repository, let's start the azure storage service
+        if (isSnapshotReady(settings, logger)) {
+            logger.debug("starting azure repository service");
+            bind(AzureStorageService.class).to(storageServiceImpl).asEagerSingleton();
+        }
+    }
+
+    /**
+     * Check if we have repository azure settings available
+     * @return true if we can use snapshot and restore
+     */
+    public static boolean isSnapshotReady(Settings settings, ESLogger logger) {
+        if (isPropertyMissing(settings, Storage.ACCOUNT) ||
+                isPropertyMissing(settings, Storage.KEY)) {
+            logger.debug("azure repository is not set using [{}] and [{}] properties",
+                    Storage.ACCOUNT,
+                    Storage.KEY);
+            return false;
+        }
+
+        logger.trace("all required properties for azure repository are set!");
+
+        return true;
+   }
+
+    public static boolean isPropertyMissing(Settings settings, String name) throws ElasticsearchException {
+        if (!Strings.hasText(settings.get(name))) {
+            return true;
+        }
+        return false;
+    }
+
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
new file mode 100644
index 0000000..487997d
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceDisableException.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+public class AzureServiceDisableException extends IllegalStateException {
+    public AzureServiceDisableException(String msg) {
+        super(msg);
+    }
+
+    public AzureServiceDisableException(String msg, Throwable cause) {
+        super(msg, cause);
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
new file mode 100644
index 0000000..4bd4f1d
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/AzureServiceRemoteException.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+public class AzureServiceRemoteException extends IllegalStateException {
+    public AzureServiceRemoteException(String msg) {
+        super(msg);
+    }
+
+    public AzureServiceRemoteException(String msg, Throwable cause) {
+        super(msg, cause);
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
new file mode 100644
index 0000000..a7c980c
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java
@@ -0,0 +1,148 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.blobstore;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.blobstore.BlobMetaData;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.support.AbstractLegacyBlobContainer;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.repositories.RepositoryException;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.net.HttpURLConnection;
+import java.net.URISyntaxException;
+import java.util.Map;
+
+/**
+ *
+ */
+public class AzureBlobContainer extends AbstractLegacyBlobContainer {
+
+    protected final ESLogger logger = Loggers.getLogger(AzureBlobContainer.class);
+    protected final AzureBlobStore blobStore;
+
+    protected final String keyPath;
+    protected final String repositoryName;
+
+    public AzureBlobContainer(String repositoryName, BlobPath path, AzureBlobStore blobStore) {
+        super(path);
+        this.blobStore = blobStore;
+        String keyPath = path.buildAsString("/");
+        if (!keyPath.isEmpty()) {
+            keyPath = keyPath + "/";
+        }
+        this.keyPath = keyPath;
+        this.repositoryName = repositoryName;
+    }
+
+    @Override
+    public boolean blobExists(String blobName) {
+        try {
+            return blobStore.client().blobExists(blobStore.container(), buildKey(blobName));
+        } catch (URISyntaxException | StorageException e) {
+            logger.warn("can not access [{}] in container {{}}: {}", blobName, blobStore.container(), e.getMessage());
+        }
+        return false;
+    }
+
+    @Override
+    public InputStream openInput(String blobName) throws IOException {
+        try {
+            return blobStore.client().getInputStream(blobStore.container(), buildKey(blobName));
+        } catch (StorageException e) {
+            if (e.getHttpStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {
+                throw new FileNotFoundException(e.getMessage());
+            }
+            throw new IOException(e);
+        } catch (URISyntaxException e) {
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public OutputStream createOutput(String blobName) throws IOException {
+        try {
+            return new AzureOutputStream(blobStore.client().getOutputStream(blobStore.container(), buildKey(blobName)));
+        } catch (StorageException e) {
+            if (e.getHttpStatusCode() == HttpURLConnection.HTTP_NOT_FOUND) {
+                throw new FileNotFoundException(e.getMessage());
+            }
+            throw new IOException(e);
+        } catch (URISyntaxException e) {
+            throw new IOException(e);
+        } catch (IllegalArgumentException e) {
+            throw new RepositoryException(repositoryName, e.getMessage());
+        }
+    }
+
+    @Override
+    public void deleteBlob(String blobName) throws IOException {
+        try {
+            blobStore.client().deleteBlob(blobStore.container(), buildKey(blobName));
+        } catch (URISyntaxException | StorageException e) {
+            logger.warn("can not access [{}] in container {{}}: {}", blobName, blobStore.container(), e.getMessage());
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobsByPrefix(@Nullable String prefix) throws IOException {
+
+        try {
+            return blobStore.client().listBlobsByPrefix(blobStore.container(), keyPath, prefix);
+        } catch (URISyntaxException | StorageException e) {
+            logger.warn("can not access [{}] in container {{}}: {}", prefix, blobStore.container(), e.getMessage());
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public void move(String sourceBlobName, String targetBlobName) throws IOException {
+        try {
+            String source = keyPath + sourceBlobName;
+            String target = keyPath + targetBlobName;
+
+            logger.debug("moving blob [{}] to [{}] in container {{}}", source, target, blobStore.container());
+
+            blobStore.client().moveBlob(blobStore.container(), source, target);
+        } catch (URISyntaxException e) {
+            logger.warn("can not move blob [{}] to [{}] in container {{}}: {}", sourceBlobName, targetBlobName, blobStore.container(), e.getMessage());
+            throw new IOException(e);
+        } catch (StorageException e) {
+            logger.warn("can not move blob [{}] to [{}] in container {{}}: {}", sourceBlobName, targetBlobName, blobStore.container(), e.getMessage());
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobs() throws IOException {
+        return listBlobsByPrefix(null);
+    }
+
+    protected String buildKey(String blobName) {
+        return keyPath + (blobName == null ? "" : blobName);
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java
new file mode 100644
index 0000000..6edcae7
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobStore.java
@@ -0,0 +1,92 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.blobstore;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.common.blobstore.BlobContainer;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.repositories.RepositoryName;
+import org.elasticsearch.repositories.RepositorySettings;
+
+import java.net.URISyntaxException;
+
+import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.CONTAINER;
+import static org.elasticsearch.repositories.azure.AzureRepository.CONTAINER_DEFAULT;
+
+/**
+ *
+ */
+public class AzureBlobStore extends AbstractComponent implements BlobStore {
+
+    private final AzureStorageService client;
+
+    private final String container;
+    private final String repositoryName;
+
+    @Inject
+    public AzureBlobStore(RepositoryName name, Settings settings, RepositorySettings repositorySettings,
+                          AzureStorageService client) throws URISyntaxException, StorageException {
+        super(settings);
+        this.client = client;
+        this.container = repositorySettings.settings().get("container", settings.get(CONTAINER, CONTAINER_DEFAULT));
+        this.repositoryName = name.getName();
+    }
+
+    @Override
+    public String toString() {
+        return container;
+    }
+
+    public AzureStorageService client() {
+        return client;
+    }
+
+    public String container() {
+        return container;
+    }
+
+    @Override
+    public BlobContainer blobContainer(BlobPath path) {
+        return new AzureBlobContainer(repositoryName, path, this);
+    }
+
+    @Override
+    public void delete(BlobPath path) {
+        String keyPath = path.buildAsString("/");
+        if (!keyPath.isEmpty()) {
+            keyPath = keyPath + "/";
+        }
+
+        try {
+            client.deleteFiles(container, keyPath);
+        } catch (URISyntaxException | StorageException e) {
+            logger.warn("can not remove [{}] in container {{}}: {}", keyPath, container, e.getMessage());
+        }
+    }
+
+    @Override
+    public void close() {
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java
new file mode 100644
index 0000000..6a95eeb
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureOutputStream.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.blobstore;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+public class AzureOutputStream extends OutputStream {
+
+    private final OutputStream blobOutputStream;
+
+    public AzureOutputStream(OutputStream blobOutputStream) {
+        this.blobOutputStream = blobOutputStream;
+    }
+
+    @Override
+    public void write(int b) throws IOException {
+        blobOutputStream.write(b);
+    }
+
+    @Override
+    public void close() throws IOException {
+        try {
+            blobOutputStream.close();
+        } catch (IOException e) {
+            // Azure is sending a "java.io.IOException: Stream is already closed."
+        }
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
new file mode 100644
index 0000000..c9b48ae
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageService.java
@@ -0,0 +1,64 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.storage;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.common.blobstore.BlobMetaData;
+
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.net.URISyntaxException;
+import java.util.Map;
+
+/**
+ * Azure Storage Service interface
+ * @see AzureStorageServiceImpl for Azure REST API implementation
+ */
+public interface AzureStorageService {
+    static public final class Storage {
+        public static final String API_IMPLEMENTATION = "cloud.azure.storage.api.impl";
+        public static final String ACCOUNT = "cloud.azure.storage.account";
+        public static final String KEY = "cloud.azure.storage.key";
+        public static final String CONTAINER = "repositories.azure.container";
+        public static final String BASE_PATH = "repositories.azure.base_path";
+        public static final String CHUNK_SIZE = "repositories.azure.chunk_size";
+        public static final String COMPRESS = "repositories.azure.compress";
+    }
+
+    boolean doesContainerExist(String container);
+
+    void removeContainer(String container) throws URISyntaxException, StorageException;
+
+    void createContainer(String container) throws URISyntaxException, StorageException;
+
+    void deleteFiles(String container, String path) throws URISyntaxException, StorageException;
+
+    boolean blobExists(String container, String blob) throws URISyntaxException, StorageException;
+
+    void deleteBlob(String container, String blob) throws URISyntaxException, StorageException;
+
+    InputStream getInputStream(String container, String blob) throws URISyntaxException, StorageException;
+
+    OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException;
+
+    Map<String,BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) throws URISyntaxException, StorageException;
+
+    void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException;
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java
new file mode 100644
index 0000000..f38b232
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceImpl.java
@@ -0,0 +1,228 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.storage;
+
+import com.microsoft.azure.storage.CloudStorageAccount;
+import com.microsoft.azure.storage.StorageException;
+import com.microsoft.azure.storage.blob.*;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.blobstore.BlobMetaData;
+import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
+import org.elasticsearch.common.collect.MapBuilder;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.repositories.RepositoryException;
+
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.Map;
+
+import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.*;
+
+/**
+ *
+ */
+public class AzureStorageServiceImpl extends AbstractLifecycleComponent<AzureStorageServiceImpl>
+    implements AzureStorageService {
+
+    private final String account;
+    private final String key;
+    private final String blob;
+
+    private CloudBlobClient client;
+
+    @Inject
+    public AzureStorageServiceImpl(Settings settings) {
+        super(settings);
+        // We try to load storage API settings from `cloud.azure.`
+        account = settings.get(ACCOUNT);
+        key = settings.get(KEY);
+        blob = "https://" + account + ".blob.core.windows.net/";
+
+        try {
+            if (account != null) {
+                logger.trace("creating new Azure storage client using account [{}], key [{}], blob [{}]", account, key, blob);
+
+                String storageConnectionString =
+                        "DefaultEndpointsProtocol=https;"
+                                + "AccountName="+ account +";"
+                                + "AccountKey=" + key;
+
+                // Retrieve storage account from connection-string.
+                CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString);
+
+                // Create the blob client.
+                client = storageAccount.createCloudBlobClient();
+            }
+        } catch (Exception e) {
+            // Can not start Azure Storage Client
+            logger.error("can not start azure storage client: {}", e.getMessage());
+        }
+    }
+
+    @Override
+    public boolean doesContainerExist(String container) {
+        try {
+            CloudBlobContainer blob_container = client.getContainerReference(container);
+            return blob_container.exists();
+        } catch (Exception e) {
+            logger.error("can not access container [{}]", container);
+        }
+        return false;
+    }
+
+    @Override
+    public void removeContainer(String container) throws URISyntaxException, StorageException {
+        CloudBlobContainer blob_container = client.getContainerReference(container);
+        // TODO Should we set some timeout and retry options?
+        /*
+        BlobRequestOptions options = new BlobRequestOptions();
+        options.setTimeoutIntervalInMs(1000);
+        options.setRetryPolicyFactory(new RetryNoRetry());
+        blob_container.deleteIfExists(options, null);
+        */
+        logger.trace("removing container [{}]", container);
+        blob_container.deleteIfExists();
+    }
+
+    @Override
+    public void createContainer(String container) throws URISyntaxException, StorageException {
+        try {
+            CloudBlobContainer blob_container = client.getContainerReference(container);
+            logger.trace("creating container [{}]", container);
+            blob_container.createIfNotExists();
+        } catch (IllegalArgumentException e) {
+            logger.trace("fails creating container [{}]", container, e.getMessage());
+            throw new RepositoryException(container, e.getMessage());
+        }
+    }
+
+    @Override
+    public void deleteFiles(String container, String path) throws URISyntaxException, StorageException {
+        logger.trace("delete files container [{}], path [{}]", container, path);
+
+        // Container name must be lower case.
+        CloudBlobContainer blob_container = client.getContainerReference(container);
+        if (blob_container.exists()) {
+            for (ListBlobItem blobItem : blob_container.listBlobs(path)) {
+                logger.trace("removing blob [{}]", blobItem.getUri());
+                deleteBlob(container, blobItem.getUri().toString());
+            }
+        }
+    }
+
+    @Override
+    public boolean blobExists(String container, String blob) throws URISyntaxException, StorageException {
+        // Container name must be lower case.
+        CloudBlobContainer blob_container = client.getContainerReference(container);
+        if (blob_container.exists()) {
+            CloudBlockBlob azureBlob = blob_container.getBlockBlobReference(blob);
+            return azureBlob.exists();
+        }
+
+        return false;
+    }
+
+    @Override
+    public void deleteBlob(String container, String blob) throws URISyntaxException, StorageException {
+        logger.trace("delete blob for container [{}], blob [{}]", container, blob);
+
+        // Container name must be lower case.
+        CloudBlobContainer blob_container = client.getContainerReference(container);
+        if (blob_container.exists()) {
+            logger.trace("container [{}]: blob [{}] found. removing.", container, blob);
+            CloudBlockBlob azureBlob = blob_container.getBlockBlobReference(blob);
+            azureBlob.delete();
+        }
+    }
+
+    @Override
+    public InputStream getInputStream(String container, String blob) throws URISyntaxException, StorageException {
+        logger.trace("reading container [{}], blob [{}]", container, blob);
+        return client.getContainerReference(container).getBlockBlobReference(blob).openInputStream();
+    }
+
+    @Override
+    public OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException {
+        logger.trace("writing container [{}], blob [{}]", container, blob);
+        return client.getContainerReference(container).getBlockBlobReference(blob).openOutputStream();
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) throws URISyntaxException, StorageException {
+        logger.debug("listing container [{}], keyPath [{}], prefix [{}]", container, keyPath, prefix);
+        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
+
+        CloudBlobContainer blobContainer = client.getContainerReference(container);
+        if (blobContainer.exists()) {
+            for (ListBlobItem blobItem : blobContainer.listBlobs(keyPath + (prefix == null ? "" : prefix))) {
+                URI uri = blobItem.getUri();
+                logger.trace("blob url [{}]", uri);
+
+                // uri.getPath is of the form /container/keyPath.* and we want to strip off the /container/
+                // this requires 1 + container.length() + 1, with each 1 corresponding to one of the /
+                String blobPath = uri.getPath().substring(1 + container.length() + 1);
+
+                CloudBlockBlob blob = blobContainer.getBlockBlobReference(blobPath);
+
+                // fetch the blob attributes from Azure (getBlockBlobReference does not do this)
+                // this is needed to retrieve the blob length (among other metadata) from Azure Storage
+                blob.downloadAttributes();
+
+                BlobProperties properties = blob.getProperties();
+                String name = blobPath.substring(keyPath.length());
+                logger.trace("blob url [{}], name [{}], size [{}]", uri, name, properties.getLength());
+                blobsBuilder.put(name, new PlainBlobMetaData(name, properties.getLength()));
+            }
+        }
+
+        return blobsBuilder.immutableMap();
+    }
+
+    @Override
+    public void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException {
+        logger.debug("moveBlob container [{}], sourceBlob [{}], targetBlob [{}]", container, sourceBlob, targetBlob);
+        CloudBlobContainer blob_container = client.getContainerReference(container);
+        CloudBlockBlob blobSource = blob_container.getBlockBlobReference(sourceBlob);
+        if (blobSource.exists()) {
+            CloudBlockBlob blobTarget = blob_container.getBlockBlobReference(targetBlob);
+            blobTarget.startCopyFromBlob(blobSource);
+            blobSource.delete();
+            logger.debug("moveBlob container [{}], sourceBlob [{}], targetBlob [{}] -> done", container, sourceBlob, targetBlob);
+        }
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+        logger.debug("starting azure storage client instance");
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+        logger.debug("stopping azure storage client instance");
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
new file mode 100644
index 0000000..da3aa8c
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/cloud/azure/storage/AzureStorageSettingsFilter.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.storage;
+
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+
+import static org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage.*;
+
+public class AzureStorageSettingsFilter extends AbstractComponent {
+
+    @Inject
+    public AzureStorageSettingsFilter(Settings settings, SettingsFilter settingsFilter) {
+        super(settings);
+        // Cloud storage API settings needed to be hidden
+        settingsFilter.addFilter(ACCOUNT);
+        settingsFilter.addFilter(KEY);
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java b/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java
new file mode 100644
index 0000000..6256115
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.repository.azure;
+
+import org.elasticsearch.cloud.azure.AzureRepositoryModule;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.repositories.RepositoriesModule;
+import org.elasticsearch.repositories.azure.AzureRepository;
+
+import java.util.Collection;
+import java.util.Collections;
+
+import static org.elasticsearch.cloud.azure.AzureRepositoryModule.isSnapshotReady;
+
+/**
+ *
+ */
+public class AzureRepositoryPlugin extends Plugin {
+
+    private final Settings settings;
+    protected final ESLogger logger = Loggers.getLogger(AzureRepositoryPlugin.class);
+
+    public AzureRepositoryPlugin(Settings settings) {
+        this.settings = settings;
+        logger.trace("starting azure repository plugin...");
+    }
+
+    @Override
+    public String name() {
+        return "repository-azure";
+    }
+
+    @Override
+    public String description() {
+        return "Azure Repository Plugin";
+    }
+
+    @Override
+    public Collection<Module> nodeModules() {
+        return Collections.singletonList((Module) new AzureRepositoryModule(settings));
+    }
+
+    public void onModule(RepositoriesModule module) {
+        if (isSnapshotReady(settings, logger)) {
+            module.registerRepository(AzureRepository.TYPE, AzureRepository.class, BlobStoreIndexShardRepository.class);
+        }
+    }
+}
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java b/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java
new file mode 100644
index 0000000..eb145d8
--- /dev/null
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureRepository.java
@@ -0,0 +1,174 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.cloud.azure.blobstore.AzureBlobStore;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.blobstore.BlobPath;
+import org.elasticsearch.common.blobstore.BlobStore;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.index.snapshots.IndexShardRepository;
+import org.elasticsearch.repositories.RepositoryName;
+import org.elasticsearch.repositories.RepositorySettings;
+import org.elasticsearch.repositories.RepositoryVerificationException;
+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
+import org.elasticsearch.snapshots.SnapshotCreationException;
+
+import java.io.IOException;
+import java.net.URISyntaxException;
+import java.util.List;
+
+/**
+ * Azure file system implementation of the BlobStoreRepository
+ * <p>
+ * Azure file system repository supports the following settings:
+ * <dl>
+ * <dt>{@code container}</dt><dd>Azure container name. Defaults to elasticsearch-snapshots</dd>
+ * <dt>{@code base_path}</dt><dd>Specifies the path within bucket to repository data. Defaults to root directory.</dd>
+ * <dt>{@code chunk_size}</dt><dd>Large file can be divided into chunks. This parameter specifies the chunk size. Defaults to 64mb.</dd>
+ * <dt>{@code compress}</dt><dd>If set to true metadata files will be stored compressed. Defaults to false.</dd>
+ * </dl>
+ */
+public class AzureRepository extends BlobStoreRepository {
+
+    public final static String TYPE = "azure";
+    public final static String CONTAINER_DEFAULT = "elasticsearch-snapshots";
+
+    static public final class Repository {
+        public static final String CONTAINER = "container";
+        public static final String CHUNK_SIZE = "chunk_size";
+        public static final String COMPRESS = "compress";
+        public static final String BASE_PATH = "base_path";
+    }
+
+    private final AzureBlobStore blobStore;
+
+    private final BlobPath basePath;
+
+    private ByteSizeValue chunkSize;
+
+    private boolean compress;
+
+    @Inject
+    public AzureRepository(RepositoryName name, RepositorySettings repositorySettings,
+                           IndexShardRepository indexShardRepository,
+                           AzureBlobStore azureBlobStore) throws IOException, URISyntaxException, StorageException {
+        super(name.getName(), repositorySettings, indexShardRepository);
+
+        String container = repositorySettings.settings().get(Repository.CONTAINER,
+                settings.get(Storage.CONTAINER, CONTAINER_DEFAULT));
+
+        this.blobStore = azureBlobStore;
+        this.chunkSize = repositorySettings.settings().getAsBytesSize(Repository.CHUNK_SIZE,
+                settings.getAsBytesSize(Storage.CHUNK_SIZE, new ByteSizeValue(64, ByteSizeUnit.MB)));
+
+        if (this.chunkSize.getMb() > 64) {
+            logger.warn("azure repository does not support yet size > 64mb. Fall back to 64mb.");
+            this.chunkSize = new ByteSizeValue(64, ByteSizeUnit.MB);
+        }
+
+        this.compress = repositorySettings.settings().getAsBoolean(Repository.COMPRESS,
+                settings.getAsBoolean(Storage.COMPRESS, false));
+        String basePath = repositorySettings.settings().get(Repository.BASE_PATH, null);
+
+        if (Strings.hasLength(basePath)) {
+            // Remove starting / if any
+            basePath = Strings.trimLeadingCharacter(basePath, '/');
+            BlobPath path = new BlobPath();
+            for(String elem : Strings.splitStringToArray(basePath, '/')) {
+                path = path.add(elem);
+            }
+            this.basePath = path;
+        } else {
+            this.basePath = BlobPath.cleanPath();
+        }
+        logger.debug("using container [{}], chunk_size [{}], compress [{}], base_path [{}]",
+                container, chunkSize, compress, basePath);
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected BlobStore blobStore() {
+        return blobStore;
+    }
+
+    @Override
+    protected BlobPath basePath() {
+        return basePath;
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected boolean isCompress() {
+        return compress;
+    }
+
+    /**
+     * {@inheritDoc}
+     */
+    @Override
+    protected ByteSizeValue chunkSize() {
+        return chunkSize;
+    }
+
+    @Override
+    public void initializeSnapshot(SnapshotId snapshotId, List<String> indices, MetaData metaData) {
+        try {
+            if (!blobStore.client().doesContainerExist(blobStore.container())) {
+                logger.debug("container [{}] does not exist. Creating...", blobStore.container());
+                blobStore.client().createContainer(blobStore.container());
+            }
+            super.initializeSnapshot(snapshotId, indices, metaData);
+        } catch (StorageException e) {
+            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
+            throw new SnapshotCreationException(snapshotId, e);
+        } catch (URISyntaxException e) {
+            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
+            throw new SnapshotCreationException(snapshotId, e);
+        }
+    }
+
+    @Override
+    public String startVerification() {
+        try {
+            if (!blobStore.client().doesContainerExist(blobStore.container())) {
+                logger.debug("container [{}] does not exist. Creating...", blobStore.container());
+                blobStore.client().createContainer(blobStore.container());
+            }
+            return super.startVerification();
+        } catch (StorageException e) {
+            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
+            throw new RepositoryVerificationException(repositoryName, "can not initialize container " + blobStore.container(), e);
+        } catch (URISyntaxException e) {
+            logger.warn("can not initialize container [{}]: [{}]", blobStore.container(), e.getMessage());
+            throw new RepositoryVerificationException(repositoryName, "can not initialize container " + blobStore.container(), e);
+        }
+    }
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
new file mode 100644
index 0000000..a1abded
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService.Storage;
+import org.elasticsearch.cloud.azure.storage.AzureStorageServiceMock;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.plugin.repository.azure.AzureRepositoryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.repositories.RepositoryMissingException;
+import org.elasticsearch.test.store.MockFSDirectoryService;
+import org.junit.After;
+import org.junit.Before;
+
+import java.net.URISyntaxException;
+import java.util.Collection;
+
+public abstract class AbstractAzureRepositoryServiceTestCase extends AbstractAzureTestCase {
+
+    public static class TestPlugin extends Plugin {
+        @Override
+        public String name() {
+            return "mock-storage-service";
+        }
+        @Override
+        public String description() {
+            return "plugs in a mock storage service for testing";
+        }
+        public void onModule(AzureRepositoryModule azureRepositoryModule) {
+            AzureRepositoryModule.storageServiceImpl = AzureStorageServiceMock.class;
+        }
+    }
+
+    protected String basePath;
+
+    public AbstractAzureRepositoryServiceTestCase(String basePath) {
+        this.basePath = basePath;
+    }
+
+    /**
+     * Deletes repositories, supports wildcard notation.
+     */
+    public static void wipeRepositories(String... repositories) {
+        // if nothing is provided, delete all
+        if (repositories.length == 0) {
+            repositories = new String[]{"*"};
+        }
+        for (String repository : repositories) {
+            try {
+                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
+            } catch (RepositoryMissingException ex) {
+                // ignore
+            }
+        }
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        Settings.Builder builder = Settings.settingsBuilder()
+                .put(Storage.CONTAINER, "snapshots");
+
+        // We use sometime deprecated settings in tests
+        builder.put(Storage.ACCOUNT, "mock_azure_account")
+                .put(Storage.KEY, "mock_azure_key");
+
+        return builder.build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureRepositoryPlugin.class, TestPlugin.class);
+    }
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
+                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
+                .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
+                .build();
+    }
+
+    @Before @After
+    public final void wipe() throws StorageException, URISyntaxException {
+        wipeRepositories();
+        cleanRepositoryFiles(basePath);
+    }
+
+    /**
+     * Purge the test container
+     */
+    public void cleanRepositoryFiles(String path) throws StorageException, URISyntaxException {
+        String container = internalCluster().getInstance(Settings.class).get("repositories.azure.container");
+        logger.info("--> remove blobs in container [{}]", container);
+        AzureStorageService client = internalCluster().getInstance(AzureStorageService.class);
+        client.deleteFiles(container, path);
+    }
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
new file mode 100644
index 0000000..b1bc5cc
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
@@ -0,0 +1,37 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.plugin.repository.azure.AzureRepositoryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.Collection;
+
+/**
+ * Base class for Azure tests.
+ */
+public abstract class AbstractAzureTestCase extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureRepositoryPlugin.class);
+    }
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureWithThirdPartyTestCase.java b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureWithThirdPartyTestCase.java
new file mode 100644
index 0000000..b7c2d37
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureWithThirdPartyTestCase.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.plugin.repository.azure.AzureRepositoryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
+
+import java.util.Collection;
+
+/**
+ * Base class for Azure tests that require credentials.
+ * <p>
+ * You must specify {@code -Dtests.thirdparty=true -Dtests.config=/path/to/config}
+ * in order to run these tests.
+ */
+@ThirdParty
+public abstract class AbstractAzureWithThirdPartyTestCase extends AbstractAzureTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put(readSettingsFromFile())
+//                .put("path.home", createTempDir())
+                .build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureRepositoryPlugin.class);
+    }
+
+    protected Settings readSettingsFromFile() {
+        Settings.Builder settings = Settings.builder();
+
+        // if explicit, just load it and don't load from env
+        try {
+            if (Strings.hasText(System.getProperty("tests.config"))) {
+                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
+            } else {
+                throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
+            }
+        } catch (SettingsException exception) {
+          throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
+        }
+        return settings.build();
+    }
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
new file mode 100644
index 0000000..8fe1923
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/cloud/azure/storage/AzureStorageServiceMock.java
@@ -0,0 +1,171 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cloud.azure.storage;
+
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.blobstore.BlobMetaData;
+import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
+import org.elasticsearch.common.collect.MapBuilder;
+import org.elasticsearch.common.component.AbstractLifecycleComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.net.URISyntaxException;
+import java.util.Locale;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+/**
+ * In memory storage for unit tests
+ */
+public class AzureStorageServiceMock extends AbstractLifecycleComponent<AzureStorageServiceMock>
+        implements AzureStorageService {
+
+    protected Map<String, ByteArrayOutputStream> blobs = new ConcurrentHashMap<>();
+
+    @Inject
+    public AzureStorageServiceMock(Settings settings) {
+        super(settings);
+    }
+
+    @Override
+    public boolean doesContainerExist(String container) {
+        return true;
+    }
+
+    @Override
+    public void removeContainer(String container) {
+    }
+
+    @Override
+    public void createContainer(String container) {
+    }
+
+    @Override
+    public void deleteFiles(String container, String path) {
+    }
+
+    @Override
+    public boolean blobExists(String container, String blob) {
+        return blobs.containsKey(blob);
+    }
+
+    @Override
+    public void deleteBlob(String container, String blob) {
+        blobs.remove(blob);
+    }
+
+    @Override
+    public InputStream getInputStream(String container, String blob) {
+        return new ByteArrayInputStream(blobs.get(blob).toByteArray());
+    }
+
+    @Override
+    public OutputStream getOutputStream(String container, String blob) throws URISyntaxException, StorageException {
+        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
+        blobs.put(blob, outputStream);
+        return outputStream;
+    }
+
+    @Override
+    public Map<String, BlobMetaData> listBlobsByPrefix(String container, String keyPath, String prefix) {
+        MapBuilder<String, BlobMetaData> blobsBuilder = MapBuilder.newMapBuilder();
+        for (String blobName : blobs.keySet()) {
+            if (startsWithIgnoreCase(blobName, prefix)) {
+                blobsBuilder.put(blobName, new PlainBlobMetaData(blobName, blobs.get(blobName).size()));
+            }
+        }
+        return blobsBuilder.immutableMap();
+    }
+
+    @Override
+    public void moveBlob(String container, String sourceBlob, String targetBlob) throws URISyntaxException, StorageException {
+        for (String blobName : blobs.keySet()) {
+            if (endsWithIgnoreCase(blobName, sourceBlob)) {
+                ByteArrayOutputStream outputStream = blobs.get(blobName);
+                blobs.put(blobName.replace(sourceBlob, targetBlob), outputStream);
+                blobs.remove(blobName);
+            }
+        }
+    }
+
+    @Override
+    protected void doStart() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doStop() throws ElasticsearchException {
+    }
+
+    @Override
+    protected void doClose() throws ElasticsearchException {
+    }
+
+    /**
+     * Test if the given String starts with the specified prefix,
+     * ignoring upper/lower case.
+     *
+     * @param str    the String to check
+     * @param prefix the prefix to look for
+     * @see java.lang.String#startsWith
+     */
+    public static boolean startsWithIgnoreCase(String str, String prefix) {
+        if (str == null || prefix == null) {
+            return false;
+        }
+        if (str.startsWith(prefix)) {
+            return true;
+        }
+        if (str.length() < prefix.length()) {
+            return false;
+        }
+        String lcStr = str.substring(0, prefix.length()).toLowerCase(Locale.ROOT);
+        String lcPrefix = prefix.toLowerCase(Locale.ROOT);
+        return lcStr.equals(lcPrefix);
+    }
+
+    /**
+     * Test if the given String ends with the specified suffix,
+     * ignoring upper/lower case.
+     *
+     * @param str    the String to check
+     * @param suffix the suffix to look for
+     * @see java.lang.String#startsWith
+     */
+    public static boolean endsWithIgnoreCase(String str, String suffix) {
+        if (str == null || suffix == null) {
+            return false;
+        }
+        if (str.endsWith(suffix)) {
+            return true;
+        }
+        if (str.length() < suffix.length()) {
+            return false;
+        }
+        String lcStr = str.substring(0, suffix.length()).toLowerCase(Locale.ROOT);
+        String lcPrefix = suffix.toLowerCase(Locale.ROOT);
+        return lcStr.equals(lcPrefix);
+    }
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureRepositoryRestIT.java b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureRepositoryRestIT.java
new file mode 100644
index 0000000..d0267d5
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureRepositoryRestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugin.repository.azure.AzureRepositoryPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class AzureRepositoryRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(AzureRepositoryPlugin.class);
+    }
+
+    public AzureRepositoryRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
new file mode 100644
index 0000000..37a21a3
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreServiceTests.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+
+import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.cloud.azure.AbstractAzureRepositoryServiceTestCase;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.snapshots.SnapshotState;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+
+@ESIntegTestCase.ClusterScope(
+        scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 1,
+        numClientNodes = 0,
+        transportClientRatio = 0.0)
+public class AzureSnapshotRestoreServiceTests extends AbstractAzureRepositoryServiceTestCase {
+
+    public AzureSnapshotRestoreServiceTests() {
+        super("/snapshot-test/repo-" + randomInt());
+    }
+
+    @Test
+    public void testSimpleWorkflow() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", basePath);
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put("base_path", basePath)
+                        .put("chunk_size", randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+}
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
new file mode 100644
index 0000000..670675a
--- /dev/null
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
@@ -0,0 +1,536 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.repositories.azure;
+
+
+import com.carrotsearch.randomizedtesting.RandomizedTest;
+import com.microsoft.azure.storage.StorageException;
+import org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.ClusterAdminClient;
+import org.elasticsearch.cloud.azure.AbstractAzureWithThirdPartyTestCase;
+import org.elasticsearch.cloud.azure.storage.AzureStorageService;
+import org.elasticsearch.cloud.azure.storage.AzureStorageServiceImpl;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.repositories.RepositoryMissingException;
+import org.elasticsearch.repositories.RepositoryVerificationException;
+import org.elasticsearch.repositories.azure.AzureRepository.Repository;
+import org.elasticsearch.snapshots.SnapshotMissingException;
+import org.elasticsearch.snapshots.SnapshotState;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.store.MockFSDirectoryService;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.net.URISyntaxException;
+import java.util.Locale;
+import java.util.concurrent.TimeUnit;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+
+/**
+ * This test needs Azure to run and -Dtests.thirdparty=true to be set
+ * and -Dtests.config=/path/to/elasticsearch.yml
+ * @see AbstractAzureWithThirdPartyTestCase
+ */
+@ClusterScope(
+        scope = ESIntegTestCase.Scope.SUITE,
+        numDataNodes = 1,
+        transportClientRatio = 0.0)
+public class AzureSnapshotRestoreTests extends AbstractAzureWithThirdPartyTestCase {
+
+    private String getRepositoryPath() {
+        String testName = "it-".concat(Strings.toUnderscoreCase(getTestName()).replaceAll("_", "-"));
+        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
+    }
+
+    private static String getContainerName() {
+        String testName = "snapshot-itest-".concat(RandomizedTest.getContext().getRunnerSeedAsString().toLowerCase(Locale.ROOT));
+        return testName.contains(" ") ? Strings.split(testName, " ")[0] : testName;
+    }
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))
+                // In snapshot tests, we explicitly disable cloud discovery
+                .put("discovery.type", "local")
+                .build();
+    }
+
+    @Override
+    public Settings indexSettings() {
+        // During restore we frequently restore index to exactly the same state it was before, that might cause the same
+        // checksum file to be written twice during restore operation
+        return Settings.builder().put(super.indexSettings())
+                .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false)
+                .put(MockFSDirectoryService.RANDOM_NO_DELETE_OPEN_FILE, false)
+                .build();
+    }
+
+    @Before @After
+    public final void wipeAzureRepositories() throws StorageException, URISyntaxException {
+        wipeRepositories();
+        cleanRepositoryFiles(
+            getContainerName(),
+            getContainerName().concat("-1"),
+            getContainerName().concat("-2"));
+    }
+
+    @Test
+    public void testSimpleWorkflow() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.admin().cluster().prepareCreateSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-3").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("--> delete some data");
+        for (int i = 0; i < 50; i++) {
+            client.prepareDelete("test-idx-1", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 50; i < 100; i++) {
+            client.prepareDelete("test-idx-2", "doc", Integer.toString(i)).get();
+        }
+        for (int i = 0; i < 100; i += 2) {
+            client.prepareDelete("test-idx-3", "doc", Integer.toString(i)).get();
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(50L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        logger.info("--> close indices");
+        client.admin().indices().prepareClose("test-idx-1", "test-idx-2").get();
+
+        logger.info("--> restore all indices from the snapshot");
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-3").get().getCount(), equalTo(50L));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion");
+        restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setWaitForCompletion(true).setIndices("test-idx-*", "-test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+    }
+
+    /**
+     * For issue #51: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/51
+     */
+    @Test
+    public void testMultipleSnapshots() throws URISyntaxException, StorageException {
+        final String indexName = "test-idx-1";
+        final String typeName = "doc";
+        final String repositoryName = "test-repo";
+        final String snapshot1Name = "test-snap-1";
+        final String snapshot2Name = "test-snap-2";
+
+        Client client = client();
+
+        logger.info("creating index [{}]", indexName);
+        createIndex(indexName);
+        ensureGreen();
+
+        logger.info("indexing first document");
+        index(indexName, typeName, Integer.toString(1), "foo", "bar " + Integer.toString(1));
+        refresh();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
+
+        logger.info("creating Azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository(repositoryName)
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                                .put(Repository.CONTAINER, getContainerName())
+                                .put(Repository.BASE_PATH, getRepositoryPath())
+                                .put(Repository.BASE_PATH, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        logger.info("creating snapshot [{}]", snapshot1Name);
+        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).setIndices(indexName).get();
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot1Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("indexing second document");
+        index(indexName, typeName, Integer.toString(2), "foo", "bar " + Integer.toString(2));
+        refresh();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(2L));
+
+        logger.info("creating snapshot [{}]", snapshot2Name);
+        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshot2Name).setWaitForCompletion(true).setIndices(indexName).get();
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots(repositoryName).setSnapshots(snapshot2Name).get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        logger.info("closing index [{}]", indexName);
+        client.admin().indices().prepareClose(indexName).get();
+
+        logger.info("attempting restore from snapshot [{}]", snapshot1Name);
+        RestoreSnapshotResponse restoreSnapshotResponse = client.admin().cluster().prepareRestoreSnapshot(repositoryName, snapshot1Name).setWaitForCompletion(true).execute().actionGet();
+        assertThat(restoreSnapshotResponse.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount(indexName).get().getCount(), equalTo(1L));
+    }
+
+    @Test
+    public void testMultipleRepositories() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse1 = client.admin().cluster().preparePutRepository("test-repo1")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName().concat("-1"))
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse1.isAcknowledged(), equalTo(true));
+        PutRepositoryResponse putRepositoryResponse2 = client.admin().cluster().preparePutRepository("test-repo2")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName().concat("-2"))
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse2.isAcknowledged(), equalTo(true));
+
+        createIndex("test-idx-1", "test-idx-2");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+
+        logger.info("--> snapshot 1");
+        CreateSnapshotResponse createSnapshotResponse1 = client.admin().cluster().prepareCreateSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").get();
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse1.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse1.getSnapshotInfo().totalShards()));
+
+        logger.info("--> snapshot 2");
+        CreateSnapshotResponse createSnapshotResponse2 = client.admin().cluster().prepareCreateSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").get();
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), greaterThan(0));
+        assertThat(createSnapshotResponse2.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse2.getSnapshotInfo().totalShards()));
+
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo1").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+        assertThat(client.admin().cluster().prepareGetSnapshots("test-repo2").setSnapshots("test-snap").get().getSnapshots().get(0).state(), equalTo(SnapshotState.SUCCESS));
+
+        // Test restore after index deletion
+        logger.info("--> delete indices");
+        cluster().wipeIndices("test-idx-1", "test-idx-2");
+        logger.info("--> restore one index after deletion from snapshot 1");
+        RestoreSnapshotResponse restoreSnapshotResponse1 = client.admin().cluster().prepareRestoreSnapshot("test-repo1", "test-snap").setWaitForCompletion(true).setIndices("test-idx-1").execute().actionGet();
+        assertThat(restoreSnapshotResponse1.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-1").get().getCount(), equalTo(100L));
+        ClusterState clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(false));
+
+        logger.info("--> restore other index after deletion from snapshot 2");
+        RestoreSnapshotResponse restoreSnapshotResponse2 = client.admin().cluster().prepareRestoreSnapshot("test-repo2", "test-snap").setWaitForCompletion(true).setIndices("test-idx-2").execute().actionGet();
+        assertThat(restoreSnapshotResponse2.getRestoreInfo().totalShards(), greaterThan(0));
+        ensureGreen();
+        assertThat(client.prepareCount("test-idx-2").get().getCount(), equalTo(100L));
+        clusterState = client.admin().cluster().prepareState().get().getState();
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-1"), equalTo(true));
+        assertThat(clusterState.getMetaData().hasIndex("test-idx-2"), equalTo(true));
+    }
+
+    /**
+     * For issue #26: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/26
+     */
+    @Test
+    public void testListBlobs_26() throws StorageException, URISyntaxException {
+        createIndex("test-idx-1", "test-idx-2", "test-idx-3");
+        ensureGreen();
+
+        logger.info("--> indexing some data");
+        for (int i = 0; i < 100; i++) {
+            index("test-idx-1", "doc", Integer.toString(i), "foo", "bar" + i);
+            index("test-idx-2", "doc", Integer.toString(i), "foo", "baz" + i);
+            index("test-idx-3", "doc", Integer.toString(i), "foo", "baz" + i);
+        }
+        refresh();
+
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository without any path");
+        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        // Get all snapshots - should be empty
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
+
+        logger.info("--> snapshot");
+        CreateSnapshotResponse createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+
+        // Get all snapshots - should have one
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
+
+        // Clean the snapshot
+        client.prepareDeleteSnapshot("test-repo", "test-snap-26").get();
+        client.prepareDeleteRepository("test-repo").get();
+
+        logger.info("-->  creating azure repository path [{}]", getRepositoryPath());
+        putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+        ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        // Get all snapshots - should be empty
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(0));
+
+        logger.info("--> snapshot");
+        createSnapshotResponse = client.prepareCreateSnapshot("test-repo", "test-snap-26").setWaitForCompletion(true).setIndices("test-idx-*").get();
+        assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
+
+        // Get all snapshots - should have one
+        assertThat(client.prepareGetSnapshots("test-repo").get().getSnapshots().size(), equalTo(1));
+
+
+    }
+
+    /**
+     * For issue #28: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/28
+     */
+    @Test
+    public void testGetDeleteNonExistingSnapshot_28() throws StorageException, URISyntaxException {
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository without any path");
+        PutRepositoryResponse putRepositoryResponse = client.preparePutRepository("test-repo").setType("azure")
+                .setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        try {
+            client.prepareGetSnapshots("test-repo").addSnapshots("nonexistingsnapshotname").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+
+        try {
+            client.prepareDeleteSnapshot("test-repo", "nonexistingsnapshotname").get();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    /**
+     * For issue #21: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21
+     */
+    @Test
+    public void testForbiddenContainerName() throws Exception {
+        checkContainerName("", false);
+        checkContainerName("es", false);
+        checkContainerName("-elasticsearch", false);
+        checkContainerName("elasticsearch--integration", false);
+        checkContainerName("elasticsearch_integration", false);
+        checkContainerName("ElAsTicsearch_integration", false);
+        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-1234", false);
+        checkContainerName("123456789-123456789-123456789-123456789-123456789-123456789-123", true);
+        checkContainerName("elasticsearch-integration", true);
+        checkContainerName("elasticsearch-integration-007", true);
+    }
+
+    /**
+     * Create repository with wrong or correct container name
+     * @param container Container name we want to create
+     * @param correct Is this container name correct
+     */
+    private void checkContainerName(final String container, final boolean correct) throws Exception {
+        logger.info("-->  creating azure repository with container name [{}]", container);
+        // It could happen that we just removed from a previous test the same container so
+        // we can not create it yet.
+        assertBusy(new Runnable() {
+
+            public void run() {
+                try {
+                    PutRepositoryResponse putRepositoryResponse = client().admin().cluster().preparePutRepository("test-repo")
+                            .setType("azure").setSettings(Settings.settingsBuilder()
+                                            .put(Repository.CONTAINER, container)
+                                            .put(Repository.BASE_PATH, getRepositoryPath())
+                                            .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                            ).get();
+                    client().admin().cluster().prepareDeleteRepository("test-repo").get();
+                    try {
+                        logger.info("--> remove container [{}]", container);
+                        cleanRepositoryFiles(container);
+                    } catch (StorageException | URISyntaxException e) {
+                        // We can ignore that as we just try to clean after the test
+                    }
+                    assertTrue(putRepositoryResponse.isAcknowledged() == correct);
+                } catch (RepositoryVerificationException e) {
+                    if (correct) {
+                        logger.debug(" -> container is being removed. Let's wait a bit...");
+                        fail();
+                    }
+                }
+            }
+        }, 5, TimeUnit.MINUTES);
+    }
+
+    /**
+     * Test case for issue #23: https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/23
+     */
+    @Test
+    public void testNonExistingRepo_23() {
+        Client client = client();
+        logger.info("-->  creating azure repository with path [{}]", getRepositoryPath());
+        PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
+                .setType("azure").setSettings(Settings.settingsBuilder()
+                        .put(Repository.CONTAINER, getContainerName())
+                        .put(Repository.BASE_PATH, getRepositoryPath())
+                        .put(Repository.CHUNK_SIZE, randomIntBetween(1000, 10000), ByteSizeUnit.BYTES)
+                ).get();
+        assertThat(putRepositoryResponse.isAcknowledged(), equalTo(true));
+
+        logger.info("--> restore non existing snapshot");
+        try {
+            client.admin().cluster().prepareRestoreSnapshot("test-repo", "no-existing-snapshot").setWaitForCompletion(true).execute().actionGet();
+            fail("Shouldn't be here");
+        } catch (SnapshotMissingException ex) {
+            // Expected
+        }
+    }
+
+    /**
+     * When a user remove a container you can not immediately create it again.
+     */
+    @Test
+    public void testRemoveAndCreateContainer() throws Exception {
+        final String container = getContainerName().concat("-testremove");
+        final AzureStorageService storageService = internalCluster().getInstance(AzureStorageService.class);
+
+        // It could happen that we run this test really close to a previous one
+        // so we might need some time to be able to create the container
+        assertBusy(new Runnable() {
+
+            public void run()  {
+                try {
+                    storageService.createContainer(container);
+                    logger.debug(" -> container created...");
+                } catch (URISyntaxException e) {
+                    // Incorrect URL. This should never happen.
+                    fail();
+                } catch (StorageException e) {
+                    // It could happen. Let's wait for a while.
+                    logger.debug(" -> container is being removed. Let's wait a bit...");
+                    fail();
+                }
+            }
+        }, 30, TimeUnit.SECONDS);
+        storageService.removeContainer(container);
+
+        ClusterAdminClient client = client().admin().cluster();
+        logger.info("-->  creating azure repository while container is being removed");
+        try {
+            client.preparePutRepository("test-repo").setType("azure")
+                    .setSettings(Settings.settingsBuilder()
+                            .put(Repository.CONTAINER, container)
+                    ).get();
+            fail("we should get a RepositoryVerificationException");
+        } catch (RepositoryVerificationException e) {
+            // Fine we expect that
+        }
+    }
+
+    /**
+     * Deletes repositories, supports wildcard notation.
+     */
+    public static void wipeRepositories(String... repositories) {
+        // if nothing is provided, delete all
+        if (repositories.length == 0) {
+            repositories = new String[]{"*"};
+        }
+        for (String repository : repositories) {
+            try {
+                client().admin().cluster().prepareDeleteRepository(repository).execute().actionGet();
+            } catch (RepositoryMissingException ex) {
+                // ignore
+            }
+        }
+    }
+
+    /**
+     * Purge the test containers
+     */
+    public void cleanRepositoryFiles(String... containers) throws StorageException, URISyntaxException {
+        Settings settings = readSettingsFromFile();
+        AzureStorageService client = new AzureStorageServiceImpl(settings);
+        for (String container : containers) {
+            client.removeContainer(container);
+        }
+    }
+}
diff --git a/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml b/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml
new file mode 100644
index 0000000..a77304b
--- /dev/null
+++ b/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Azure Repository component
+#
+"Azure Repository loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: repository-azure  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/repository-s3/LICENSE.txt b/plugins/repository-s3/LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/repository-s3/LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/repository-s3/NOTICE.txt b/plugins/repository-s3/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/repository-s3/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/repository-s3/pom.xml b/plugins/repository-s3/pom.xml
index 44d1307..cd68af7 100644
--- a/plugins/repository-s3/pom.xml
+++ b/plugins/repository-s3/pom.xml
@@ -19,7 +19,7 @@
         <tests.jvms>1</tests.jvms>
         <tests.rest.suite>repository_s3</tests.rest.suite>
         <tests.rest.load_packaged>false</tests.rest.load_packaged>
-        <xlint.options>-Xlint:-rawtypes</xlint.options>
+        <xlint.options>-Xlint:-rawtypes,-deprecation</xlint.options>
     </properties>
 
     <dependencies>
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index 474f6c8..81b7e31 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -43,7 +43,7 @@ import java.util.Map;
 public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Service> implements AwsS3Service {
 
     /**
-     * (acceskey, endpoint) -> client
+     * (acceskey, endpoint) -&gt; client
      */
     private Map<Tuple<String, String>, AmazonS3Client> clients = new HashMap<Tuple<String,String>, AmazonS3Client>();
 
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
index 67e6889..dc74315 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java
@@ -38,17 +38,17 @@ import java.util.List;
 
 /**
  * DefaultS3OutputStream uploads data to the AWS S3 service using 2 modes: single and multi part.
- * <p/>
+ * <p>
  * When the length of the chunk is lower than buffer_size, the chunk is uploaded with a single request.
  * Otherwise multiple requests are made, each of buffer_size (except the last one which can be lower than buffer_size).
- * <p/>
+ * <p>
  * Quick facts about S3:
- * <p/>
+ * <p>
  * Maximum object size:                 5 TB
  * Maximum number of parts per upload:  10,000
  * Part numbers:                        1 to 10,000 (inclusive)
- * Part size:                           5 MB to 5 GB, last part can be < 5 MB
- * <p/>
+ * Part size:                           5 MB to 5 GB, last part can be &lt; 5 MB
+ * <p>
  * See http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html
  * See http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
  */
@@ -94,11 +94,6 @@ public class DefaultS3OutputStream extends S3OutputStream {
 
     /**
      * Upload data using a single request.
-     *
-     * @param bytes
-     * @param off
-     * @param len
-     * @throws IOException
      */
     private void upload(byte[] bytes, int off, int len) throws IOException {
         try (ByteArrayInputStream is = new ByteArrayInputStream(bytes, off, len)) {
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
index 4be35ba..8ee9d35 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
@@ -38,7 +38,7 @@ import java.util.Locale;
 
 /**
  * Shared file system implementation of the BlobStoreRepository
- * <p/>
+ * <p>
  * Shared file system repository supports the following settings
  * <dl>
  * <dt>{@code bucket}</dt><dd>S3 bucket</dd>
@@ -68,7 +68,6 @@ public class S3Repository extends BlobStoreRepository {
      * @param repositorySettings   repository settings
      * @param indexShardRepository index shard repository
      * @param s3Service            S3 service
-     * @throws IOException
      */
     @Inject
     public S3Repository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, AwsS3Service s3Service) throws IOException {
diff --git a/plugins/store-smb/LICENSE.txt b/plugins/store-smb/LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/store-smb/LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/store-smb/licenses/no_deps.txt b/plugins/store-smb/licenses/no_deps.txt
new file mode 100644
index 0000000..8cce254
--- /dev/null
+++ b/plugins/store-smb/licenses/no_deps.txt
@@ -0,0 +1 @@
+This plugin has no third party dependencies
diff --git a/plugins/store-smb/pom.xml b/plugins/store-smb/pom.xml
new file mode 100644
index 0000000..9ce7f4a
--- /dev/null
+++ b/plugins/store-smb/pom.xml
@@ -0,0 +1,47 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!-- Licensed to Elasticsearch under one or more contributor
+license agreements. See the NOTICE file distributed with this work for additional
+information regarding copyright ownership. ElasticSearch licenses this file to you
+under the Apache License, Version 2.0 (the "License"); you may not use this
+file except in compliance with the License. You may obtain a copy of the
+License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by
+applicable law or agreed to in writing, software distributed under the License
+is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied. See the License for the specific language
+governing permissions and limitations under the License. -->
+
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+    <modelVersion>4.0.0</modelVersion>
+
+    <parent>
+        <groupId>org.elasticsearch.plugin</groupId>
+        <artifactId>plugins</artifactId>
+        <version>3.0.0-SNAPSHOT</version>
+    </parent>
+
+    <artifactId>store-smb</artifactId>
+    <name>Plugin: Store: SMB</name>
+    <description>The Store SMB plugin adds support for SMB stores.</description>
+
+    <properties>
+        <elasticsearch.plugin.classname>org.elasticsearch.plugin.store.smb.SMBStorePlugin</elasticsearch.plugin.classname>
+        <tests.jvms>1</tests.jvms>
+        <tests.rest.suite>store_smb</tests.rest.suite>
+        <tests.rest.load_packaged>false</tests.rest.load_packaged>
+    </properties>
+
+    <dependencies>
+    </dependencies>
+
+    <build>
+        <plugins>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-assembly-plugin</artifactId>
+            </plugin>
+        </plugins>
+    </build>
+
+</project>
diff --git a/plugins/store-smb/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java b/plugins/store-smb/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java
new file mode 100644
index 0000000..1e783fd
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/apache/lucene/store/SmbDirectoryWrapper.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.store;
+
+import java.io.FilterOutputStream;
+import java.io.IOException;
+import java.nio.channels.Channels;
+import java.nio.file.Files;
+import java.nio.file.StandardOpenOption;
+
+/**
+ * This class is used to wrap an existing {@link org.apache.lucene.store.FSDirectory} so that
+ * the new shard segment files will be opened for Read and Write access.
+ * <p>
+ * When storing index files on an SMB share like Azure File Service, opening the file for Read
+ * access can save a lot of roundtrips to the storage server and thus offering better performance.
+ */
+public final class SmbDirectoryWrapper extends FilterDirectory {
+
+    private final FSDirectory fsDirectory;
+
+    public SmbDirectoryWrapper(FSDirectory in) {
+        super(in);
+        fsDirectory = in;
+    }
+
+    @Override
+    public IndexOutput createOutput(String name, IOContext context) throws IOException {
+        fsDirectory.ensureOpen();
+        fsDirectory.ensureCanWrite(name);
+        return new SmbFSIndexOutput(name);
+    }
+
+    /**
+     * Copied from final inner class {@link org.apache.lucene.store.FSDirectory.FSIndexOutput}
+     */
+    final class SmbFSIndexOutput extends OutputStreamIndexOutput {
+        /**
+         * The maximum chunk size is 8192 bytes, because {@link java.io.FileOutputStream} mallocs
+         * a native buffer outside of stack if the write buffer size is larger.
+         */
+        static final int CHUNK_SIZE = 8192;
+
+        private final String name;
+
+        public SmbFSIndexOutput(String name) throws IOException {
+            super("SmbFSIndexOutput(path=\"" + fsDirectory.getDirectory().resolve(name) + "\")", new FilterOutputStream(Channels.newOutputStream(Files.newByteChannel(fsDirectory.getDirectory().resolve(name), StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING, StandardOpenOption.READ, StandardOpenOption.WRITE))) {
+                // This implementation ensures, that we never write more than CHUNK_SIZE bytes:
+                @Override
+                public void write(byte[] b, int offset, int length) throws IOException {
+                    while (length > 0) {
+                        final int chunk = Math.min(length, CHUNK_SIZE);
+                        out.write(b, offset, chunk);
+                        length -= chunk;
+                        offset += chunk;
+                    }
+                }
+            }, CHUNK_SIZE);
+            this.name = name;
+        }
+    }
+}
diff --git a/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java
new file mode 100644
index 0000000..47efbd0
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsDirectoryService.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store.smbmmapfs;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.MMapDirectory;
+import org.apache.lucene.store.SmbDirectoryWrapper;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.settings.IndexSettings;
+import org.elasticsearch.index.shard.ShardPath;
+import org.elasticsearch.index.store.FsDirectoryService;
+import org.elasticsearch.index.store.IndexStore;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbMmapFsDirectoryService extends FsDirectoryService {
+
+    @Inject
+    public SmbMmapFsDirectoryService(@IndexSettings Settings indexSettings, IndexStore indexStore, ShardPath path) {
+        super(indexSettings, indexStore, path);
+    }
+
+    @Override
+    protected Directory newFSDirectory(Path location, LockFactory lockFactory) throws IOException {
+        logger.debug("wrapping MMapDirectory for SMB");
+        return new SmbDirectoryWrapper(new MMapDirectory(location, buildLockFactory(indexSettings)));
+    }
+}
diff --git a/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java
new file mode 100644
index 0000000..0422975
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbmmapfs/SmbMmapFsIndexStore.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store.smbmmapfs;
+
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.settings.IndexSettings;
+import org.elasticsearch.index.settings.IndexSettingsService;
+import org.elasticsearch.index.store.DirectoryService;
+import org.elasticsearch.index.store.IndexStore;
+import org.elasticsearch.indices.store.IndicesStore;
+
+public class SmbMmapFsIndexStore extends IndexStore {
+
+    @Inject
+    public SmbMmapFsIndexStore(Index index, @IndexSettings Settings indexSettings,
+                               IndexSettingsService indexSettingsService, IndicesStore indicesStore) {
+        super(index, indexSettings, indexSettingsService, indicesStore);
+    }
+
+    @Override
+    public Class<? extends DirectoryService> shardDirectory() {
+        return SmbMmapFsDirectoryService.class;
+    }
+}
diff --git a/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java
new file mode 100644
index 0000000..7866090
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsDirectoryService.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store.smbsimplefs;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.SimpleFSDirectory;
+import org.apache.lucene.store.SmbDirectoryWrapper;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.settings.IndexSettings;
+import org.elasticsearch.index.shard.ShardPath;
+import org.elasticsearch.index.store.FsDirectoryService;
+import org.elasticsearch.index.store.IndexStore;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbSimpleFsDirectoryService extends FsDirectoryService {
+
+    @Inject
+    public SmbSimpleFsDirectoryService(@IndexSettings Settings indexSettings, IndexStore indexStore, ShardPath path) {
+        super(indexSettings, indexStore, path);
+    }
+
+    @Override
+    protected Directory newFSDirectory(Path location, LockFactory lockFactory) throws IOException {
+        logger.debug("wrapping SimpleFSDirectory for SMB");
+        return new SmbDirectoryWrapper(new SimpleFSDirectory(location, lockFactory));
+    }
+}
diff --git a/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java
new file mode 100644
index 0000000..4813344
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/elasticsearch/index/store/smbsimplefs/SmbSimpleFsIndexStore.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store.smbsimplefs;
+
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.settings.IndexSettings;
+import org.elasticsearch.index.settings.IndexSettingsService;
+import org.elasticsearch.index.store.DirectoryService;
+import org.elasticsearch.index.store.IndexStore;
+import org.elasticsearch.indices.store.IndicesStore;
+
+public class SmbSimpleFsIndexStore extends IndexStore {
+
+    @Inject
+    public SmbSimpleFsIndexStore(Index index, @IndexSettings Settings indexSettings,
+                                 IndexSettingsService indexSettingsService, IndicesStore indicesStore) {
+        super(index, indexSettings, indexSettingsService, indicesStore);
+    }
+
+    @Override
+    public Class<? extends DirectoryService> shardDirectory() {
+        return SmbSimpleFsDirectoryService.class;
+    }
+}
+
diff --git a/plugins/store-smb/src/main/java/org/elasticsearch/plugin/store/smb/SMBStorePlugin.java b/plugins/store-smb/src/main/java/org/elasticsearch/plugin/store/smb/SMBStorePlugin.java
new file mode 100644
index 0000000..5a5ace3
--- /dev/null
+++ b/plugins/store-smb/src/main/java/org/elasticsearch/plugin/store/smb/SMBStorePlugin.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plugin.store.smb;
+
+import org.elasticsearch.index.store.IndexStoreModule;
+import org.elasticsearch.index.store.smbmmapfs.SmbMmapFsIndexStore;
+import org.elasticsearch.index.store.smbsimplefs.SmbSimpleFsIndexStore;
+import org.elasticsearch.plugins.Plugin;
+
+public class SMBStorePlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "store-smb";
+    }
+
+    @Override
+    public String description() {
+        return "SMB Store Plugin";
+    }
+
+    public void onModule(IndexStoreModule storeModule) {
+        storeModule.addIndexStore("smb_mmap_fs", SmbMmapFsIndexStore.class);
+        storeModule.addIndexStore("smb_simple_fs", SmbSimpleFsIndexStore.class);
+    }
+}
diff --git a/plugins/store-smb/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java b/plugins/store-smb/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java
new file mode 100644
index 0000000..4c6c230
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/apache/lucene/store/ESBaseDirectoryTestCase.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.store;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.elasticsearch.bootstrap.BootstrapForTesting;
+import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
+
+import com.carrotsearch.randomizedtesting.annotations.Listeners;
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+/**
+ * Extends Lucene's BaseDirectoryTestCase with ES test behavior.
+ */
+@Listeners({
+  ReproduceInfoPrinter.class
+})
+@TimeoutSuite(millis = TimeUnits.HOUR)
+@LuceneTestCase.SuppressReproduceLine
+@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
+public abstract class ESBaseDirectoryTestCase extends BaseDirectoryTestCase {
+    static {
+        BootstrapForTesting.ensureInitialized();
+    }
+}
diff --git a/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java b/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
new file mode 100644
index 0000000..43c61d8
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbMMapDirectoryTests.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.store;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbMMapDirectoryTests extends ESBaseDirectoryTestCase {
+
+    @Override
+    protected Directory getDirectory(Path file) throws IOException {
+        return new SmbDirectoryWrapper(new MMapDirectory(file));
+    }
+}
\ No newline at end of file
diff --git a/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java b/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
new file mode 100644
index 0000000..208eb6c
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/apache/lucene/store/SmbSimpleFSDirectoryTests.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.lucene.store;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
+public class SmbSimpleFSDirectoryTests extends ESBaseDirectoryTestCase {
+
+    @Override
+    protected Directory getDirectory(Path file) throws IOException {
+        return new SmbDirectoryWrapper(new SimpleFSDirectory(file));
+    }
+}
diff --git a/plugins/store-smb/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
new file mode 100644
index 0000000..770e819
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/AbstractAzureFsTestCase.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.plugin.store.smb.SMBStorePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.util.Collection;
+
+import static org.hamcrest.Matchers.is;
+
+abstract public class AbstractAzureFsTestCase extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(SMBStorePlugin.class);
+    }
+
+    @Test
+    public void testAzureFs() {
+        // Create an index and index some documents
+        createIndex("test");
+        long nbDocs = randomIntBetween(10, 1000);
+        for (long i = 0; i < nbDocs; i++) {
+            index("test", "doc", "" + i, "foo", "bar");
+        }
+        refresh();
+        SearchResponse response = client().prepareSearch("test").get();
+        assertThat(response.getHits().totalHits(), is(nbDocs));
+    }
+}
diff --git a/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SMBStoreRestIT.java b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SMBStoreRestIT.java
new file mode 100644
index 0000000..649af17
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SMBStoreRestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugin.store.smb.SMBStorePlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class SMBStoreRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(SMBStorePlugin.class);
+    }
+
+    public SMBStoreRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
new file mode 100644
index 0000000..0f9e874
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbMMapFsTests.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.common.settings.Settings;
+
+
+public class SmbMMapFsTests extends AbstractAzureFsTestCase {
+
+    @Override
+    public Settings indexSettings() {
+        return Settings.builder()
+                .put(super.indexSettings())
+                .put("index.store.type", "smb_mmap_fs")
+                .build();
+    }
+
+}
diff --git a/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
new file mode 100644
index 0000000..ed15718
--- /dev/null
+++ b/plugins/store-smb/src/test/java/org/elasticsearch/index/store/SmbSimpleFsTests.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.store;
+
+import org.elasticsearch.common.settings.Settings;
+
+
+public class SmbSimpleFsTests extends AbstractAzureFsTestCase {
+    @Override
+    public Settings indexSettings() {
+        return Settings.builder()
+                .put(super.indexSettings())
+                .put("index.store.type", "smb_simple_fs")
+                .build();
+    }
+}
diff --git a/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml b/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml
new file mode 100644
index 0000000..155a39b
--- /dev/null
+++ b/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for SMB Store component
+#
+"SMB Store loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: store-smb  }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/pom.xml b/pom.xml
index 6c23819..54014ee 100644
--- a/pom.xml
+++ b/pom.xml
@@ -42,6 +42,7 @@
         <!-- Path warnings must be ignored because maven doesnt create the classes output
              dir when no source files exist (eg for distribution or qa modules) -->
         <xlint.options>-Xlint:-path</xlint.options>
+        <doclint.options>-Xdoclint:-missing</doclint.options>
 
         <!-- libraries -->
         <lucene.version>5.4.0</lucene.version>
@@ -613,7 +614,9 @@
                             <arg>-XDignore.symbol.file</arg>
                             <arg>-Xlint:all</arg>
                             <arg>${xlint.options}</arg>
-                            <!-- DISABLED: incompatible with java 9 <arg>-Werror</arg> -->
+                            <arg>-Xdoclint:all/private</arg>
+                            <arg>${doclint.options}</arg>
+                            <arg>${javac.werror}</arg>
                         </compilerArgs>
                     </configuration>
                 </plugin>
@@ -1050,10 +1053,8 @@ encoding/<project>=UTF-8
                                 <name>.settings/org.eclipse.jdt.core.prefs</name>
                                 <content>
                                <![CDATA[eclipse.preferences.version=1
-org.eclipse.jdt.core.compiler.annotation.inheritNullAnnotations=enabled
-org.eclipse.jdt.core.compiler.annotation.missingNonNullByDefaultAnnotation=ignore
-org.eclipse.jdt.core.compiler.annotation.nullable=org.elasticsearch.common.Nullable
-org.eclipse.jdt.core.compiler.annotation.nullanalysis=enabled
+# Shut down null analysis because we don't have all the required annotations (Nonnull and NonNullByDefault).
+org.eclipse.jdt.core.compiler.annotation.nullanalysis=disabled
 org.eclipse.jdt.core.compiler.codegen.targetPlatform=1.8
 org.eclipse.jdt.core.compiler.compliance=1.8
 org.eclipse.jdt.core.compiler.problem.forbiddenReference=warning
@@ -1146,6 +1147,23 @@ org.eclipse.jdt.ui.text.custom_code_templates=<?xml version\="1.0" encoding\="UT
                     <artifactId>maven-antrun-plugin</artifactId>
                     <version>1.8</version>
                     <executions>
+                        <!-- remove this hack when maven works with java 9 properly -->
+                        <execution>
+                            <id>set-werror</id>
+                            <phase>validate</phase>
+                            <goals>
+                                <goal>run</goal>
+                            </goals>
+                            <configuration>
+                                <target>
+                                    <!-- if we are on java 1.9.* we turn off Werror or maven crushes! -->
+                                    <condition property="javac.werror" value="-Aboguskey=bogusvalue" else="-Werror" >
+                                        <matches pattern="1\.9\..+$" string="${java.runtime.version}" />
+                                    </condition>
+                                </target>
+                                <exportAntProperties>true</exportAntProperties>
+                            </configuration>
+                        </execution>
                         <execution>
                             <id>check-invalid-patterns</id>
                             <phase>validate</phase>
@@ -1263,10 +1281,14 @@ org.eclipse.jdt.ui.text.custom_code_templates=<?xml version\="1.0" encoding\="UT
                     <version>2.0.0</version>
                 </plugin>
                 <plugin>
-                    <!-- We just declare which plugin version to use. Each project can have then its own settings -->
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-javadoc-plugin</artifactId>
                     <version>2.10.3</version>
+                    <configuration>
+                        <!-- Doclint defaults to very strict. While it'd be great to have wonderful Javadocs we
+                             prefer working Javadocs over total failure. -->
+                        <additionalparam>-Xdoclint:none</additionalparam>
+                    </configuration>
                 </plugin>
                 <plugin>
                     <!-- We just declare which plugin version to use. Each project can have then its own settings -->
diff --git a/qa/pom.xml b/qa/pom.xml
index e6bda14..7cc28fe 100644
--- a/qa/pom.xml
+++ b/qa/pom.xml
@@ -38,47 +38,12 @@
 
     <!-- typical layout -->
     <build>
-        <resources>
-            <resource>
-                <directory>src/main/resources</directory>
-                <filtering>true</filtering>
-                <includes>
-                    <include>**/*.properties</include>
-                </includes>
-            </resource>
-        </resources>
-
         <testResources>
             <testResource>
-                <directory>src/test/java</directory>
-                <includes>
-                    <include>**/*.json</include>
-                    <include>**/*.txt</include>
-                </includes>
-            </testResource>
-            <testResource>
                 <directory>src/test/resources</directory>
-                <excludes>
-                    <exclude>elasticsearch.yml</exclude>
-                    <exclude>**/*.properties</exclude>
-                </excludes>
-            </testResource>
-            <testResource>
-                <directory>src/test/resources</directory>
-                <filtering>true</filtering>
-                <includes>
-                    <include>elasticsearch.yml</include>
-                    <include>**/*.properties</include>
-                </includes>
-            </testResource>
-            <!-- REST API specification and test suites -->
-            <testResource>
-                <directory>${project.basedir}/rest-api-spec</directory>
                 <filtering>true</filtering>
-                <targetPath>rest-api-spec</targetPath>
                 <includes>
-                    <include>api/*.json</include>
-                    <include>test/**/*.yaml</include>
+                    <include>rest-api-spec/**</include>
                 </includes>
             </testResource>
             <!-- REST API specifications copied from main Elasticsearch specs
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
index 327bd2c..6e912cf 100644
--- a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -46,12 +46,12 @@ import static org.hamcrest.Matchers.notNullValue;
 /**
  * {@link ESSmokeClientTestCase} is an abstract base class to run integration
  * tests against an external Elasticsearch Cluster.
- * <p/>
+ * <p>
  * You can define a list of transport addresses from where you can reach your cluster
  * by setting "tests.cluster" system property. It defaults to "localhost:9300".
- * <p/>
+ * <p>
  * All tests can be run from maven using mvn install as maven will start an external cluster first.
- * <p/>
+ * <p>
  * If you want to debug this module from your IDE, then start an external cluster by yourself
  * then run JUnit. If you changed the default port, set "tests.cluster=localhost:PORT" when running
  * your test.
diff --git a/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml b/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
deleted file mode 100644
index 99446f2..0000000
--- a/qa/smoke-test-multinode/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-# Integration tests for smoke testing multi-node IT
-# If the local machine which is running the test is low on disk space
-# We can have one unassigned shard
----
-"cluster health basic test, wait for both nodes to join":
-  - do:
-      cluster.health:
-        wait_for_nodes: 2
-
-  - is_true:   cluster_name
-  - is_false:  timed_out
-  - gte:       { number_of_nodes:         2 }
-  - gte:       { number_of_data_nodes:    2 }
diff --git a/qa/smoke-test-multinode/src/test/resources/rest-api-spec/test/smoke_test_multinode/10_basic.yaml b/qa/smoke-test-multinode/src/test/resources/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
new file mode 100644
index 0000000..99446f2
--- /dev/null
+++ b/qa/smoke-test-multinode/src/test/resources/rest-api-spec/test/smoke_test_multinode/10_basic.yaml
@@ -0,0 +1,13 @@
+# Integration tests for smoke testing multi-node IT
+# If the local machine which is running the test is low on disk space
+# We can have one unassigned shard
+---
+"cluster health basic test, wait for both nodes to join":
+  - do:
+      cluster.health:
+        wait_for_nodes: 2
+
+  - is_true:   cluster_name
+  - is_false:  timed_out
+  - gte:       { number_of_nodes:         2 }
+  - gte:       { number_of_data_nodes:    2 }
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index 128dab1..9aa983c 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -282,7 +282,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-azure</artifactId>
+                   <artifactId>cloud-gce</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -290,7 +290,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>cloud-gce</artifactId>
+                   <artifactId>delete-by-query</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -298,7 +298,7 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
-                   <artifactId>delete-by-query</artifactId>
+                   <artifactId>discovery-azure</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
@@ -354,12 +354,28 @@
 
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>repository-azure</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
                    <artifactId>repository-s3</artifactId>
                    <version>${elasticsearch.version}</version>
                    <type>zip</type>
                    <overWrite>true</overWrite>
                  </artifactItem>
 
+                 <artifactItem>
+                   <groupId>org.elasticsearch.plugin</groupId>
+                   <artifactId>store-smb</artifactId>
+                   <version>${elasticsearch.version}</version>
+                   <type>zip</type>
+                   <overWrite>true</overWrite>
+                 </artifactItem>
+
                  <!-- Internal plugins for test purpose only -->
                  <artifactItem>
                    <groupId>org.elasticsearch.plugin</groupId>
diff --git a/qa/smoke-test-plugins/rest-api-spec/test/smoke_test_plugins/10_basic.yaml b/qa/smoke-test-plugins/rest-api-spec/test/smoke_test_plugins/10_basic.yaml
deleted file mode 100644
index dbb0922..0000000
--- a/qa/smoke-test-plugins/rest-api-spec/test/smoke_test_plugins/10_basic.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-# Integration tests for smoke testing plugins
-#
-"Correct Plugin Count":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - length:  { nodes.$master.plugins: ${expected.plugin.count}  }
diff --git a/qa/smoke-test-plugins/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_basic.yaml b/qa/smoke-test-plugins/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_basic.yaml
new file mode 100644
index 0000000..dbb0922
--- /dev/null
+++ b/qa/smoke-test-plugins/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_basic.yaml
@@ -0,0 +1,13 @@
+# Integration tests for smoke testing plugins
+#
+"Correct Plugin Count":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - length:  { nodes.$master.plugins: ${expected.plugin.count}  }
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index 1c36872..7e53968 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -143,13 +143,13 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>cloud-aws</artifactId>
+                                    <artifactId>discovery-azure</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
-                                    <artifactId>cloud-azure</artifactId>
+                                    <artifactId>discovery-ec2</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
                                 </artifactItem>
@@ -209,6 +209,24 @@
                                 </artifactItem>
                                 <artifactItem>
                                     <groupId>org.elasticsearch.plugin</groupId>
+                                    <artifactId>repository-azure</artifactId>
+                                    <version>${elasticsearch.version}</version>
+                                    <type>zip</type>
+                                </artifactItem>
+                                <artifactItem>
+                                    <groupId>org.elasticsearch.plugin</groupId>
+                                    <artifactId>repository-s3</artifactId>
+                                    <version>${elasticsearch.version}</version>
+                                    <type>zip</type>
+                                </artifactItem>
+                                <artifactItem>
+                                    <groupId>org.elasticsearch.plugin</groupId>
+                                    <artifactId>store-smb</artifactId>
+                                    <version>${elasticsearch.version}</version>
+                                    <type>zip</type>
+                                </artifactItem>
+                                <artifactItem>
+                                    <groupId>org.elasticsearch.plugin</groupId>
                                     <artifactId>site-example</artifactId>
                                     <version>${elasticsearch.version}</version>
                                     <type>zip</type>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
index 3b5acc7..2ceeb2a 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
@@ -72,6 +72,17 @@ setup() {
     verify_archive_installation
 }
 
+@test "[TAR] elasticsearch fails if java executable is not found" {
+  local JAVA=$(which java)
+
+  sudo chmod -x $JAVA
+  run "$ESHOME/bin/elasticsearch"
+  sudo chmod +x $JAVA
+
+  [ "$status" -eq 1 ]
+  [[ "$output" == *"Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME"* ]]
+}
+
 ##################################
 # Check that Elasticsearch is working
 ##################################
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index 491371e..aa9f784 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -131,6 +131,18 @@ fi
     remove_jvm_example
 }
 
+@test "[$GROUP] fail if java executable is not found" {
+  [ "$GROUP" == "TAR PLUGINS" ] || skip "Test case only supported by TAR PLUGINS"
+  local JAVA=$(which java)
+
+  sudo chmod -x $JAVA
+  run "$ESHOME/bin/plugin"
+  sudo chmod +x $JAVA
+
+  [ "$status" -eq 1 ]
+  [[ "$output" == *"Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME"* ]]
+}
+
 # Note that all of the tests from here to the end of the file expect to be run
 # in sequence and don't take well to being run one at a time.
 @test "[$GROUP] install jvm-example plugin" {
diff --git a/rest-api-spec/LICENSE.txt b/rest-api-spec/LICENSE.txt
deleted file mode 100644
index 75fd7ff..0000000
--- a/rest-api-spec/LICENSE.txt
+++ /dev/null
@@ -1,13 +0,0 @@
-Copyright (c) 2013-2015 Elasticsearch
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
