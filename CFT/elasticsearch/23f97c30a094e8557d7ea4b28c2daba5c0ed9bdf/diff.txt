diff --git a/Vagrantfile b/Vagrantfile
index ab0e322..7c76e23 100644
--- a/Vagrantfile
+++ b/Vagrantfile
@@ -32,7 +32,10 @@ Vagrant.configure(2) do |config|
   end
   config.vm.define "vivid" do |config|
     config.vm.box = "ubuntu/vivid64"
-    ubuntu_common config
+    ubuntu_common config, extra: <<-SHELL
+      # Install Jayatana so we can work around it being present.
+      [ -f /usr/share/java/jayatanaag.jar ] || install jayatana
+    SHELL
   end
   # Wheezy's backports don't contain Openjdk 8 and the backflips required to
   # get the sun jdk on there just aren't worth it. We have jessie for testing
@@ -116,11 +119,11 @@ SOURCE_PROMPT
   end
 end
 
-def ubuntu_common(config)
-  deb_common config, 'apt-add-repository -y ppa:openjdk-r/ppa > /dev/null 2>&1', 'openjdk-r-*'
+def ubuntu_common(config, extra: '')
+  deb_common config, 'apt-add-repository -y ppa:openjdk-r/ppa > /dev/null 2>&1', 'openjdk-r-*', extra: extra
 end
 
-def deb_common(config, add_openjdk_repository_command, openjdk_list)
+def deb_common(config, add_openjdk_repository_command, openjdk_list, extra: '')
   # http://foo-o-rama.com/vagrant--stdin-is-not-a-tty--fix.html
   config.vm.provision "fix-no-tty", type: "shell" do |s|
       s.privileged = false
@@ -137,6 +140,7 @@ def deb_common(config, add_openjdk_repository_command, openjdk_list)
         (echo "Importing java-8 ppa" &&
           #{add_openjdk_repository_command} &&
           apt-get update)
+      #{extra}
 SHELL
   )
 end
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 87348f9..4c82c28 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -482,7 +482,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         RESOURCE_NOT_FOUND_EXCEPTION(org.elasticsearch.ResourceNotFoundException.class, org.elasticsearch.ResourceNotFoundException::new, 19),
         ACTION_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionTransportException.class, org.elasticsearch.transport.ActionTransportException::new, 20),
         ELASTICSEARCH_GENERATION_EXCEPTION(org.elasticsearch.ElasticsearchGenerationException.class, org.elasticsearch.ElasticsearchGenerationException::new, 21),
-        CREATE_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.CreateFailedEngineException.class, org.elasticsearch.index.engine.CreateFailedEngineException::new, 22),
+        //      22 was CreateFailedEngineException
         INDEX_SHARD_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardStartedException.class, org.elasticsearch.index.shard.IndexShardStartedException::new, 23),
         SEARCH_CONTEXT_MISSING_EXCEPTION(org.elasticsearch.search.SearchContextMissingException.class, org.elasticsearch.search.SearchContextMissingException::new, 24),
         SCRIPT_EXCEPTION(org.elasticsearch.script.ScriptException.class, org.elasticsearch.script.ScriptException::new, 25),
@@ -514,7 +514,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         INDEX_SHARD_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.IndexShardAlreadyExistsException.class, org.elasticsearch.index.IndexShardAlreadyExistsException::new, 51),
         VERSION_CONFLICT_ENGINE_EXCEPTION(org.elasticsearch.index.engine.VersionConflictEngineException.class, org.elasticsearch.index.engine.VersionConflictEngineException::new, 52),
         ENGINE_EXCEPTION(org.elasticsearch.index.engine.EngineException.class, org.elasticsearch.index.engine.EngineException::new, 53),
-        DOCUMENT_ALREADY_EXISTS_EXCEPTION(org.elasticsearch.index.engine.DocumentAlreadyExistsException.class, org.elasticsearch.index.engine.DocumentAlreadyExistsException::new, 54),
+        // 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException
         NO_SUCH_NODE_EXCEPTION(org.elasticsearch.action.NoSuchNodeException.class, org.elasticsearch.action.NoSuchNodeException::new, 55),
         SETTINGS_EXCEPTION(org.elasticsearch.common.settings.SettingsException.class, org.elasticsearch.common.settings.SettingsException::new, 56),
         INDEX_TEMPLATE_MISSING_EXCEPTION(org.elasticsearch.indices.IndexTemplateMissingException.class, org.elasticsearch.indices.IndexTemplateMissingException::new, 57),
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 3fa9533..a610d8d 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -259,6 +259,8 @@ public class Version {
     public static final Version V_2_0_0_beta1 = new Version(V_2_0_0_beta1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_beta2_ID = 2000002;
     public static final Version V_2_0_0_beta2 = new Version(V_2_0_0_beta2_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
+    public static final int V_2_0_0_rc1_ID = 2000051;
+    public static final Version V_2_0_0_rc1 = new Version(V_2_0_0_rc1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_ID = 2000099;
     public static final Version V_2_0_0 = new Version(V_2_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_1_0_ID = 2010099;
@@ -287,6 +289,8 @@ public class Version {
                 return V_2_1_0;
             case V_2_0_0_ID:
                 return V_2_0_0;
+            case V_2_0_0_rc1_ID:
+                return V_2_0_0_rc1;
             case V_2_0_0_beta2_ID:
                 return V_2_0_0_beta2;
             case V_2_0_0_beta1_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
index dd0968e..ff31bb7 100644
--- a/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
+++ b/core/src/main/java/org/elasticsearch/action/UnavailableShardsException.java
@@ -21,6 +21,7 @@ package org.elasticsearch.action;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.HppcMaps;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.rest.RestStatus;
@@ -32,8 +33,8 @@ import java.io.IOException;
  */
 public class UnavailableShardsException extends ElasticsearchException {
 
-    public UnavailableShardsException(@Nullable ShardId shardId, String message) {
-        super(buildMessage(shardId, message));
+    public UnavailableShardsException(@Nullable ShardId shardId, String message, Object... args) {
+        super(buildMessage(shardId, message), args);
     }
 
     private static String buildMessage(ShardId shardId, String message) {
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index c071885..0f00b87 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -47,7 +47,6 @@ import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -97,6 +96,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     protected TransportRequestOptions transportOptions() {
         return BulkAction.INSTANCE.transportOptions(settings);
     }
+
     @Override
     protected BulkShardResponse newResponseInstance() {
         return new BulkShardResponse();
@@ -416,7 +416,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 } catch (Throwable t) {
                     t = ExceptionsHelper.unwrapCause(t);
                     boolean retry = false;
-                    if (t instanceof VersionConflictEngineException || (t instanceof DocumentAlreadyExistsException && translate.operation() == UpdateHelper.Operation.UPSERT)) {
+                    if (t instanceof VersionConflictEngineException) {
                         retry = true;
                     }
                     return new UpdateResult(translate, indexRequest, retry, t, null);
@@ -460,20 +460,12 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                     SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, indexRequest.source()).index(shardId.getIndex()).type(indexRequest.type()).id(indexRequest.id())
                             .routing(indexRequest.routing()).parent(indexRequest.parent()).timestamp(indexRequest.timestamp()).ttl(indexRequest.ttl());
 
-                    final Engine.IndexingOperation operation;
-                    if (indexRequest.opType() == IndexRequest.OpType.INDEX) {
-                        operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
-                    } else {
-                        assert indexRequest.opType() == IndexRequest.OpType.CREATE : indexRequest.opType();
-                        operation = indexShard.prepareCreate(sourceToParse,
-                                indexRequest.version(), indexRequest.versionType(),
-                                Engine.Operation.Origin.REPLICA);
-                    }
+                    final Engine.Index operation = indexShard.prepareIndex(sourceToParse, indexRequest.version(), indexRequest.versionType(), Engine.Operation.Origin.REPLICA);
                     Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
                     if (update != null) {
                         throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
                     }
-                    operation.execute(indexShard);
+                    indexShard.index(operation);
                     location = locationToSync(location, operation.getTranslogLocation());
                 } catch (Throwable e) {
                     // if its not an ignore replica failure, we need to make sure to bubble up the failure
@@ -500,7 +492,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
             }
         }
 
-       processAfter(request.refresh(), indexShard, location);
+        processAfter(request.refresh(), indexShard, location);
     }
 
     private void applyVersion(BulkItemRequest item, long version, VersionType versionType) {
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index c171ae9..ad7b9c1 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -49,14 +49,14 @@ import static org.elasticsearch.action.ValidateActions.addValidationError;
 /**
  * Index request to index a typed JSON document into a specific index and make it searchable. Best
  * created using {@link org.elasticsearch.client.Requests#indexRequest(String)}.
- * <p>
+ *
  * The index requires the {@link #index()}, {@link #type(String)}, {@link #id(String)} and
  * {@link #source(byte[])} to be set.
- * <p>
+ *
  * The source (content to index) can be set in its bytes form using ({@link #source(byte[])}),
  * its string form ({@link #source(String)}) or using a {@link org.elasticsearch.common.xcontent.XContentBuilder}
  * ({@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}).
- * <p>
+ *
  * If the {@link #id(String)} is not set, it will be automatically generated.
  *
  * @see IndexResponse
@@ -114,7 +114,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
         public static OpType fromString(String sOpType) {
             String lowersOpType = sOpType.toLowerCase(Locale.ROOT);
-            switch(lowersOpType){
+            switch (lowersOpType) {
                 case "create":
                     return OpType.CREATE;
                 case "index":
@@ -216,6 +216,14 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
+
+        if (opType() == OpType.CREATE) {
+            if (versionType != VersionType.INTERNAL || version != Versions.MATCH_DELETED) {
+                validationException = addValidationError("create operations do not support versioning. use index instead", validationException);
+                return validationException;
+            }
+        }
+
         if (!versionType.validateVersionForWrites(version)) {
             validationException = addValidationError("illegal version value [" + version + "] for version type [" + versionType.name() + "]", validationException);
         }
@@ -370,7 +378,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     /**
      * Sets the document source to index.
-     * <p>
+     *
      * Note, its preferable to either set it using {@link #source(org.elasticsearch.common.xcontent.XContentBuilder)}
      * or using the {@link #source(byte[])}.
      */
@@ -480,6 +488,10 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
      */
     public IndexRequest opType(OpType opType) {
         this.opType = opType;
+        if (opType == OpType.CREATE) {
+            version(Versions.MATCH_DELETED);
+            versionType(VersionType.INTERNAL);
+        }
         return this;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 3e98f1a..63b8237 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -54,7 +54,7 @@ import org.elasticsearch.transport.TransportService;
 
 /**
  * Performs the index operation.
- * <p>
+ *
  * Allows for the following settings:
  * <ul>
  * <li><b>autoCreateIndex</b>: When set to <tt>true</tt>, will automatically create an index if one does not exists.
@@ -167,6 +167,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
 
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(null, request, indexShard);
+
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
         processAfter(request.refresh(), indexShard, location);
@@ -180,18 +181,12 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.REPLICA, request.source()).index(shardId.getIndex()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
 
-        final Engine.IndexingOperation operation;
-        if (request.opType() == IndexRequest.OpType.INDEX) {
-            operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            operation = indexShard.prepareCreate(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
-        }
+        final Engine.Index operation = indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.REPLICA);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         if (update != null) {
             throw new RetryOnReplicaException(shardId, "Mappings are not available on the replica yet, triggered update: " + update);
         }
-        operation.execute(indexShard);
+        indexShard.index(operation);
         processAfter(request.refresh(), indexShard, operation.getTranslogLocation());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index d7fca31..f3db2b6 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -56,7 +56,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.mapper.Mapping;
@@ -188,9 +187,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         if (cause instanceof VersionConflictEngineException) {
             return true;
         }
-        if (cause instanceof DocumentAlreadyExistsException) {
-            return true;
-        }
         return false;
     }
 
@@ -1036,22 +1032,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     /** Utility method to create either an index or a create operation depending
      *  on the {@link OpType} of the request. */
-    private final Engine.IndexingOperation prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
+    private final Engine.Index prepareIndexOperationOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) {
         SourceToParse sourceToParse = SourceToParse.source(SourceToParse.Origin.PRIMARY, request.source()).index(request.index()).type(request.type()).id(request.id())
                 .routing(request.routing()).parent(request.parent()).timestamp(request.timestamp()).ttl(request.ttl());
-        if (request.opType() == IndexRequest.OpType.INDEX) {
             return indexShard.prepareIndex(sourceToParse, request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        } else {
-            assert request.opType() == IndexRequest.OpType.CREATE : request.opType();
-            return indexShard.prepareCreate(sourceToParse,
-                    request.version(), request.versionType(), Engine.Operation.Origin.PRIMARY);
-        }
+
     }
 
     /** Execute the given {@link IndexRequest} on a primary shard, throwing a
      *  {@link RetryOnPrimaryException} if the operation needs to be re-tried. */
     protected final WriteResult<IndexResponse> executeIndexRequestOnPrimary(BulkShardRequest shardRequest, IndexRequest request, IndexShard indexShard) throws Throwable {
-        Engine.IndexingOperation operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
+        Engine.Index operation = prepareIndexOperationOnPrimary(shardRequest, request, indexShard);
         Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
         final ShardId shardId = indexShard.shardId();
         if (update != null) {
@@ -1064,7 +1055,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                         "Dynamics mappings are not available on the node that holds the primary yet");
             }
         }
-        final boolean created = operation.execute(indexShard);
+        final boolean created = indexShard.index(operation);
 
         // update the version on request so it will happen on the replicas
         final long version = operation.version();
diff --git a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
index 2e815da..5f4f942 100644
--- a/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.support.single.instance;
 
+import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.UnavailableShardsException;
@@ -35,6 +36,7 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.shard.ShardId;
@@ -42,6 +44,7 @@ import org.elasticsearch.node.NodeClosedException;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
+import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Supplier;
 
@@ -111,9 +114,8 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
         private volatile ClusterStateObserver observer;
         private ShardIterator shardIt;
         private DiscoveryNodes nodes;
-        private final AtomicBoolean operationStarted = new AtomicBoolean();
 
-        private AsyncSingleAction(Request request, ActionListener<Response> listener) {
+        AsyncSingleAction(Request request, ActionListener<Response> listener) {
             this.request = request;
             this.listener = listener;
         }
@@ -123,14 +125,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
             doStart();
         }
 
-        protected boolean doStart() {
+        protected void doStart() {
             nodes = observer.observedState().nodes();
             try {
                 ClusterBlockException blockException = checkGlobalBlock(observer.observedState());
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -138,13 +140,14 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 request.concreteIndex(indexNameExpressionResolver.concreteSingleIndex(observer.observedState(), request));
                 // check if we need to execute, and if not, return
                 if (!resolveRequest(observer.observedState(), request, listener)) {
-                    return true;
+                    listener.onFailure(new IllegalStateException(LoggerMessageFormat.format("{} request {} could not be resolved", new ShardId(request.index, request.shardId), actionName)));
+                    return;
                 }
                 blockException = checkRequestBlock(observer.observedState(), request);
                 if (blockException != null) {
                     if (blockException.retryable()) {
                         retry(blockException);
-                        return false;
+                        return;
                     } else {
                         throw blockException;
                     }
@@ -152,13 +155,13 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 shardIt = shards(observer.observedState(), request);
             } catch (Throwable e) {
                 listener.onFailure(e);
-                return true;
+                return;
             }
 
             // no shardIt, might be in the case between index gateway recovery and shardIt initialization
             if (shardIt.size() == 0) {
                 retry(null);
-                return false;
+                return;
             }
 
             // this transport only make sense with an iterator that returns a single shard routing (like primary)
@@ -169,11 +172,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
             if (!shard.active()) {
                 retry(null);
-                return false;
-            }
-
-            if (!operationStarted.compareAndSet(false, true)) {
-                return true;
+                return;
             }
 
             request.shardId = shardIt.shardId().id();
@@ -197,24 +196,30 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
 
                 @Override
                 public void handleException(TransportException exp) {
+                    Throwable cause = exp.unwrapCause();
                     // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
-                    if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
+                    if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException ||
                             retryOnFailure(exp)) {
-                        operationStarted.set(false);
-                        // we already marked it as started when we executed it (removed the listener) so pass false
-                        // to re-add to the cluster listener
-                        retry(null);
+                        retry(cause);
                     } else {
                         listener.onFailure(exp);
                     }
                 }
             });
-            return true;
         }
 
         void retry(final @Nullable Throwable failure) {
             if (observer.isTimedOut()) {
                 // we running as a last attempt after a timeout has happened. don't retry
+                Throwable listenFailure = failure;
+                if (listenFailure == null) {
+                    if (shardIt == null) {
+                        listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [{}], request: {}", request.timeout(), actionName);
+                    } else {
+                        listenFailure = new UnavailableShardsException(shardIt.shardId(), "[{}] shardIt, [{}] active : Timeout waiting for [{}], request: {}", shardIt.size(), shardIt.sizeActive(), request.timeout(), actionName);
+                    }
+                }
+                listener.onFailure(listenFailure);
                 return;
             }
 
@@ -232,17 +237,7 @@ public abstract class TransportInstanceSingleOperationAction<Request extends Ins
                 @Override
                 public void onTimeout(TimeValue timeout) {
                     // just to be on the safe side, see if we can start it now?
-                    if (!doStart()) {
-                        Throwable listenFailure = failure;
-                        if (listenFailure == null) {
-                            if (shardIt == null) {
-                                listenFailure = new UnavailableShardsException(new ShardId(request.concreteIndex(), -1), "Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            } else {
-                                listenFailure = new UnavailableShardsException(shardIt.shardId(), "[" + shardIt.size() + "] shardIt, [" + shardIt.sizeActive() + "] active : Timeout waiting for [" + timeout + "], request: " + request.toString());
-                            }
-                        }
-                        listener.onFailure(listenFailure);
-                    }
+                    doStart();
                 }
             }, request.timeout());
         }
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index 7479416..2a639c8 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -48,9 +48,8 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
-import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndexAlreadyExistsException;
 import org.elasticsearch.indices.IndicesService;
@@ -170,7 +169,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
         final UpdateHelper.Result result = updateHelper.prepare(request, indexShard);
         switch (result.operation()) {
             case UPSERT:
-                IndexRequest upsertRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest upsertRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference upsertSourceBytes = upsertRequest.source();
                 indexAction.execute(upsertRequest, new ActionListener<IndexResponse>() {
@@ -189,7 +188,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                     @Override
                     public void onFailure(Throwable e) {
                         e = ExceptionsHelper.unwrapCause(e);
-                        if (e instanceof VersionConflictEngineException || e instanceof DocumentAlreadyExistsException) {
+                        if (e instanceof VersionConflictEngineException) {
                             if (retryCount < request.retryOnConflict()) {
                                 threadPool.executor(executor()).execute(new ActionRunnable<UpdateResponse>(listener) {
                                     @Override
@@ -205,7 +204,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case INDEX:
-                IndexRequest indexRequest = new IndexRequest((IndexRequest)result.action(), request);
+                IndexRequest indexRequest = new IndexRequest(result.action(), request);
                 // we fetch it from the index request so we don't generate the bytes twice, its already done in the index request
                 final BytesReference indexSourceBytes = indexRequest.source();
                 indexAction.execute(indexRequest, new ActionListener<IndexResponse>() {
@@ -235,7 +234,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 });
                 break;
             case DELETE:
-                DeleteRequest deleteRequest = new DeleteRequest((DeleteRequest)result.action(), request);
+                DeleteRequest deleteRequest = new DeleteRequest(result.action(), request);
                 deleteAction.execute(deleteRequest, new ActionListener<DeleteResponse>() {
                     @Override
                     public void onResponse(DeleteResponse response) {
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 5b8c14a..9ebb2c9 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.PidFile;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.Terminal;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.CreationException;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
@@ -195,7 +194,7 @@ final class Bootstrap {
     private static void setupLogging(Settings settings, Environment environment) {
         try {
             Class.forName("org.apache.log4j.Logger");
-            LogConfigurator.configure(settings);
+            LogConfigurator.configure(settings, true);
         } catch (ClassNotFoundException e) {
             // no log4j
         } catch (NoClassDefFoundError e) {
@@ -249,13 +248,13 @@ final class Bootstrap {
 
         Environment environment = initialSettings(foreground);
         Settings settings = environment.settings();
+        setupLogging(settings, environment);
+        checkForCustomConfFile();
 
         if (environment.pidFile() != null) {
             PidFile.create(environment.pidFile(), true);
         }
 
-        setupLogging(settings, environment);
-
         if (System.getProperty("es.max-open-files", "false").equals("true")) {
             ESLogger logger = Loggers.getLogger(Bootstrap.class);
             logger.info("max_open_files [{}]", ProcessProbe.getInstance().getMaxFileDescriptorCount());
@@ -330,4 +329,21 @@ final class Bootstrap {
             System.err.flush();
         }
     }
+
+    private static void checkForCustomConfFile() {
+        String confFileSetting = System.getProperty("es.default.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.default.config");
+        confFileSetting = System.getProperty("es.config");
+        checkUnsetAndMaybeExit(confFileSetting, "es.config");
+        confFileSetting = System.getProperty("elasticsearch.config");
+        checkUnsetAndMaybeExit(confFileSetting, "elasticsearch.config");
+    }
+
+    private static void checkUnsetAndMaybeExit(String confFileSetting, String settingName) {
+        if (confFileSetting != null && confFileSetting.isEmpty() == false) {
+            ESLogger logger = Loggers.getLogger(Bootstrap.class);
+            logger.info("{} is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.", settingName);
+            System.exit(1);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
index b13c799..e3925aa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
@@ -73,7 +73,7 @@ public class MappingUpdatedAction extends AbstractComponent {
             throw new IllegalArgumentException("_default_ mapping should not be updated");
         }
         return client.preparePutMapping(index).setType(type).setSource(mappingUpdate.toString())
-            .setMasterNodeTimeout(timeout).setTimeout(timeout);
+                .setMasterNodeTimeout(timeout).setTimeout(timeout);
     }
 
     public void updateMappingOnMaster(String index, String type, Mapping mappingUpdate, final TimeValue timeout, final MappingUpdateListener listener) {
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
index 8f92864..a715d34 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
@@ -70,6 +70,7 @@ public class LogConfigurator {
             .put("socketHub", "org.apache.log4j.net.SocketHubAppender")
             .put("syslog", "org.apache.log4j.net.SyslogAppender")
             .put("telnet", "org.apache.log4j.net.TelnetAppender")
+            .put("terminal", "org.elasticsearch.common.logging.log4j.TerminalAppender")
                     // policies
             .put("timeBased", "org.apache.log4j.rolling.TimeBasedRollingPolicy")
             .put("sizeBased", "org.apache.log4j.rolling.SizeBasedTriggeringPolicy")
@@ -83,15 +84,24 @@ public class LogConfigurator {
             .put("xml", "org.apache.log4j.XMLLayout")
             .immutableMap();
 
-    public static void configure(Settings settings) {
+    /**
+     * Consolidates settings and converts them into actual log4j settings, then initializes loggers and appenders.
+     *
+     * @param settings      custom settings that should be applied
+     * @param resolveConfig controls whether the logging conf file should be read too or not.
+     */
+    public static void configure(Settings settings, boolean resolveConfig) {
         if (loaded) {
             return;
         }
         loaded = true;
         // TODO: this is partly a copy of InternalSettingsPreparer...we should pass in Environment and not do all this...
         Environment environment = new Environment(settings);
+
         Settings.Builder settingsBuilder = settingsBuilder();
-        resolveConfig(environment, settingsBuilder);
+        if (resolveConfig) {
+            resolveConfig(environment, settingsBuilder);
+        }
         settingsBuilder
                 .putProperties("elasticsearch.", System.getProperties())
                 .putProperties("es.", System.getProperties());
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/TerminalAppender.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/TerminalAppender.java
new file mode 100644
index 0000000..3c60c44
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/TerminalAppender.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.common.logging.log4j;
+
+import org.apache.log4j.AppenderSkeleton;
+import org.apache.log4j.spi.LoggingEvent;
+import org.elasticsearch.common.cli.Terminal;
+
+/**
+ * TerminalAppender logs event to Terminal.DEFAULT. It is used for example by the PluginManagerCliParser.
+ * */
+public class TerminalAppender extends AppenderSkeleton {
+    @Override
+    protected void append(LoggingEvent event) {
+        Terminal.DEFAULT.println(event.getRenderedMessage());
+    }
+
+    @Override
+    public void close() {
+    }
+
+    @Override
+    public boolean requiresLayout() {
+        return false;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
index 77eb218..55586d8 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/uid/Versions.java
@@ -33,10 +33,24 @@ import java.util.concurrent.ConcurrentMap;
 /** Utility class to resolve the Lucene doc ID and version for a given uid. */
 public class Versions {
 
-    public static final long MATCH_ANY = -3L; // Version was not specified by the user
+    /** used to indicate the write operation should succeed regardless of current version **/
+    public static final long MATCH_ANY = -3L;
+
+    /** indicates that the current document was not found in lucene and in the version map */
     public static final long NOT_FOUND = -1L;
+
+    /**
+     * used when the document is old and doesn't contain any version information in the index
+     * see {@link PerThreadIDAndVersionLookup#lookup(org.apache.lucene.util.BytesRef)}
+     */
     public static final long NOT_SET = -2L;
 
+    /**
+     * used to indicate that the write operation should be executed if the document is currently deleted
+     * i.e., not found in the index and/or found as deleted (with version) in the version map
+     */
+    public static final long MATCH_DELETED = -4L;
+
     // TODO: is there somewhere else we can store these?
     private static final ConcurrentMap<IndexReader, CloseableThreadLocal<PerThreadIDAndVersionLookup>> lookupStates = ConcurrentCollections.newConcurrentMapWithAggressiveConcurrency();
 
diff --git a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
index 0d5c3cb..fe84284 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
@@ -16,7 +16,6 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.index;
 
 import org.elasticsearch.common.Nullable;
@@ -35,7 +34,6 @@ import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 /**
@@ -60,10 +58,9 @@ public final class IndexServicesProvider {
     private final EngineFactory factory;
     private final BigArrays bigArrays;
     private final IndexSearcherWrapper indexSearcherWrapper;
-    private final IndexingMemoryController indexingMemoryController;
 
     @Inject
-    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper, IndexingMemoryController indexingMemoryController) {
+    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper) {
         this.indicesLifecycle = indicesLifecycle;
         this.threadPool = threadPool;
         this.mapperService = mapperService;
@@ -79,7 +76,6 @@ public final class IndexServicesProvider {
         this.factory = factory;
         this.bigArrays = bigArrays;
         this.indexSearcherWrapper = indexSearcherWrapper;
-        this.indexingMemoryController = indexingMemoryController;
     }
 
     public IndicesLifecycle getIndicesLifecycle() {
@@ -138,11 +134,5 @@ public final class IndexServicesProvider {
         return bigArrays;
     }
 
-    public IndexSearcherWrapper getIndexSearcherWrapper() {
-        return indexSearcherWrapper;
-    }
-
-    public IndexingMemoryController getIndexingMemoryController() {
-        return indexingMemoryController;
-    }
+    public IndexSearcherWrapper getIndexSearcherWrapper() { return indexSearcherWrapper; }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/VersionType.java b/core/src/main/java/org/elasticsearch/index/VersionType.java
index a5d8cae..b8f998b 100644
--- a/core/src/main/java/org/elasticsearch/index/VersionType.java
+++ b/core/src/main/java/org/elasticsearch/index/VersionType.java
@@ -31,24 +31,37 @@ import java.io.IOException;
 public enum VersionType implements Writeable<VersionType> {
     INTERNAL((byte) 0) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return isVersionConflict(currentVersion, expectedVersion, deleted);
+        }
+
+        @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return "document already exists (current version [" + currentVersion + "])";
+            }
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
         }
 
         @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
-            return isVersionConflict(currentVersion, expectedVersion);
+            return isVersionConflict(currentVersion, expectedVersion, false);
         }
 
-        private boolean isVersionConflict(long currentVersion, long expectedVersion) {
+        @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        private boolean isVersionConflict(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
                 return false;
             }
-            if (currentVersion == Versions.NOT_FOUND) {
-                return true;
+            if (expectedVersion == Versions.MATCH_DELETED) {
+                return deleted == false;
             }
             if (currentVersion != expectedVersion) {
                 return true;
@@ -63,8 +76,7 @@ public enum VersionType implements Writeable<VersionType> {
 
         @Override
         public boolean validateVersionForWrites(long version) {
-            // not allowing Versions.NOT_FOUND as it is not a valid input value.
-            return version > 0L || version == Versions.MATCH_ANY;
+            return version > 0L || version == Versions.MATCH_ANY || version == Versions.MATCH_DELETED;
         }
 
         @Override
@@ -82,7 +94,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL((byte) 1) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -99,6 +111,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher or equal to the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -116,6 +133,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -133,7 +155,7 @@ public enum VersionType implements Writeable<VersionType> {
     },
     EXTERNAL_GTE((byte) 2) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -150,6 +172,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            return "current version [" + currentVersion + "] is higher than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
@@ -167,6 +194,11 @@ public enum VersionType implements Writeable<VersionType> {
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            return "current version [" + currentVersion + "] is different than the one provided [" + expectedVersion + "]";
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -187,7 +219,7 @@ public enum VersionType implements Writeable<VersionType> {
      */
     FORCE((byte) 3) {
         @Override
-        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion) {
+        public boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
             if (currentVersion == Versions.NOT_SET) {
                 return false;
             }
@@ -195,17 +227,27 @@ public enum VersionType implements Writeable<VersionType> {
                 return false;
             }
             if (expectedVersion == Versions.MATCH_ANY) {
-                return true;
+                throw new IllegalStateException("you must specify a version when use VersionType.FORCE");
             }
             return false;
         }
 
         @Override
+        public String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted) {
+            throw new AssertionError("VersionType.FORCE should never result in a write conflict");
+        }
+
+        @Override
         public boolean isVersionConflictForReads(long currentVersion, long expectedVersion) {
             return false;
         }
 
         @Override
+        public String explainConflictForReads(long currentVersion, long expectedVersion) {
+            throw new AssertionError("VersionType.FORCE should never result in a read conflict");
+        }
+
+        @Override
         public long updateVersion(long currentVersion, long expectedVersion) {
             return expectedVersion;
         }
@@ -237,18 +279,47 @@ public enum VersionType implements Writeable<VersionType> {
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
      * @return true if versions conflict false o.w.
      */
-    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion);
+    public abstract boolean isVersionConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
+
+
+    /**
+     * Returns a human readable explanation for a version conflict on write.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForWrites(long, long, boolean)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the write operation
+     * @param deleted         true if the document is currently deleted (note that #currentVersion will typically be
+     *                        {@link Versions#NOT_FOUND}, but may be something else if the document was recently deleted
+     */
+    public abstract String explainConflictForWrites(long currentVersion, long expectedVersion, boolean deleted);
 
     /**
      * Checks whether the current version conflicts with the expected version, based on the current version type.
      *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
      * @return true if versions conflict false o.w.
      */
     public abstract boolean isVersionConflictForReads(long currentVersion, long expectedVersion);
 
     /**
+     * Returns a human readable explanation for a version conflict on read.
+     *
+     * Note that this method is only called if {@link #isVersionConflictForReads(long, long)} returns true;
+     *
+     * @param currentVersion  the current version for the document
+     * @param expectedVersion the version specified for the read operation
+     */
+    public abstract String explainConflictForReads(long currentVersion, long expectedVersion);
+
+    /**
      * Returns the new version for a document, based on its current one and the specified in the request
      *
      * @return new version
diff --git a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
deleted file mode 100644
index 32d9ee6..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/CreateFailedEngineException.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- *
- */
-public class CreateFailedEngineException extends EngineException {
-
-    private final String type;
-
-    private final String id;
-
-    public CreateFailedEngineException(ShardId shardId, String type, String id, Throwable cause) {
-        super(shardId, "Create failed for [" + type + "#" + id + "]", cause);
-        Objects.requireNonNull(type, "type must not be null");
-        Objects.requireNonNull(id, "id must not be null");
-        this.type = type;
-        this.id = id;
-    }
-
-    public CreateFailedEngineException(StreamInput in) throws IOException{
-        super(in);
-        type = in.readString();
-        id = in.readString();
-    }
-
-    public String type() {
-        return this.type;
-    }
-
-    public String id() {
-        return this.id;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(type);
-        out.writeString(id);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java b/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
deleted file mode 100644
index 467dd8c..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/DocumentAlreadyExistsException.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class DocumentAlreadyExistsException extends EngineException {
-
-    public DocumentAlreadyExistsException(ShardId shardId, String type, String id) {
-        super(shardId, "[" + type + "][" + id + "]: document already exists");
-    }
-
-    public DocumentAlreadyExistsException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.CONFLICT;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index 70fa7e0..7ef1d14 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -45,7 +45,6 @@ import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
-import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.Translog;
@@ -60,7 +59,6 @@ import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
-import java.util.function.Supplier;
 
 /**
  *
@@ -144,7 +142,8 @@ public abstract class Engine implements Closeable {
         return new MergeStats();
     }
 
-    /** A throttling class that can be activated, causing the
+    /**
+     * A throttling class that can be activated, causing the
      * {@code acquireThrottle} method to block on a lock when throttling
      * is enabled
      */
@@ -203,9 +202,7 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public abstract void create(Create create) throws EngineException;
-
-    public abstract boolean index(Index index) throws EngineException;
+    public abstract boolean index(Index operation) throws EngineException;
 
     public abstract void delete(Delete delete) throws EngineException;
 
@@ -216,7 +213,8 @@ public abstract class Engine implements Closeable {
     /**
      * Attempts to do a special commit where the given syncID is put into the commit data. The attempt
      * succeeds if there are not pending writes in lucene and the current point is equal to the expected one.
-     * @param syncId id of this sync
+     *
+     * @param syncId           id of this sync
      * @param expectedCommitId the expected value of
      * @return true if the sync commit was made, false o.w.
      */
@@ -243,7 +241,8 @@ public abstract class Engine implements Closeable {
             if (get.versionType().isVersionConflictForReads(docIdAndVersion.version, get.version())) {
                 Releasables.close(searcher);
                 Uid uid = Uid.createUid(get.uid().text());
-                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), docIdAndVersion.version, get.version());
+                throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                        get.versionType().explainConflictForReads(docIdAndVersion.version, get.version()));
             }
         }
 
@@ -328,7 +327,7 @@ public abstract class Engine implements Closeable {
         } catch (IOException e) {
             // Fall back to reading from the store if reading from the commit fails
             try {
-                return store. readLastCommittedSegmentsInfo();
+                return store.readLastCommittedSegmentsInfo();
             } catch (IOException e2) {
                 e2.addSuppressed(e);
                 throw e2;
@@ -366,9 +365,6 @@ public abstract class Engine implements Closeable {
         stats.addIndexWriterMaxMemoryInBytes(0);
     }
 
-    /** How much heap Lucene's IndexWriter is using */
-    abstract public long indexWriterRAMBytesUsed();
-
     protected Segment[] getSegmentInfo(SegmentInfos lastCommittedSegmentInfos, boolean verbose) {
         ensureOpen();
         Map<String, Segment> segments = new HashMap<>();
@@ -472,7 +468,8 @@ public abstract class Engine implements Closeable {
 
     /**
      * Flushes the state of the engine including the transaction log, clearing memory.
-     * @param force if <code>true</code> a lucene commit is executed even if no changes need to be committed.
+     *
+     * @param force         if <code>true</code> a lucene commit is executed even if no changes need to be committed.
      * @param waitIfOngoing if <code>true</code> this call will block until all currently running flushes have finished.
      *                      Otherwise this call will return without blocking.
      * @return the commit Id for the resulting commit
@@ -610,94 +607,43 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public static interface Operation {
-        static enum Type {
-            CREATE,
-            INDEX,
-            DELETE
-        }
-
-        static enum Origin {
-            PRIMARY,
-            REPLICA,
-            RECOVERY
-        }
-
-        Type opType();
-
-        Origin origin();
-
-        /**
-         * Returns operation start time in nanoseconds.
-         */
-        long startTime();
-    }
-
-    public static abstract class IndexingOperation implements Operation {
-
+    public static abstract class Operation {
         private final Term uid;
-        private final ParsedDocument doc;
         private long version;
         private final VersionType versionType;
         private final Origin origin;
         private Translog.Location location;
-
         private final long startTime;
         private long endTime;
 
-        public IndexingOperation(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+        public Operation(Term uid, long version, VersionType versionType, Origin origin, long startTime) {
             this.uid = uid;
-            this.doc = doc;
             this.version = version;
             this.versionType = versionType;
             this.origin = origin;
             this.startTime = startTime;
         }
 
-        public IndexingOperation(Term uid, ParsedDocument doc) {
-            this(uid, doc, Versions.MATCH_ANY, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
+        public static enum Origin {
+            PRIMARY,
+            REPLICA,
+            RECOVERY
         }
 
-        @Override
         public Origin origin() {
             return this.origin;
         }
 
-        public ParsedDocument parsedDoc() {
-            return this.doc;
-        }
-
         public Term uid() {
             return this.uid;
         }
 
-        public String type() {
-            return this.doc.type();
-        }
-
-        public String id() {
-            return this.doc.id();
-        }
-
-        public String routing() {
-            return this.doc.routing();
-        }
-
-        public long timestamp() {
-            return this.doc.timestamp();
-        }
-
-        public long ttl() {
-            return this.doc.ttl();
-        }
-
         public long version() {
             return this.version;
         }
 
         public void updateVersion(long version) {
             this.version = version;
-            this.doc.version().setLongValue(version);
         }
 
         public void setTranslogLocation(Translog.Location location) {
@@ -712,19 +658,9 @@ public abstract class Engine implements Closeable {
             return this.versionType;
         }
 
-        public String parent() {
-            return this.doc.parent();
-        }
-
-        public List<Document> docs() {
-            return this.doc.docs();
-        }
-
-        public BytesReference source() {
-            return this.doc.source();
-        }
-
-        @Override
+        /**
+         * Returns operation start time in nanoseconds.
+         */
         public long startTime() {
             return this.startTime;
         }
@@ -739,78 +675,77 @@ public abstract class Engine implements Closeable {
         public long endTime() {
             return this.endTime;
         }
-
-        /**
-         * Execute this operation against the provided {@link IndexShard} and
-         * return whether the document was created.
-         */
-        public abstract boolean execute(IndexShard shard);
     }
 
-    public static final class Create extends IndexingOperation {
+    public static class Index extends Operation {
 
-        public Create(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        private final ParsedDocument doc;
+
+        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
+            super(uid, version, versionType, origin, startTime);
+            this.doc = doc;
         }
 
-        public Create(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public Index(Term uid, ParsedDocument doc) {
+            this(uid, doc, Versions.MATCH_ANY);
         }
 
-        @Override
-        public Type opType() {
-            return Type.CREATE;
+        public Index(Term uid, ParsedDocument doc, long version) {
+            this(uid, doc, version, VersionType.INTERNAL, Origin.PRIMARY, System.nanoTime());
         }
 
-        @Override
-        public boolean execute(IndexShard shard) {
-            shard.create(this);
-            return true;
+        public ParsedDocument parsedDoc() {
+            return this.doc;
         }
-    }
 
-    public static final class Index extends IndexingOperation {
+        public String type() {
+            return this.doc.type();
+        }
 
-        public Index(Term uid, ParsedDocument doc, long version, VersionType versionType, Origin origin, long startTime) {
-            super(uid, doc, version, versionType, origin, startTime);
+        public String id() {
+            return this.doc.id();
         }
 
-        public Index(Term uid, ParsedDocument doc) {
-            super(uid, doc);
+        public String routing() {
+            return this.doc.routing();
         }
 
-        @Override
-        public Type opType() {
-            return Type.INDEX;
+        public long timestamp() {
+            return this.doc.timestamp();
+        }
+
+        public long ttl() {
+            return this.doc.ttl();
         }
 
         @Override
-        public boolean execute(IndexShard shard) {
-            return shard.index(this);
+        public void updateVersion(long version) {
+            super.updateVersion(version);
+            this.doc.version().setLongValue(version);
+        }
+
+        public String parent() {
+            return this.doc.parent();
+        }
+
+        public List<Document> docs() {
+            return this.doc.docs();
+        }
+
+        public BytesReference source() {
+            return this.doc.source();
         }
     }
 
-    public static class Delete implements Operation {
+    public static class Delete extends Operation {
         private final String type;
         private final String id;
-        private final Term uid;
-        private long version;
-        private final VersionType versionType;
-        private final Origin origin;
         private boolean found;
 
-        private final long startTime;
-        private long endTime;
-        private Translog.Location location;
-
         public Delete(String type, String id, Term uid, long version, VersionType versionType, Origin origin, long startTime, boolean found) {
+            super(uid, version, versionType, origin, startTime);
             this.type = type;
             this.id = id;
-            this.uid = uid;
-            this.version = version;
-            this.versionType = versionType;
-            this.origin = origin;
-            this.startTime = startTime;
             this.found = found;
         }
 
@@ -822,16 +757,6 @@ public abstract class Engine implements Closeable {
             this(template.type(), template.id(), template.uid(), template.version(), versionType, template.origin(), template.startTime(), template.found());
         }
 
-        @Override
-        public Type opType() {
-            return Type.DELETE;
-        }
-
-        @Override
-        public Origin origin() {
-            return this.origin;
-        }
-
         public String type() {
             return this.type;
         }
@@ -840,53 +765,14 @@ public abstract class Engine implements Closeable {
             return this.id;
         }
 
-        public Term uid() {
-            return this.uid;
-        }
-
         public void updateVersion(long version, boolean found) {
-            this.version = version;
+            updateVersion(version);
             this.found = found;
         }
 
-        /**
-         * before delete execution this is the version to be deleted. After this is the version of the "delete" transaction record.
-         */
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return this.versionType;
-        }
-
         public boolean found() {
             return this.found;
         }
-
-        @Override
-        public long startTime() {
-            return this.startTime;
-        }
-
-        public void endTime(long endTime) {
-            this.endTime = endTime;
-        }
-
-        /**
-         * Returns operation end time in nanoseconds.
-         */
-        public long endTime() {
-            return this.endTime;
-        }
-
-        public void setTranslogLocation(Translog.Location location) {
-            this.location = location;
-        }
-
-        public Translog.Location getTranslogLocation() {
-            return this.location;
-        }
     }
 
     public static class DeleteByQuery {
@@ -1139,12 +1025,18 @@ public abstract class Engine implements Closeable {
 
         @Override
         public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
+            if (this == o) {
+                return true;
+            }
+            if (o == null || getClass() != o.getClass()) {
+                return false;
+            }
 
             CommitId commitId = (CommitId) o;
 
-            if (!Arrays.equals(id, commitId.id)) return false;
+            if (!Arrays.equals(id, commitId.id)) {
+                return false;
+            }
 
             return true;
         }
@@ -1155,5 +1047,6 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public void onSettingsChanged() {}
+    public void onSettingsChanged() {
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
index d553003..a100cd4 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java
@@ -40,7 +40,6 @@ import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.indices.IndicesWarmer;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.concurrent.TimeUnit;
@@ -108,6 +107,8 @@ public final class EngineConfig {
 
     public static final TimeValue DEFAULT_REFRESH_INTERVAL = new TimeValue(1, TimeUnit.SECONDS);
     public static final TimeValue DEFAULT_GC_DELETES = TimeValue.timeValueSeconds(60);
+    public static final ByteSizeValue DEFAULT_INDEX_BUFFER_SIZE = new ByteSizeValue(64, ByteSizeUnit.MB);
+    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
 
     public static final String DEFAULT_VERSION_MAP_SIZE = "25%";
 
@@ -138,8 +139,7 @@ public final class EngineConfig {
         this.failedEngineListener = failedEngineListener;
         this.compoundOnFlush = indexSettings.getAsBoolean(EngineConfig.INDEX_COMPOUND_ON_FLUSH, compoundOnFlush);
         codecName = indexSettings.get(EngineConfig.INDEX_CODEC_SETTING, EngineConfig.DEFAULT_CODEC_NAME);
-        // We start up inactive and rely on IndexingMemoryController to give us our fair share once we start indexing:
-        indexingBufferSize = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
+        indexingBufferSize = DEFAULT_INDEX_BUFFER_SIZE;
         gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES_SETTING, EngineConfig.DEFAULT_GC_DELETES).millis();
         versionMapSizeSetting = indexSettings.get(INDEX_VERSION_MAP_SIZE, DEFAULT_VERSION_MAP_SIZE);
         updateVersionMapSize();
@@ -258,10 +258,10 @@ public final class EngineConfig {
 
     /**
      * Returns a {@link org.elasticsearch.index.indexing.ShardIndexingService} used inside the engine to inform about
-     * pre and post index and create operations. The operations are used for statistic purposes etc.
+     * pre and post index. The operations are used for statistic purposes etc.
      *
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#postCreate(org.elasticsearch.index.engine.Engine.Create)
-     * @see org.elasticsearch.index.indexing.ShardIndexingService#preCreate(org.elasticsearch.index.engine.Engine.Create)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#postIndex(Engine.Index)
+     * @see org.elasticsearch.index.indexing.ShardIndexingService#preIndex(Engine.Index)
      *
      */
     public ShardIndexingService getIndexingService() {
diff --git a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
index d7487ef..23f6be7 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/EngineException.java
@@ -30,16 +30,16 @@ import java.io.IOException;
  */
 public class EngineException extends ElasticsearchException {
 
-    public EngineException(ShardId shardId, String msg) {
-        this(shardId, msg, null);
+    public EngineException(ShardId shardId, String msg, Object... params) {
+        this(shardId, msg, null, params);
     }
 
-    public EngineException(ShardId shardId, String msg, Throwable cause) {
-        super(msg, cause);
+    public EngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(msg, cause, params);
         setShard(shardId);
     }
 
-    public EngineException(StreamInput in) throws IOException{
+    public EngineException(StreamInput in) throws IOException {
         super(in);
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 4a1d6aa..a325f3a 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -316,7 +316,8 @@ public class InternalEngine extends Engine {
                     }
                     if (get.versionType().isVersionConflictForReads(versionValue.version(), get.version())) {
                         Uid uid = Uid.createUid(get.uid().text());
-                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(), versionValue.version(), get.version());
+                        throw new VersionConflictEngineException(shardId, uid.type(), uid.id(),
+                                get.versionType().explainConflictForReads(versionValue.version(), get.version()));
                     }
                     Translog.Operation op = translog.read(versionValue.translogLocation());
                     if (op != null) {
@@ -331,96 +332,7 @@ public class InternalEngine extends Engine {
     }
 
     @Override
-    public void create(Create create) throws EngineException {
-        try (ReleasableLock lock = readLock.acquire()) {
-            ensureOpen();
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                // Don't throttle recovery operations
-                innerCreate(create);
-            } else {
-                try (Releasable r = throttle.acquireThrottle()) {
-                    innerCreate(create);
-                }
-            }
-        } catch (OutOfMemoryError | IllegalStateException | IOException t) {
-            maybeFailEngine("create", t);
-            throw new CreateFailedEngineException(shardId, create.type(), create.id(), t);
-        }
-        checkVersionMapRefresh();
-    }
-
-    private void innerCreate(Create create) throws IOException {
-        synchronized (dirtyLock(create.uid())) {
-            final long currentVersion;
-            final VersionValue versionValue;
-            versionValue = versionMap.getUnderLock(create.uid().bytes());
-            if (versionValue == null) {
-                currentVersion = loadCurrentVersionFromIndex(create.uid());
-            } else {
-                if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
-                    currentVersion = Versions.NOT_FOUND; // deleted, and GC
-                } else {
-                    currentVersion = versionValue.version();
-                }
-            }
-            innerCreateUnderLock(create, currentVersion, versionValue);
-        }
-    }
-
-    private void innerCreateUnderLock(Create create, long currentVersion, VersionValue versionValue) throws IOException {
-
-        // same logic as index
-        long updatedVersion;
-        long expectedVersion = create.version();
-        if (create.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else {
-                throw new VersionConflictEngineException(shardId, create.type(), create.id(), currentVersion, expectedVersion);
-            }
-        }
-        updatedVersion = create.versionType().updateVersion(currentVersion, expectedVersion);
-
-        // if the doc exists
-        boolean doUpdate = false;
-        if ((versionValue != null && versionValue.delete() == false) || (versionValue == null && currentVersion != Versions.NOT_FOUND)) {
-            if (create.origin() == Operation.Origin.RECOVERY) {
-                return;
-            } else if (create.origin() == Operation.Origin.REPLICA) {
-                // #7142: the primary already determined it's OK to index this document, and we confirmed above that the version doesn't
-                // conflict, so we must also update here on the replica to remain consistent:
-                doUpdate = true;
-            } else {
-                // On primary, we throw DAEE if the _uid is already in the index with an older version:
-                assert create.origin() == Operation.Origin.PRIMARY;
-                throw new DocumentAlreadyExistsException(shardId, create.type(), create.id());
-            }
-        }
-
-        create.updateVersion(updatedVersion);
-
-        if (doUpdate) {
-            if (create.docs().size() > 1) {
-                indexWriter.updateDocuments(create.uid(), create.docs());
-            } else {
-                indexWriter.updateDocument(create.uid(), create.docs().get(0));
-            }
-        } else {
-            if (create.docs().size() > 1) {
-                indexWriter.addDocuments(create.docs());
-            } else {
-                indexWriter.addDocument(create.docs().get(0));
-            }
-        }
-        Translog.Location translogLocation = translog.add(new Translog.Create(create));
-
-        versionMap.putUnderLock(create.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
-        create.setTranslogLocation(translogLocation);
-        indexingService.postCreateUnderLock(create);
-    }
-
-    @Override
-    public boolean index(Index index) throws EngineException {
+    public boolean index(Index index) {
         final boolean created;
         try (ReleasableLock lock = readLock.acquire()) {
             ensureOpen();
@@ -440,40 +352,16 @@ public class InternalEngine extends Engine {
         return created;
     }
 
-    /**
-     * Forces a refresh if the versionMap is using too much RAM
-     */
-    private void checkVersionMapRefresh() {
-        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
-            try {
-                if (isClosed.get()) {
-                    // no point...
-                    return;
-                }
-                // Now refresh to clear versionMap:
-                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
-                    @Override
-                    public void run() {
-                        try {
-                            refresh("version_table_full");
-                        } catch (EngineClosedException ex) {
-                            // ignore
-                        }
-                    }
-                });
-            } catch (EsRejectedExecutionException ex) {
-                // that is fine too.. we might be shutting down
-            }
-        }
-    }
-
     private boolean innerIndex(Index index) throws IOException {
         synchronized (dirtyLock(index.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(index.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(index.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -481,19 +369,20 @@ public class InternalEngine extends Engine {
                 }
             }
 
-            long updatedVersion;
             long expectedVersion = index.version();
-            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (index.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (index.origin() == Operation.Origin.RECOVERY) {
                     return false;
                 } else {
-                    throw new VersionConflictEngineException(shardId, index.type(), index.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, index.type(), index.id(),
+                            index.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
-            updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
+            long updatedVersion = index.versionType().updateVersion(currentVersion, expectedVersion);
 
             final boolean created;
             index.updateVersion(updatedVersion);
+
             if (currentVersion == Versions.NOT_FOUND) {
                 // document does not exists, we can optimize for create
                 created = true;
@@ -518,11 +407,39 @@ public class InternalEngine extends Engine {
 
             versionMap.putUnderLock(index.uid().bytes(), new VersionValue(updatedVersion, translogLocation));
             index.setTranslogLocation(translogLocation);
+
             indexingService.postIndexUnderLock(index);
             return created;
         }
     }
 
+    /**
+     * Forces a refresh if the versionMap is using too much RAM
+     */
+    private void checkVersionMapRefresh() {
+        if (versionMap.ramBytesUsedForRefresh() > config().getVersionMapSize().bytes() && versionMapRefreshPending.getAndSet(true) == false) {
+            try {
+                if (isClosed.get()) {
+                    // no point...
+                    return;
+                }
+                // Now refresh to clear versionMap:
+                engineConfig.getThreadPool().executor(ThreadPool.Names.REFRESH).execute(new Runnable() {
+                    @Override
+                    public void run() {
+                        try {
+                            refresh("version_table_full");
+                        } catch (EngineClosedException ex) {
+                            // ignore
+                        }
+                    }
+                });
+            } catch (EsRejectedExecutionException ex) {
+                // that is fine too.. we might be shutting down
+            }
+        }
+    }
+
     @Override
     public void delete(Delete delete) throws EngineException {
         try (ReleasableLock lock = readLock.acquire()) {
@@ -549,10 +466,13 @@ public class InternalEngine extends Engine {
     private void innerDelete(Delete delete) throws IOException {
         synchronized (dirtyLock(delete.uid())) {
             final long currentVersion;
+            final boolean deleted;
             VersionValue versionValue = versionMap.getUnderLock(delete.uid().bytes());
             if (versionValue == null) {
                 currentVersion = loadCurrentVersionFromIndex(delete.uid());
+                deleted = currentVersion == Versions.NOT_FOUND;
             } else {
+                deleted = versionValue.delete();
                 if (engineConfig.isEnableGcDeletes() && versionValue.delete() && (engineConfig.getThreadPool().estimatedTimeInMillis() - versionValue.time()) > engineConfig.getGcDeletesInMillis()) {
                     currentVersion = Versions.NOT_FOUND; // deleted, and GC
                 } else {
@@ -562,11 +482,12 @@ public class InternalEngine extends Engine {
 
             long updatedVersion;
             long expectedVersion = delete.version();
-            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion)) {
+            if (delete.versionType().isVersionConflictForWrites(currentVersion, expectedVersion, deleted)) {
                 if (delete.origin() == Operation.Origin.RECOVERY) {
                     return;
                 } else {
-                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(), currentVersion, expectedVersion);
+                    throw new VersionConflictEngineException(shardId, delete.type(), delete.id(),
+                            delete.versionType().explainConflictForWrites(currentVersion, expectedVersion, deleted));
                 }
             }
             updatedVersion = delete.versionType().updateVersion(currentVersion, expectedVersion);
@@ -905,11 +826,6 @@ public class InternalEngine extends Engine {
     }
 
     @Override
-    public long indexWriterRAMBytesUsed() {
-        return indexWriter.ramBytesUsed();
-    }
-
-    @Override
     public List<Segment> segments(boolean verbose) {
         try (ReleasableLock lock = readLock.acquire()) {
             Segment[] segmentsArr = getSegmentInfo(lastCommittedSegmentInfos, verbose);
diff --git a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
index dcc0e7d..89ed81a 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
@@ -103,11 +103,6 @@ public class ShadowEngine extends Engine {
 
 
     @Override
-    public void create(Create create) throws EngineException {
-        throw new UnsupportedOperationException(shardId + " create operation not allowed on shadow engine");
-    }
-
-    @Override
     public boolean index(Index index) throws EngineException {
         throw new UnsupportedOperationException(shardId + " index operation not allowed on shadow engine");
     }
@@ -245,9 +240,4 @@ public class ShadowEngine extends Engine {
         return lastCommittedSegmentInfos;
     }
 
-    @Override
-    public long indexWriterRAMBytesUsed() {
-        // No IndexWriter
-        throw new UnsupportedOperationException("ShadowEngine has no IndexWriter");
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
index 8c2d352..9b038c6 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/VersionConflictEngineException.java
@@ -29,8 +29,16 @@ import java.io.IOException;
  */
 public class VersionConflictEngineException extends EngineException {
 
-    public VersionConflictEngineException(ShardId shardId, String type, String id, long current, long provided) {
-        super(shardId, "[" + type + "][" + id + "]: version conflict, current [" + current + "], provided [" + provided + "]");
+    public VersionConflictEngineException(ShardId shardId, String type, String id, String explanation) {
+        this(shardId, null, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, Throwable cause, String type, String id, String explanation) {
+        this(shardId, "[{}][{}]: version conflict, {}", cause, type, id, explanation);
+    }
+
+    public VersionConflictEngineException(ShardId shardId, String msg, Throwable cause, Object... params) {
+        super(shardId, msg, cause, params);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
index 858453f..651bc40 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java
@@ -28,39 +28,8 @@ public abstract class IndexingOperationListener {
     /**
      * Called before the indexing occurs.
      */
-    public Engine.Create preCreate(Engine.Create create) {
-        return create;
-    }
-
-    /**
-     * Called after the indexing occurs, under a locking scheme to maintain
-     * concurrent updates to the same doc.
-     * <p>
-     * Note, long operations should not occur under this callback.
-     */
-    public void postCreateUnderLock(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred.
-     */
-    public void postCreate(Engine.Create create) {
-
-    }
-
-    /**
-     * Called after create index operation occurred with exception.
-     */
-    public void postCreate(Engine.Create create, Throwable ex) {
-
-    }
-
-    /**
-     * Called before the indexing occurs.
-     */
-    public Engine.Index preIndex(Engine.Index index) {
-        return index;
+    public Engine.Index preIndex(Engine.Index operation) {
+        return operation;
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
index ea45db2..292c2a1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java
@@ -128,10 +128,6 @@ public final class IndexingSlowLog {
         postIndexing(index.parsedDoc(), tookInNanos);
     }
 
-    void postCreate(Engine.Create create, long tookInNanos) {
-        postIndexing(create.parsedDoc(), tookInNanos);
-    }
-
     /**
      * Reads how much of the source to log. The user can specify any value they
      * like and numbers are interpreted the maximum number of characters to log
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
index 4f0ea77..d1abbf1 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
@@ -85,25 +85,6 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         listeners.remove(listener);
     }
 
-    public Engine.Create preCreate(Engine.Create create) {
-        totalStats.indexCurrent.inc();
-        typeStats(create.type()).indexCurrent.inc();
-        for (IndexingOperationListener listener : listeners) {
-            create = listener.preCreate(create);
-        }
-        return create;
-    }
-
-    public void postCreateUnderLock(Engine.Create create) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreateUnderLock(create);
-            } catch (Exception e) {
-                logger.warn("postCreateUnderLock listener [{}] failed", e, listener);
-            }
-        }
-    }
-
     public void throttlingActivated() {
         totalStats.setThrottled(true);
     }
@@ -112,40 +93,13 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
         totalStats.setThrottled(false);
     }
 
-    public void postCreate(Engine.Create create) {
-        long took = create.endTime() - create.startTime();
-        totalStats.indexMetric.inc(took);
-        totalStats.indexCurrent.dec();
-        StatsHolder typeStats = typeStats(create.type());
-        typeStats.indexMetric.inc(took);
-        typeStats.indexCurrent.dec();
-        slowLog.postCreate(create, took);
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create);
-            } catch (Exception e) {
-                logger.warn("postCreate listener [{}] failed", e, listener);
-            }
-        }
-    }
-
-    public void postCreate(Engine.Create create, Throwable ex) {
-        for (IndexingOperationListener listener : listeners) {
-            try {
-                listener.postCreate(create, ex);
-            } catch (Throwable t) {
-                logger.warn("postCreate listener [{}] failed", t, listener);
-            }
-        }
-    }
-
-    public Engine.Index preIndex(Engine.Index index) {
+    public Engine.Index preIndex(Engine.Index operation) {
         totalStats.indexCurrent.inc();
-        typeStats(index.type()).indexCurrent.inc();
+        typeStats(operation.type()).indexCurrent.inc();
         for (IndexingOperationListener listener : listeners) {
-            index = listener.preIndex(index);
+            operation = listener.preIndex(operation);
         }
-        return index;
+        return operation;
     }
 
     public void postIndexUnderLock(Engine.Index index) {
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index d811f1f..1f8a4c6 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -242,29 +242,12 @@ public final class PercolatorQueriesRegistry extends AbstractIndexShardComponent
     private class RealTimePercolatorOperationListener extends IndexingOperationListener {
 
         @Override
-        public Engine.Create preCreate(Engine.Create create) {
+        public Engine.Index preIndex(Engine.Index operation) {
             // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                parsePercolatorDocument(create.id(), create.source());
+            if (PercolatorService.TYPE_NAME.equals(operation.type())) {
+                parsePercolatorDocument(operation.id(), operation.source());
             }
-            return create;
-        }
-
-        @Override
-        public void postCreateUnderLock(Engine.Create create) {
-            // add the query under a doc lock
-            if (PercolatorService.TYPE_NAME.equals(create.type())) {
-                addPercolateQuery(create.id(), create.source());
-            }
-        }
-
-        @Override
-        public Engine.Index preIndex(Engine.Index index) {
-            // validate the query here, before we index
-            if (PercolatorService.TYPE_NAME.equals(index.type())) {
-                parsePercolatorDocument(index.id(), index.source());
-            }
-            return index;
+            return operation;
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index 9a224fa..a265427 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -202,7 +202,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
     }
     
     /** Returns validation method for coordinates. */
-    public GeoValidationMethod getValidationMethod(GeoValidationMethod method) {
+    public GeoValidationMethod getValidationMethod() {
         return this.validationMethod;
     }
 
@@ -221,6 +221,7 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
             }
         }
 
+        GeoPoint point = new GeoPoint(this.point);
         if (GeoValidationMethod.isCoerce(validationMethod)) {
             GeoUtils.normalizePoint(point, true, true);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 31bc889..72c226f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -70,7 +70,7 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     // and Equals so ShapeBuilder can be used here
     private BytesReference shapeBytes;
 
-    private SpatialStrategy strategy = null;
+    private SpatialStrategy strategy;
 
     private final String indexedShapeId;
     private final String indexedShapeType;
@@ -429,7 +429,9 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
             }
         }
         builder.relation = ShapeRelation.DISJOINT.readFrom(in);
-        builder.strategy = SpatialStrategy.RECURSIVE.readFrom(in);
+        if (in.readBoolean()) {
+            builder.strategy = SpatialStrategy.RECURSIVE.readFrom(in);
+        }
         return builder;
     }
 
@@ -447,7 +449,12 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
             out.writeOptionalString(indexedShapePath);
         }
         relation.writeTo(out);
-        strategy.writeTo(out);
+        if (strategy == null) {
+            out.writeBoolean(false);
+        } else {
+            out.writeBoolean(true);
+            strategy.writeTo(out);
+        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
index 3c81393..e133abd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/exp/ExponentialDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class ExponentialDecayFunctionBuilder extends DecayFunctionBuilder<ExponentialDecayFunctionBuilder> {
 
-    private static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
+    public static final DecayFunction EXP_DECAY_FUNCTION = new ExponentialDecayScoreFunction();
 
     public ExponentialDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
index 621b22a..618503a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/gauss/GaussDecayFunctionBuilder.java
@@ -27,7 +27,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class GaussDecayFunctionBuilder extends DecayFunctionBuilder<GaussDecayFunctionBuilder> {
 
-    private static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
+    public static final DecayFunction GAUSS_DECAY_FUNCTION = new GaussScoreFunction();
 
     public GaussDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
index 2e63aed..f321ee1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/lin/LinearDecayFunctionBuilder.java
@@ -26,7 +26,7 @@ import org.elasticsearch.index.query.functionscore.DecayFunctionBuilder;
 
 public class LinearDecayFunctionBuilder extends DecayFunctionBuilder<LinearDecayFunctionBuilder> {
 
-    private static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
+    public static final DecayFunction LINEAR_DECAY_FUNCTION = new LinearDecayScoreFunction();
 
     public LinearDecayFunctionBuilder(String fieldName, Object origin, Object scale, Object offset) {
         super(fieldName, origin, scale, offset);
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index e274636..4db52b6 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -43,7 +43,6 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.metrics.MeanMetric;
 import org.elasticsearch.common.settings.Settings;
@@ -85,8 +84,8 @@ import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
-import org.elasticsearch.index.store.Store.MetadataSnapshot;
 import org.elasticsearch.index.store.Store;
+import org.elasticsearch.index.store.Store.MetadataSnapshot;
 import org.elasticsearch.index.store.StoreFileMetaData;
 import org.elasticsearch.index.store.StoreStats;
 import org.elasticsearch.index.suggest.stats.ShardSuggestMetric;
@@ -101,7 +100,6 @@ import org.elasticsearch.index.warmer.WarmerStats;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
 import org.elasticsearch.indices.cache.query.IndicesQueryCache;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.indices.recovery.RecoveryFailedException;
 import org.elasticsearch.indices.recovery.RecoveryState;
 import org.elasticsearch.percolator.PercolatorService;
@@ -120,7 +118,6 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
-
 public class IndexShard extends AbstractIndexShardComponent implements IndexSettingsService.Listener {
 
     private final ThreadPool threadPool;
@@ -193,13 +190,6 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
 
     private final IndexSearcherWrapper searcherWrapper;
 
-    /** True if this shard is still indexing (recently) and false if we've been idle for long enough (as periodically checked by {@link
-     *  IndexingMemoryController}). */
-    private final AtomicBoolean active = new AtomicBoolean();
-
-    private volatile long lastWriteNS;
-    private final IndexingMemoryController indexingMemoryController;
-
     @Inject
     public IndexShard(ShardId shardId, @IndexSettings Settings indexSettings, ShardPath path, Store store, IndexServicesProvider provider) {
         super(shardId, indexSettings);
@@ -252,16 +242,11 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.flushThresholdSize = indexSettings.getAsBytesSize(INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, new ByteSizeValue(512, ByteSizeUnit.MB));
         this.disableFlush = indexSettings.getAsBoolean(INDEX_TRANSLOG_DISABLE_FLUSH, false);
         this.indexShardOperationCounter = new IndexShardOperationCounter(logger, shardId);
-        this.indexingMemoryController = provider.getIndexingMemoryController();
-
         this.searcherWrapper = provider.getIndexSearcherWrapper();
         this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, queryParserService, indexingService, mapperService, indexFieldDataService);
         if (mapperService.hasMapping(PercolatorService.TYPE_NAME)) {
             percolatorQueriesRegistry.enableRealTimePercolator();
         }
-
-        // We start up inactive
-        active.set(false);
     }
 
     public Store store() {
@@ -293,7 +278,9 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return indexFieldDataService;
     }
 
-    public MapperService mapperService() { return mapperService;}
+    public MapperService mapperService() {
+        return mapperService;
+    }
 
     public ShardSearchStats searchService() {
         return this.searchService;
@@ -438,41 +425,6 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         return previousState;
     }
 
-    public Engine.Create prepareCreate(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        try {
-            return prepareCreate(docMapper(source.type()), source, version, versionType, origin);
-        } catch (Throwable t) {
-            verifyNotClosed(t);
-            throw t;
-        }
-    }
-
-    static Engine.Create prepareCreate(DocumentMapperForType docMapper, SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
-        long startTime = System.nanoTime();
-        ParsedDocument doc = docMapper.getDocumentMapper().parse(source);
-        if (docMapper.getMapping() != null) {
-            doc.addDynamicMappingsUpdate(docMapper.getMapping());
-        }
-        return new Engine.Create(docMapper.getDocumentMapper().uidMapper().term(doc.uid().stringValue()), doc, version, versionType, origin, startTime);
-    }
-
-    public void create(Engine.Create create) {
-        ensureWriteAllowed(create);
-        markLastWrite(create);
-        create = indexingService.preCreate(create);
-        try {
-            if (logger.isTraceEnabled()) {
-                logger.trace("index [{}][{}]{}", create.type(), create.id(), create.docs());
-            }
-            getEngine().create(create);
-            create.endTime(System.nanoTime());
-        } catch (Throwable ex) {
-            indexingService.postCreate(create, ex);
-            throw ex;
-        }
-        indexingService.postCreate(create);
-    }
-
     public Engine.Index prepareIndex(SourceToParse source, long version, VersionType versionType, Engine.Operation.Origin origin) {
         try {
             return prepareIndex(docMapper(source.type()), source, version, versionType, origin);
@@ -496,8 +448,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
      * updated.
      */
     public boolean index(Engine.Index index) {
-        ensureWriteAllowed(index);
-        markLastWrite(index);
+        writeAllowed(index.origin());
         index = indexingService.preIndex(index);
         final boolean created;
         try {
@@ -521,8 +472,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     }
 
     public void delete(Engine.Delete delete) {
-        ensureWriteAllowed(delete);
-        markLastWrite(delete);
+        writeAllowed(delete.origin());
         delete = indexingService.preDelete(delete);
         try {
             if (logger.isTraceEnabled()) {
@@ -932,24 +882,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         }
     }
 
-    /** Returns timestamp of last indexing operation */
-    public long getLastWriteNS() {
-        return lastWriteNS;
-    }
-
-    /** Records timestamp of the last write operation, possibly switching {@code active} to true if we were inactive. */
-    private void markLastWrite(Engine.Operation op) {
-        lastWriteNS = op.startTime();
-        if (active.getAndSet(true) == false) {
-            // We are currently inactive, but a new write operation just showed up, so we now notify IMC
-            // to wake up and fix our indexing buffer.  We could do this async instead, but cost should
-            // be low, and it's rare this happens.
-            indexingMemoryController.forceCheck();
-        }
-    }
-
-    private void ensureWriteAllowed(Engine.Operation op) throws IllegalIndexShardStateException {
-        Engine.Operation.Origin origin = op.origin();
+    private void writeAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException {
         IndexShardState state = this.state; // one time volatile read
 
         if (origin == Engine.Operation.Origin.PRIMARY) {
@@ -1011,8 +944,6 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.failedEngineListener.delegates.add(failedEngineListener);
     }
 
-    /** Change the indexing and translog buffer sizes.  If {@code IndexWriter} is currently using more than
-     *  the new buffering indexing size then we do a refresh to free up the heap. */
     public void updateBufferSize(ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
 
         final EngineConfig config = engineConfig;
@@ -1031,50 +962,27 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
             // so we push changes these changes down to IndexWriter:
             engine.onSettingsChanged();
 
-            long iwBytesUsed = engine.indexWriterRAMBytesUsed();
-
-            String message = LoggerMessageFormat.format("updating index_buffer_size from [{}] to [{}]; IndexWriter now using [{}] bytes",
-                                                        preValue, shardIndexingBufferSize, iwBytesUsed);
-
-            if (iwBytesUsed > shardIndexingBufferSize.bytes()) {
-                // our allowed buffer was changed to less than we are currently using; we ask IW to refresh
-                // so it clears its buffers (otherwise it won't clear until the next indexing/delete op)
-                logger.debug(message + "; now refresh to clear IndexWriter memory");
-
-                // TODO: should IW have an API to move segments to disk, but not refresh?  Its flush method is protected...
+            if (shardIndexingBufferSize == EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER) {
+                // it's inactive: make sure we do a refresh / full IW flush in this case, since the memory
+                // changes only after a "data" change has happened to the writer
+                // the index writer lazily allocates memory and a refresh will clean it all up.
+                logger.debug("updating index_buffer_size from [{}] to (inactive) [{}]", preValue, shardIndexingBufferSize);
                 try {
                     refresh("update index buffer");
                 } catch (Throwable e) {
-                    logger.warn("failed to refresh after decreasing index buffer", e);
+                    logger.warn("failed to refresh after setting shard to inactive", e);
                 }
             } else {
-                logger.debug(message);
+                logger.debug("updating index_buffer_size from [{}] to [{}]", preValue, shardIndexingBufferSize);
             }
         }
 
         engine.getTranslog().updateBuffer(shardTranslogBufferSize);
     }
 
-    /** Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last
-     *  indexing operation, and become inactive (reducing indexing and translog buffers to tiny values) if so.  This returns true
-     *  if the shard is inactive. */
-    public boolean checkIdle(long inactiveTimeNS) {
-        if (System.nanoTime() - lastWriteNS >= inactiveTimeNS) {
-            boolean wasActive = active.getAndSet(false);
-            if (wasActive) {
-                updateBufferSize(IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
-                logger.debug("shard is now inactive");
-                indicesLifecycle.onShardInactive(this);
-            }
-        }
-
-        return active.get() == false;
-    }
-
-    /** Returns {@code true} if this shard is active (has seen indexing ops in the last {@link
-     *  IndexingMemoryController#SHARD_INACTIVE_TIME_SETTING} (default 5 minutes), else {@code false}. */
-    public boolean getActive() {
-        return active.get();
+    public void markAsInactive() {
+        updateBufferSize(EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER, TranslogConfig.INACTIVE_SHARD_TRANSLOG_BUFFER);
+        indicesLifecycle.onShardInactive(this);
     }
 
     public final boolean isFlushOnClose() {
@@ -1559,6 +1467,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     /**
      * Schedules a flush if needed but won't schedule more than one flush concurrently. The flush will be executed on the
      * Flush thread-pool asynchronously.
+     *
      * @return <code>true</code> if a new flush is scheduled otherwise <code>false</code>.
      */
     public boolean maybeFlush() {
diff --git a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
index d3e8f97..c81b9e5 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.index.shard;
 
-import java.io.IOException;
-
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexServicesProvider;
@@ -29,6 +27,8 @@ import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.store.Store;
 
+import java.io.IOException;
+
 /**
  * ShadowIndexShard extends {@link IndexShard} to add file synchronization
  * from the primary when a flush happens. It also ensures that a replica being
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index f893ec4..23551df 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -145,19 +145,7 @@ public class TranslogRecoveryPerformer {
     public void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates) {
         try {
             switch (operation.opType()) {
-                case CREATE:
-                    Translog.Create create = (Translog.Create) operation;
-                    Engine.Create engineCreate = IndexShard.prepareCreate(docMapper(create.type()),
-                            source(create.source()).index(shardId.getIndex()).type(create.type()).id(create.id())
-                                    .routing(create.routing()).parent(create.parent()).timestamp(create.timestamp()).ttl(create.ttl()),
-                            create.version(), create.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY);
-                    maybeAddMappingUpdate(engineCreate.type(), engineCreate.parsedDoc().dynamicMappingsUpdate(), engineCreate.id(), allowMappingUpdates);
-                    if (logger.isTraceEnabled()) {
-                        logger.trace("[translog] recover [create] op of [{}][{}]", create.type(), create.id());
-                    }
-                    engine.create(engineCreate);
-                    break;
-                case SAVE:
+                case INDEX:
                     Translog.Index index = (Translog.Index) operation;
                     Engine.Index engineIndex = IndexShard.prepareIndex(docMapper(index.type()), source(index.source()).type(index.type()).id(index.id())
                                     .routing(index.routing()).parent(index.parent()).timestamp(index.timestamp()).ttl(index.ttl()),
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 5084895..a8f2801 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -465,11 +465,10 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     }
 
     /**
-     * Adds a created / delete / index operations to the transaction log.
+     * Adds a delete / index operations to the transaction log.
      *
      * @see org.elasticsearch.index.translog.Translog.Operation
-     * @see org.elasticsearch.index.translog.Translog.Create
-     * @see org.elasticsearch.index.translog.Translog.Index
+     * @see Index
      * @see org.elasticsearch.index.translog.Translog.Delete
      */
     public Location add(Operation operation) throws TranslogException {
@@ -874,8 +873,9 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
      */
     public interface Operation extends Streamable {
         enum Type {
+            @Deprecated
             CREATE((byte) 1),
-            SAVE((byte) 2),
+            INDEX((byte) 2),
             DELETE((byte) 3),
             DELETE_BY_QUERY((byte) 4);
 
@@ -894,7 +894,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                     case 1:
                         return CREATE;
                     case 2:
-                        return SAVE;
+                        return INDEX;
                     case 3:
                         return DELETE;
                     case 4:
@@ -929,199 +929,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    public static class Create implements Operation {
-        public static final int SERIALIZATION_FORMAT = 6;
-
-        private String id;
-        private String type;
-        private BytesReference source;
-        private String routing;
-        private String parent;
-        private long timestamp;
-        private long ttl;
-        private long version = Versions.MATCH_ANY;
-        private VersionType versionType = VersionType.INTERNAL;
-
-        public Create() {
-        }
-
-        public Create(Engine.Create create) {
-            this.id = create.id();
-            this.type = create.type();
-            this.source = create.source();
-            this.routing = create.routing();
-            this.parent = create.parent();
-            this.timestamp = create.timestamp();
-            this.ttl = create.ttl();
-            this.version = create.version();
-            this.versionType = create.versionType();
-        }
-
-        public Create(String type, String id, byte[] source) {
-            this.id = id;
-            this.type = type;
-            this.source = new BytesArray(source);
-        }
-
-        @Override
-        public Type opType() {
-            return Type.CREATE;
-        }
-
-        @Override
-        public long estimateSize() {
-            return ((id.length() + type.length()) * 2) + source.length() + 12;
-        }
-
-        public String id() {
-            return this.id;
-        }
-
-        public BytesReference source() {
-            return this.source;
-        }
-
-        public String type() {
-            return this.type;
-        }
-
-        public String routing() {
-            return this.routing;
-        }
-
-        public String parent() {
-            return this.parent;
-        }
-
-        public long timestamp() {
-            return this.timestamp;
-        }
-
-        public long ttl() {
-            return this.ttl;
-        }
-
-        public long version() {
-            return this.version;
-        }
-
-        public VersionType versionType() {
-            return versionType;
-        }
-
-        @Override
-        public Source getSource() {
-            return new Source(source, routing, parent, timestamp, ttl);
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            int version = in.readVInt(); // version
-            id = in.readString();
-            type = in.readString();
-            source = in.readBytesReference();
-            if (version >= 1) {
-                if (in.readBoolean()) {
-                    routing = in.readString();
-                }
-            }
-            if (version >= 2) {
-                if (in.readBoolean()) {
-                    parent = in.readString();
-                }
-            }
-            if (version >= 3) {
-                this.version = in.readLong();
-            }
-            if (version >= 4) {
-                this.timestamp = in.readLong();
-            }
-            if (version >= 5) {
-                this.ttl = in.readLong();
-            }
-            if (version >= 6) {
-                this.versionType = VersionType.fromValue(in.readByte());
-            }
-
-            assert versionType.validateVersionForWrites(version);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(SERIALIZATION_FORMAT);
-            out.writeString(id);
-            out.writeString(type);
-            out.writeBytesReference(source);
-            if (routing == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(routing);
-            }
-            if (parent == null) {
-                out.writeBoolean(false);
-            } else {
-                out.writeBoolean(true);
-                out.writeString(parent);
-            }
-            out.writeLong(version);
-            out.writeLong(timestamp);
-            out.writeLong(ttl);
-            out.writeByte(versionType.getValue());
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-
-            Create create = (Create) o;
-
-            if (timestamp != create.timestamp ||
-                    ttl != create.ttl ||
-                    version != create.version ||
-                    id.equals(create.id) == false ||
-                    type.equals(create.type) == false ||
-                    source.equals(create.source) == false) {
-                return false;
-            }
-            if (routing != null ? !routing.equals(create.routing) : create.routing != null) {
-                return false;
-            }
-            if (parent != null ? !parent.equals(create.parent) : create.parent != null) {
-                return false;
-            }
-            return versionType == create.versionType;
-
-        }
-
-        @Override
-        public int hashCode() {
-            int result = id.hashCode();
-            result = 31 * result + type.hashCode();
-            result = 31 * result + source.hashCode();
-            result = 31 * result + (routing != null ? routing.hashCode() : 0);
-            result = 31 * result + (parent != null ? parent.hashCode() : 0);
-            result = 31 * result + (int) (timestamp ^ (timestamp >>> 32));
-            result = 31 * result + (int) (ttl ^ (ttl >>> 32));
-            result = 31 * result + (int) (version ^ (version >>> 32));
-            result = 31 * result + versionType.hashCode();
-            return result;
-        }
-
-        @Override
-        public String toString() {
-            return "Create{" +
-                    "id='" + id + '\'' +
-                    ", type='" + type + '\'' +
-                    '}';
-        }
-    }
-
     public static class Index implements Operation {
         public static final int SERIALIZATION_FORMAT = 6;
 
@@ -1158,7 +965,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
 
         @Override
         public Type opType() {
-            return Type.SAVE;
+            return Type.INDEX;
         }
 
         @Override
@@ -1667,13 +1474,14 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     static Translog.Operation newOperationFromType(Translog.Operation.Type type) throws IOException {
         switch (type) {
             case CREATE:
-                return new Translog.Create();
+                // the deserialization logic in Index was identical to that of Create when create was deprecated
+                return new Index();
             case DELETE:
                 return new Translog.Delete();
             case DELETE_BY_QUERY:
                 return new Translog.DeleteByQuery();
-            case SAVE:
-                return new Translog.Index();
+            case INDEX:
+                return new Index();
             default:
                 throw new IOException("No type for [" + type + "]");
         }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
index 30ab814..4d74961 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogConfig.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.translog.Translog.TranslogGeneration;
-import org.elasticsearch.indices.memory.IndexingMemoryController;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.nio.file.Path;
@@ -43,6 +42,7 @@ public final class TranslogConfig {
     public static final String INDEX_TRANSLOG_FS_TYPE = "index.translog.fs.type";
     public static final String INDEX_TRANSLOG_BUFFER_SIZE = "index.translog.fs.buffer_size";
     public static final String INDEX_TRANSLOG_SYNC_INTERVAL = "index.translog.sync_interval";
+    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
 
     private final TimeValue syncInterval;
     private final BigArrays bigArrays;
@@ -73,7 +73,7 @@ public final class TranslogConfig {
         this.threadPool = threadPool;
         this.bigArrays = bigArrays;
         this.type = TranslogWriter.Type.fromString(indexSettings.get(INDEX_TRANSLOG_FS_TYPE, TranslogWriter.Type.BUFFERED.name()));
-        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER).bytes(); // Not really interesting, updated by IndexingMemoryController...
+        this.bufferSize = (int) indexSettings.getAsBytesSize(INDEX_TRANSLOG_BUFFER_SIZE, ByteSizeValue.parseBytesSizeValue("64k", INDEX_TRANSLOG_BUFFER_SIZE)).bytes(); // Not really interesting, updated by IndexingMemoryController...
 
         syncInterval = indexSettings.getAsTime(INDEX_TRANSLOG_SYNC_INTERVAL, TimeValue.timeValueSeconds(5));
         if (syncInterval.millis() > 0 && threadPool != null) {
diff --git a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
index 90bb4c4..a84fff3 100644
--- a/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
+++ b/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java
@@ -29,10 +29,12 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.EngineClosedException;
+import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.translog.Translog;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -40,6 +42,9 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.util.*;
 import java.util.concurrent.ScheduledFuture;
 
+/**
+ *
+ */
 public class IndexingMemoryController extends AbstractLifecycleComponent<IndexingMemoryController> {
 
     /** How much heap (% or bytes) we will share across all actively indexing shards on this node (default: 10%). */
@@ -78,12 +83,6 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
     /** How frequently we check shards to find inactive ones (default: 30 seconds). */
     public static final String SHARD_INACTIVE_INTERVAL_TIME_SETTING = "indices.memory.interval";
 
-    /** Once a shard becomes inactive, we reduce the {@code IndexWriter} buffer to this value (500 KB) to let active shards use the heap instead. */
-    public static final ByteSizeValue INACTIVE_SHARD_INDEXING_BUFFER = ByteSizeValue.parseBytesSizeValue("500kb", "INACTIVE_SHARD_INDEXING_BUFFER");
-
-    /** Once a shard becomes inactive, we reduce the {@code Translog} buffer to this value (1 KB) to let active shards use the heap instead. */
-    public static final ByteSizeValue INACTIVE_SHARD_TRANSLOG_BUFFER = ByteSizeValue.parseBytesSizeValue("1kb", "INACTIVE_SHARD_TRANSLOG_BUFFER");
-
     private final ThreadPool threadPool;
     private final IndicesService indicesService;
 
@@ -165,6 +164,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
         this.statusChecker = new ShardsIndicesStatusChecker();
 
+
         logger.debug("using indexing buffer size [{}], with {} [{}], {} [{}], {} [{}], {} [{}]",
                 this.indexingBuffer,
                 MIN_SHARD_INDEX_BUFFER_SIZE_SETTING, this.minShardIndexBufferSize,
@@ -175,7 +175,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
 
     @Override
     protected void doStart() {
-        // it's fine to run it on the scheduler thread, no busy work
+        // its fine to run it on the scheduler thread, no busy work
         this.scheduler = threadPool.scheduleWithFixedDelay(statusChecker, interval);
     }
 
@@ -240,7 +240,6 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return null;
     }
 
-    /** set new indexing and translog buffers on this shard.  this may cause the shard to refresh to free up heap. */
     protected void updateShardBuffers(ShardId shardId, ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
@@ -256,86 +255,105 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         }
     }
 
-    /** returns {@link IndexShard#getActive} if the shard exists, else null */
-    protected Boolean getShardActive(ShardId shardId) {
+
+    /** returns the current translog status (generation id + ops) for the given shard id. Returns null if unavailable. */
+    protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
         final IndexShard indexShard = getShard(shardId);
         if (indexShard == null) {
             return null;
         }
-        return indexShard.getActive();
+        final Translog translog;
+        try {
+            translog = indexShard.getTranslog();
+        } catch (EngineClosedException e) {
+            // not ready yet to be checked for activity
+            return null;
+        }
+
+        ShardIndexingStatus status = new ShardIndexingStatus();
+        status.translogId = translog.currentFileGeneration();
+        status.translogNumberOfOperations = translog.totalOperations();
+        return status;
     }
 
-    /** check if any shards active status changed, now. */
-    public void forceCheck() {
+    // used for tests
+    void forceCheck() {
         statusChecker.run();
     }
 
     class ShardsIndicesStatusChecker implements Runnable {
 
-        // True if the shard was active last time we checked
-        private final Map<ShardId,Boolean> shardWasActive = new HashMap<>();
+        private final Map<ShardId, ShardIndexingStatus> shardsIndicesStatus = new HashMap<>();
 
         @Override
-        public synchronized void run() {
+        public void run() {
             EnumSet<ShardStatusChangeType> changes = purgeDeletedAndClosedShards();
 
-            updateShardStatuses(changes);
+            final List<ShardId> activeToInactiveIndexingShards = new ArrayList<>();
+            final int activeShards = updateShardStatuses(changes, activeToInactiveIndexingShards);
+            for (ShardId indexShard : activeToInactiveIndexingShards) {
+                markShardAsInactive(indexShard);
+            }
 
             if (changes.isEmpty() == false) {
                 // Something changed: recompute indexing buffers:
-                calcAndSetShardBuffers("[" + changes + "]");
+                calcAndSetShardBuffers(activeShards, "[" + changes + "]");
             }
         }
 
         /**
-         * goes through all existing shards and check whether there are changes in their active status
+         * goes through all existing shards and check whether the changes their active status
+         *
+         * @return the current count of active shards
          */
-        private void updateShardStatuses(EnumSet<ShardStatusChangeType> changes) {
+        private int updateShardStatuses(EnumSet<ShardStatusChangeType> changes, List<ShardId> activeToInactiveIndexingShards) {
+            int activeShards = 0;
             for (ShardId shardId : availableShards()) {
 
-                // Is the shard active now?
-                Boolean isActive = getShardActive(shardId);
+                final ShardIndexingStatus currentStatus = getTranslogStatus(shardId);
 
-                if (isActive == null) {
+                if (currentStatus == null) {
                     // shard was closed..
                     continue;
                 }
 
-                // Was the shard active last time we checked?
-                Boolean wasActive = shardWasActive.get(shardId);
-
-                if (wasActive == null) {
-                    // First time we are seeing this shard
-                    shardWasActive.put(shardId, isActive);
+                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
+                if (status == null) {
+                    status = currentStatus;
+                    shardsIndicesStatus.put(shardId, status);
                     changes.add(ShardStatusChangeType.ADDED);
-                } else if (isActive) {
-                    // Shard is active now
-                    if (wasActive == false) {
-                        // Shard became active itself, since we last checked (due to new indexing op arriving)
+                } else {
+                    final boolean lastActiveIndexing = status.activeIndexing;
+                    status.updateWith(currentTimeInNanos(), currentStatus, inactiveTime.nanos());
+                    if (lastActiveIndexing && (status.activeIndexing == false)) {
+                        activeToInactiveIndexingShards.add(shardId);
+                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
+                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise, setting size to [{}]",
+                                shardId,
+                                inactiveTime, EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER);
+                    } else if ((lastActiveIndexing == false) && status.activeIndexing) {
                         changes.add(ShardStatusChangeType.BECAME_ACTIVE);
                         logger.debug("marking shard {} as active indexing wise", shardId);
-                        shardWasActive.put(shardId, true);
-                    } else if (checkIdle(shardId, inactiveTime.nanos()) == Boolean.TRUE) {
-                        // Make shard inactive now
-                        changes.add(ShardStatusChangeType.BECAME_INACTIVE);
-                        logger.debug("marking shard {} as inactive (inactive_time[{}]) indexing wise",
-                                     shardId,
-                                     inactiveTime);
-                        shardWasActive.put(shardId, false);
                     }
                 }
+
+                if (status.activeIndexing) {
+                    activeShards++;
+                }
             }
+
+            return activeShards;
         }
 
         /**
          * purge any existing statuses that are no longer updated
          *
-         * @return the changes applied
+         * @return true if any change
          */
         private EnumSet<ShardStatusChangeType> purgeDeletedAndClosedShards() {
             EnumSet<ShardStatusChangeType> changes = EnumSet.noneOf(ShardStatusChangeType.class);
 
-            Iterator<ShardId> statusShardIdIterator = shardWasActive.keySet().iterator();
+            Iterator<ShardId> statusShardIdIterator = shardsIndicesStatus.keySet().iterator();
             while (statusShardIdIterator.hasNext()) {
                 ShardId shardId = statusShardIdIterator.next();
                 if (shardAvailable(shardId) == false) {
@@ -346,25 +364,12 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
             return changes;
         }
 
-        private void calcAndSetShardBuffers(String reason) {
-
-            // Count how many shards are now active:
-            int activeShardCount = 0;
-            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
-                if (ent.getValue()) {
-                    activeShardCount++;
-                }
-            }
-
-            // TODO: we could be smarter here by taking into account how RAM the IndexWriter on each shard
-            // is actually using (using IW.ramBytesUsed), so that small indices (e.g. Marvel) would not
-            // get the same indexing buffer as large indices.  But it quickly gets tricky...
-            if (activeShardCount == 0) {
+        private void calcAndSetShardBuffers(int activeShards, String reason) {
+            if (activeShards == 0) {
                 logger.debug("no active shards (reason={})", reason);
                 return;
             }
-
-            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShardCount);
+            ByteSizeValue shardIndexingBufferSize = new ByteSizeValue(indexingBuffer.bytes() / activeShards);
             if (shardIndexingBufferSize.bytes() < minShardIndexBufferSize.bytes()) {
                 shardIndexingBufferSize = minShardIndexBufferSize;
             }
@@ -372,7 +377,7 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardIndexingBufferSize = maxShardIndexBufferSize;
             }
 
-            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShardCount);
+            ByteSizeValue shardTranslogBufferSize = new ByteSizeValue(translogBuffer.bytes() / activeShards);
             if (shardTranslogBufferSize.bytes() < minShardTranslogBufferSize.bytes()) {
                 shardTranslogBufferSize = minShardTranslogBufferSize;
             }
@@ -380,12 +385,11 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
                 shardTranslogBufferSize = maxShardTranslogBufferSize;
             }
 
-            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShardCount, shardIndexingBufferSize, shardTranslogBufferSize);
-
-            for (Map.Entry<ShardId,Boolean> ent : shardWasActive.entrySet()) {
-                if (ent.getValue()) {
-                    // This shard is active
-                    updateShardBuffers(ent.getKey(), shardIndexingBufferSize, shardTranslogBufferSize);
+            logger.debug("recalculating shard indexing buffer (reason={}), total is [{}] with [{}] active shards, each shard set to indexing=[{}], translog=[{}]", reason, indexingBuffer, activeShards, shardIndexingBufferSize, shardTranslogBufferSize);
+            for (ShardId shardId : availableShards()) {
+                ShardIndexingStatus status = shardsIndicesStatus.get(shardId);
+                if (status == null || status.activeIndexing) {
+                    updateShardBuffers(shardId, shardIndexingBufferSize, shardTranslogBufferSize);
                 }
             }
         }
@@ -395,14 +399,13 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         return System.nanoTime();
     }
 
-    /** ask this shard to check now whether it is inactive, and reduces its indexing and translog buffers if so.  returns Boolean.TRUE if
-     *  it did deactive, Boolean.FALSE if it did not, and null if the shard is unknown */
-    protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
+    // update inactive indexing buffer size
+    protected void markShardAsInactive(ShardId shardId) {
         String ignoreReason = null;
         final IndexShard shard = getShard(shardId);
         if (shard != null) {
             try {
-                return shard.checkIdle(inactiveTimeNS);
+                shard.markAsInactive();
             } catch (EngineClosedException e) {
                 // ignore
                 ignoreReason = "EngineClosedException";
@@ -416,10 +419,47 @@ public class IndexingMemoryController extends AbstractLifecycleComponent<Indexin
         if (ignoreReason != null) {
             logger.trace("ignore [{}] while marking shard {} as inactive", ignoreReason, shardId);
         }
-        return null;
     }
 
     private static enum ShardStatusChangeType {
         ADDED, DELETED, BECAME_ACTIVE, BECAME_INACTIVE
     }
+
+    static class ShardIndexingStatus {
+        long translogId = -1;
+        long translogNumberOfOperations = -1;
+        boolean activeIndexing = true;
+        long idleSinceNanoTime = -1; // contains the first time we saw this shard with no operations done on it
+
+
+        /** update status based on a new sample. updates all internal variables */
+        public void updateWith(long currentNanoTime, ShardIndexingStatus current, long inactiveNanoInterval) {
+            final boolean idle = (translogId == current.translogId && translogNumberOfOperations == current.translogNumberOfOperations);
+            if (activeIndexing && idle) {
+                // no indexing activity detected.
+                if (idleSinceNanoTime < 0) {
+                    // first time we see this, start the clock.
+                    idleSinceNanoTime = currentNanoTime;
+                } else if ((currentNanoTime - idleSinceNanoTime) > inactiveNanoInterval) {
+                    // shard is inactive. mark it as such.
+                    activeIndexing = false;
+                }
+            } else if (activeIndexing == false  // we weren't indexing before
+                    && idle == false // but we do now
+                    && current.translogNumberOfOperations > 0 // but only if we're really sure - see note bellow
+                    ) {
+                // since we sync flush once a shard becomes inactive, the translog id can change, however that
+                // doesn't mean the an indexing operation has happened. Note that if we're really unlucky and a flush happens
+                // immediately after an indexing operation we may not become active immediately. The following
+                // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay
+                // inactive
+
+                activeIndexing = true;
+                idleSinceNanoTime = -1;
+            }
+
+            translogId = current.translogId;
+            translogNumberOfOperations = current.translogNumberOfOperations;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index bd15a6a..3f35ddf 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -73,7 +73,7 @@ public class InternalSettingsPreparer {
      * and then replacing all property placeholders. If a {@link Terminal} is provided and configuration settings are loaded,
      * settings with a value of <code>${prompt.text}</code> or <code>${prompt.secret}</code> will result in a prompt for
      * the setting to the user.
-     * @param input The initial settings to use
+     * @param input The custom settings to use. These are not overwritten by settings in the configuration file.
      * @param terminal the Terminal to use for input/output
      * @return the {@link Settings} and {@link Environment} as a {@link Tuple}
      */
@@ -83,42 +83,20 @@ public class InternalSettingsPreparer {
         initializeSettings(output, input, true);
         Environment environment = new Environment(output.build());
 
-        // TODO: can we simplify all of this and have a single filename, which is looked up in the config dir?
-        boolean loadFromEnv = true;
-        if (useSystemProperties(input)) {
-            // if its default, then load it, but also load form env
-            if (Strings.hasText(System.getProperty("es.default.config"))) {
-                // TODO: we don't allow multiple config files, but having loadFromEnv true here allows just that
-                loadFromEnv = true;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.default.config")));
-            }
-            // TODO: these should be elseifs so that multiple files cannot be loaded
-            // if explicit, just load it and don't load from env
-            if (Strings.hasText(System.getProperty("es.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("es.config")));
-            }
-            if (Strings.hasText(System.getProperty("elasticsearch.config"))) {
-                loadFromEnv = false;
-                output.loadFromPath(environment.configFile().resolve(System.getProperty("elasticsearch.config")));
-            }
-        }
-        if (loadFromEnv) {
-            boolean settingsFileFound = false;
-            Set<String> foundSuffixes = new HashSet<>();
-            for (String allowedSuffix : ALLOWED_SUFFIXES) {
-                Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
-                if (Files.exists(path)) {
-                    if (!settingsFileFound) {
-                        output.loadFromPath(path);
-                    }
-                    settingsFileFound = true;
-                    foundSuffixes.add(allowedSuffix);
+        boolean settingsFileFound = false;
+        Set<String> foundSuffixes = new HashSet<>();
+        for (String allowedSuffix : ALLOWED_SUFFIXES) {
+            Path path = environment.configFile().resolve("elasticsearch" + allowedSuffix);
+            if (Files.exists(path)) {
+                if (!settingsFileFound) {
+                    output.loadFromPath(path);
                 }
+                settingsFileFound = true;
+                foundSuffixes.add(allowedSuffix);
             }
-            if (foundSuffixes.size() > 1) {
-                throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
-            }
+        }
+        if (foundSuffixes.size() > 1) {
+            throw new SettingsException("multiple settings files found with suffixes: " + Strings.collectionToDelimitedString(foundSuffixes, ","));
         }
 
         // re-initialize settings now that the config file has been loaded
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
index db6afd1..5d83fa2 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManagerCliParser.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.CliTool;
 import org.elasticsearch.common.cli.CliToolConfig;
 import org.elasticsearch.common.cli.Terminal;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.logging.log4j.LogConfigurator;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -39,7 +38,6 @@ import java.util.Locale;
 
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.option;
-import static org.elasticsearch.common.settings.Settings.EMPTY;
 
 public class PluginManagerCliParser extends CliTool {
 
@@ -51,8 +49,21 @@ public class PluginManagerCliParser extends CliTool {
             .build();
 
     public static void main(String[] args) {
-        Environment env = InternalSettingsPreparer.prepareEnvironment(EMPTY, Terminal.DEFAULT);
-        LogConfigurator.configure(env.settings());
+        // initialize default for es.logger.level because we will not read the logging.yml
+        String loggerLevel = System.getProperty("es.logger.level", "INFO");
+        // Set the appender for all potential log files to terminal so that other components that use the logger print out the
+        // same terminal.
+        // The reason for this is that the plugin cli cannot be configured with a file appender because when the plugin command is
+        // executed there is no way of knowing where the logfiles should be placed. For example, if elasticsearch
+        // is run as service then the logs should be at /var/log/elasticsearch but when started from the tar they should be at es.home/logs.
+        // Therefore we print to Terminal.
+        Environment env = InternalSettingsPreparer.prepareEnvironment(Settings.builder()
+                .put("appender.terminal.type", "terminal")
+                .put("rootLogger", "${es.logger.level}, terminal")
+                .put("es.logger.level", loggerLevel)
+                .build(), Terminal.DEFAULT);
+        // configure but do not read the logging conf file
+        LogConfigurator.configure(env.settings(), false);
         int status = new PluginManagerCliParser().execute(args).status();
         System.exit(status);
     }
diff --git a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
index 548c1b3..71154a5 100644
--- a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
@@ -94,16 +94,6 @@ public class NativeScriptEngineService extends AbstractComponent implements Scri
     }
 
     @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        return executable(compiledScript, vars).run();
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-
-    @Override
     public void close() {
     }
 
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/ScriptEngineService.java
index 9660857..993c95a 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptEngineService.java
@@ -42,10 +42,6 @@ public interface ScriptEngineService extends Closeable {
 
     SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars);
 
-    Object execute(CompiledScript compiledScript, Map<String, Object> vars);
-
-    Object unwrap(Object value);
-
     /**
      * Handler method called when a script is removed from the Guava cache.
      *
diff --git a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
index b5d4e96..3affd0c 100644
--- a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.script.mustache;
 
 import com.github.mustachejava.Mustache;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -34,7 +33,6 @@ import org.elasticsearch.script.ScriptException;
 import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.lookup.SearchLookup;
 
-import java.io.IOException;
 import java.lang.ref.SoftReference;
 import java.util.Collections;
 import java.util.Map;
@@ -88,29 +86,6 @@ public class MustacheScriptEngineService extends AbstractComponent implements Sc
         return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
     }
 
-    /**
-     * Execute a compiled template object (as retrieved from the compile method)
-     * and fill potential place holders with the variables given.
-     *
-     * @param template
-     *            compiled template object.
-     * @param vars
-     *            map of variables to use during substitution.
-     *
-     * @return the processed string with all given variables substitued.
-     * */
-    @Override
-    public Object execute(CompiledScript template, Map<String, Object> vars) {
-        BytesStreamOutput result = new BytesStreamOutput();
-        try (UTF8StreamWriter writer = utf8StreamWriter().setOutput(result)) {
-            ((Mustache) template.compiled()).execute(writer, vars);
-        } catch (Exception e) {
-            logger.error("Error executing " + template, e);
-            throw new ScriptException("Error executing " + template, e);
-        }
-        return result.bytes();
-    }
-
     @Override
     public String[] types() {
         return new String[] {NAME};
@@ -139,11 +114,6 @@ public class MustacheScriptEngineService extends AbstractComponent implements Sc
     }
 
     @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-
-    @Override
     public void close() {
         // Nothing to do here
     }
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 9a260f0..9297c6b 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -20,7 +20,6 @@ package org.elasticsearch;
 
 import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
-
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -31,12 +30,7 @@ import org.elasticsearch.client.AbstractClientHeadersTestCase;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IllegalShardRoutingStateException;
-import org.elasticsearch.cluster.routing.RoutingTableValidation;
-import org.elasticsearch.cluster.routing.RoutingValidationException;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
@@ -55,7 +49,6 @@ import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.index.engine.CreateFailedEngineException;
 import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
@@ -139,9 +132,9 @@ public class ExceptionSerializationTests extends ESTestCase {
                         Class<?> clazz = loadClass(filename);
                         if (ignore.contains(clazz) == false) {
                             if (Modifier.isAbstract(clazz.getModifiers()) == false && Modifier.isInterface(clazz.getModifiers()) == false && isEsException(clazz)) {
-                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
+                                if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz) == false && ElasticsearchException.class.equals(clazz.getEnclosingClass()) == false) {
                                     notRegistered.add(clazz);
-                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>)clazz)) {
+                                } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz)) {
                                     registered.add(clazz);
                                     try {
                                         if (clazz.getDeclaredMethod("writeTo", StreamOutput.class) != null) {
@@ -199,7 +192,7 @@ public class ExceptionSerializationTests extends ESTestCase {
     }
 
     public static final class TestException extends ElasticsearchException {
-        public TestException(StreamInput in) throws IOException{
+        public TestException(StreamInput in) throws IOException {
             super(in);
         }
     }
@@ -247,7 +240,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ex.getIndex(), "foo");
         assertEquals(ex.getMessage(), "fobar");
 
-        ex = serialize(new QueryShardException((Index)null, null, null));
+        ex = serialize(new QueryShardException((Index) null, null, null));
         assertNull(ex.getIndex());
         assertNull(ex.getMessage());
     }
@@ -282,22 +275,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(-3, alreadyExpiredException.now());
     }
 
-    public void testCreateFailedEngineException() throws IOException {
-        CreateFailedEngineException ex = serialize(new CreateFailedEngineException(new ShardId("idx", 2), "type", "id", null));
-        assertEquals(ex.getShardId(), new ShardId("idx", 2));
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertNull(ex.getCause());
-
-        ex = serialize(new CreateFailedEngineException(null, "type", "id", new NullPointerException()));
-        assertNull(ex.getShardId());
-        assertEquals("type", ex.type());
-        assertEquals("id", ex.id());
-        assertTrue(ex.getCause() instanceof NullPointerException);
-    }
-
     public void testMergeMappingException() throws IOException {
-        MergeMappingException ex = serialize(new MergeMappingException(new String[] {"one", "two"}));
+        MergeMappingException ex = serialize(new MergeMappingException(new String[]{"one", "two"}));
         assertArrayEquals(ex.failures(), new String[]{"one", "two"});
     }
 
@@ -342,7 +321,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("the dude abides!", ex.name());
         assertEquals("index_template [the dude abides!] already exists", ex.getMessage());
 
-        ex = serialize(new IndexTemplateAlreadyExistsException((String)null));
+        ex = serialize(new IndexTemplateAlreadyExistsException((String) null));
         assertNull(ex.name());
         assertEquals("index_template [null] already exists", ex.getMessage());
     }
@@ -449,7 +428,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals(ctx.shardTarget(), ex.shard());
     }
 
-    public void testIllegalIndexShardStateException()throws IOException {
+    public void testIllegalIndexShardStateException() throws IOException {
         ShardId id = new ShardId("foo", 1);
         IndexShardState state = randomFrom(IndexShardState.values());
         IllegalIndexShardStateException ex = serialize(new IllegalIndexShardStateException(id, state, "come back later buddy"));
@@ -480,7 +459,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("baam", ex.getMessage());
         assertTrue(ex.getCause() instanceof NullPointerException);
         assertEquals(empty.length, ex.shardFailures().length);
-        ShardSearchFailure[] one = new ShardSearchFailure[] {
+        ShardSearchFailure[] one = new ShardSearchFailure[]{
                 new ShardSearchFailure(new IllegalArgumentException("nono!"))
         };
 
@@ -521,7 +500,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("index_template [name] missing", ex.getMessage());
         assertEquals("name", ex.name());
 
-        ex = serialize(new IndexTemplateMissingException((String)null));
+        ex = serialize(new IndexTemplateMissingException((String) null));
         assertEquals("index_template [null] missing", ex.getMessage());
         assertNull(ex.name());
     }
@@ -570,8 +549,8 @@ public class ExceptionSerializationTests extends ESTestCase {
         ex = serialize(new NotSerializableExceptionWrapper(new IllegalArgumentException("nono!")));
         assertEquals("{\"type\":\"illegal_argument_exception\",\"reason\":\"nono!\"}", toXContent(ex));
 
-        Throwable[] unknowns = new Throwable[] {
-                new JsonParseException("foobar", new JsonLocation(new Object(), 1,2,3,4)),
+        Throwable[] unknowns = new Throwable[]{
+                new JsonParseException("foobar", new JsonLocation(new Object(), 1, 2, 3, 4)),
                 new ClassCastException("boom boom boom"),
                 new IOException("booom")
         };
@@ -609,7 +588,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         UnknownHeaderException uhe = new UnknownHeaderException("msg", status);
         uhe.addHeader("foo", "foo", "bar");
 
-        ElasticsearchException serialize = serialize((ElasticsearchException)uhe);
+        ElasticsearchException serialize = serialize((ElasticsearchException) uhe);
         assertTrue(serialize instanceof NotSerializableExceptionWrapper);
         NotSerializableExceptionWrapper e = (NotSerializableExceptionWrapper) serialize;
         assertEquals("msg", e.getMessage());
@@ -684,7 +663,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(19, org.elasticsearch.ResourceNotFoundException.class);
         ids.put(20, org.elasticsearch.transport.ActionTransportException.class);
         ids.put(21, org.elasticsearch.ElasticsearchGenerationException.class);
-        ids.put(22, org.elasticsearch.index.engine.CreateFailedEngineException.class);
+        ids.put(22, null); // was CreateFailedEngineException
         ids.put(23, org.elasticsearch.index.shard.IndexShardStartedException.class);
         ids.put(24, org.elasticsearch.search.SearchContextMissingException.class);
         ids.put(25, org.elasticsearch.script.ScriptException.class);
@@ -716,7 +695,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(51, org.elasticsearch.index.IndexShardAlreadyExistsException.class);
         ids.put(52, org.elasticsearch.index.engine.VersionConflictEngineException.class);
         ids.put(53, org.elasticsearch.index.engine.EngineException.class);
-        ids.put(54, org.elasticsearch.index.engine.DocumentAlreadyExistsException.class);
+        ids.put(54, null); // was DocumentAlreadyExistsException, which is superseded with VersionConflictEngineException
         ids.put(55, org.elasticsearch.action.NoSuchNodeException.class);
         ids.put(56, org.elasticsearch.common.settings.SettingsException.class);
         ids.put(57, org.elasticsearch.indices.IndexTemplateMissingException.class);
@@ -813,7 +792,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         }
 
         for (ElasticsearchException.ElasticsearchExceptionHandle handle : ElasticsearchException.ElasticsearchExceptionHandle.values()) {
-            assertEquals((int)reverse.get(handle.exceptionClass), handle.id);
+            assertEquals((int) reverse.get(handle.exceptionClass), handle.id);
         }
 
         for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) {
diff --git a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
index 1cdea96..7c08a0d 100644
--- a/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/index/IndexRequestTests.java
@@ -18,12 +18,18 @@
  */
 package org.elasticsearch.action.index;
 
+import org.elasticsearch.index.VersionType;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
-import static org.hamcrest.Matchers.equalTo;
+
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.*;
 
 /**
-  */
+ */
 public class IndexRequestTests extends ESTestCase {
 
     @Test
@@ -39,9 +45,23 @@ public class IndexRequestTests extends ESTestCase {
         assertThat(IndexRequest.OpType.fromString(indexUpper), equalTo(IndexRequest.OpType.INDEX));
     }
 
-    @Test(expected= IllegalArgumentException.class)
-    public void testReadBogusString(){
+    @Test(expected = IllegalArgumentException.class)
+    public void testReadBogusString() {
         String foobar = "foobar";
         IndexRequest.OpType.fromString(foobar);
     }
+
+    public void testCreateOperationRejectsVersions() {
+        Set<VersionType> allButInternalSet = new HashSet<>(Arrays.asList(VersionType.values()));
+        allButInternalSet.remove(VersionType.INTERNAL);
+        VersionType[] allButInternal = allButInternalSet.toArray(new VersionType[]{});
+        IndexRequest request = new IndexRequest("index", "type", "1");
+        request.opType(IndexRequest.OpType.CREATE);
+        request.versionType(randomFrom(allButInternal));
+        assertThat(request.validate().validationErrors(), not(empty()));
+
+        request.versionType(VersionType.INTERNAL);
+        request.version(randomIntBetween(0, Integer.MAX_VALUE));
+        assertThat(request.validate().validationErrors(), not(empty()));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
new file mode 100644
index 0000000..fce4312
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationActionTests.java
@@ -0,0 +1,316 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.support.single.instance;
+
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.IndicesRequest;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.PlainActionFuture;
+import org.elasticsearch.action.support.replication.ClusterStateCreationUtils;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlock;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.block.ClusterBlocks;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.cluster.TestClusterService;
+import org.elasticsearch.test.transport.CapturingTransport;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.ConnectTransportException;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportService;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.function.Supplier;
+
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class TransportInstanceSingleOperationActionTests extends ESTestCase {
+
+    private static ThreadPool THREAD_POOL;
+
+    private TestClusterService clusterService;
+    private CapturingTransport transport;
+    private TransportService transportService;
+
+    private TestTransportInstanceSingleOperationAction action;
+
+    public static class Request extends InstanceShardOperationRequest<Request> {
+        public Request() {
+        }
+    }
+
+    public static class Response extends ActionResponse {
+        public Response() {
+        }
+    }
+
+    class TestTransportInstanceSingleOperationAction extends TransportInstanceSingleOperationAction<Request, Response> {
+        private final Map<ShardId, Object> shards = new HashMap<>();
+
+        public TestTransportInstanceSingleOperationAction(Settings settings, String actionName, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, Supplier<Request> request) {
+            super(settings, actionName, THREAD_POOL, TransportInstanceSingleOperationActionTests.this.clusterService, transportService, actionFilters, indexNameExpressionResolver, request);
+        }
+
+        public Map<ShardId, Object> getResults() {
+            return shards;
+        }
+
+        @Override
+        protected String executor() {
+            return ThreadPool.Names.SAME;
+        }
+
+        @Override
+        protected void shardOperation(Request request, ActionListener<Response> listener) {
+            throw new UnsupportedOperationException("Not implemented in test class");
+        }
+
+        @Override
+        protected Response newResponse() {
+            return new Response();
+        }
+
+        @Override
+        protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+            return true;
+        }
+
+        @Override
+        protected ShardIterator shards(ClusterState clusterState, Request request) {
+            return clusterState.routingTable().index(request.concreteIndex()).shard(request.shardId).primaryShardIt();
+        }
+    }
+
+    class MyResolver extends IndexNameExpressionResolver {
+        public MyResolver() {
+            super(Settings.EMPTY);
+        }
+
+        @Override
+        public String[] concreteIndices(ClusterState state, IndicesRequest request) {
+            return request.indices();
+        }
+    }
+
+    @BeforeClass
+    public static void startThreadPool() {
+        THREAD_POOL = new ThreadPool(TransportInstanceSingleOperationActionTests.class.getSimpleName());
+    }
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        transport = new CapturingTransport();
+        clusterService = new TestClusterService(THREAD_POOL);
+        transportService = new TransportService(transport, THREAD_POOL);
+        transportService.start();
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        );
+    }
+
+    @AfterClass
+    public static void destroyThreadPool() {
+        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);
+        // since static must set to null to be eligible for collection
+        THREAD_POOL = null;
+    }
+
+    public void testGlobalBlock() {
+        Request request = new Request();
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        ClusterBlocks.Builder block = ClusterBlocks.builder()
+                .addGlobalBlock(new ClusterBlock(1, "", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
+        clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
+        try {
+            action.new AsyncSingleAction(request, listener).start();
+            listener.get();
+            fail("expected ClusterBlockException");
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, ClusterBlockException.class) == null) {
+                logger.info("expected ClusterBlockException  but got ", t);
+                fail("expected ClusterBlockException");
+            }
+        }
+    }
+
+    public void testBasicRequestWorks() throws InterruptedException, ExecutionException, TimeoutException {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testFailureWithoutRetry() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        // this should not trigger retry or anything and the listener should report exception immediately
+        transport.handleResponse(requestId, new TransportException("a generic transport exception", new Exception("generic test exception")));
+
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, TransportException.class) == null) {
+                logger.info("expected TransportException  but got ", t);
+                fail("expected and TransportException");
+            }
+        }
+    }
+
+    public void testSuccessAfterRetryWithClusterStateUpdate() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.INITIALIZING));
+        action.new AsyncSingleAction(request, listener).start();
+        // this should fail because primary not initialized
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        // this time it should work
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testSuccessAfterRetryWithExcpetionFromTransport() throws Exception {
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        boolean local = randomBoolean();
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        // trigger cluster state observer
+        clusterService.setState(ClusterStateCreationUtils.state("test", local, ShardRoutingState.STARTED));
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        transport.handleResponse(transport.capturedRequests()[0].requestId, new Response());
+        listener.get();
+    }
+
+    public void testRetryOfAnAlreadyTimedOutRequest() throws Exception {
+        Request request = new Request().index("test").timeout(new TimeValue(0, TimeUnit.MILLISECONDS));
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(1));
+        long requestId = transport.capturedRequests()[0].requestId;
+        transport.clear();
+        DiscoveryNode node = clusterService.state().getNodes().getLocalNode();
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+
+        // wait until the timeout was triggered and we actually tried to send for the second time
+        assertBusy(new Runnable() {
+            @Override
+            public void run() {
+                assertThat(transport.capturedRequests().length, equalTo(1));
+            }
+        });
+
+        // let it fail the second time too
+        requestId = transport.capturedRequests()[0].requestId;
+        transport.handleResponse(requestId, new ConnectTransportException(node, "test exception"));
+        try {
+            // result should return immediately
+            assertTrue(listener.isDone());
+            listener.get();
+            fail("this should fail with a transport exception");
+        } catch (ExecutionException t) {
+            if (ExceptionsHelper.unwrap(t, ConnectTransportException.class) == null) {
+                logger.info("expected ConnectTransportException  but got ", t);
+                fail("expected and ConnectTransportException");
+            }
+        }
+    }
+
+    public void testUnresolvableRequestDoesNotHang() throws InterruptedException, ExecutionException, TimeoutException {
+        action = new TestTransportInstanceSingleOperationAction(
+                Settings.EMPTY,
+                "indices:admin/test_unresolvable",
+                transportService,
+                new ActionFilters(new HashSet<ActionFilter>()),
+                new MyResolver(),
+                Request::new
+        ) {
+            @Override
+            protected boolean resolveRequest(ClusterState state, Request request, ActionListener<Response> listener) {
+                return false;
+            }
+        };
+        Request request = new Request().index("test");
+        request.shardId = 0;
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        clusterService.setState(ClusterStateCreationUtils.state("test", randomBoolean(), ShardRoutingState.STARTED));
+        action.new AsyncSingleAction(request, listener).start();
+        assertThat(transport.capturedRequests().length, equalTo(0));
+        try {
+            listener.get();
+        } catch (Throwable t) {
+            if (ExceptionsHelper.unwrap(t, IllegalStateException.class) == null) {
+                logger.info("expected IllegalStateException  but got ", t);
+                fail("expected and IllegalStateException");
+            }
+        }
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java b/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
index c1bdf00..d0cd387 100644
--- a/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
@@ -57,7 +57,7 @@ public class Log4jESLoggerTests extends ESTestCase {
                 .put("path.conf", configDir.toAbsolutePath())
                 .put("path.home", createTempDir().toString())
                 .build();
-        LogConfigurator.configure(settings);
+        LogConfigurator.configure(settings, true);
 
         esTestLogger = Log4jESLoggerFactory.getLogger("test");
         Logger testLogger = ((Log4jESLogger) esTestLogger).logger();
diff --git a/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java b/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
index 199f94c..2b84bec 100644
--- a/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
@@ -32,8 +32,10 @@ import org.junit.Test;
 
 import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
+import java.nio.file.OpenOption;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
+import java.util.Arrays;
 
 import static org.hamcrest.Matchers.*;
 
@@ -56,7 +58,7 @@ public class LoggingConfigurationTests extends ESTestCase {
                     .put("path.conf", configDir.toAbsolutePath())
                     .put("path.home", createTempDir().toString())
                     .build();
-            LogConfigurator.configure(settings);
+            LogConfigurator.configure(settings, true);
 
             ESLogger esLogger = Log4jESLoggerFactory.getLogger("test");
             Logger logger = ((Log4jESLogger) esLogger).logger();
@@ -157,20 +159,20 @@ public class LoggingConfigurationTests extends ESTestCase {
     public void testResolveOrder() throws Exception {
         Path tmpDir = createTempDir();
         Path loggingConf = tmpDir.resolve(loggingConfiguration("yaml"));
-        Files.write(loggingConf, "logger.test: INFO, file\n".getBytes(StandardCharsets.UTF_8));
+        Files.write(loggingConf, "logger.test_resolve_order: INFO, file\n".getBytes(StandardCharsets.UTF_8));
         Files.write(loggingConf, "appender.file.type: file\n".getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
         Environment environment = InternalSettingsPreparer.prepareEnvironment(
                 Settings.builder()
                         .put("path.conf", tmpDir.toAbsolutePath())
                         .put("path.home", createTempDir().toString())
-                        .put("logger.test", "TRACE, console")
+                        .put("logger.test_resolve_order", "TRACE, console")
                         .put("appender.console.type", "console")
                         .put("appender.console.layout.type", "consolePattern")
                         .put("appender.console.layout.conversionPattern", "[%d{ISO8601}][%-5p][%-25c] %m%n")
                         .build(), new CliToolTestCase.MockTerminal());
-        LogConfigurator.configure(environment.settings());
+        LogConfigurator.configure(environment.settings(), true);
         // args should overwrite whatever is in the config
-        ESLogger esLogger = Log4jESLoggerFactory.getLogger("test");
+        ESLogger esLogger = Log4jESLoggerFactory.getLogger("test_resolve_order");
         Logger logger = ((Log4jESLogger) esLogger).logger();
         Appender appender = logger.getAppender("console");
         assertThat(appender, notNullValue());
@@ -179,6 +181,31 @@ public class LoggingConfigurationTests extends ESTestCase {
         assertThat(appender, nullValue());
     }
 
+    // tests that config file is not read when we call LogConfigurator.configure(Settings, false)
+    @Test
+    public void testConfigNotRead() throws Exception {
+        Path tmpDir = createTempDir();
+        Path loggingConf = tmpDir.resolve(loggingConfiguration("yaml"));
+        Files.write(loggingConf,
+                Arrays.asList(
+                        "logger.test_config_not_read: INFO, console",
+                        "appender.console.type: console"),
+                StandardCharsets.UTF_8);
+        Environment environment = InternalSettingsPreparer.prepareEnvironment(
+                Settings.builder()
+                        .put("path.conf", tmpDir.toAbsolutePath())
+                        .put("path.home", createTempDir().toString())
+                        .build(), new CliToolTestCase.MockTerminal());
+        LogConfigurator.configure(environment.settings(), false);
+        ESLogger esLogger = Log4jESLoggerFactory.getLogger("test_config_not_read");
+
+        assertNotNull(esLogger);
+        Logger logger = ((Log4jESLogger) esLogger).logger();
+        Appender appender = logger.getAppender("console");
+        // config was not read
+        assertNull(appender);
+    }
+
     private static String loggingConfiguration(String suffix) {
         return "logging." + randomAsciiOfLength(randomIntBetween(0, 10)) + "." + suffix;
     }
diff --git a/core/src/test/java/org/elasticsearch/get/GetActionIT.java b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
index 55b104d..b26e3ec 100644
--- a/core/src/test/java/org/elasticsearch/get/GetActionIT.java
+++ b/core/src/test/java/org/elasticsearch/get/GetActionIT.java
@@ -25,11 +25,7 @@ import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetRequestBuilder;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.get.MultiGetRequest;
-import org.elasticsearch.action.get.MultiGetRequestBuilder;
-import org.elasticsearch.action.get.MultiGetResponse;
+import org.elasticsearch.action.get.*;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
@@ -53,14 +49,7 @@ import java.util.Set;
 import static java.util.Collections.singleton;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class GetActionIT extends ESIntegTestCase {
 
@@ -600,7 +589,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
         //Version from Lucene index
@@ -623,7 +612,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getResponse().getSourceAsMap().get("field").toString(), equalTo("value1"));
         assertThat(response.getResponses()[2].getFailure(), notNullValue());
         assertThat(response.getResponses()[2].getFailure().getId(), equalTo("1"));
-        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict, current [1], provided [2]"));
+        assertThat(response.getResponses()[2].getFailure().getMessage(), startsWith("[type1][1]: version conflict"));
         assertThat(response.getResponses()[2].getFailure().getFailure(), instanceOf(VersionConflictEngineException.class));
 
 
@@ -648,7 +637,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
@@ -674,7 +663,7 @@ public class GetActionIT extends ESIntegTestCase {
         assertThat(response.getResponses()[1].getFailure(), notNullValue());
         assertThat(response.getResponses()[1].getFailure().getId(), equalTo("2"));
         assertThat(response.getResponses()[1].getIndex(), equalTo("test"));
-        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict, current [2], provided [1]"));
+        assertThat(response.getResponses()[1].getFailure().getMessage(), startsWith("[type1][2]: version conflict"));
         assertThat(response.getResponses()[2].getId(), equalTo("2"));
         assertThat(response.getResponses()[2].getIndex(), equalTo("test"));
         assertThat(response.getResponses()[2].getFailure(), nullValue());
diff --git a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
index e4a97d2..3f7ea54 100644
--- a/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/VersionTypeTests.java
@@ -29,26 +29,31 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testInternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, Versions.MATCH_ANY));
         // if we don't have a version in the index we accept everything
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_SET, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_SET, Versions.MATCH_ANY));
 
         // if we didn't find a version (but the index does support it), we don't like it unless MATCH_ANY
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_ANY, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
+        // deletes
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.MATCH_DELETED, true));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, Versions.MATCH_DELETED, true));
+
+
         // and the stupid usual case
-        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10));
+        assertFalse(VersionType.INTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
         assertFalse(VersionType.INTERNAL.isVersionConflictForReads(10, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(9, 10));
-        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.INTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertTrue(VersionType.INTERNAL.isVersionConflictForReads(10, 9));
 
 // Old indexing code, dictating behavior
@@ -99,23 +104,23 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, 10));
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(Versions.NOT_FOUND, Versions.MATCH_ANY));
 
         // and the standard behavior
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL.isVersionConflictForReads(9, 10));
@@ -137,14 +142,14 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testExternalGTEVersionConflict() throws Exception {
 
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
         // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean()));
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -152,9 +157,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10));
-        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForWrites(10, 9, randomBoolean()));
 
         assertFalse(VersionType.EXTERNAL_GTE.isVersionConflictForReads(10, 10));
         assertTrue(VersionType.EXTERNAL_GTE.isVersionConflictForReads(9, 10));
@@ -166,14 +171,20 @@ public class VersionTypeTests extends ESTestCase {
     @Test
     public void testForceVersionConflict() throws Exception {
 
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10));
-        // MATCH_ANY must throw an exception in the case of external version, as the version must be set! it used as the new value
-        assertTrue(VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_SET, 10, randomBoolean()));
+
+        // MATCH_ANY must throw an exception in the case of force version, as the version must be set! it used as the new value
+        try {
+            VersionType.FORCE.isVersionConflictForWrites(10, Versions.MATCH_ANY, randomBoolean());
+            fail();
+        } catch (IllegalStateException e) {
+            //yes!!
+        }
 
         // if we didn't find a version (but the index does support it), we always accept
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, Versions.NOT_FOUND, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(Versions.NOT_FOUND, 10, randomBoolean()));
 
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, Versions.NOT_FOUND));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(Versions.NOT_FOUND, 10));
@@ -181,9 +192,9 @@ public class VersionTypeTests extends ESTestCase {
 
 
         // and the standard behavior
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10));
-        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(9, 10, randomBoolean()));
+        assertFalse(VersionType.FORCE.isVersionConflictForWrites(10, 9, randomBoolean()));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(9, 10));
         assertFalse(VersionType.FORCE.isVersionConflictForReads(10, 9));
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index 4b7de9b..8accf4d 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -96,7 +96,6 @@ import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
-import java.util.regex.Pattern;
 
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.PRIMARY;
@@ -105,8 +104,6 @@ import static org.hamcrest.Matchers.*;
 
 public class InternalEngineTests extends ESTestCase {
 
-    private static final Pattern PARSE_LEGACY_ID_PATTERN = Pattern.compile("^" + Translog.TRANSLOG_FILE_PREFIX + "(\\d+)((\\.recovering))?$");
-
     protected final ShardId shardId = new ShardId(new Index("index"), 1);
 
     protected ThreadPool threadPool;
@@ -273,10 +270,10 @@ public class InternalEngineTests extends ESTestCase {
 
             // create a doc and refresh
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -310,7 +307,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.onSettingsChanged();
 
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -358,7 +355,7 @@ public class InternalEngineTests extends ESTestCase {
             engine.config().setCompoundOnFlush(true);
             engine.onSettingsChanged();
             ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("4"), doc4));
+            engine.index(new Engine.Index(newUid("4"), doc4));
             engine.refresh("test");
 
             segments = engine.segments(false);
@@ -392,7 +389,7 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.isEmpty(), equalTo(true));
 
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -400,10 +397,10 @@ public class InternalEngineTests extends ESTestCase {
             assertThat(segments.get(0).ramTree, notNullValue());
 
             ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-            engine.create(new Engine.Create(newUid("2"), doc2));
+            engine.index(new Engine.Index(newUid("2"), doc2));
             engine.refresh("test");
             ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-            engine.create(new Engine.Create(newUid("3"), doc3));
+            engine.index(new Engine.Index(newUid("3"), doc3));
             engine.refresh("test");
 
             segments = engine.segments(true);
@@ -473,7 +470,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = engine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -524,7 +521,7 @@ public class InternalEngineTests extends ESTestCase {
     /* */
     public void testConcurrentGetAndFlush() throws Exception {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         final AtomicReference<Engine.GetResult> latestGetResult = new AtomicReference<>();
         latestGetResult.set(engine.get(new Engine.Get(true, newUid("1"))));
@@ -569,7 +566,7 @@ public class InternalEngineTests extends ESTestCase {
         Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -661,7 +658,7 @@ public class InternalEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -722,7 +719,7 @@ public class InternalEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = engine.acquireSearcher("test");
@@ -758,7 +755,7 @@ public class InternalEngineTests extends ESTestCase {
                      new LogByteSizeMergePolicy()), false)) {
             final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             Engine.CommitId commitID = engine.flush();
             assertThat(commitID, equalTo(new Engine.CommitId(store.readLastCommittedSegmentsInfo().getId())));
             byte[] wrongBytes = Base64.decode(commitID.toString());
@@ -766,7 +763,7 @@ public class InternalEngineTests extends ESTestCase {
             Engine.CommitId wrongId = new Engine.CommitId(wrongBytes);
             assertEquals("should fail to sync flush with wrong id (but no docs)", engine.syncFlush(syncId + "1", wrongId),
                     Engine.SyncedFlushResult.COMMIT_MISMATCH);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             assertEquals("should fail to sync flush with right id but pending doc", engine.syncFlush(syncId + "2", commitID),
                     Engine.SyncedFlushResult.PENDING_OPERATIONS);
             commitID = engine.flush();
@@ -780,7 +777,7 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushSurvivesEngineRestart() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
@@ -799,14 +796,14 @@ public class InternalEngineTests extends ESTestCase {
     public void testSycnedFlushVanishesOnReplay() throws IOException {
         final String syncId = randomUnicodeOfCodepointLengthBetween(10, 20);
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        engine.create(new Engine.Create(newUid("1"), doc));
+        engine.index(new Engine.Index(newUid("1"), doc));
         final Engine.CommitId commitID = engine.flush();
         assertEquals("should succeed to flush commit with right id and no pending doc", engine.syncFlush(syncId, commitID),
                 Engine.SyncedFlushResult.SUCCESS);
         assertEquals(store.readLastCommittedSegmentsInfo().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         assertEquals(engine.getLastCommittedSegmentInfos().getUserData().get(Engine.SYNC_COMMIT_ID), syncId);
         doc = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), new BytesArray("{}"), null);
-        engine.create(new Engine.Create(newUid("2"), doc));
+        engine.index(new Engine.Index(newUid("2"), doc));
         EngineConfig config = engine.config();
         engine.close();
         final MockDirectoryWrapper directory = DirectoryUtils.getLeaf(store.directory(), MockDirectoryWrapper.class);
@@ -823,28 +820,16 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningNewCreate() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
+        create = new Engine.Index(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
+        replicaEngine.index(create);
         assertThat(create.version(), equalTo(1l));
     }
 
     @Test
-    public void testExternalVersioningNewCreate() {
-        ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 12, VersionType.EXTERNAL, Engine.Operation.Origin.PRIMARY, 0);
-        engine.create(create);
-        assertThat(create.version(), equalTo(12l));
-
-        create = new Engine.Create(newUid("1"), doc, create.version(), create.versionType().versionTypeForReplicationAndRecovery(), REPLICA, 0);
-        replicaEngine.create(create);
-        assertThat(create.version(), equalTo(12l));
-    }
-
-    @Test
     public void testVersioningNewIndex() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
         Engine.Index index = new Engine.Index(newUid("1"), doc);
@@ -1107,9 +1092,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1164,9 +1149,9 @@ public class InternalEngineTests extends ESTestCase {
         }
 
         // we shouldn't be able to create as well
-        Engine.Create create = new Engine.Create(newUid("1"), doc, 2l, VersionType.INTERNAL, PRIMARY, 0);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
         } catch (VersionConflictEngineException e) {
             // all is well
         }
@@ -1175,15 +1160,15 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsException() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1191,17 +1176,17 @@ public class InternalEngineTests extends ESTestCase {
     @Test
     public void testVersioningCreateExistsExceptionWithFlush() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocument(), B_1, null);
-        Engine.Create create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
-        engine.create(create);
+        Engine.Index create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
+        engine.index(create);
         assertThat(create.version(), equalTo(1l));
 
         engine.flush();
 
-        create = new Engine.Create(newUid("1"), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, 0);
+        create = new Engine.Index(newUid("1"), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, 0);
         try {
-            engine.create(create);
+            engine.index(create);
             fail();
-        } catch (DocumentAlreadyExistsException e) {
+        } catch (VersionConflictEngineException e) {
             // all is well
         }
     }
@@ -1365,13 +1350,13 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
 
             // Again, with TRACE, which should log IndexWriter output:
             rootLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertTrue(mockAppender.sawIndexWriterMessage);
 
@@ -1400,14 +1385,14 @@ public class InternalEngineTests extends ESTestCase {
         try {
             // First, with DEBUG, which should NOT log IndexWriter output:
             ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-            engine.create(new Engine.Create(newUid("1"), doc));
+            engine.index(new Engine.Index(newUid("1"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertFalse(mockAppender.sawIndexWriterIFDMessage);
 
             // Again, with TRACE, which should only log IndexWriter IFD output:
             iwIFDLogger.setLevel(Level.TRACE);
-            engine.create(new Engine.Create(newUid("2"), doc));
+            engine.index(new Engine.Index(newUid("2"), doc));
             engine.flush();
             assertFalse(mockAppender.sawIndexWriterMessage);
             assertTrue(mockAppender.sawIndexWriterIFDMessage);
@@ -1607,8 +1592,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1660,8 +1645,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1761,8 +1746,8 @@ public class InternalEngineTests extends ESTestCase {
                 final int numExtraDocs = randomIntBetween(1, 10);
                 for (int i = 0; i < numExtraDocs; i++) {
                     ParsedDocument doc = testParsedDocument("extra" + Integer.toString(i), "extra" + Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-                    Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-                    engine.create(firstIndexRequest);
+                    Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+                    engine.index(firstIndexRequest);
                     assertThat(firstIndexRequest.version(), equalTo(1l));
                 }
                 engine.refresh("test");
@@ -1790,8 +1775,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1839,8 +1824,8 @@ public class InternalEngineTests extends ESTestCase {
         int randomId = randomIntBetween(numDocs + 1, numDocs + 10);
         String uuidValue = "test#" + Integer.toString(randomId);
         ParsedDocument doc = testParsedDocument(uuidValue, Integer.toString(randomId), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-        Engine.Create firstIndexRequest = new Engine.Create(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
-        engine.create(firstIndexRequest);
+        Engine.Index firstIndexRequest = new Engine.Index(newUid(uuidValue), doc, 1, VersionType.EXTERNAL, PRIMARY, System.nanoTime());
+        engine.index(firstIndexRequest);
         assertThat(firstIndexRequest.version(), equalTo(1l));
         if (flush) {
             engine.flush();
@@ -1920,8 +1905,8 @@ public class InternalEngineTests extends ESTestCase {
         final int numDocs = randomIntBetween(1, 10);
         for (int i = 0; i < numDocs; i++) {
             ParsedDocument doc = testParsedDocument(Integer.toString(i), Integer.toString(i), "test", null, -1, -1, testDocument(), new BytesArray("{}"), null);
-            Engine.Create firstIndexRequest = new Engine.Create(newUid(Integer.toString(i)), doc, Versions.MATCH_ANY, VersionType.INTERNAL, PRIMARY, System.nanoTime());
-            engine.create(firstIndexRequest);
+            Engine.Index firstIndexRequest = new Engine.Index(newUid(Integer.toString(i)), doc, Versions.MATCH_DELETED, VersionType.INTERNAL, PRIMARY, System.nanoTime());
+            engine.index(firstIndexRequest);
             assertThat(firstIndexRequest.version(), equalTo(1l));
         }
         engine.refresh("test");
@@ -1939,7 +1924,7 @@ public class InternalEngineTests extends ESTestCase {
         engine.close();
 
         Translog translog = new Translog(new TranslogConfig(shardId, createTempDir(), Settings.EMPTY, Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, threadPool));
-        translog.add(new Translog.Create("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
+        translog.add(new Translog.Index("test", "SomeBogusId", "{}".getBytes(Charset.forName("UTF-8"))));
         assertEquals(generation.translogFileGeneration, translog.currentFileGeneration());
         translog.close();
 
diff --git a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
index b5987a9..4c132f3 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java
@@ -236,7 +236,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testCommitStats() {
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         CommitStats stats1 = replicaEngine.commitStats();
         assertThat(stats1.getGeneration(), greaterThan(0l));
@@ -271,10 +271,10 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a doc and refresh
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -334,7 +334,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -407,7 +407,7 @@ public class ShadowEngineTests extends ESTestCase {
         primaryEngine.onSettingsChanged();
 
         ParsedDocument doc4 = testParsedDocument("4", "4", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("4"), doc4));
+        primaryEngine.index(new Engine.Index(newUid("4"), doc4));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(false);
@@ -441,7 +441,7 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.isEmpty(), equalTo(true));
 
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -449,10 +449,10 @@ public class ShadowEngineTests extends ESTestCase {
         assertThat(segments.get(0).ramTree, notNullValue());
 
         ParsedDocument doc2 = testParsedDocument("2", "2", "test", null, -1, -1, testDocumentWithTextField(), B_2, null);
-        primaryEngine.create(new Engine.Create(newUid("2"), doc2));
+        primaryEngine.index(new Engine.Index(newUid("2"), doc2));
         primaryEngine.refresh("test");
         ParsedDocument doc3 = testParsedDocument("3", "3", "test", null, -1, -1, testDocumentWithTextField(), B_3, null);
-        primaryEngine.create(new Engine.Create(newUid("3"), doc3));
+        primaryEngine.index(new Engine.Index(newUid("3"), doc3));
         primaryEngine.refresh("test");
 
         segments = primaryEngine.segments(true);
@@ -480,7 +480,7 @@ public class ShadowEngineTests extends ESTestCase {
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
         try {
-            replicaEngine.create(new Engine.Create(newUid("1"), doc));
+            replicaEngine.index(new Engine.Index(newUid("1"), doc));
             fail("should have thrown an exception");
         } catch (UnsupportedOperationException e) {}
         replicaEngine.refresh("test");
@@ -517,7 +517,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         replicaEngine.refresh("test");
 
@@ -573,7 +573,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -700,7 +700,7 @@ public class ShadowEngineTests extends ESTestCase {
         document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -784,7 +784,7 @@ public class ShadowEngineTests extends ESTestCase {
 
         // create a document
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
 
         // its not there...
         searchResult = primaryEngine.acquireSearcher("test");
@@ -830,7 +830,7 @@ public class ShadowEngineTests extends ESTestCase {
     @Test
     public void testFailEngineOnCorruption() {
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
         MockDirectoryWrapper leaf = DirectoryUtils.getLeaf(replicaEngine.config().getStore().directory(), MockDirectoryWrapper.class);
         leaf.setRandomIOExceptionRate(1.0);
@@ -869,7 +869,7 @@ public class ShadowEngineTests extends ESTestCase {
     public void testFailStart() throws IOException {
         // Need a commit point for this
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, testDocumentWithTextField(), B_1, null);
-        primaryEngine.create(new Engine.Create(newUid("1"), doc));
+        primaryEngine.index(new Engine.Index(newUid("1"), doc));
         primaryEngine.flush();
 
         // this test fails if any reader, searcher or directory is not closed - MDW FTW
@@ -957,7 +957,7 @@ public class ShadowEngineTests extends ESTestCase {
         ParseContext.Document document = testDocumentWithTextField();
         document.add(new Field(SourceFieldMapper.NAME, B_1.toBytes(), SourceFieldMapper.Defaults.FIELD_TYPE));
         ParsedDocument doc = testParsedDocument("1", "1", "test", null, -1, -1, document, B_1, null);
-        pEngine.create(new Engine.Create(newUid("1"), doc));
+        pEngine.index(new Engine.Index(newUid("1"), doc));
         pEngine.flush(true, true);
 
         t.join();
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
index 9dd1a55..19e48aa 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoDistanceRangeQueryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
+import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
 import org.junit.Test;
@@ -108,8 +109,12 @@ public class GeoDistanceRangeQueryTests extends AbstractQueryTestCase<GeoDistanc
         GeoDistanceRangeQuery geoQuery = (GeoDistanceRangeQuery) query;
         assertThat(geoQuery.fieldName(), equalTo(queryBuilder.fieldName()));
         if (queryBuilder.point() != null) {
-            assertThat(geoQuery.lat(), equalTo(queryBuilder.point().lat()));
-            assertThat(geoQuery.lon(), equalTo(queryBuilder.point().lon()));
+            GeoPoint expectedPoint = new GeoPoint(queryBuilder.point());
+            if (GeoValidationMethod.isCoerce(queryBuilder.getValidationMethod())) {
+                GeoUtils.normalizePoint(expectedPoint, true, true);
+            }
+            assertThat(geoQuery.lat(), equalTo(expectedPoint.lat()));
+            assertThat(geoQuery.lon(), equalTo(expectedPoint.lon()));
         }
         assertThat(geoQuery.geoDistance(), equalTo(queryBuilder.geoDistance()));
         if (queryBuilder.from() != null && queryBuilder.from() instanceof Number) {
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
index a4ac66c..0a2034d 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeoShapeQueryBuilderTests.java
@@ -77,10 +77,12 @@ public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQue
                 builder.indexedShapePath(indexedShapePath);
             }
         }
-        SpatialStrategy strategy = randomFrom(SpatialStrategy.values());
-        builder.strategy(strategy);
-        if (strategy != SpatialStrategy.TERM) {
-            builder.relation(randomFrom(ShapeRelation.values()));
+        if (randomBoolean()) {
+            SpatialStrategy strategy = randomFrom(SpatialStrategy.values());
+            builder.strategy(strategy);
+            if (strategy != SpatialStrategy.TERM) {
+                builder.relation(randomFrom(ShapeRelation.values()));
+            }
         }
         return builder;
     }
@@ -105,9 +107,7 @@ public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQue
         } catch (IOException ex) {
             throw new ElasticsearchException("boom", ex);
         }
-        GetResponse response = new GetResponse(new GetResult(indexedShapeIndex, indexedShapeType, indexedShapeId, 0, true, new BytesArray(
-                json), null));
-        return response;
+        return new GetResponse(new GetResult(indexedShapeIndex, indexedShapeType, indexedShapeId, 0, true, new BytesArray(json), null));
     }
 
     @After
@@ -149,7 +149,7 @@ public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQue
     @Test
     public void testNoShape() throws IOException {
         try {
-            GeoShapeQueryBuilder builder = new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, (ShapeBuilder) null);
+            new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, (ShapeBuilder) null);
             fail("exception expected");
         } catch (IllegalArgumentException e) {
             // expected
@@ -158,12 +158,12 @@ public class GeoShapeQueryBuilderTests extends AbstractQueryTestCase<GeoShapeQue
 
     @Test(expected = IllegalArgumentException.class)
     public void testNoIndexedShape() throws IOException {
-        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, (String) null, "type");
+        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, null, "type");
     }
 
     @Test(expected = IllegalArgumentException.class)
     public void testNoIndexedShapeType() throws IOException {
-        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, "id", (String) null);
+        new GeoShapeQueryBuilder(GEO_SHAPE_FIELD_NAME, "id", null);
     }
 
     @Test(expected=IllegalArgumentException.class)
diff --git a/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
new file mode 100644
index 0000000..bc2272e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/functionscore/FunctionScoreTests.java
@@ -0,0 +1,534 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query.functionscore;
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.lucene.search.function.*;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.fielddata.*;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.functionscore.exp.ExponentialDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
+import org.elasticsearch.index.query.functionscore.lin.LinearDecayFunctionBuilder;
+import org.elasticsearch.search.MultiValueMode;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.concurrent.ExecutionException;
+
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class FunctionScoreTests extends ESTestCase {
+
+    private static final String UNSUPPORTED = "Method not implemented. This is just a stub for testing.";
+
+
+    /**
+     * Stub for IndexFieldData. Needed by some score functions. Returns 1 as count always.
+     */
+    private static class IndexFieldDataStub implements IndexFieldData<AtomicFieldData> {
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicFieldData load(LeafReaderContext context) {
+            return new AtomicFieldData() {
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    return new SortedBinaryDocValues() {
+                        @Override
+                        public void setDocument(int docId) {
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+
+                        @Override
+                        public BytesRef valueAt(int index) {
+                            return new BytesRef("0");
+                        }
+                    };
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public IndexFieldData.XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, IndexFieldData.XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    /**
+     * Stub for IndexNumericFieldData needed by some score functions. Returns 1 as value always.
+     */
+    private static class IndexNumericFieldDataStub implements IndexNumericFieldData {
+
+        @Override
+        public NumericType getNumericType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public MappedFieldType.Names getFieldNames() {
+            return new MappedFieldType.Names("test");
+        }
+
+        @Override
+        public FieldDataType getFieldDataType() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public AtomicNumericFieldData load(LeafReaderContext context) {
+            return new AtomicNumericFieldData() {
+                @Override
+                public SortedNumericDocValues getLongValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedNumericDoubleValues getDoubleValues() {
+                    return new SortedNumericDoubleValues() {
+                        @Override
+                        public void setDocument(int doc) {
+                        }
+
+                        @Override
+                        public double valueAt(int index) {
+                            return 1;
+                        }
+
+                        @Override
+                        public int count() {
+                            return 1;
+                        }
+                    };
+                }
+
+                @Override
+                public ScriptDocValues getScriptValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public SortedBinaryDocValues getBytesValues() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public long ramBytesUsed() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public Collection<Accountable> getChildResources() {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+
+                @Override
+                public void close() {
+                }
+            };
+        }
+
+        @Override
+        public AtomicNumericFieldData loadDirect(LeafReaderContext context) throws Exception {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public XFieldComparatorSource comparatorSource(@Nullable Object missingValue, MultiValueMode sortMode, XFieldComparatorSource.Nested nested) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public void clear(IndexReader reader) {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+
+        @Override
+        public Index index() {
+            throw new UnsupportedOperationException(UNSUPPORTED);
+        }
+    }
+
+    private static final ScoreFunction RANDOM_SCORE_FUNCTION = new RandomScoreFunction(0, 0, new IndexFieldDataStub());
+    private static final ScoreFunction FIELD_VALUE_FACTOR_FUNCTION = new FieldValueFactorFunction("test", 1, FieldValueFactorFunction.Modifier.LN, new Double(1), null);
+    private static final ScoreFunction GAUSS_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, GaussDecayFunctionBuilder.GAUSS_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction EXP_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, ExponentialDecayFunctionBuilder.EXP_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction LIN_DECAY_FUNCTION = new DecayFunctionBuilder.NumericFieldDataScoreFunction(0, 1, 0.1, 0, LinearDecayFunctionBuilder.LINEAR_DECAY_FUNCTION, new IndexNumericFieldDataStub(), MultiValueMode.MAX);
+    private static final ScoreFunction WEIGHT_FACTOR_FUNCTION = new WeightFactorFunction(4);
+    private static final String TEXT = "The way out is through.";
+    private static final String FIELD = "test";
+    private static final Term TERM = new Term(FIELD, "through");
+    private Directory dir;
+    private IndexWriter w;
+    private DirectoryReader reader;
+    private IndexSearcher searcher;
+
+    @Before
+    public void initSearcher() throws IOException {
+        dir = newDirectory();
+        w = new IndexWriter(dir, newIndexWriterConfig(new StandardAnalyzer()));
+        Document d = new Document();
+        d.add(new TextField(FIELD, TEXT, Field.Store.YES));
+        d.add(new TextField("_uid", "1", Field.Store.YES));
+        w.addDocument(d);
+        w.commit();
+        reader = DirectoryReader.open(w, true);
+        searcher = newSearcher(reader);
+    }
+
+    @After
+    public void closeAllTheReaders() throws IOException {
+        reader.close();
+        w.close();
+        dir.close();
+    }
+
+    @Test
+    public void testExplainFunctionScoreQuery() throws IOException {
+
+        Explanation functionExplanation = getFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)");
+        assertThat(functionExplanation.getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "Function for field test:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFunctionScoreExplanation(searcher, WEIGHT_FACTOR_FUNCTION);
+        checkFunctionScoreExplanation(functionExplanation, "product of:");
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].toString(), equalTo("1.0 = constant score 1.0 - no function provided\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].toString(), equalTo("4.0 = weight\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails().length, equalTo(0));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction scoreFunction) throws IOException {
+        FunctionScoreQuery functionScoreQuery = new FunctionScoreQuery(new TermQuery(TERM), scoreFunction, 0.0f, CombineFunction.AVG, 100);
+        Weight weight = searcher.createNormalizedWeight(functionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public void checkFunctionScoreExplanation(Explanation randomExplanation, String functionExpl) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo(functionExpl));
+    }
+
+    @Test
+    public void testExplainFiltersFunctionScoreQuery() throws IOException {
+        Explanation functionExplanation = getFiltersFunctionScoreExplanation(searcher, RANDOM_SCORE_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, FIELD_VALUE_FACTOR_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, GAUSS_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, EXP_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher, LIN_DECAY_FUNCTION);
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        // now test all together
+        functionExplanation = getFiltersFunctionScoreExplanation(searcher
+                , RANDOM_SCORE_FUNCTION
+                , FIELD_VALUE_FACTOR_FUNCTION
+                , GAUSS_DECAY_FUNCTION
+                , EXP_DECAY_FUNCTION
+                , LIN_DECAY_FUNCTION
+        );
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "random score function (seed: 0)", 0);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[0].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "field value function: ln(doc['test'].value?:1.0 * factor=1.0)", 1);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[1].getDetails()[1].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 2);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(-0.5*pow(MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)],2.0)/0.21714724095162594)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[2].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 3);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = exp(- MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)] * 2.3025850929940455)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[3].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+
+        checkFiltersFunctionScoreExplanation(functionExplanation, "Function for field test:", 4);
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].toString(), equalTo("0.1 = max(0.0, ((1.1111111111111112 - MAX[Math.max(Math.abs(1.0(=doc value) - 0.0(=origin))) - 0.0(=offset), 0)])/1.1111111111111112)\n"));
+        assertThat(functionExplanation.getDetails()[0].getDetails()[4].getDetails()[1].getDetails()[0].getDetails().length, equalTo(0));
+    }
+
+    public Explanation getFiltersFunctionScoreExplanation(IndexSearcher searcher, ScoreFunction... scoreFunctions) throws IOException {
+        FiltersFunctionScoreQuery filtersFunctionScoreQuery = getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode.AVG, CombineFunction.AVG, scoreFunctions);
+        Weight weight = searcher.createNormalizedWeight(filtersFunctionScoreQuery, true);
+        Explanation explanation = weight.explain(searcher.getIndexReader().leaves().get(0), 0);
+        return explanation.getDetails()[1];
+    }
+
+    public FiltersFunctionScoreQuery getFiltersFunctionScoreQuery(FiltersFunctionScoreQuery.ScoreMode scoreMode, CombineFunction combineFunction, ScoreFunction... scoreFunctions) {
+        FiltersFunctionScoreQuery.FilterFunction[] filterFunctions = new FiltersFunctionScoreQuery.FilterFunction[scoreFunctions.length];
+        for (int i = 0; i < scoreFunctions.length; i++) {
+            filterFunctions[i] = new FiltersFunctionScoreQuery.FilterFunction(
+                    new TermQuery(TERM), scoreFunctions[i]);
+        }
+        return new FiltersFunctionScoreQuery(new TermQuery(TERM), scoreMode, filterFunctions, Float.MAX_VALUE, Float.MAX_VALUE * -1, combineFunction);
+    }
+
+    public void checkFiltersFunctionScoreExplanation(Explanation randomExplanation, String functionExpl, int whichFunction) {
+        assertThat(randomExplanation.getDescription(), equalTo("min of:"));
+        assertThat(randomExplanation.getDetails()[0].getDescription(), equalTo("function score, score mode [avg]"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDescription(), equalTo("function score, product of:"));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[0].getDescription(), equalTo("match filter: " + FIELD + ":" + TERM.text()));
+        assertThat(randomExplanation.getDetails()[0].getDetails()[whichFunction].getDetails()[1].getDescription(), equalTo(functionExpl));
+    }
+
+    private static float[] randomFloats(int size) {
+        float[] weights = new float[size];
+        for (int i = 0; i < weights.length; i++) {
+            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-5f;
+        }
+        return weights;
+    }
+
+    private static class ScoreFunctionStub extends ScoreFunction {
+        private float score;
+
+        ScoreFunctionStub(float score) {
+            super(CombineFunction.REPLACE);
+            this.score = score;
+        }
+
+        @Override
+        public LeafScoreFunction getLeafScoreFunction(LeafReaderContext ctx) throws IOException {
+            return new LeafScoreFunction() {
+                @Override
+                public double score(int docId, float subQueryScore) {
+                    return score;
+                }
+
+                @Override
+                public Explanation explainScore(int docId, Explanation subQueryScore) throws IOException {
+                    throw new UnsupportedOperationException(UNSUPPORTED);
+                }
+            };
+        }
+
+        @Override
+        public boolean needsScores() {
+            return false;
+        }
+
+        @Override
+        protected boolean doEquals(ScoreFunction other) {
+            return false;
+        }
+    }
+
+    @Test
+    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
+        int numFunctions = randomIntBetween(1, 3);
+        float[] weights = randomFloats(numFunctions);
+        float[] scores = randomFloats(numFunctions);
+        ScoreFunctionStub[] scoreFunctionStubs = new ScoreFunctionStub[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            scoreFunctionStubs[i] = new ScoreFunctionStub(scores[i]);
+        }
+        WeightFactorFunction[] weightFunctionStubs = new WeightFactorFunction[numFunctions];
+        for (int i = 0; i < numFunctions; i++) {
+            weightFunctionStubs[i] = new WeightFactorFunction(weights[i], scoreFunctionStubs[i]);
+        }
+
+        FiltersFunctionScoreQuery filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MULTIPLY
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double score = 1;
+        for (int i = 0; i < weights.length; i++) {
+            score *= weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / score, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.SUM
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.AVG
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double norm = 0;
+        sum = 0;
+        for (int i = 0; i < weights.length; i++) {
+            norm += weights[i];
+            sum += weights[i] * scores[i];
+        }
+        assertThat(scoreWithWeight * norm / sum, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MIN
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double min = Double.POSITIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            min = Math.min(min, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / min, closeTo(1, 1.e-5d));
+
+        filtersFunctionScoreQueryWithWeights = getFiltersFunctionScoreQuery(
+                FiltersFunctionScoreQuery.ScoreMode.MAX
+                , CombineFunction.REPLACE
+                , weightFunctionStubs
+        );
+
+        topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        scoreWithWeight = topDocsWithWeights.scoreDocs[0].score;
+        double max = Double.NEGATIVE_INFINITY;
+        for (int i = 0; i < weights.length; i++) {
+            max = Math.max(max, weights[i] * scores[i]);
+        }
+        assertThat(scoreWithWeight / max, closeTo(1, 1.e-5d));
+    }
+
+    @Test
+    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
+        FunctionScoreQuery filtersFunctionScoreQueryWithWeights = new FunctionScoreQuery(new MatchAllDocsQuery(), new WeightFactorFunction(2), 0.0f, CombineFunction.MULTIPLY, 100);
+        TopDocs topDocsWithWeights = searcher.search(filtersFunctionScoreQueryWithWeights, 1);
+        float score = topDocsWithWeights.scoreDocs[0].score;
+        assertThat(score, equalTo(2.0f));
+    }
+}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index a952df2..e370106 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -41,6 +41,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -69,7 +70,6 @@ import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.index.settings.IndexSettingsService;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
 import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;
@@ -95,10 +95,7 @@ import java.util.concurrent.CyclicBarrier;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.EMPTY_PARAMS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
@@ -342,8 +339,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         client().prepareIndex("test", "test").setSource("{}").get();
         ensureGreen("test");
         IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        Boolean result = indicesService.indexService("test").getShardOrNull(0).checkIdle(0);
-        assertEquals(Boolean.TRUE, result);
+        indicesService.indexService("test").getShardOrNull(0).markAsInactive();
         assertBusy(new Runnable() { // should be very very quick
             @Override
             public void run() {
@@ -629,9 +625,9 @@ public class IndexShardTests extends ESSingleNodeTestCase {
 
         shardIndexingService.addListener(new IndexingOperationListener() {
             @Override
-            public Engine.Index preIndex(Engine.Index index) {
+            public Engine.Index preIndex(Engine.Index operation) {
                 preIndexCalled.set(true);
-                return super.preIndex(index);
+                return super.preIndex(operation);
             }
         });
 
@@ -958,7 +954,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
             }
         };
 
-        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController());
+        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper);
         IndexShard newShard = new IndexShard(shard.shardId(), shard.indexSettings, shard.shardPath(), shard.store(), newProvider);
 
         ShardRoutingHelper.reinit(routing);
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index 8764d1a..e345e20 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -183,14 +183,14 @@ public class TranslogTests extends ESTestCase {
 
     @Test
     public void testRead() throws IOException {
-        Translog.Location loc1 = translog.add(new Translog.Create("test", "1", new byte[]{1}));
-        Translog.Location loc2 = translog.add(new Translog.Create("test", "2", new byte[]{2}));
+        Translog.Location loc1 = translog.add(new Translog.Index("test", "1", new byte[]{1}));
+        Translog.Location loc2 = translog.add(new Translog.Index("test", "2", new byte[]{2}));
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
         translog.sync();
         assertThat(translog.read(loc1).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{1})));
         assertThat(translog.read(loc2).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{2})));
-        Translog.Location loc3 = translog.add(new Translog.Create("test", "2", new byte[]{3}));
+        Translog.Location loc3 = translog.add(new Translog.Index("test", "2", new byte[]{3}));
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
         translog.sync();
         assertThat(translog.read(loc3).getSource().source.toBytesArray(), equalTo(new BytesArray(new byte[]{3})));
@@ -215,19 +215,13 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
-        snapshot = translog.newSnapshot();
-        assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
-        assertThat(snapshot.estimatedTotalOperations(), equalTo(1));
-        snapshot.close();
-
-        addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("3")));
+        addToTranslogAndList(translog, ops, new Translog.Delete(newUid("2")));
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
         assertThat(snapshot.estimatedTotalOperations(), equalTo(ops.size()));
@@ -235,17 +229,13 @@ public class TranslogTests extends ESTestCase {
 
         snapshot = translog.newSnapshot();
 
-        Translog.Create create = (Translog.Create) snapshot.next();
-        assertThat(create != null, equalTo(true));
-        assertThat(create.source().toBytes(), equalTo(new byte[]{1}));
-
         Translog.Index index = (Translog.Index) snapshot.next();
         assertThat(index != null, equalTo(true));
-        assertThat(index.source().toBytes(), equalTo(new byte[]{2}));
+        assertThat(index.source().toBytes(), equalTo(new byte[]{1}));
 
         Translog.Delete delete = (Translog.Delete) snapshot.next();
         assertThat(delete != null, equalTo(true));
-        assertThat(delete.uid(), equalTo(newUid("3")));
+        assertThat(delete.uid(), equalTo(newUid("2")));
 
         assertThat(snapshot.next(), equalTo(null));
 
@@ -290,28 +280,22 @@ public class TranslogTests extends ESTestCase {
         assertThat((int) firstOperationPosition, greaterThan(CodecUtil.headerLength(TranslogWriter.TRANSLOG_CODEC)));
         assertThat(lastSize, equalTo(firstOperationPosition));
 
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         stats = stats();
         assertThat(stats.estimatedNumberOfOperations(), equalTo(1l));
         assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
         lastSize = stats.translogSizeInBytes().bytes();
 
-        translog.add(new Translog.Index("test", "2", new byte[]{2}));
+        translog.add(new Translog.Delete(newUid("2")));
         stats = stats();
         assertThat(stats.estimatedNumberOfOperations(), equalTo(2l));
         assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
         lastSize = stats.translogSizeInBytes().bytes();
 
         translog.add(new Translog.Delete(newUid("3")));
-        stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
-        assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
-        lastSize = stats.translogSizeInBytes().bytes();
-
-        translog.add(new Translog.Delete(newUid("4")));
         translog.prepareCommit();
         stats = stats();
-        assertThat(stats.estimatedNumberOfOperations(), equalTo(4l));
+        assertThat(stats.estimatedNumberOfOperations(), equalTo(3l));
         assertThat(stats.translogSizeInBytes().bytes(), greaterThan(lastSize));
 
         translog.commit();
@@ -327,7 +311,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
 
         snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.equalsTo(ops));
@@ -354,7 +338,7 @@ public class TranslogTests extends ESTestCase {
         assertThat(snapshot, SnapshotMatchers.size(0));
         snapshot.close();
 
-        addToTranslogAndList(translog, ops, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, ops, new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot1 = translog.newSnapshot();
 
         addToTranslogAndList(translog, ops, new Translog.Index("test", "2", new byte[]{2}));
@@ -375,7 +359,7 @@ public class TranslogTests extends ESTestCase {
 
     public void testSnapshotOnClosedTranslog() throws IOException {
         assertTrue(Files.exists(translogDir.resolve(Translog.getFilename(1))));
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         translog.close();
         try {
             Translog.Snapshot snapshot = translog.newSnapshot();
@@ -388,7 +372,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void deleteOnSnapshotRelease() throws Exception {
         ArrayList<Translog.Operation> firstOps = new ArrayList<>();
-        addToTranslogAndList(translog, firstOps, new Translog.Create("test", "1", new byte[]{1}));
+        addToTranslogAndList(translog, firstOps, new Translog.Index("test", "1", new byte[]{1}));
 
         Translog.Snapshot firstSnapshot = translog.newSnapshot();
         assertThat(firstSnapshot.estimatedTotalOperations(), equalTo(1));
@@ -463,10 +447,7 @@ public class TranslogTests extends ESTestCase {
                             Translog.Operation op;
                             switch (randomFrom(Translog.Operation.Type.values())) {
                                 case CREATE:
-                                    op = new Translog.Create("test", threadId + "_" + opCount,
-                                            randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
-                                    break;
-                                case SAVE:
+                                case INDEX:
                                     op = new Translog.Index("test", threadId + "_" + opCount,
                                             randomUnicodeOfLengthBetween(1, 20 * 1024).getBytes("UTF-8"));
                                     break;
@@ -508,7 +489,7 @@ public class TranslogTests extends ESTestCase {
             Translog.Operation expectedOp = locationOperation.operation;
             assertEquals(expectedOp.opType(), op.opType());
             switch (op.opType()) {
-                case SAVE:
+                case INDEX:
                     Translog.Index indexOp = (Translog.Index) op;
                     Translog.Index expIndexOp = (Translog.Index) expectedOp;
                     assertEquals(expIndexOp.id(), indexOp.id());
@@ -518,16 +499,6 @@ public class TranslogTests extends ESTestCase {
                     assertEquals(expIndexOp.version(), indexOp.version());
                     assertEquals(expIndexOp.versionType(), indexOp.versionType());
                     break;
-                case CREATE:
-                    Translog.Create createOp = (Translog.Create) op;
-                    Translog.Create expCreateOp = (Translog.Create) expectedOp;
-                    assertEquals(expCreateOp.id(), createOp.id());
-                    assertEquals(expCreateOp.routing(), createOp.routing());
-                    assertEquals(expCreateOp.type(), createOp.type());
-                    assertEquals(expCreateOp.source(), createOp.source());
-                    assertEquals(expCreateOp.version(), createOp.version());
-                    assertEquals(expCreateOp.versionType(), createOp.versionType());
-                    break;
                 case DELETE:
                     Translog.Delete delOp = (Translog.Delete) op;
                     Translog.Delete expDelOp = (Translog.Delete) expectedOp;
@@ -550,7 +521,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -574,7 +545,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
             String ascii = randomAsciiOfLengthBetween(1, 50);
-            locations.add(translog.add(new Translog.Create("test", "" + op, ascii.getBytes("UTF-8"))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, ascii.getBytes("UTF-8"))));
         }
         translog.sync();
 
@@ -638,7 +609,7 @@ public class TranslogTests extends ESTestCase {
     @Test
     public void testVerifyTranslogIsNotDeleted() throws IOException {
         assertFileIsPresent(translog, 1);
-        translog.add(new Translog.Create("test", "1", new byte[]{1}));
+        translog.add(new Translog.Index("test", "1", new byte[]{1}));
         Translog.Snapshot snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.size(1));
         assertFileIsPresent(translog, 1);
@@ -686,9 +657,7 @@ public class TranslogTests extends ESTestCase {
                         final Translog.Operation op;
                         switch (Translog.Operation.Type.values()[((int) (id % Translog.Operation.Type.values().length))]) {
                             case CREATE:
-                                op = new Translog.Create("type", "" + id, new byte[]{(byte) id});
-                                break;
-                            case SAVE:
+                            case INDEX:
                                 op = new Translog.Index("type", "" + id, new byte[]{(byte) id});
                                 break;
                             case DELETE:
@@ -830,12 +799,12 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            final Translog.Location location = translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+            final Translog.Location location = translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
             if (randomBoolean()) {
                 assertTrue("at least one operation pending", translog.syncNeeded());
                 assertTrue("this operation has not been synced", translog.ensureSynced(location));
                 assertFalse("the last call to ensureSycned synced all previous ops", translog.syncNeeded()); // we are the last location so everything should be synced
-                translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
+                translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8"))));
                 assertTrue("one pending operation", translog.syncNeeded());
                 assertFalse("this op has been synced before", translog.ensureSynced(location)); // not syncing now
                 assertTrue("we only synced a previous operation yet", translog.syncNeeded());
@@ -858,7 +827,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int count = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(++count).getBytes(Charset.forName("UTF-8")))));
             if (rarely() && translogOperations > op+1) {
                 translog.commit();
             }
@@ -887,14 +856,14 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         int lastSynced = -1;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (frequently()) {
                 translog.sync();
                 lastSynced = op;
             }
         }
         assertEquals(translogOperations, translog.totalOperations());
-        final Translog.Location lastLocation = translog.add(new Translog.Create("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
+        final Translog.Location lastLocation = translog.add(new Translog.Index("test", "" + translogOperations, Integer.toString(translogOperations).getBytes(Charset.forName("UTF-8"))));
 
         final Checkpoint checkpoint = Checkpoint.read(translog.location().resolve(Translog.CHECKPOINT_FILE_NAME));
         try (final ImmutableTranslogReader reader = translog.openReader(translog.location().resolve(Translog.getFilename(translog.currentFileGeneration())), checkpoint)) {
@@ -975,7 +944,7 @@ public class TranslogTests extends ESTestCase {
         int minUncommittedOp = -1;
         final boolean commitOften = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             final boolean commit = commitOften ? frequently() : rarely();
             if (commit && op < translogOperations-1) {
                 translog.commit();
@@ -1017,7 +986,7 @@ public class TranslogTests extends ESTestCase {
         Translog.TranslogGeneration translogGeneration = null;
         final boolean sync = randomBoolean();
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (op == prepareOp) {
                 translogGeneration = translog.getGeneration();
                 translog.prepareCommit();
@@ -1068,7 +1037,7 @@ public class TranslogTests extends ESTestCase {
         List<Translog.Operation> ops = new ArrayList<>();
         int translogOperations = randomIntBetween(10, 100);
         for (int op = 0; op < translogOperations; op++) {
-            Translog.Create test = new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
+            Translog.Index test = new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")));
             ops.add(test);
         }
         Translog.writeOperations(out, ops);
@@ -1083,8 +1052,8 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(10, 100);
         try(Translog translog2 = create(createTempDir())) {
             for (int op = 0; op < translogOperations; op++) {
-                locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
-                locations2.add(translog2.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+                locations2.add(translog2.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             }
             int iters = randomIntBetween(10, 100);
             for (int i = 0; i < iters; i++) {
@@ -1110,7 +1079,7 @@ public class TranslogTests extends ESTestCase {
         int translogOperations = randomIntBetween(1, 10);
         int firstUncommitted = 0;
         for (int op = 0; op < translogOperations; op++) {
-            locations.add(translog.add(new Translog.Create("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
+            locations.add(translog.add(new Translog.Index("test", "" + op, Integer.toString(op).getBytes(Charset.forName("UTF-8")))));
             if (randomBoolean()) {
                 translog.commit();
                 firstUncommitted = op + 1;
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
index 451fdf3..283124d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java
@@ -45,7 +45,7 @@ public class TranslogVersionTests extends ESTestCase {
             assertThat("a version0 stream is returned", reader instanceof LegacyTranslogReader, equalTo(true));
             try (final Translog.Snapshot snapshot = reader.newSnapshot()) {
                 final Translog.Operation operation = snapshot.next();
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.SAVE, equalTo(true));
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
                 Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("1"));
                 assertThat(op.type(), equalTo("doc"));
@@ -73,8 +73,8 @@ public class TranslogVersionTests extends ESTestCase {
 
                 Translog.Operation operation = snapshot.next();
 
-                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.CREATE, equalTo(true));
-                Translog.Create op = (Translog.Create) operation;
+                assertThat("operation is the correct type correctly", operation.opType() == Translog.Operation.Type.INDEX, equalTo(true));
+                Translog.Index op = (Translog.Index) operation;
                 assertThat(op.id(), equalTo("Bwiq98KFSb6YjJQGeSpeiw"));
                 assertThat(op.type(), equalTo("doc"));
                 assertThat(op.source().toUtf8(), equalTo("{\"body\": \"foo\"}"));
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
index e14cc22..aaca771 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerIT.java
@@ -82,7 +82,7 @@ public class IndexingMemoryControllerIT extends ESIntegTestCase {
         index("test1", "type", "1", "f", 1);
 
         // make shard the shard buffer was set to inactive size
-        final ByteSizeValue inactiveBuffer = IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER;
+        final ByteSizeValue inactiveBuffer = EngineConfig.INACTIVE_SHARD_INDEXING_BUFFER;
         if (awaitBusy(() -> getIWBufferSize("test1") == inactiveBuffer.bytes()) == false) {
             fail("failed to update shard indexing buffer size for test1 index to [" + inactiveBuffer + "]; got: " + getIWBufferSize("test1"));
         }
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
index d3d9e96..f6e21db 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java
@@ -22,17 +22,13 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
@@ -43,28 +39,44 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         final static ByteSizeValue INACTIVE = new ByteSizeValue(-1);
 
+        final Map<ShardId, Long> translogIds = new HashMap<>();
+        final Map<ShardId, Long> translogOps = new HashMap<>();
+
         final Map<ShardId, ByteSizeValue> indexingBuffers = new HashMap<>();
         final Map<ShardId, ByteSizeValue> translogBuffers = new HashMap<>();
 
-        final Map<ShardId, Long> lastIndexTimeNanos = new HashMap<>();
-        final Set<ShardId> activeShards = new HashSet<>();
-
         long currentTimeSec = TimeValue.timeValueNanos(System.nanoTime()).seconds();
 
         public MockController(Settings settings) {
             super(Settings.builder()
                             .put(SHARD_INACTIVE_INTERVAL_TIME_SETTING, "200h") // disable it
-                            .put(SHARD_INACTIVE_TIME_SETTING, "1ms") // nearly immediate
+                            .put(SHARD_INACTIVE_TIME_SETTING, "0s") // immediate
                             .put(settings)
                             .build(),
                     null, null, 100 * 1024 * 1024); // fix jvm mem size to 100mb
         }
 
+        public void incTranslog(ShardId shard1, int id, int ops) {
+            setTranslog(shard1, translogIds.get(shard1) + id, translogOps.get(shard1) + ops);
+        }
+
+        public void setTranslog(ShardId id, long translogId, long ops) {
+            translogIds.put(id, translogId);
+            translogOps.put(id, ops);
+        }
+
         public void deleteShard(ShardId id) {
+            translogIds.remove(id);
+            translogOps.remove(id);
             indexingBuffers.remove(id);
             translogBuffers.remove(id);
         }
 
+        public void assertActive(ShardId id) {
+            assertThat(indexingBuffers.get(id), not(equalTo(INACTIVE)));
+            assertThat(translogBuffers.get(id), not(equalTo(INACTIVE)));
+        }
+
         public void assertBuffers(ShardId id, ByteSizeValue indexing, ByteSizeValue translog) {
             assertThat(indexingBuffers.get(id), equalTo(indexing));
             assertThat(translogBuffers.get(id), equalTo(translog));
@@ -82,53 +94,43 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         @Override
         protected List<ShardId> availableShards() {
-            return new ArrayList<>(indexingBuffers.keySet());
+            return new ArrayList<>(translogIds.keySet());
         }
 
         @Override
         protected boolean shardAvailable(ShardId shardId) {
-            return indexingBuffers.containsKey(shardId);
+            return translogIds.containsKey(shardId);
         }
 
         @Override
-        protected Boolean getShardActive(ShardId shardId) {
-            return activeShards.contains(shardId);
+        protected void markShardAsInactive(ShardId shardId) {
+            indexingBuffers.put(shardId, INACTIVE);
+            translogBuffers.put(shardId, INACTIVE);
         }
 
         @Override
-        protected void updateShardBuffers(ShardId shardId, ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
-            indexingBuffers.put(shardId, shardIndexingBufferSize);
-            translogBuffers.put(shardId, shardTranslogBufferSize);
+        protected ShardIndexingStatus getTranslogStatus(ShardId shardId) {
+            if (!shardAvailable(shardId)) {
+                return null;
+            }
+            ShardIndexingStatus status = new ShardIndexingStatus();
+            status.translogId = translogIds.get(shardId);
+            status.translogNumberOfOperations = translogOps.get(shardId);
+            return status;
         }
 
         @Override
-        protected Boolean checkIdle(ShardId shardId, long inactiveTimeNS) {
-            Long ns = lastIndexTimeNanos.get(shardId);
-            if (ns == null) {
-                return null;
-            } else if (currentTimeInNanos() - ns >= inactiveTimeNS) {
-                indexingBuffers.put(shardId, INACTIVE);
-                translogBuffers.put(shardId, INACTIVE);
-                activeShards.remove(shardId);
-                return true;
-            } else {
-                return false;
-            }
+        protected void updateShardBuffers(ShardId shardId, ByteSizeValue shardIndexingBufferSize, ByteSizeValue shardTranslogBufferSize) {
+            indexingBuffers.put(shardId, shardIndexingBufferSize);
+            translogBuffers.put(shardId, shardTranslogBufferSize);
         }
 
         public void incrementTimeSec(int sec) {
             currentTimeSec += sec;
         }
 
-        public void simulateIndexing(ShardId shardId) {
-            lastIndexTimeNanos.put(shardId, currentTimeInNanos());
-            if (indexingBuffers.containsKey(shardId) == false) {
-                // First time we are seeing this shard; start it off with inactive buffers as IndexShard does:
-                indexingBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_INDEXING_BUFFER);
-                translogBuffers.put(shardId, IndexingMemoryController.INACTIVE_SHARD_TRANSLOG_BUFFER);
-            }
-            activeShards.add(shardId);
-            forceCheck();
+        public void simulateFlush(ShardId shard) {
+            setTranslog(shard, translogIds.get(shard) + 1, 0);
         }
     }
 
@@ -137,12 +139,14 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .put(IndexingMemoryController.INDEX_BUFFER_SIZE_SETTING, "10mb")
                 .put(IndexingMemoryController.TRANSLOG_BUFFER_SIZE_SETTING, "100kb").build());
         final ShardId shard1 = new ShardId("test", 1);
-        controller.simulateIndexing(shard1);
+        controller.setTranslog(shard1, randomInt(10), randomInt(10));
+        controller.forceCheck();
         controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
 
         // add another shard
         final ShardId shard2 = new ShardId("test", 2);
-        controller.simulateIndexing(shard2);
+        controller.setTranslog(shard2, randomInt(10), randomInt(10));
+        controller.forceCheck();
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
@@ -157,7 +161,8 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
         // add a new one
         final ShardId shard3 = new ShardId("test", 3);
-        controller.simulateIndexing(shard3);
+        controller.setTranslog(shard3, randomInt(10), randomInt(10));
+        controller.forceCheck();
         controller.assertBuffers(shard3, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB)); // translog is maxed at 64K
     }
 
@@ -169,42 +174,48 @@ public class IndexingMemoryControllerTests extends ESTestCase {
                 .build());
 
         final ShardId shard1 = new ShardId("test", 1);
-        controller.simulateIndexing(shard1);
+        controller.setTranslog(shard1, 0, 0);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.simulateIndexing(shard2);
+        controller.setTranslog(shard2, 0, 0);
+        controller.forceCheck();
         controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
         controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
         // index into both shards, move the clock and see that they are still active
-        controller.simulateIndexing(shard1);
-        controller.simulateIndexing(shard2);
-
+        controller.setTranslog(shard1, randomInt(2), randomInt(2) + 1);
+        controller.setTranslog(shard2, randomInt(2) + 1, randomInt(2));
+        // the controller doesn't know when the ops happened, so even if this is more
+        // than the inactive time the shard is still marked as active
         controller.incrementTimeSec(10);
         controller.forceCheck();
+        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
-        // both shards now inactive
-        controller.assertInActive(shard1);
-        controller.assertInActive(shard2);
-
-        // index into one shard only, see it becomes active
-        controller.simulateIndexing(shard1);
-        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
-        controller.assertInActive(shard2);
-
-        controller.incrementTimeSec(3); // increment but not enough to become inactive
+        // index into one shard only, see other shard is made inactive correctly
+        controller.incTranslog(shard1, randomInt(2), randomInt(2) + 1);
+        controller.forceCheck(); // register what happened with the controller (shard is still active)
+        controller.incrementTimeSec(3); // increment but not enough
         controller.forceCheck();
-        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
-        controller.assertInActive(shard2);
+        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
 
         controller.incrementTimeSec(3); // increment some more
         controller.forceCheck();
-        controller.assertInActive(shard1);
+        controller.assertBuffers(shard1, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
         controller.assertInActive(shard2);
 
+        if (randomBoolean()) {
+            // once a shard gets inactive it will be synced flushed and a new translog generation will be made
+            controller.simulateFlush(shard2);
+            controller.forceCheck();
+            controller.assertInActive(shard2);
+        }
+
         // index some and shard becomes immediately active
-        controller.simulateIndexing(shard2);
-        controller.assertInActive(shard1);
-        controller.assertBuffers(shard2, new ByteSizeValue(10, ByteSizeUnit.MB), new ByteSizeValue(64, ByteSizeUnit.KB));
+        controller.incTranslog(shard2, randomInt(2), 1 + randomInt(2)); // we must make sure translog ops is never 0
+        controller.forceCheck();
+        controller.assertBuffers(shard1, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
+        controller.assertBuffers(shard2, new ByteSizeValue(5, ByteSizeUnit.MB), new ByteSizeValue(50, ByteSizeUnit.KB));
     }
 
     public void testMinShardBufferSizes() {
@@ -262,9 +273,10 @@ public class IndexingMemoryControllerTests extends ESTestCase {
 
     protected void assertTwoActiveShards(MockController controller, ByteSizeValue indexBufferSize, ByteSizeValue translogBufferSize) {
         final ShardId shard1 = new ShardId("test", 1);
-        controller.simulateIndexing(shard1);
+        controller.setTranslog(shard1, 0, 0);
         final ShardId shard2 = new ShardId("test", 2);
-        controller.simulateIndexing(shard2);
+        controller.setTranslog(shard2, 0, 0);
+        controller.forceCheck();
         controller.assertBuffers(shard1, indexBufferSize, translogBufferSize);
         controller.assertBuffers(shard2, indexBufferSize, translogBufferSize);
 
diff --git a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
index aa7cd45..1cdac14 100644
--- a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
+++ b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
@@ -104,16 +104,6 @@ public class MockScriptEngine implements ScriptEngineService {
     }
 
     @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        return null;
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return null;
-    }
-
-    @Override
     public void scriptRemoved(@Nullable CompiledScript script) {
     }
 
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index c14c2e8..efda8d2 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -286,16 +286,6 @@ public class ScriptModesTests extends ESTestCase {
         }
 
         @Override
-        public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-            return null;
-        }
-
-        @Override
-        public Object unwrap(Object value) {
-            return null;
-        }
-
-        @Override
         public void close() {
 
         }
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
index a0afe8f..26ba580 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
@@ -497,16 +497,6 @@ public class ScriptServiceTests extends ESTestCase {
         }
 
         @Override
-        public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-            return null;
-        }
-
-        @Override
-        public Object unwrap(Object value) {
-            return null;
-        }
-
-        @Override
         public void close() {
 
         }
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
index b44d180..28ae808 100644
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
@@ -54,7 +54,7 @@ public class MustacheScriptEngineTests extends ESTestCase {
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
             Map<String, Object> vars = new HashMap<>();
             vars.put("boost_val", "0.3");
-            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
             assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
                     new String(o.toBytes(), Charset.forName("UTF-8")));
@@ -65,7 +65,7 @@ public class MustacheScriptEngineTests extends ESTestCase {
             Map<String, Object> vars = new HashMap<>();
             vars.put("boost_val", "0.3");
             vars.put("body_val", "\"quick brown\"");
-            BytesReference o = (BytesReference) qe.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars);
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
             assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
                     new String(o.toBytes(), Charset.forName("UTF-8")));
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index c2c2782..983fb52 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -823,33 +823,4 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         }
     }
-
-    @Test
-    public void testExplainString() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
-                        .endObject().startObject("num").field("type", "double").endObject().endObject().endObject().endObject()));
-        ensureYellow();
-
-
-        client().prepareIndex().setType("type1").setId("1").setIndex("test")
-                .setSource(jsonBuilder().startObject().field("test", "value").array("num", 0.5, 0.7).endObject()).get();
-
-        refresh();
-
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true)
-                                .query(functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 1.0, 5.0, 1.0)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 1.0, 5.0, 1.0))
-                                }).boostMode(CombineFunction.REPLACE)))).get();
-        String explanation = response.getHits().getAt(0).getExplanation().toString();
-        assertThat(explanation, containsString(" 1.0 = exp(-0.5*pow(MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)],2.0)/18.033688011112044)"));
-        assertThat(explanation, containsString("1.0 = max(0.0, ((10.0 - MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)])/10.0)"));
-        assertThat(explanation, containsString("1.0 = exp(- MIN[Math.max(Math.abs(0.5(=doc value) - 1.0(=origin))) - 1.0(=offset), 0), Math.max(Math.abs(0.7(=doc value) - 1.0(=origin))) - 1.0(=offset), 0)] * 0.13862943611198905)"));
-
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
index 5296e76..93c29e0 100644
--- a/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
+++ b/core/src/test/java/org/elasticsearch/versioning/SimpleVersioningIT.java
@@ -18,15 +18,6 @@
  */
 package org.elasticsearch.versioning;
 
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicInteger;
-
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.action.bulk.BulkResponse;
@@ -37,12 +28,15 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
 import org.elasticsearch.index.engine.FlushNotAllowedEngineException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicInteger;
+
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
@@ -100,7 +94,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // deleting with a lower version works.
-        long v= randomIntBetween(12,14);
+        long v = randomIntBetween(12, 14);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.FORCE).get();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -136,7 +130,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         // Delete with a higher or equal version deletes all versions up to the given one.
-        long v= randomIntBetween(14,17);
+        long v = randomIntBetween(14, 17);
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(v).setVersionType(VersionType.EXTERNAL_GTE).execute().actionGet();
         assertThat(deleteResponse.isFound(), equalTo(true));
         assertThat(deleteResponse.getVersion(), equalTo(v));
@@ -165,7 +159,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(indexResponse.getVersion(), equalTo(14l));
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(13).setVersionType(VersionType.EXTERNAL).execute(),
-                     VersionConflictEngineException.class);
+                VersionConflictEngineException.class);
 
         if (randomBoolean()) {
             refresh();
@@ -176,8 +170,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // deleting with a lower version fails.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
         // Delete with a higher version deletes all versions up to the given one.
         DeleteResponse deleteResponse = client().prepareDelete("test", "type", "1").setVersion(17).setVersionType(VersionType.EXTERNAL).execute().actionGet();
@@ -186,8 +180,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Deleting with a lower version keeps on failing after a delete.
         assertThrows(
-            client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
-            VersionConflictEngineException.class);
+                client().prepareDelete("test", "type", "1").setVersion(2).setVersionType(VersionType.EXTERNAL).execute(),
+                VersionConflictEngineException.class);
 
 
         // But delete with a higher version is OK.
@@ -206,8 +200,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         assertThat(deleteResponse.getVersion(), equalTo(20l));
 
         // Make sure that the next delete will be GC. Note we do it on the index settings so it will be cleaned up
-        HashMap<String,Object> newSettings = new HashMap<>();
-        newSettings.put("index.gc_deletes",-1);
+        HashMap<String, Object> newSettings = new HashMap<>();
+        newSettings.put("index.gc_deletes", -1);
         client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
 
         Thread.sleep(300); // gc works based on estimated sampled time. Give it a chance...
@@ -221,7 +215,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
     public void testRequireUnitsOnUpdateSettings() throws Exception {
         createIndex("test");
         ensureGreen();
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "42");
         try {
             client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet();
@@ -262,22 +256,12 @@ public class SimpleVersioningIT extends ESIntegTestCase {
                 VersionConflictEngineException.class);
 
         assertThrows(
-            client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
-            VersionConflictEngineException.class);
-
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
-        assertThrows(
-            client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+                client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
-        assertThrows(
-                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(2).execute(),
-                DocumentAlreadyExistsException.class);
+                client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
+                VersionConflictEngineException.class);
 
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -334,10 +318,8 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         assertThrows(client().prepareIndex("test", "type", "1").setSource("field1", "value1_1").setVersion(1).execute(),
                 VersionConflictEngineException.class);
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
-                VersionConflictEngineException.class);
 
-        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").setVersion(1).execute(),
+        assertThrows(client().prepareIndex("test", "type", "1").setCreate(true).setSource("field1", "value1_1").execute(),
                 VersionConflictEngineException.class);
 
         assertThrows(client().prepareDelete("test", "type", "1").setVersion(1).execute(), VersionConflictEngineException.class);
@@ -377,90 +359,94 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         IDSource ids;
         final Random random = getRandom();
         switch (random.nextInt(6)) {
-        case 0:
-            // random simple
-            if (VERBOSE) {
-                System.out.println("TEST: use random simple ids");
-            }
-            ids = new IDSource() {
+            case 0:
+                // random simple
+                if (VERBOSE) {
+                    System.out.println("TEST: use random simple ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomSimpleString(random);
                     }
                 };
-            break;
-        case 1:
-            // random realistic unicode
-            if (VERBOSE) {
-                System.out.println("TEST: use random realistic unicode ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 1:
+                // random realistic unicode
+                if (VERBOSE) {
+                    System.out.println("TEST: use random realistic unicode ids");
+                }
+                ids = new IDSource() {
                     @Override
                     public String next() {
                         return TestUtil.randomRealisticUnicodeString(random);
                     }
                 };
-            break;
-        case 2:
-            // sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 2:
+                // sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use seuquential ids");
+                }
+                ids = new IDSource() {
                     int upto;
+
                     @Override
                     public String next() {
                         return Integer.toString(upto++);
                     }
                 };
-            break;
-        case 3:
-            // zero-pad sequential
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad seuquential ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 3:
+                // zero-pad sequential
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad seuquential ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%0" + TestUtil.nextInt(random, 4, 20) + "d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         String s = Integer.toString(upto++);
                         return zeroPad.substring(zeroPad.length() - s.length()) + s;
                     }
                 };
-            break;
-        case 4:
-            // random long
-            if (VERBOSE) {
-                System.out.println("TEST: use random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 4:
+                // random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        case 5:
-            // zero-pad random long
-            if (VERBOSE) {
-                System.out.println("TEST: use zero-pad random long ids");
-            }
-            ids = new IDSource() {
+                break;
+            case 5:
+                // zero-pad random long
+                if (VERBOSE) {
+                    System.out.println("TEST: use zero-pad random long ids");
+                }
+                ids = new IDSource() {
                     final int radix = TestUtil.nextInt(random, Character.MIN_RADIX, Character.MAX_RADIX);
                     final String zeroPad = String.format(Locale.ROOT, "%015d", 0);
                     int upto;
+
                     @Override
                     public String next() {
                         return Long.toString(random.nextLong() & 0x3ffffffffffffffL, radix);
                     }
                 };
-            break;
-        default:
-            throw new AssertionError();
+                break;
+            default:
+                throw new AssertionError();
         }
 
         return ids;
@@ -530,7 +516,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
             } else {
                 sb.append("  response: null");
             }
-            
+
             return sb.toString();
         }
     }
@@ -547,7 +533,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         // TODO: not great we don't test deletes GC here:
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "1000000h");
         assertAcked(client().admin().indices().prepareUpdateSettings("test").setSettings(newSettings).execute().actionGet());
 
@@ -584,14 +570,14 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Attach random versions to them:
         long version = 0;
-        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs/2, numIDs*(TEST_NIGHTLY ? 8 : 2))];
-        final Map<String,IDAndVersion> truth = new HashMap<>();
+        final IDAndVersion[] idVersions = new IDAndVersion[TestUtil.nextInt(random, numIDs / 2, numIDs * (TEST_NIGHTLY ? 8 : 2))];
+        final Map<String, IDAndVersion> truth = new HashMap<>();
 
         if (VERBOSE) {
             System.out.println("TEST: use " + numIDs + " ids; " + idVersions.length + " operations");
         }
 
-        for(int i=0;i<idVersions.length;i++) {
+        for (int i = 0; i < idVersions.length; i++) {
 
             if (useMonotonicVersion) {
                 version += TestUtil.nextInt(random, 1, 10);
@@ -612,7 +598,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         // Shuffle
-        for(int i = idVersions.length - 1; i > 0; i--) {
+        for (int i = idVersions.length - 1; i > 0; i--) {
             int index = random.nextInt(i + 1);
             IDAndVersion x = idVersions[index];
             idVersions[index] = idVersions[i];
@@ -620,7 +606,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         }
 
         if (VERBOSE) {
-            for(IDAndVersion idVersion : idVersions) {
+            for (IDAndVersion idVersion : idVersions) {
                 System.out.println("id=" + idVersion.id + " version=" + idVersion.version + " delete?=" + idVersion.delete + " truth?=" + (truth.get(idVersion.id) == idVersion));
             }
         }
@@ -629,109 +615,87 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         final CountDownLatch startingGun = new CountDownLatch(1);
         Thread[] threads = new Thread[TestUtil.nextInt(random, 1, TEST_NIGHTLY ? 20 : 5)];
         final long startTime = System.nanoTime();
-        for(int i=0;i<threads.length;i++) {
+        for (int i = 0; i < threads.length; i++) {
             final int threadID = i;
             threads[i] = new Thread() {
-                    @Override
-                    public void run() {
-                        try {
-                            //final Random threadRandom = RandomizedContext.current().getRandom();
-                            final Random threadRandom = getRandom();
-                            startingGun.await();
-                            while (true) {
-
-                                // TODO: sometimes use bulk:
-
-                                int index = upto.getAndIncrement();
-                                if (index >= idVersions.length) {
-                                    break;
-                                }
-                                if (VERBOSE && index % 100 == 0) {
-                                    System.out.println(Thread.currentThread().getName() + ": index=" + index);
-                                }
-                                IDAndVersion idVersion = idVersions[index];
-
-                                String id = idVersion.id;
-                                idVersion.threadID = threadID;
-                                idVersion.indexStartTime = System.nanoTime()-startTime;
-                                long version = idVersion.version;
-                                if (idVersion.delete) {
-                                    try {
-                                        idVersion.response = client().prepareDelete("test", "type", id)
+                @Override
+                public void run() {
+                    try {
+                        //final Random threadRandom = RandomizedContext.current().getRandom();
+                        final Random threadRandom = getRandom();
+                        startingGun.await();
+                        while (true) {
+
+                            // TODO: sometimes use bulk:
+
+                            int index = upto.getAndIncrement();
+                            if (index >= idVersions.length) {
+                                break;
+                            }
+                            if (VERBOSE && index % 100 == 0) {
+                                System.out.println(Thread.currentThread().getName() + ": index=" + index);
+                            }
+                            IDAndVersion idVersion = idVersions[index];
+
+                            String id = idVersion.id;
+                            idVersion.threadID = threadID;
+                            idVersion.indexStartTime = System.nanoTime() - startTime;
+                            long version = idVersion.version;
+                            if (idVersion.delete) {
+                                try {
+                                    idVersion.response = client().prepareDelete("test", "type", id)
                                             .setVersion(version)
                                             .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                    } catch (VersionConflictEngineException vcee) {
-                                        // OK: our version is too old
-                                        assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                        idVersion.versionConflict = true;
-                                    }
-                                } else {
-                                    for (int x=0;x<2;x++) {
-                                        // Try create first:
-                                    
-                                        IndexRequest.OpType op;
-                                        if (x == 0) {
-                                            op = IndexRequest.OpType.CREATE;
-                                        } else {
-                                            op = IndexRequest.OpType.INDEX;
-                                        }
-                                    
-                                        // index document
-                                        try {
-                                            idVersion.response = client().prepareIndex("test", "type", id)
-                                                .setSource("foo", "bar")
-                                                .setOpType(op)
-                                                .setVersion(version)
-                                                .setVersionType(VersionType.EXTERNAL).execute().actionGet();
-                                            break;
-                                        } catch (DocumentAlreadyExistsException daee) {
-                                            if (x == 0) {
-                                                // OK: id was already indexed by another thread, now use index:
-                                                idVersion.alreadyExists = true;
-                                            } else {
-                                                // Should not happen with op=INDEX:
-                                                throw daee;
-                                            }
-                                        } catch (VersionConflictEngineException vcee) {
-                                            // OK: our version is too old
-                                            assertThat(version, lessThanOrEqualTo(truth.get(id).version));
-                                            idVersion.versionConflict = true;
-                                        }
-                                    }
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                idVersion.indexFinishTime = System.nanoTime()-startTime;
-
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime()-startTime));
-                                    refresh();
-                                    System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime()-startTime));
+                            } else {
+                                try {
+                                    idVersion.response = client().prepareIndex("test", "type", id)
+                                            .setSource("foo", "bar")
+                                            .setVersion(version).setVersionType(VersionType.EXTERNAL).get();
+
+                                } catch (VersionConflictEngineException vcee) {
+                                    // OK: our version is too old
+                                    assertThat(version, lessThanOrEqualTo(truth.get(id).version));
+                                    idVersion.versionConflict = true;
                                 }
-                                if (threadRandom.nextInt(100) == 7) {
-                                    System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime()-startTime));
-                                    try {
-                                        flush();
-                                    } catch (FlushNotAllowedEngineException fnaee) {
-                                        // OK
-                                    }
-                                    System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime()-startTime));
+                            }
+                            idVersion.indexFinishTime = System.nanoTime() - startTime;
+
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now refresh at " + (System.nanoTime() - startTime));
+                                refresh();
+                                System.out.println(threadID + ": TEST: refresh done at " + (System.nanoTime() - startTime));
+                            }
+                            if (threadRandom.nextInt(100) == 7) {
+                                System.out.println(threadID + ": TEST: now flush at " + (System.nanoTime() - startTime));
+                                try {
+                                    flush();
+                                } catch (FlushNotAllowedEngineException fnaee) {
+                                    // OK
                                 }
+                                System.out.println(threadID + ": TEST: flush done at " + (System.nanoTime() - startTime));
                             }
-                        } catch (Exception e) {
-                            throw new RuntimeException(e);
                         }
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
                     }
-                };
+                }
+            };
             threads[i].start();
         }
 
         startingGun.countDown();
-        for(Thread thread : threads) {
+        for (Thread thread : threads) {
             thread.join();
         }
 
         // Verify against truth:
         boolean failed = false;
-        for(String id : ids) {
+        for (String id : ids) {
             long expected;
             IDAndVersion idVersion = truth.get(id);
             if (idVersion != null && idVersion.delete == false) {
@@ -748,7 +712,7 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         if (failed) {
             System.out.println("All versions:");
-            for(int i=0;i<idVersions.length;i++) {
+            for (int i = 0; i < idVersions.length; i++) {
                 System.out.println("i=" + i + " " + idVersions[i]);
             }
             fail("wrong versions for some IDs");
@@ -760,36 +724,36 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // We require only one shard for this test, so that the 2nd delete provokes pruning the deletes map:
         client()
-            .admin()
-            .indices()
-            .prepareCreate("test")
-            .setSettings(Settings.settingsBuilder()
-                         .put("index.number_of_shards", 1))
-            .execute().
-            actionGet();
+                .admin()
+                .indices()
+                .prepareCreate("test")
+                .setSettings(Settings.settingsBuilder()
+                        .put("index.number_of_shards", 1))
+                .execute().
+                actionGet();
 
         ensureGreen();
 
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "10ms");
         newSettings.put("index.refresh_interval", "-1");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -798,20 +762,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
 
         // ThreadPool.estimatedTimeInMillis has default granularity of 200 msec, so we must sleep at least that long; sleep much longer in
         // case system is busy:
@@ -819,20 +783,20 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete an unrelated doc (provokes pruning deletes from versionMap)
         client()
-            .prepareDelete("test", "type", "id2")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id2")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should still reflect delete:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 
     @Test
@@ -842,25 +806,25 @@ public class SimpleVersioningIT extends ESIntegTestCase {
         ensureGreen();
 
         // We test deletes, but can't rely on wall-clock delete GC:
-        HashMap<String,Object> newSettings = new HashMap<>();
+        HashMap<String, Object> newSettings = new HashMap<>();
         newSettings.put("index.gc_deletes", "0ms");
         client()
-            .admin()
-            .indices()
-            .prepareUpdateSettings("test")
-            .setSettings(newSettings)
-            .execute()
-            .actionGet();
+                .admin()
+                .indices()
+                .prepareUpdateSettings("test")
+                .setSettings(newSettings)
+                .execute()
+                .actionGet();
 
         // Index a doc:
         client()
-            .prepareIndex("test", "type", "id")
-            .setSource("foo", "bar")
-            .setOpType(IndexRequest.OpType.INDEX)
-            .setVersion(10)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareIndex("test", "type", "id")
+                .setSource("foo", "bar")
+                .setOpType(IndexRequest.OpType.INDEX)
+                .setVersion(10)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         if (randomBoolean()) {
             // Force refresh so the add is sometimes visible in the searcher:
@@ -869,19 +833,19 @@ public class SimpleVersioningIT extends ESIntegTestCase {
 
         // Delete it
         client()
-            .prepareDelete("test", "type", "id")
-            .setVersion(11)
-            .setVersionType(VersionType.EXTERNAL)
-            .execute()
-            .actionGet();
+                .prepareDelete("test", "type", "id")
+                .setVersion(11)
+                .setVersionType(VersionType.EXTERNAL)
+                .execute()
+                .actionGet();
 
         // Real-time get should reflect delete even though index.gc_deletes is 0:
         assertThat("doc should have been deleted",
-                   client()
-                   .prepareGet("test", "type", "id")
-                   .execute()
-                   .actionGet()
-                   .getVersion(),
-                   equalTo(-1L));
+                client()
+                        .prepareGet("test", "type", "id")
+                        .execute()
+                        .actionGet()
+                        .getVersion(),
+                equalTo(-1L));
     }
 }
diff --git a/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip
new file mode 100644
index 0000000..44e78b4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/index-2.0.0-rc1.zip differ
diff --git a/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip
new file mode 100644
index 0000000..b15cedf
Binary files /dev/null and b/core/src/test/resources/indices/bwc/repo-2.0.0-rc1.zip differ
diff --git a/dev-tools/prepare_release_candidate.py b/dev-tools/prepare_release_candidate.py
index 5f83893..24450a6 100644
--- a/dev-tools/prepare_release_candidate.py
+++ b/dev-tools/prepare_release_candidate.py
@@ -63,7 +63,7 @@ To install the deb from an APT repo:
 
 APT line sources.list line:
 
-deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/debian/ stable main
+deb http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/debian/ stable main
 
 To install the RPM, create a YUM file like:
 
@@ -73,7 +73,7 @@ containing:
 
 [elasticsearch-2.0]
 name=Elasticsearch repository for packages
-baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/centos
+baseurl=http://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
@@ -300,7 +300,7 @@ if __name__ == "__main__":
   ensure_checkout_is_clean()
   if not re.match('(\d+\.\d+)\.*',release_version):
     raise RuntimeError('illegal release version format: %s' % (release_version))
-  major_minor_version = re.match('(\d+\.\d+)\.*',release_version).group(1)
+  package_repo_version = '%s.x' % re.match('(\d+)\.*', release_version).group(1)
 
   print('*** Preparing release version: [%s]' % release_version)
 
@@ -348,13 +348,13 @@ if __name__ == "__main__":
   # repository push commands
   s3cmd_sync_to_staging_bucket_cmd = 's3cmd sync -P %s s3://%s/elasticsearch/staging/%s-%s/org/' % (localRepoElasticsearch, bucket, release_version, shortHash)
   s3_bucket_sync_to = '%s/elasticsearch/staging/%s-%s/repos/' % (bucket, release_version, shortHash)
-  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (major_minor_version, s3_bucket_sync_to)
+  s3cmd_sync_official_repo_cmd = 's3cmd sync s3://packages.elasticsearch.org/elasticsearch/%s s3://%s' % (package_repo_version, s3_bucket_sync_to)
 
-  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, major_minor_version)
+  debs3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/debian' % (release_version, shortHash, package_repo_version)
   debs3_upload_cmd = 'deb-s3 upload --preserve-versions %s/distribution/deb/elasticsearch/%s/elasticsearch-%s.deb -b %s --prefix %s --sign %s --arch amd64' % (localRepoElasticsearch, release_version, release_version, bucket, debs3_prefix, gpg_key)
   debs3_list_cmd = 'deb-s3 list -b %s --prefix %s' % (bucket, debs3_prefix)
   debs3_verify_cmd = 'deb-s3 verify -b %s --prefix %s' % (bucket, debs3_prefix)
-  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, major_minor_version)
+  rpms3_prefix = 'elasticsearch/staging/%s-%s/repos/%s/centos' % (release_version, shortHash, package_repo_version)
   rpms3_upload_cmd = 'rpm-s3 -v -b %s -p %s --sign --visibility public-read -k 0 %s' % (bucket, rpms3_prefix, rpm)
 
   if deploy_s3:
@@ -397,7 +397,7 @@ if __name__ == "__main__":
     print('NOTE: Running s3cmd might require you to create a config file with your credentials, if the s3cmd does not support suppliying them via the command line!')
 
   print('*** Once the release is deployed and published send out the following mail to dev@elastic.co:')
-  string_format_dict = {'version' : release_version, 'hash': shortHash, 'major_minor_version' : major_minor_version, 'bucket': bucket}
+  string_format_dict = {'version' : release_version, 'hash': shortHash, 'package_repo_version' : package_repo_version, 'bucket': bucket}
   print(MAIL_TEMPLATE % string_format_dict)
 
   print('')
@@ -406,7 +406,7 @@ if __name__ == "__main__":
 
   print('')
   print('To publish the release and the repo on S3 execute the following commands:')
-  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(major_minor_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(major_minor_version)s'  % string_format_dict)
+  print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/repos/%(package_repo_version)s/ s3://packages.elasticsearch.org/elasticsearch/%(package_repo_version)s'  % string_format_dict)
   print('   s3cmd cp --recursive s3://%(bucket)s/elasticsearch/staging/%(version)s-%(hash)s/org/ s3://%(bucket)s/elasticsearch/release/org'  % string_format_dict)
   print('Now go ahead and tag the release:')
   print('   git tag -a v%(version)s %(hash)s'  % string_format_dict)
diff --git a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
index 67d139e..1588e11 100644
--- a/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
+++ b/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties
@@ -24,7 +24,7 @@
 # jvm=true
 # classname=foo.bar.BazPlugin
 # description=My cool plugin
-# version=2.0
+# version=2.0.0-rc1
 # elasticsearch.version=2.0
 # java.version=1.7
 #
@@ -64,6 +64,10 @@ classname=${elasticsearch.plugin.classname}
 java.version=${maven.compiler.target}
 #
 # 'elasticsearch.version' version of elasticsearch compiled against
+# You will have to release a new version of the plugin for each new
+# elasticsearch release. This version is checked when the plugin
+# is loaded so Elasticsearch will refuse to start in the presence of
+# plugins with the incorrect elasticsearch.version.
 elasticsearch.version=${elasticsearch.version}
 #
 ### deprecated elements for jvm plugins :
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index 7237674..182398d 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -76,6 +76,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -110,7 +111,7 @@
                                 <data>
                                     <src>${project.build.directory}/generated-packaging/deb/bin</src>
                                     <type>directory</type>
-                                    <includes>elasticsearch,elasticsearch.in.sh,plugin</includes>
+                                    <includes>elasticsearch,elasticsearch.in.sh,plugin,elasticsearch-systemd-pre-exec</includes>
                                     <mapper>
                                         <type>perm</type>
                                         <prefix>${packaging.elasticsearch.bin.dir}</prefix>
diff --git a/distribution/deb/src/main/packaging/init.d/elasticsearch b/distribution/deb/src/main/packaging/init.d/elasticsearch
index 9ea2beb..3a82bbe 100755
--- a/distribution/deb/src/main/packaging/init.d/elasticsearch
+++ b/distribution/deb/src/main/packaging/init.d/elasticsearch
@@ -74,9 +74,6 @@ DATA_DIR=/var/lib/$NAME
 # Elasticsearch configuration directory
 CONF_DIR=/etc/$NAME
 
-# Elasticsearch configuration file (elasticsearch.yml)
-CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Maximum number of VMA (Virtual Memory Areas) a process can own
 MAX_MAP_COUNT=262144
 
@@ -93,10 +90,16 @@ if [ -f "$DEFAULT" ]; then
 	. "$DEFAULT"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 # Define other required variables
 PID_FILE="$PID_DIR/$NAME.pid"
 DAEMON=$ES_HOME/bin/elasticsearch
-DAEMON_OPTS="-d -p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
+DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
 
 export ES_HEAP_SIZE
 export ES_HEAP_NEWSIZE
diff --git a/distribution/deb/src/main/packaging/packaging.properties b/distribution/deb/src/main/packaging/packaging.properties
index f268cde..3635928 100644
--- a/distribution/deb/src/main/packaging/packaging.properties
+++ b/distribution/deb/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/default/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=deb
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index 37f7203..1e3004c 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -79,6 +79,7 @@
                                         <include>bin/elasticsearch</include>
                                         <include>bin/elasticsearch.in.sh</include>
                                         <include>bin/plugin</include>
+                                        <include>bin/elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </resource>
                             </resources>
@@ -127,6 +128,7 @@
                                         <include>elasticsearch</include>
                                         <include>elasticsearch.in.sh</include>
                                         <include>plugin</include>
+                                        <include>elasticsearch-systemd-pre-exec</include>
                                     </includes>
                                 </source>
                             </sources>
diff --git a/distribution/rpm/src/main/packaging/init.d/elasticsearch b/distribution/rpm/src/main/packaging/init.d/elasticsearch
index 9626dfc..924c678 100644
--- a/distribution/rpm/src/main/packaging/init.d/elasticsearch
+++ b/distribution/rpm/src/main/packaging/init.d/elasticsearch
@@ -40,7 +40,7 @@ MAX_MAP_COUNT=${packaging.os.max.map.count}
 LOG_DIR="${packaging.elasticsearch.log.dir}"
 DATA_DIR="${packaging.elasticsearch.data.dir}"
 CONF_DIR="${packaging.elasticsearch.conf.dir}"
-CONF_FILE="${packaging.elasticsearch.conf.dir}/elasticsearch.yml"
+
 PID_DIR="${packaging.elasticsearch.pid.dir}"
 
 # Source the default env file
@@ -49,6 +49,12 @@ if [ -f "$ES_ENV_FILE" ]; then
     . "$ES_ENV_FILE"
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 exec="$ES_HOME/bin/elasticsearch"
 prog="elasticsearch"
 pidfile="$PID_DIR/${prog}.pid"
@@ -83,7 +89,6 @@ checkJava() {
 start() {
     checkJava
     [ -x $exec ] || exit 5
-    [ -f $CONF_FILE ] || exit 6
     if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then
         echo "MAX_LOCKED_MEMORY is set - ES_HEAP_SIZE must also be set"
         return 7
diff --git a/distribution/rpm/src/main/packaging/packaging.properties b/distribution/rpm/src/main/packaging/packaging.properties
index b5bf28a..bc4af5f 100644
--- a/distribution/rpm/src/main/packaging/packaging.properties
+++ b/distribution/rpm/src/main/packaging/packaging.properties
@@ -6,7 +6,6 @@ packaging.env.file=/etc/sysconfig/elasticsearch
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=${packaging.elasticsearch.conf.dir}
-packaging.plugin.default.config.file=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 
 # Simple marker to check that properties are correctly overridden
 packaging.type=rpm
diff --git a/distribution/src/main/packaging/env/elasticsearch b/distribution/src/main/packaging/env/elasticsearch
index cdf05bb..0c01d4f 100644
--- a/distribution/src/main/packaging/env/elasticsearch
+++ b/distribution/src/main/packaging/env/elasticsearch
@@ -8,9 +8,6 @@
 # Elasticsearch configuration directory
 #CONF_DIR=${packaging.elasticsearch.conf.dir}
 
-# Elasticsearch configuration file
-#CONF_FILE=$CONF_DIR/elasticsearch.yml
-
 # Elasticsearch data directory
 #DATA_DIR=${packaging.elasticsearch.data.dir}
 
diff --git a/distribution/src/main/packaging/packaging.properties b/distribution/src/main/packaging/packaging.properties
index ff95c9d..be5b604 100644
--- a/distribution/src/main/packaging/packaging.properties
+++ b/distribution/src/main/packaging/packaging.properties
@@ -8,7 +8,6 @@ packaging.env.file=
 
 # Default configuration directory and file to use in bin/plugin script
 packaging.plugin.default.config.dir=$ES_HOME/config
-packaging.plugin.default.config.file=$ES_HOME/config/elasticsearch.yml
 
 # Default values for min/max heap memory allocated to elasticsearch java process
 packaging.elasticsearch.heap.min=256m
diff --git a/distribution/src/main/packaging/systemd/elasticsearch.service b/distribution/src/main/packaging/systemd/elasticsearch.service
index cdcad9d..d8f56f7 100644
--- a/distribution/src/main/packaging/systemd/elasticsearch.service
+++ b/distribution/src/main/packaging/systemd/elasticsearch.service
@@ -7,7 +7,6 @@ After=network-online.target
 [Service]
 Environment=ES_HOME=${packaging.elasticsearch.home.dir}
 Environment=CONF_DIR=${packaging.elasticsearch.conf.dir}
-Environment=CONF_FILE=${packaging.elasticsearch.conf.dir}/elasticsearch.yml
 Environment=DATA_DIR=${packaging.elasticsearch.data.dir}
 Environment=LOG_DIR=${packaging.elasticsearch.log.dir}
 Environment=PID_DIR=${packaging.elasticsearch.pid.dir}
@@ -18,12 +17,13 @@ WorkingDirectory=${packaging.elasticsearch.home.dir}
 User=${packaging.elasticsearch.user}
 Group=${packaging.elasticsearch.group}
 
+ExecStartPre=${packaging.elasticsearch.bin.dir}/elasticsearch-systemd-pre-exec
+
 ExecStart=${packaging.elasticsearch.bin.dir}/elasticsearch \
                                                 -Des.pidfile=${PID_DIR}/elasticsearch.pid \
                                                 -Des.default.path.home=${ES_HOME} \
                                                 -Des.default.path.logs=${LOG_DIR} \
                                                 -Des.default.path.data=${DATA_DIR} \
-                                                -Des.default.config=${CONF_FILE} \
                                                 -Des.default.path.conf=${CONF_DIR}
 
 # Connects standard output to /dev/null
diff --git a/distribution/src/main/resources/bin/elasticsearch b/distribution/src/main/resources/bin/elasticsearch
index 878fcff..66f4657 100755
--- a/distribution/src/main/resources/bin/elasticsearch
+++ b/distribution/src/main/resources/bin/elasticsearch
@@ -42,10 +42,10 @@
 # Be aware that you will be entirely responsible for populating the needed
 # environment variables.
 
-
 # Maven will replace the project.name with elasticsearch below. If that
 # hasn't been done, we assume that this is not a packaged version and the
 # user has forgotten to run Maven to create a package.
+
 IS_PACKAGED_VERSION='${project.parent.artifactId}'
 if [ "$IS_PACKAGED_VERSION" != "distributions" ]; then
     cat >&2 << EOF
diff --git a/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
new file mode 100755
index 0000000..a51d639
--- /dev/null
+++ b/distribution/src/main/resources/bin/elasticsearch-systemd-pre-exec
@@ -0,0 +1,7 @@
+#!/bin/sh
+
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
diff --git a/distribution/src/main/resources/bin/plugin b/distribution/src/main/resources/bin/plugin
index c466d48..35dbe3a 100755
--- a/distribution/src/main/resources/bin/plugin
+++ b/distribution/src/main/resources/bin/plugin
@@ -1,5 +1,6 @@
 #!/bin/sh
 
+
 CDPATH=""
 SCRIPT="$0"
 
@@ -21,17 +22,10 @@ ES_HOME=`dirname "$SCRIPT"`/..
 # make ELASTICSEARCH_HOME absolute
 ES_HOME=`cd "$ES_HOME"; pwd`
 
+
 # Sets the default values for elasticsearch variables used in this script
 if [ -z "$CONF_DIR" ]; then
   CONF_DIR="${packaging.plugin.default.config.dir}"
-
-  if [ -z "$CONF_FILE" ]; then
-    CONF_FILE="$CONF_DIR/elasticsearch.yml"
-  fi
-fi
-
-if [ -z "$CONF_FILE" ]; then
-  CONF_FILE="${packaging.plugin.default.config.file}"
 fi
 
 # The default env file is defined at building/packaging time.
@@ -66,6 +60,12 @@ if [ "x$JAVA_TOOL_OPTIONS" != "x" ]; then
     unset JAVA_TOOL_OPTIONS
 fi
 
+# CONF_FILE setting was removed
+if [ ! -z "$CONF_FILE" ]; then
+    echo "CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed."
+    exit 1
+fi
+
 if [ -x "$JAVA_HOME/bin/java" ]; then
     JAVA=$JAVA_HOME/bin/java
 else
@@ -105,16 +105,6 @@ if [ -e "$CONF_DIR" ]; then
   esac
 fi
 
-if [ -e "$CONF_FILE" ]; then
-  case "$properties" in
-    *-Des.default.config=*|*-Des.config=*)
-    ;;
-    *)
-      properties="$properties -Des.default.config=\"$CONF_FILE\""
-    ;;
-  esac
-fi
-
 # full hostname passed through cut for portability on systems that do not support hostname -s
 # export on separate line for shells that do not support combining definition and export
 HOSTNAME=`hostname | cut -d. -f1`
diff --git a/distribution/src/main/resources/bin/service.bat b/distribution/src/main/resources/bin/service.bat
index 06c9c64..9822e6b 100644
--- a/distribution/src/main/resources/bin/service.bat
+++ b/distribution/src/main/resources/bin/service.bat
@@ -5,6 +5,8 @@ TITLE Elasticsearch Service ${project.version}
 
 if NOT DEFINED JAVA_HOME goto err
 
+if not "%CONF_FILE%" == "" goto conffileset
+
 set SCRIPT_DIR=%~dp0
 for %%I in ("%SCRIPT_DIR%..") do set ES_HOME=%%~dpfI
 
@@ -147,9 +149,7 @@ if "%DATA_DIR%" == "" set DATA_DIR=%ES_HOME%\data
 
 if "%CONF_DIR%" == "" set CONF_DIR=%ES_HOME%\config
 
-if "%CONF_FILE%" == "" set CONF_FILE=%ES_HOME%\config\elasticsearch.yml
-
-set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.config="%CONF_FILE%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
+set ES_PARAMS=-Delasticsearch;-Des.path.home="%ES_HOME%";-Des.default.path.home="%ES_HOME%";-Des.default.path.logs="%LOG_DIR%";-Des.default.path.data="%DATA_DIR%";-Des.default.path.conf="%CONF_DIR%"
 
 set JVM_OPTS=%JAVA_OPTS: =;%
 
@@ -207,4 +207,8 @@ set /a conv=%conv% * 1024
 set "%~2=%conv%"
 goto:eof
 
+:conffileset
+echo CONF_FILE setting is no longer supported. elasticsearch.yml must be placed in the config directory and cannot be renamed.
+goto:eof
+
 ENDLOCAL
diff --git a/docs/plugins/authors.asciidoc b/docs/plugins/authors.asciidoc
index 2ce05fd..e0db081 100644
--- a/docs/plugins/authors.asciidoc
+++ b/docs/plugins/authors.asciidoc
@@ -43,6 +43,72 @@ instance, see
 https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/pom.xml[`plugins/site-example/pom.xml`].
 
 [float]
+==== Mandatory elements for all plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`description`              |String  | simple summary of the plugin
+
+|`version`                  |String  | plugin's version
+
+|`name`                     |String  | the plugin name
+
+|=======================================================================
+
+
+
+[float]
+==== Mandatory elements for Java plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`jvm`                      |Boolean | true if the `classname` class should be loaded
+from jar files in the root directory of the plugin.
+Note that only jar files in the root directory are added to the classpath for the plugin!
+If you need other resources, package them into a resources jar.
+
+|`classname`                |String  | the name of the class to load, fully-qualified.
+
+|`java.version`             |String  | version of java the code is built against.
+Use the system property `java.specification.version`. Version string must be a sequence
+of nonnegative decimal integers separated by "."'s and may have leading zeros.
+
+|`elasticsearch.version`    |String  | version of elasticsearch compiled against.
+
+|=======================================================================
+
+[IMPORTANT]
+.Plugin release lifecycle
+==============================================
+
+You will have to release a new version of the plugin for each new elasticsearch release.
+This version is checked when the plugin is loaded so Elasticsearch will refuse to start
+in the presence of plugins with the incorrect `elasticsearch.version`.
+
+==============================================
+
+
+[float]
+==== Mandatory elements for Site plugins
+
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Element                    | Type   | Description
+
+|`site`                     |Boolean | true to indicate contents of the `_site/`
+directory in the root of the plugin should be served.
+
+|=======================================================================
+
+
+[float]
 === Testing your plugin
 
 When testing a Java plugin, it will only be auto-loaded if it is in the
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
index fa1ac07..5ac2085 100644
--- a/docs/plugins/discovery-ec2.asciidoc
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -165,6 +165,37 @@ The following are a list of settings (prefixed with `discovery.ec2`) that can fu
     Defaults to `3s`. If no unit like `ms`, `s` or `m` is specified,
     milliseconds are used.
 
+
+[IMPORTANT]
+.Binding the network host
+==============================================
+
+It's important to define `network.host` as by default it's bound to `localhost`.
+
+You can use {ref}/modules-network.html[core network host settings] or
+<<discovery-ec2-network-host,ec2 specific host settings>>:
+
+==============================================
+
+[[discovery-ec2-network-host]]
+===== EC2 Network Host
+
+When the `discovery-ec2` plugin is installed, the following are also allowed
+as valid network host settings:
+
+[cols="<,<",options="header",]
+|==================================================================
+|EC2 Host Value |Description
+|`_ec2:privateIpv4_` |The private IP address (ipv4) of the machine.
+|`_ec2:privateDns_` |The private host of the machine.
+|`_ec2:publicIpv4_` |The public IP address (ipv4) of the machine.
+|`_ec2:publicDns_` |The public host of the machine.
+|`_ec2:privateIp_` |equivalent to _ec2:privateIpv4_.
+|`_ec2:publicIp_` |equivalent to _ec2:publicIpv4_.
+|`_ec2_` |equivalent to _ec2:privateIpv4_.
+|==================================================================
+
+
 [[discovery-ec2-permissions]]
 ===== Recommended EC2 Permissions
 
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
index bae6613..52ff574 100644
--- a/docs/plugins/plugin-script.asciidoc
+++ b/docs/plugins/plugin-script.asciidoc
@@ -131,6 +131,8 @@ Plugins can be removed manually, by deleting the appropriate directory under
 sudo bin/plugin remove [pluginname]
 -----------------------------------
 
+After a Java plugin has been removed, you will need to restart the node to complete the removal process.
+
 === Other command line parameters
 
 The `plugin` scripts supports a number of other command line parameters:
diff --git a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
index cdd7601..5afff0c 100644
--- a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
@@ -281,7 +281,7 @@ had a value.
 {
     "aggs" : {
         "publish_date" : {
-             "datehistogram" : {
+             "date_histogram" : {
                  "field" : "publish_date",
                  "interval": "year",
                  "missing": "2000-01-01" <1>
diff --git a/docs/reference/aggregations/pipeline.asciidoc b/docs/reference/aggregations/pipeline.asciidoc
index 4410db3..e4cdae5 100644
--- a/docs/reference/aggregations/pipeline.asciidoc
+++ b/docs/reference/aggregations/pipeline.asciidoc
@@ -2,8 +2,6 @@
 
 == Pipeline Aggregations
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Pipeline aggregations work on the outputs produced from other aggregations rather than from document sets, adding
diff --git a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
index b2b9d93f..541ffec 100644
--- a/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/avg-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-avg-bucket-aggregation]]
 === Avg Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the (mean) average value of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
index 81372c1..b1bbfcd 100644
--- a/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-script-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-script-aggregation]]
 === Bucket Script Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which can perform per bucket computations on specified metrics 
diff --git a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
index cef1e67..2d80abc 100644
--- a/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/bucket-selector-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-bucket-selector-aggregation]]
 === Bucket Selector Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which executes a script which determines whether the current bucket will be retained 
diff --git a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
index 823c5c8..e29dbbe 100644
--- a/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/cumulative-sum-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-cumulative-sum-aggregation]]
 === Cumulative Sum Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the cumulative sum of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
index 48296ca..f68a811 100644
--- a/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/derivative-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-derivative-aggregation]]
 === Derivative Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A parent pipeline aggregation which calculates the derivative of a specified metric in a parent histogram (or date_histogram) 
diff --git a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
index bbf610a..0a44685 100644
--- a/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/extended-stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-extended-stats-bucket-aggregation]]
 === Extended Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
index 310a643..96094d0 100644
--- a/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/max-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-max-bucket-aggregation]]
 === Max Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the maximum value of a specified metric in a sibling aggregation
diff --git a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
index 11d3d55..c970384 100644
--- a/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/min-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-min-bucket-aggregation]]
 === Min Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which identifies the bucket(s) with the minimum value of a specified metric in a sibling aggregation 
diff --git a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
index 6fe91cb..968c596 100644
--- a/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/movavg-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-movavg-aggregation]]
 === Moving Average Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Given an ordered series of data, the Moving Average aggregation will slide a window across the data and emit the average
diff --git a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
index 2476969..4e6423f 100644
--- a/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/percentiles-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-percentiles-bucket-aggregation]]
 === Percentiles Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates percentiles across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
index 7193510..17cfea9 100644
--- a/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/serial-diff-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-serialdiff-aggregation]]
 === Serial Differencing Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 Serial differencing is a technique where values in a time series are subtracted from itself at
diff --git a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
index 7d6d24d..f524032 100644
--- a/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/stats-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-stats-bucket-aggregation]]
 === Stats Bucket Aggregation
 
-coming[2.1.0]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates a variety of stats across all bucket of a specified metric in a sibling aggregation.
diff --git a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
index 56d786f..52022b3 100644
--- a/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
+++ b/docs/reference/aggregations/pipeline/sum-bucket-aggregation.asciidoc
@@ -1,8 +1,6 @@
 [[search-aggregations-pipeline-sum-bucket-aggregation]]
 === Sum Bucket Aggregation
 
-coming[2.0.0-beta1]
-
 experimental[]
 
 A sibling pipeline aggregation which calculates the sum across all bucket of a specified metric in a sibling aggregation. 
diff --git a/docs/reference/docs/bulk.asciidoc b/docs/reference/docs/bulk.asciidoc
index b9b7d47..ef066eb 100644
--- a/docs/reference/docs/bulk.asciidoc
+++ b/docs/reference/docs/bulk.asciidoc
@@ -131,8 +131,6 @@ operation based on the `_parent` / `_routing` mapping.
 [[bulk-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 Each bulk item can include the timestamp value using the
 `_timestamp`/`timestamp` field. It automatically follows the behavior of
 the index operation based on the `_timestamp` mapping.
@@ -141,8 +139,6 @@ the index operation based on the `_timestamp` mapping.
 [[bulk-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Each bulk item can include the ttl value using the `_ttl`/`ttl` field.
 It automatically follows the behavior of the index operation based on
 the `_ttl` mapping.
diff --git a/docs/reference/docs/index_.asciidoc b/docs/reference/docs/index_.asciidoc
index ab542c5..089af3e 100644
--- a/docs/reference/docs/index_.asciidoc
+++ b/docs/reference/docs/index_.asciidoc
@@ -257,8 +257,6 @@ specified using the `routing` parameter.
 [[index-timestamp]]
 === Timestamp
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 A document can be indexed with a `timestamp` associated with it. The
 `timestamp` value of a document can be set using the `timestamp`
 parameter. For example:
@@ -281,8 +279,6 @@ page>>.
 [[index-ttl]]
 === TTL
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 
 A document can be indexed with a `ttl` (time to live) associated with
 it. Expired documents will be expunged automatically. The expiration
diff --git a/docs/reference/docs/termvectors.asciidoc b/docs/reference/docs/termvectors.asciidoc
index 7530ff7..0e10843 100644
--- a/docs/reference/docs/termvectors.asciidoc
+++ b/docs/reference/docs/termvectors.asciidoc
@@ -81,8 +81,6 @@ omit :
 [float]
 ==== Distributed frequencies
 
-coming[2.0.0-beta1]
-
 Setting `dfs` to `true` (default is `false`) will return the term statistics
 or the field statistics of the entire index, and not just at the shard. Use it
 with caution as distributed frequencies can have a serious performance impact.
@@ -90,8 +88,6 @@ with caution as distributed frequencies can have a serious performance impact.
 [float]
 ==== Terms Filtering
 
-coming[2.0.0-beta1]
-
 With the parameter `filter`, the terms returned could also be filtered based
 on their tf-idf scores. This could be useful in order find out a good
 characteristic vector of a document. This feature works in a similar manner to
diff --git a/docs/reference/getting-started.asciidoc b/docs/reference/getting-started.asciidoc
index 2731fca..a30046e 100755
--- a/docs/reference/getting-started.asciidoc
+++ b/docs/reference/getting-started.asciidoc
@@ -862,7 +862,7 @@ In the previous section, we skipped over a little detail called the document sco
 
 But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.
 
-The <<query-dsl-range-query,`range` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
+The <<query-dsl-bool-query,`bool` query>> that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the <<query-dsl-range-query,`range` query>>, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
 
 This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.
 
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index e36a66a..8e34747 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -1,11 +1,12 @@
 [[elasticsearch-reference]]
 = Elasticsearch Reference
 
-:version:   2.0.0-beta1
-:branch:    2.0
-:jdk:       1.8.0_25
-:defguide:  https://www.elastic.co/guide/en/elasticsearch/guide/current
-:plugins:   https://www.elastic.co/guide/en/elasticsearch/plugins/master
+:version:       3.0.0-beta1
+:major-version: 3.x
+:branch:        3.0
+:jdk:           1.8.0_25
+:defguide:      https://www.elastic.co/guide/en/elasticsearch/guide/current
+:plugins:       https://www.elastic.co/guide/en/elasticsearch/plugins/master
 
 include::getting-started.asciidoc[]
 
diff --git a/docs/reference/indices/analyze.asciidoc b/docs/reference/indices/analyze.asciidoc
index d17f409..1a256a6 100644
--- a/docs/reference/indices/analyze.asciidoc
+++ b/docs/reference/indices/analyze.asciidoc
@@ -16,8 +16,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 If text parameter is provided as array of strings, it is analyzed as a multi-valued field.
 
 [source,js]
@@ -29,8 +27,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Or by building a custom transient analyzer out of tokenizers,
 token filters and char filters. Token filters can use the shorter 'filters'
 parameter name:
@@ -53,8 +49,6 @@ curl -XGET 'localhost:9200/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 It can also run against a specific index:
 
 [source,js]
@@ -78,8 +72,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Also, the analyzer can be derived based on a field mapping, for example:
 
 [source,js]
@@ -91,8 +83,6 @@ curl -XGET 'localhost:9200/test/_analyze' -d '
 }'
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 Will cause the analysis to happen based on the analyzer configured in the
 mapping for `obj1.field1` (and if not, the default index analyzer).
 
diff --git a/docs/reference/mapping/fields/parent-field.asciidoc b/docs/reference/mapping/fields/parent-field.asciidoc
index 22e46c4..64f4a99 100644
--- a/docs/reference/mapping/fields/parent-field.asciidoc
+++ b/docs/reference/mapping/fields/parent-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-parent-field]]
 === `_parent` field
 
-added[2.0.0-beta1,The parent-child implementation has been completely rewritten. It is advisable to reindex any 1.x indices which use parent-child to take advantage of the new optimizations]
-
 A parent-child relationship can be established between documents in the same
 index by making one mapping type the parent of another:
 
diff --git a/docs/reference/mapping/fields/timestamp-field.asciidoc b/docs/reference/mapping/fields/timestamp-field.asciidoc
index 3f4bf8a..5971a02 100644
--- a/docs/reference/mapping/fields/timestamp-field.asciidoc
+++ b/docs/reference/mapping/fields/timestamp-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-timestamp-field]]
 === `_timestamp` field
 
-deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
-
 The `_timestamp` field, when enabled, allows a timestamp to be indexed and
 stored with a document. The timestamp may be specified manually, generated
 automatically, or set to a default value:
diff --git a/docs/reference/mapping/fields/ttl-field.asciidoc b/docs/reference/mapping/fields/ttl-field.asciidoc
index 5394d28..07ce8a8 100644
--- a/docs/reference/mapping/fields/ttl-field.asciidoc
+++ b/docs/reference/mapping/fields/ttl-field.asciidoc
@@ -1,8 +1,6 @@
 [[mapping-ttl-field]]
 === `_ttl` field
 
-deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
-
 Some types of documents, such as session data or special offers, come with an
 expiration date. The `_ttl` field allows you to specify the minimum time a
 document should live, after which time the document is deleted automatically.
diff --git a/docs/reference/migration/migrate_2_0/java.asciidoc b/docs/reference/migration/migrate_2_0/java.asciidoc
index 9871df4..65bfaef 100644
--- a/docs/reference/migration/migrate_2_0/java.asciidoc
+++ b/docs/reference/migration/migrate_2_0/java.asciidoc
@@ -32,6 +32,13 @@ Settings settings = Settings.settingsBuilder()
 Client client = TransportClient.builder().settings(settings).build();
 --------------------------------------------------
 
+==== Exception are only thrown on total failure
+
+Previously, many APIs would throw an exception if any shard failed to execute
+the request. Now the exception is only thrown if all shards fail the request.
+The responses for these APIs will always have a `getShardFailures` method that
+you can and should check for failures.
+
 ==== Automatically thread client listeners
 
 Previously, the user had to set request listener threads to `true` when on the
@@ -109,7 +116,7 @@ new InetSocketTransportAddress(new InetSocketAddress("127.0.0.1", 0));
 Elasticsearch used to shade its dependencies and to relocate packages. We no longer use shading or relocation.
 You might need to change your imports to the original package names:
 
-* `com.google.common` was `org.elasticsearch.common` 
+* `com.google.common` was `org.elasticsearch.common`
 * `com.carrotsearch.hppc` was `org.elasticsearch.common.hppc`
 * `jsr166e` was `org.elasticsearch.common.util.concurrent.jsr166e`
 * `com.fasterxml.jackson` was `org.elasticsearch.common.jackson`
@@ -121,4 +128,3 @@ You might need to change your imports to the original package names:
 * `com.tdunning.math.stats` was `org.elasticsearch.common.stats`
 * `org.apache.commons.lang` was `org.elasticsearch.common.lang`
 * `org.apache.commons.cli` was `org.elasticsearch.common.cli.commons`
- 
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index 17d36c5..0be16cb 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -164,3 +164,13 @@ the `logging.yml` configuration file with the `file.layout.conversionPattern`
 setting.
 
 Remove mapping.date.round_ceil setting for date math parsing #8889 (issues: #8556, #8598)
+
+==== Custom config file
+
+It is no longer possible to specify a custom config file with the `CONF_FILE`
+environment variable, or the `-Des.config`, `-Des.default.config`, or
+`-Delasticsearch.config` parameters.
+
+Instead, the config file must be named `elasticsearch.yml` and must be located
+in the default `config/` directory, or in the directory specified in the
+`CONF_DIR` environment variable.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 897098f..db61b30 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -266,3 +266,8 @@ of string values: see `FilterFunctionScoreQuery.ScoreMode` and `CombineFunction`
 
 For simplicity, only one way of adding the ids to the existing list (empty by default)  is left: `addIds(String...)`
 
+==== DocumentAlreadyExistsException removed
+
+`DocumentAlreadyExistsException` is removed and a `VersionConflictException` is thrown instead (with a better
+error description). This will influence code that use the `IndexRequest.opType()` or `IndexRequest.create()`
+to index a document only if it doesn't already exist.
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index dc6aca7..70b4d8c 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -51,20 +51,8 @@ provided network interface. For example `_en0:ipv4_`.
 provided network interface. For example `_en0:ipv6_`.
 |=======================================================================
 
-When the `discovery-ec2` plugin is installed, the following are also allowed
-as valid network host settings:
-
-[cols="<,<",options="header",]
-|==================================================================
-|EC2 Host Value |Description
-|`_ec2:privateIpv4_` |The private IP address (ipv4) of the machine.
-|`_ec2:privateDns_` |The private host of the machine.
-|`_ec2:publicIpv4_` |The public IP address (ipv4) of the machine.
-|`_ec2:publicDns_` |The public host of the machine.
-|`_ec2_` |Less verbose option for the private ip address.
-|`_ec2:privateIp_` |Less verbose option for the private ip address.
-|`_ec2:publicIp_` |Less verbose option for the public ip address.
-|==================================================================
+When the `discovery-ec2` plugin is installed, you can use
+{plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[ec2 specific host settings].
 
 [float]
 [[tcp-settings]]
diff --git a/docs/reference/modules/scripting.asciidoc b/docs/reference/modules/scripting.asciidoc
index 7729ce2..7b2e43c 100644
--- a/docs/reference/modules/scripting.asciidoc
+++ b/docs/reference/modules/scripting.asciidoc
@@ -624,16 +624,3 @@ power of the second argument.
 |`hypot(x, y)` |Returns sqrt(_x2_ + _y2_) without intermediate overflow
 or underflow.
 |=======================================================================
-
-[float]
-=== Arithmetic precision in MVEL
-
-When dividing two numbers using MVEL based scripts, the engine tries to
-be smart and adheres to the default behaviour of java. This means if you
-divide two integers (you might have configured the fields as integer in
-the mapping), the result will also be an integer. This means, if a
-calculation like `1/num` is happening in your scripts and `num` is an
-integer with the value of `8`, the result is `0` even though you were
-expecting it to be `0.125`. You may need to enforce precision by
-explicitly using a double like `1.0/num` in order to get the expected
-result.
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index 4830e43..50ee4df 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -121,7 +121,7 @@ The following settings are supported:
  using size value notation, i.e. 1g, 10m, 5k. Defaults to `null` (unlimited chunk size).
 `max_restore_bytes_per_sec`:: Throttles per node restore rate. Defaults to `40mb` per second.
 `max_snapshot_bytes_per_sec`:: Throttles per node snapshot rate. Defaults to `40mb` per second.
-`readonly`:: Makes repository read-only. coming[2.1.0]  Defaults to `false`.
+`readonly`:: Makes repository read-only.  Defaults to `false`.
 
 [float]
 ===== Read-only URL Repository
@@ -259,7 +259,7 @@ GET /_snapshot/my_backup/_all
 -----------------------------------
 // AUTOSENSE
 
-coming[2.0.0-beta1] A currently running snapshot can be retrieved using the following command:
+A currently running snapshot can be retrieved using the following command:
 
 [source,sh]
 -----------------------------------
diff --git a/docs/reference/query-dsl/mlt-query.asciidoc b/docs/reference/query-dsl/mlt-query.asciidoc
index 9c42881..ee4b695 100644
--- a/docs/reference/query-dsl/mlt-query.asciidoc
+++ b/docs/reference/query-dsl/mlt-query.asciidoc
@@ -149,7 +149,7 @@ input, the other one for term selection and for query formation.
 ==== Document Input Parameters
 
 [horizontal]
-`like`:: coming[2.0.0-beta1]
+`like`::
 The only *required* parameter of the MLT query is `like` and follows a
 versatile syntax, in which the user can specify free form text and/or a single
 or multiple documents (see examples above). The syntax to specify documents is
@@ -162,7 +162,7 @@ follows a similar syntax to the `per_field_analyzer` parameter of the
 Additionally, to provide documents not necessarily present in the index,
 <<docs-termvectors-artificial-doc,artificial documents>> are also supported.
 
-`unlike`:: coming[2.0.0-beta1] 
+`unlike`:: 
 The `unlike` parameter is used in conjunction with `like` in order not to
 select terms found in a chosen set of documents. In other words, we could ask
 for documents `like: "Apple"`, but `unlike: "cake crumble tree"`. The syntax
@@ -172,10 +172,10 @@ is the same as `like`.
 A list of fields to fetch and analyze the text from. Defaults to the `_all`
 field for free text and to all possible fields for document inputs.
 
-`like_text`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`like_text`::
 The text to find documents like it.
 
-`ids` or `docs`:: deprecated[2.0.0-beta1,Replaced by `like`]
+`ids` or `docs`::
 A list of documents following the same syntax as the <<docs-multi-get,Multi GET API>>.
 
 [float]
diff --git a/docs/reference/search/request/scroll.asciidoc b/docs/reference/search/request/scroll.asciidoc
index 2921441..825564d 100644
--- a/docs/reference/search/request/scroll.asciidoc
+++ b/docs/reference/search/request/scroll.asciidoc
@@ -63,8 +63,6 @@ curl -XGET <1> 'localhost:9200/_search/scroll' <2> -d'
 '
 --------------------------------------------------
 
-coming[2.0.0-beta1, body based parameters were added in 2.0.0]
-
 <1> `GET` or `POST` can be used.
 <2> The URL should not include the `index` or `type` name -- these
     are specified in the original `search` request instead.
@@ -151,8 +149,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 Multiple scroll IDs can be passed as array:
 
 [source,js]
@@ -163,8 +159,6 @@ curl -XDELETE localhost:9200/_search/scroll -d '
 }'
 ---------------------------------------
 
-coming[2.0.0-beta1, Body based parameters were added in 2.0.0]
-
 All search contexts can be cleared with the `_all` parameter:
 
 [source,js]
diff --git a/docs/reference/setup/as-a-service.asciidoc b/docs/reference/setup/as-a-service.asciidoc
index 01bbd2d..50454ca 100644
--- a/docs/reference/setup/as-a-service.asciidoc
+++ b/docs/reference/setup/as-a-service.asciidoc
@@ -22,7 +22,6 @@ Each package features a configuration file, which allows you to set the followin
 `LOG_DIR`::               Log directory, defaults to `/var/log/elasticsearch`
 `DATA_DIR`::              Data directory, defaults to `/var/lib/elasticsearch`
 `CONF_DIR`::              Configuration file directory (which needs to include `elasticsearch.yml` and `logging.yml` files), defaults to `/etc/elasticsearch`
-`CONF_FILE`::             Path to configuration file, defaults to `/etc/elasticsearch/elasticsearch.yml`
 `ES_JAVA_OPTS`::          Any additional java options you may want to apply. This may be useful, if you need to set the `node.name` property, but do not want to change the `elasticsearch.yml` configuration file, because it is distributed via a provisioning system like puppet or chef. Example: `ES_JAVA_OPTS="-Des.node.name=search-01"`
 `RESTART_ON_UPGRADE`::    Configure restart on package upgrade, defaults to `false`. This means you will have to restart your elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster.
 `ES_GC_LOG_FILE` ::       The absolute log file path for creating a garbage collection logfile, which is done by the JVM. Note that this logfile can grow pretty quick and thus is disabled by default.
@@ -72,9 +71,9 @@ sudo service elasticsearch start
 
 
 [float]
-===== Using systemd
+==== Using systemd
 
-Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch`. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
+Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch` if the system is rpm based and `/etc/default/elasticsearch` if it is deb. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
 
 [source,sh]
 --------------------------------------------------
diff --git a/docs/reference/setup/configuration.asciidoc b/docs/reference/setup/configuration.asciidoc
index 45c384b..0768891 100644
--- a/docs/reference/setup/configuration.asciidoc
+++ b/docs/reference/setup/configuration.asciidoc
@@ -298,14 +298,6 @@ Enter value for [node.name]:
 NOTE: Elasticsearch will not start if `${prompt.text}` or `${prompt.secret}`
 is used in the settings and the process is run as a service or in the background.
 
-The location of the configuration file can be set externally using a
-system property:
-
-[source,sh]
---------------------------------------------------
-$ elasticsearch -Des.config=/path/to/config/file
---------------------------------------------------
-
 [float]
 [[configuration-index-settings]]
 === Index Settings
diff --git a/docs/reference/setup/repositories.asciidoc b/docs/reference/setup/repositories.asciidoc
index 79b8959..70b000e 100644
--- a/docs/reference/setup/repositories.asciidoc
+++ b/docs/reference/setup/repositories.asciidoc
@@ -6,7 +6,7 @@ binary packages, but no source packages, as the packages are created as part of
 build.
 
 We have split the major versions in separate urls to avoid accidental upgrades across major version.
-For all 0.90.x releases use 0.90 as version number, for 1.0.x use 1.0, for 1.1.x use 1.1 etc.
+For all 2.x releases use 2.x as version number, for 3.x.y use 3.x etc...
 
 We use the PGP key https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
 Elasticsearch Signing Key, with fingerprint
@@ -25,11 +25,11 @@ Download and install the Public Signing Key:
 wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
 --------------------------------------------------
 
-Save the repository definition to  `/etc/apt/sources.list.d/elasticsearch-{branch}.list`:
+Save the repository definition to  +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+:
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-echo "deb http://packages.elastic.co/elasticsearch/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{branch}.list
+echo "deb http://packages.elastic.co/elasticsearch/{major-version}/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-{major-version}.list
 --------------------------------------------------
 
 [WARNING]
@@ -57,9 +57,9 @@ If two entries exist for the same Elasticsearch repository, you will see an erro
 
 ["literal",subs="attributes,callouts"]
 
-Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{branch}/debian/ ...`
+Duplicate sources.list entry http://packages.elastic.co/elasticsearch/{major-version}/debian/ ...`
 
-Examine +/etc/apt/sources.list.d/elasticsearch-{branch}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
+Examine +/etc/apt/sources.list.d/elasticsearch-{major-version}.list+ for the duplicate entry or locate the duplicate entry amongst the files in `/etc/apt/sources.list.d/` and the `/etc/apt/sources.list` file.
 ==================================================
 
 Configure Elasticsearch to automatically start during bootup. If your
@@ -93,9 +93,9 @@ in a file with a `.repo` suffix, for example `elasticsearch.repo`
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-[elasticsearch-{branch}]
-name=Elasticsearch repository for {branch}.x packages
-baseurl=http://packages.elastic.co/elasticsearch/{branch}/centos
+[elasticsearch-{major-version}]
+name=Elasticsearch repository for {major-version} packages
+baseurl=http://packages.elastic.co/elasticsearch/{major-version}/centos
 gpgcheck=1
 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
diff --git a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
index 71668c4..72a1dd7 100644
--- a/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
+++ b/plugins/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
@@ -237,17 +237,6 @@ public class ExpressionScriptEngineService extends AbstractComponent implements
     }
 
     @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        ExpressionExecutableScript expressionExecutableScript = new ExpressionExecutableScript(compiledScript, vars);
-        return expressionExecutableScript.run();
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-
-    @Override
     public void close() {}
 
     @Override
diff --git a/plugins/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java b/plugins/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
index 1644eff..d1e7160 100644
--- a/plugins/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
+++ b/plugins/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
@@ -38,7 +38,6 @@ import org.codehaus.groovy.control.customizers.CompilationCustomizer;
 import org.codehaus.groovy.control.customizers.ImportCustomizer;
 import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.bootstrap.BootstrapInfo;
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.hash.MessageDigests;
@@ -245,25 +244,6 @@ public class GroovyScriptEngineService extends AbstractComponent implements Scri
         };
     }
 
-    @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        try {
-            Map<String, Object> allVars = new HashMap<>();
-            if (vars != null) {
-                allVars.putAll(vars);
-            }
-            Script scriptObject = createScript(compiledScript.compiled(), allVars);
-            return scriptObject.run();
-        } catch (Exception e) {
-            throw new ScriptException("failed to execute " + compiledScript, e);
-        }
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return value;
-    }
-
     public static final class GroovyScript implements ExecutableScript, LeafSearchScript {
 
         private final CompiledScript compiledScript;
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
index 6cba9ae..fa6e91b 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.messy.tests;
 
-import java.nio.charset.StandardCharsets;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.bulk.BulkItemResponse;
@@ -49,21 +47,15 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.concurrent.CyclicBarrier;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class BulkTests extends ESIntegTestCase {
 
@@ -190,8 +182,8 @@ public class BulkTests extends ESIntegTestCase {
         assertThat(((UpdateResponse) bulkResponse.getItems()[2].getResponse()).getVersion(), equalTo(3l));
 
         bulkResponse = client().prepareBulk()
-                .add(client().prepareIndex("test", "type", "e1").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
-                .add(client().prepareIndex("test", "type", "e2").setCreate(true).setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e1").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
+                .add(client().prepareIndex("test", "type", "e2").setSource("field", "1").setVersion(10).setVersionType(VersionType.EXTERNAL))
                 .add(client().prepareIndex("test", "type", "e1").setSource("field", "2").setVersion(12).setVersionType(VersionType.EXTERNAL)).get();
 
         assertTrue(((IndexResponse) bulkResponse.getItems()[0].getResponse()).isCreated());
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
index 99b334e..51fc5a4 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
@@ -19,20 +19,12 @@
 
 package org.elasticsearch.messy.tests;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
-import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
@@ -49,385 +41,26 @@ import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.randomFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
 import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.closeTo;
 import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
 import static org.hamcrest.Matchers.is;
 
 public class FunctionScoreTests extends ESIntegTestCase {
 
     static final String TYPE = "type";
     static final String INDEX = "index";
-    static final String TEXT_FIELD = "text_field";
-    static final String DOUBLE_FIELD = "double_field";
-    static final String GEO_POINT_FIELD = "geo_point_field";
-    static final XContentBuilder SIMPLE_DOC;
-    static final XContentBuilder MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD;
 
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-    
-    @Test
-    public void testExplainQueryOnlyOnce() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate("test").addMapping(
-                "type1",
-                jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
-                        .endObject().startObject("num").field("type", "float").endObject().endObject().endObject().endObject()));
-        ensureYellow();
-
-        client().prepareIndex()
-                .setType("type1")
-                .setId("1")
-                .setIndex("test")
-                .setSource(
-                        jsonBuilder().startObject().field("test", "value").field("num", 10).endObject()).get();
-        refresh();
-
-        SearchResponse response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(exponentialDecayFunction("num", 5, 5)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(linearDecayFunction("num", 5, 5))
-                                })))).get();
-        String explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), fieldValueFactorFunction("num"))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-
-        response = client().search(
-                searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                        searchSource().explain(true).query(
-                                functionScoreQuery(termQuery("test", "value"), randomFunction(10))))).get();
-        explanation = response.getHits().getAt(0).explanation().toString();
-
-        checkQueryExplanationAppearsOnlyOnce(explanation);
-    }
-
-    private void checkQueryExplanationAppearsOnlyOnce(String explanation) {
-        // use some substring of the query explanation and see if it appears twice
-        String queryExplanation = "idf(docFreq=1, maxDocs=1)";
-        int queryExplanationIndex = explanation.indexOf(queryExplanation, 0);
-        assertThat(queryExplanationIndex, greaterThan(-1));
-        queryExplanationIndex = explanation.indexOf(queryExplanation, queryExplanationIndex + 1);
-        assertThat(queryExplanationIndex, equalTo(-1));
-    }
-
-    static {
-        XContentBuilder simpleDoc;
-        XContentBuilder mappingWithDoubleAndGeoPointAndTestField;
-        try {
-            simpleDoc = jsonBuilder().startObject()
-                    .field(TEXT_FIELD, "value")
-                    .startObject(GEO_POINT_FIELD)
-                    .field("lat", 10)
-                    .field("lon", 20)
-                    .endObject()
-                    .field(DOUBLE_FIELD, Math.E)
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        SIMPLE_DOC = simpleDoc;
-        try {
-
-            mappingWithDoubleAndGeoPointAndTestField = jsonBuilder().startObject()
-                    .startObject(TYPE)
-                    .startObject("properties")
-                    .startObject(TEXT_FIELD)
-                    .field("type", "string")
-                    .endObject()
-                    .startObject(GEO_POINT_FIELD)
-                    .field("type", "geo_point")
-                    .endObject()
-                    .startObject(DOUBLE_FIELD)
-                    .field("type", "double")
-                    .endObject()
-                    .endObject()
-                    .endObject()
-                    .endObject();
-        } catch (IOException e) {
-            throw new ElasticsearchException("Exception while initializing FunctionScoreIT", e);
-        }
-        MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD = mappingWithDoubleAndGeoPointAndTestField;
-    }
-
-    @Test
-    public void testExplain() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(3))
-                                })).explain(true))).actionGet();
-
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("6.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  6.0 = min of:\n    6.0 = function score, score mode [multiply]\n      1.0 = function score, product of:\n        1.0 = match filter: *:*\n        1.0 = Function for field geo_point_field:\n          1.0 = exp(-0.5*pow(MIN of: [Math.max(arcDistance([10.0, 20.0](=doc value),[10.0, 20.0](=origin)) - 0.0(=offset), 0)],2.0)/7.213475204444817E11)\n      2.0 = function score, product of:\n        1.0 = match filter: *:*\n        2.0 = product of:\n          1.0 = field value function: ln(doc['double_field'].value * factor=1.0)\n          2.0 = weight\n      3.0 = function score, product of:\n        1.0 = match filter: *:*\n        3.0 = product of:\n          1.0 = script score function, computed with script:\"[script: _index['text_field']['value'].tf(), type: inline, lang: null, params: null]\n            1.0 = _score: \n              1.0 = ConstantScore(text_field:value), product of:\n                1.0 = boost\n                1.0 = queryNorm\n          3.0 = weight\n    3.4028235E38 = maxBoost\n"));
-        responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), weightFactorFunction(4.0f)))
-                                .explain(true))).actionGet();
-        assertThat(
-                responseWithWeights.getHits().getAt(0).getExplanation().toString(),
-                equalTo("4.0 = function score, product of:\n  1.0 = ConstantScore(text_field:value), product of:\n    1.0 = boost\n    1.0 = queryNorm\n  4.0 = min of:\n    4.0 = product of:\n      1.0 = constant score 1.0 - no function provided\n      4.0 = weight\n    3.4028235E38 = maxBoost\n"));
-
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTest() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE, MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD
-        ));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        SearchResponse response = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km")),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")))
-                                })))).actionGet();
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(
-                        searchSource().query(
-                                functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), new FunctionScoreQueryBuilder.FilterFunctionBuilder[]{
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km").setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN).setWeight(2)),
-                                        new FunctionScoreQueryBuilder.FilterFunctionBuilder(scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()")).setWeight(2))
-                                })))).actionGet();
-
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).getScore(), is(1.0f));
-        assertThat(responseWithWeights.getHits().getAt(0).getScore(), is(8.0f));
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestWithRandomWeightsAndRandomCombineMode() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        XContentBuilder doc = SIMPLE_DOC;
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        float[] weights = createRandomWeights(scoreFunctionBuilders.length);
-        float[] scores = getScores(scoreFunctionBuilders);
-        int weightscounter = 0;
-        FunctionScoreQueryBuilder.FilterFunctionBuilder[] filterFunctionBuilders = new FunctionScoreQueryBuilder.FilterFunctionBuilder[scoreFunctionBuilders.length];
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            filterFunctionBuilders[weightscounter] = new FunctionScoreQueryBuilder.FilterFunctionBuilder(builder.setWeight(weights[weightscounter]));
-            weightscounter++;
-        }
-        FiltersFunctionScoreQuery.ScoreMode scoreMode = randomFrom(FiltersFunctionScoreQuery.ScoreMode.AVG, FiltersFunctionScoreQuery.ScoreMode.SUM,
-                FiltersFunctionScoreQuery.ScoreMode.MIN, FiltersFunctionScoreQuery.ScoreMode.MAX, FiltersFunctionScoreQuery.ScoreMode.MULTIPLY);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), filterFunctionBuilders).scoreMode(scoreMode);
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        double expectedScore = computeExpectedScore(weights, scores, scoreMode);
-        assertThat((float) expectedScore / responseWithWeights.getHits().getAt(0).getScore(), is(1.0f));
-    }
-
-    protected double computeExpectedScore(float[] weights, float[] scores, FiltersFunctionScoreQuery.ScoreMode scoreMode) {
-        double expectedScore;
-        switch(scoreMode) {
-            case MULTIPLY:
-                expectedScore = 1.0;
-                break;
-            case MAX:
-                expectedScore = Float.MAX_VALUE * -1.0;
-                break;
-            case MIN:
-                expectedScore = Float.MAX_VALUE;
-                break;
-            default:
-                expectedScore = 0.0;
-                break;
-        }
-
-        float weightSum = 0;
-        for (int i = 0; i < weights.length; i++) {
-            double functionScore = (double) weights[i] * scores[i];
-            weightSum += weights[i];
-            switch(scoreMode) {
-                case AVG:
-                    expectedScore += functionScore;
-                    break;
-                case MAX:
-                    expectedScore = Math.max(functionScore, expectedScore);
-                    break;
-                case MIN:
-                    expectedScore = Math.min(functionScore, expectedScore);
-                    break;
-                case SUM:
-                    expectedScore += functionScore;
-                    break;
-                case MULTIPLY:
-                    expectedScore *= functionScore;
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-        }
-        if (scoreMode == FiltersFunctionScoreQuery.ScoreMode.AVG) {
-            expectedScore /= weightSum;
-        }
-        return expectedScore;
-    }
-
-    @Test
-    public void simpleWeightedFunctionsTestSingleFunction() throws IOException, ExecutionException, InterruptedException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        XContentBuilder doc = jsonBuilder().startObject()
-                .field(TEXT_FIELD, "value")
-                .startObject(GEO_POINT_FIELD)
-                .field("lat", 12)
-                .field("lon", 21)
-                .endObject()
-                .field(DOUBLE_FIELD, 10)
-                .endObject();
-        index(INDEX, TYPE, "1", doc);
-        refresh();
-        ScoreFunctionBuilder[] scoreFunctionBuilders = getScoreFunctionBuilders();
-        ScoreFunctionBuilder scoreFunctionBuilder = scoreFunctionBuilders[randomInt(3)];
-        float[] weights = createRandomWeights(1);
-        float[] scores = getScores(scoreFunctionBuilder);
-        FunctionScoreQueryBuilder withWeights = functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), scoreFunctionBuilder.setWeight(weights[0]));
-
-        SearchResponse responseWithWeights = client().search(
-                searchRequest().source(searchSource().query(withWeights))
-        ).actionGet();
-
-        assertThat( (double) scores[0] * weights[0]/ responseWithWeights.getHits().getAt(0).getScore(), closeTo(1.0, 1.e-6));
-
-    }
-
-    private float[] getScores(ScoreFunctionBuilder... scoreFunctionBuilders) {
-        float[] scores = new float[scoreFunctionBuilders.length];
-        int scorecounter = 0;
-        for (ScoreFunctionBuilder builder : scoreFunctionBuilders) {
-            SearchResponse response = client().search(
-                    searchRequest().source(
-                            searchSource().query(
-                                    functionScoreQuery(constantScoreQuery(termQuery(TEXT_FIELD, "value")), builder)
-                            ))).actionGet();
-            scores[scorecounter] = response.getHits().getAt(0).getScore();
-            scorecounter++;
-        }
-        return scores;
-    }
-
-    private float[] createRandomWeights(int size) {
-        float[] weights = new float[size];
-        for (int i = 0; i < weights.length; i++) {
-            weights[i] = randomFloat() * (randomBoolean() ? 1.0f : -1.0f) * randomInt(100) + 1.e-6f;
-        }
-        return weights;
-    }
-
-    public ScoreFunctionBuilder[] getScoreFunctionBuilders() {
-        ScoreFunctionBuilder[] builders = new ScoreFunctionBuilder[4];
-        builders[0] = gaussDecayFunction(GEO_POINT_FIELD, new GeoPoint(10, 20), "1000km");
-        builders[1] = randomFunction(10);
-        builders[2] = fieldValueFactorFunction(DOUBLE_FIELD).modifier(FieldValueFactorFunction.Modifier.LN);
-        builders[3] = scriptFunction(new Script("_index['" + TEXT_FIELD + "']['value'].tf()"));
-        return builders;
-    }
 
-    @Test
-    public void checkWeightOnlyCreatesBoostFunction() throws IOException {
-        assertAcked(prepareCreate(INDEX).addMapping(
-                TYPE,
-                MAPPING_WITH_DOUBLE_AND_GEO_POINT_AND_TEXT_FIELD));
-        ensureYellow();
-
-        index(INDEX, TYPE, "1", SIMPLE_DOC);
-        refresh();
-        String query =jsonBuilder().startObject()
-                .startObject("query")
-                .startObject("function_score")
-                .startArray("functions")
-                .startObject()
-                .field("weight",2)
-                .endObject()
-                .endArray()
-                .endObject()
-                .endObject()
-                .endObject().string();
-        SearchResponse response = client().search(
-                searchRequest().source(new BytesArray(query))
-                ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-
-        query =jsonBuilder().startObject()
-                .startObject("query")
-                .startObject("function_score")
-                .field("weight",2)
-                .endObject()
-                .endObject()
-                .endObject().string();
-        response = client().search(
-                searchRequest().source(new BytesArray(query))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(new WeightBuilder().setWeight(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-        response = client().search(
-                searchRequest().source(searchSource().query(functionScoreQuery(weightFactorFunction(2.0f))))
-        ).actionGet();
-        assertSearchResponse(response);
-        assertThat(response.getHits().getAt(0).score(), equalTo(2.0f));
-    }
 
     @Test
     public void testScriptScoresNested() throws IOException {
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
index 3c2b230..dc76eda 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
@@ -44,12 +44,7 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Semaphore;
@@ -58,12 +53,7 @@ import java.util.concurrent.TimeUnit;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class UpdateTests extends ESIntegTestCase {
 
@@ -258,25 +248,26 @@ public class UpdateTests extends ESIntegTestCase {
                         .setVersionType(VersionType.EXTERNAL).execute(),
                 ActionRequestValidationException.class);
 
+
+        // With force version
+        client().prepareUpdate(indexOrAlias(), "type", "2")
+                .setScript(new Script("ctx._source.text = 'v10'", ScriptService.ScriptType.INLINE, null, null))
+                .setVersion(10).setVersionType(VersionType.FORCE).get();
+
+        GetResponse get = get("test", "type", "2");
+        assertThat(get.getVersion(), equalTo(10l));
+        assertThat((String) get.getSource().get("text"), equalTo("v10"));
+
         // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
 
         // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
         client().prepareUpdate(indexOrAlias(), "type", "3")
                 .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
                 .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
-        GetResponse get = get("test", "type", "3");
+        get = get("test", "type", "3");
         assertThat(get.getVersion(), equalTo(1l));
         assertThat((String) get.getSource().get("text"), equalTo("v0"));
 
-        // With force version
-        client().prepareUpdate(indexOrAlias(), "type", "4")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setVersionType(VersionType.FORCE).setUpsert("{ \"text\": \"v0\" }").get();
-
-        get = get("test", "type", "4");
-        assertThat(get.getVersion(), equalTo(10l));
-        assertThat((String) get.getSource().get("text"), equalTo("v0"));
-
 
         // retry on conflict is rejected:
         assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 7e8e26c..008a83e 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -1933,14 +1933,14 @@ public class SearchQueryIT extends ESIntegTestCase {
     public void testMinScore() throws ExecutionException, InterruptedException {
         createIndex("test");
 
-        indexRandom(true,
-                client().prepareIndex("test", "test", "1").setSource("score", 1.5),
-                client().prepareIndex("test", "test", "2").setSource("score", 1.0),
-                client().prepareIndex("test", "test", "3").setSource("score", 2.0),
-                client().prepareIndex("test", "test", "4").setSource("score", 0.5));
+        client().prepareIndex("test", "test", "1").setSource("score", 1.5).get();
+        client().prepareIndex("test", "test", "2").setSource("score", 1.0).get();
+        client().prepareIndex("test", "test", "3").setSource("score", 2.0).get();
+        client().prepareIndex("test", "test", "4").setSource("score", 0.5).get();
+        refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test").setQuery(
-                functionScoreQuery(ScoreFunctionBuilders.fieldValueFactorFunction("score")).setMinScore(1.5f)).get();
+                functionScoreQuery(ScoreFunctionBuilders.fieldValueFactorFunction("score").missing(1.0)).setMinScore(1.5f)).get();
         assertHitCount(searchResponse, 2);
         assertFirstHit(searchResponse, hasId("3"));
         assertSecondHit(searchResponse, hasId("1"));
diff --git a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
index 0adf9ca..70fc5f4 100644
--- a/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
+++ b/plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
@@ -178,31 +178,6 @@ public class JavaScriptScriptEngineService extends AbstractComponent implements
         }
     }
 
-    @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        Context ctx = Context.enter();
-        ctx.setWrapFactory(wrapFactory);
-        try {
-            Script script = (Script) compiledScript.compiled();
-            Scriptable scope = ctx.newObject(globalScope);
-            scope.setPrototype(globalScope);
-            scope.setParentScope(null);
-
-            for (Map.Entry<String, Object> entry : vars.entrySet()) {
-                ScriptableObject.putProperty(scope, entry.getKey(), entry.getValue());
-            }
-            Object ret = script.exec(ctx, scope);
-            return ScriptValueConverter.unwrapValue(ret);
-        } finally {
-            Context.exit();
-        }
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return ScriptValueConverter.unwrapValue(value);
-    }
-
     private String generateScriptName() {
         return "Script" + counter.incrementAndGet() + ".js";
     }
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java
index 13fe237..f0a3181 100644
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java
@@ -57,7 +57,7 @@ public class JavaScriptScriptEngineTests extends ESTestCase {
     @Test
     public void testSimpleEquation() {
         Map<String, Object> vars = new HashMap<String, Object>();
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testSimpleEquation", "js", se.compile("1 + 2")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testSimpleEquation", "js", se.compile("1 + 2")), vars).run();
         assertThat(((Number) o).intValue(), equalTo(3));
     }
 
@@ -68,21 +68,21 @@ public class JavaScriptScriptEngineTests extends ESTestCase {
         Map<String, Object> obj2 = MapBuilder.<String, Object>newMapBuilder().put("prop2", "value2").map();
         Map<String, Object> obj1 = MapBuilder.<String, Object>newMapBuilder().put("prop1", "value1").put("obj2", obj2).put("l", Arrays.asList("2", "1")).map();
         vars.put("obj1", obj1);
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "js", se.compile("obj1")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "js", se.compile("obj1")), vars).run();
         assertThat(o, instanceOf(Map.class));
         obj1 = (Map<String, Object>) o;
         assertThat((String) obj1.get("prop1"), equalTo("value1"));
         assertThat((String) ((Map<String, Object>) obj1.get("obj2")).get("prop2"), equalTo("value2"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "js", se.compile("obj1.l[0]")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "js", se.compile("obj1.l[0]")), vars).run();
         assertThat(((String) o), equalTo("2"));
     }
 
     @Test
     public void testJavaScriptObjectToMap() {
         Map<String, Object> vars = new HashMap<String, Object>();
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testJavaScriptObjectToMap", "js",
-                se.compile("var obj1 = {}; obj1.prop1 = 'value1'; obj1.obj2 = {}; obj1.obj2.prop2 = 'value2'; obj1")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testJavaScriptObjectToMap", "js",
+                se.compile("var obj1 = {}; obj1.prop1 = 'value1'; obj1.obj2 = {}; obj1.obj2.prop2 = 'value2'; obj1")), vars).run();
         Map obj1 = (Map) o;
         assertThat((String) obj1.get("prop1"), equalTo("value1"));
         assertThat((String) ((Map<String, Object>) obj1.get("obj2")).get("prop2"), equalTo("value2"));
@@ -97,9 +97,10 @@ public class JavaScriptScriptEngineTests extends ESTestCase {
         ctx.put("obj1", obj1);
         vars.put("ctx", ctx);
 
-        se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testJavaScriptObjectMapInter", "js",
+        ExecutableScript executable = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testJavaScriptObjectMapInter", "js",
                 se.compile("ctx.obj2 = {}; ctx.obj2.prop2 = 'value2'; ctx.obj1.prop1 = 'uvalue1'")), vars);
-        ctx = (Map<String, Object>) se.unwrap(vars.get("ctx"));
+        executable.run();
+        ctx = (Map<String, Object>) executable.unwrap(vars.get("ctx"));
         assertThat(ctx.containsKey("obj1"), equalTo(true));
         assertThat((String) ((Map<String, Object>) ctx.get("obj1")).get("prop1"), equalTo("uvalue1"));
         assertThat(ctx.containsKey("obj2"), equalTo(true));
@@ -130,22 +131,22 @@ public class JavaScriptScriptEngineTests extends ESTestCase {
         Map<String, Object> obj1 = MapBuilder.<String, Object>newMapBuilder().put("prop1", "value1").put("obj2", obj2).map();
         vars.put("l", Arrays.asList("1", "2", "3", obj1));
 
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
-                se.compile("l.length")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
+                se.compile("l.length")), vars).run();
         assertThat(((Number) o).intValue(), equalTo(4));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
-                se.compile("l[0]")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
+                se.compile("l[0]")), vars).run();
         assertThat(((String) o), equalTo("1"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
-                se.compile("l[3]")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
+                se.compile("l[3]")), vars).run();
         obj1 = (Map<String, Object>) o;
         assertThat((String) obj1.get("prop1"), equalTo("value1"));
         assertThat((String) ((Map<String, Object>) obj1.get("obj2")).get("prop2"), equalTo("value2"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
-                se.compile("l[3].prop1")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessInScript", "js",
+                se.compile("l[3].prop1")), vars).run();
         assertThat(((String) o), equalTo("value1"));
     }
 
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
index 1d9090d..b639ed9 100644
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
@@ -149,7 +149,7 @@ public class JavaScriptScriptMultiThreadedTests extends ESTestCase {
                             long addition = x + y;
                             runtimeVars.put("x", x);
                             runtimeVars.put("y", y);
-                            long result = ((Number) se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), runtimeVars)).longValue();
+                            long result = ((Number) se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testExecutableNoRuntimeParams", "js", compiled), runtimeVars).run()).longValue();
                             assertThat(result, equalTo(addition));
                         }
                     } catch (Throwable t) {
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptSecurityTests.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptSecurityTests.java
index 36636eb..887317f 100644
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptSecurityTests.java
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptSecurityTests.java
@@ -50,7 +50,7 @@ public class JavaScriptSecurityTests extends ESTestCase {
     /** runs a script */
     private void doTest(String script) {
         Map<String, Object> vars = new HashMap<String, Object>();
-        se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "test", "js", se.compile(script)), vars);
+        se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "test", "js", se.compile(script)), vars).run();
     }
     
     /** asserts that a script runs without exception */
diff --git a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java
index 9cb44ef..bb7eb31 100644
--- a/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java
+++ b/plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java
@@ -43,14 +43,14 @@ public class SimpleBench {
         for (int i = 0; i < 1000; i++) {
             vars.put("x", i);
             vars.put("y", i + 1);
-            se.execute(compiledScript, vars);
+            se.executable(compiledScript, vars).run();
         }
 
         final long ITER = 100000;
 
         StopWatch stopWatch = new StopWatch().start();
         for (long i = 0; i < ITER; i++) {
-            se.execute(compiledScript, vars);
+            se.executable(compiledScript, vars).run();
         }
         System.out.println("Execute Took: " + stopWatch.stop().lastTaskTime());
 
diff --git a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
index 87bfbb5..3dfa4bc 100644
--- a/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
+++ b/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
@@ -125,23 +125,6 @@ public class PythonScriptEngineService extends AbstractComponent implements Scri
     }
 
     @Override
-    public Object execute(CompiledScript compiledScript, Map<String, Object> vars) {
-        PyObject pyVars = Py.java2py(vars);
-        interp.setLocals(pyVars);
-        // eval the script with reduced privileges
-        PyObject ret = evalRestricted((PyCode) compiledScript.compiled());
-        if (ret == null) {
-            return null;
-        }
-        return ret.__tojava__(Object.class);
-    }
-
-    @Override
-    public Object unwrap(Object value) {
-        return unwrapValue(value);
-    }
-
-    @Override
     public void close() {
         interp.cleanup();
     }
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java
index 5634ec3..979da65 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java
@@ -59,7 +59,7 @@ public class PythonScriptEngineTests extends ESTestCase {
     @Test
     public void testSimpleEquation() {
         Map<String, Object> vars = new HashMap<String, Object>();
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testSimpleEquation", "python", se.compile("1 + 2")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testSimpleEquation", "python", se.compile("1 + 2")), vars).run();
         assertThat(((Number) o).intValue(), equalTo(3));
     }
 
@@ -70,13 +70,13 @@ public class PythonScriptEngineTests extends ESTestCase {
         Map<String, Object> obj2 = MapBuilder.<String, Object>newMapBuilder().put("prop2", "value2").map();
         Map<String, Object> obj1 = MapBuilder.<String, Object>newMapBuilder().put("prop1", "value1").put("obj2", obj2).put("l", Arrays.asList("2", "1")).map();
         vars.put("obj1", obj1);
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "python", se.compile("obj1")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "python", se.compile("obj1")), vars).run();
         assertThat(o, instanceOf(Map.class));
         obj1 = (Map<String, Object>) o;
         assertThat((String) obj1.get("prop1"), equalTo("value1"));
         assertThat((String) ((Map<String, Object>) obj1.get("obj2")).get("prop2"), equalTo("value2"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "python", se.compile("obj1['l'][0]")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testMapAccess", "python", se.compile("obj1['l'][0]")), vars).run();
         assertThat(((String) o), equalTo("2"));
     }
 
@@ -89,9 +89,10 @@ public class PythonScriptEngineTests extends ESTestCase {
         ctx.put("obj1", obj1);
         vars.put("ctx", ctx);
 
-        se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testObjectInterMap", "python",
+        ExecutableScript executable = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testObjectInterMap", "python",
                 se.compile("ctx['obj2'] = { 'prop2' : 'value2' }; ctx['obj1']['prop1'] = 'uvalue1'")), vars);
-        ctx = (Map<String, Object>) se.unwrap(vars.get("ctx"));
+        executable.run();
+        ctx = (Map<String, Object>) executable.unwrap(vars.get("ctx"));
         assertThat(ctx.containsKey("obj1"), equalTo(true));
         assertThat((String) ((Map<String, Object>) ctx.get("obj1")).get("prop1"), equalTo("uvalue1"));
         assertThat(ctx.containsKey("obj2"), equalTo(true));
@@ -109,15 +110,15 @@ public class PythonScriptEngineTests extends ESTestCase {
 //        Object o = se.execute(se.compile("l.length"), vars);
 //        assertThat(((Number) o).intValue(), equalTo(4));
 
-        Object o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[0]")), vars);
+        Object o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[0]")), vars).run();
         assertThat(((String) o), equalTo("1"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[3]")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[3]")), vars).run();
         obj1 = (Map<String, Object>) o;
         assertThat((String) obj1.get("prop1"), equalTo("value1"));
         assertThat((String) ((Map<String, Object>) obj1.get("obj2")).get("prop2"), equalTo("value2"));
 
-        o = se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[3]['prop1']")), vars);
+        o = se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "testAccessListInScript", "python", se.compile("l[3]['prop1']")), vars).run();
         assertThat(((String) o), equalTo("value1"));
     }
 
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
index 6798dab..81ebf69 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
@@ -158,7 +158,7 @@ public class PythonScriptMultiThreadedTests extends ESTestCase {
                             long addition = x + y;
                             runtimeVars.put("x", x);
                             runtimeVars.put("y", y);
-                            long result = ((Number) se.execute(compiledScript, runtimeVars)).longValue();
+                            long result = ((Number) se.executable(compiledScript, runtimeVars).run()).longValue();
                             assertThat(result, equalTo(addition));
                         }
                     } catch (Throwable t) {
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
index 745a109..fd60607 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
@@ -53,7 +53,7 @@ public class PythonSecurityTests extends ESTestCase {
     /** runs a script */
     private void doTest(String script) {
         Map<String, Object> vars = new HashMap<String, Object>();
-        se.execute(new CompiledScript(ScriptService.ScriptType.INLINE, "test", "python", se.compile(script)), vars);
+        se.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "test", "python", se.compile(script)), vars).run();
     }
     
     /** asserts that a script runs without exception */
@@ -68,7 +68,10 @@ public class PythonSecurityTests extends ESTestCase {
             fail("did not get expected exception");
         } catch (PyException expected) {
             Throwable cause = expected.getCause();
-            assertNotNull("null cause for exception: " + expected, cause);
+            // TODO: fix jython localization bugs: https://github.com/elastic/elasticsearch/issues/13967
+            // this is the correct assert:
+            // assertNotNull("null cause for exception: " + expected, cause);
+            assertNotNull("null cause for exception", cause);
             assertTrue("unexpected exception: " + cause, cause instanceof SecurityException);
         }
     }
diff --git a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java
index 4fab7dd..60e792c 100644
--- a/plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java
+++ b/plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java
@@ -44,14 +44,14 @@ public class SimpleBench {
         for (int i = 0; i < 1000; i++) {
             vars.put("x", i);
             vars.put("y", i + 1);
-            se.execute(compiledScript, vars);
+            se.executable(compiledScript, vars).run();
         }
 
         final long ITER = 100000;
 
         StopWatch stopWatch = new StopWatch().start();
         for (long i = 0; i < ITER; i++) {
-            se.execute(compiledScript, vars);
+            se.executable(compiledScript, vars).run();
         }
         System.out.println("Execute Took: " + stopWatch.stop().lastTaskTime());
 
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
index 2ceeb2a..383375f 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats
@@ -31,6 +31,7 @@
 # Load test utilities
 load packaging_test_utils
 load tar
+load plugins
 
 setup() {
     skip_not_tar_gz
@@ -91,12 +92,9 @@ setup() {
     # starting Elasticsearch so we don't have to wait for elasticsearch to scan for
     # them.
     install_elasticsearch_test_scripts
-
+    ESPLUGIN_COMMAND_USER=elasticsearch install_and_check_plugin lang groovy
     start_elasticsearch_service
-
     run_elasticsearch_tests
-
     stop_elasticsearch_service
-
     rm -rf "/tmp/elasticsearch"
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/30_deb_package.bats b/qa/vagrant/src/test/resources/packaging/scripts/30_deb_package.bats
index d0a6921..553f867 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/30_deb_package.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/30_deb_package.bats
@@ -32,6 +32,7 @@
 # Load test utilities
 load packaging_test_utils
 load os_package
+load plugins
 
 # Cleans everything for the 1st execution
 setup() {
@@ -85,6 +86,7 @@ setup() {
     # starting Elasticsearch so we don't have to wait for elasticsearch to scan for
     # them.
     install_elasticsearch_test_scripts
+    ESPLUGIN_COMMAND_USER=root install_and_check_plugin lang groovy
     start_elasticsearch_service
     run_elasticsearch_tests
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/40_rpm_package.bats b/qa/vagrant/src/test/resources/packaging/scripts/40_rpm_package.bats
index 91b6d8c..7f447e5 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/40_rpm_package.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/40_rpm_package.bats
@@ -31,6 +31,7 @@
 # Load test utilities
 load packaging_test_utils
 load os_package
+load plugins
 
 # Cleans everything for the 1st execution
 setup() {
@@ -80,6 +81,7 @@ setup() {
     # starting Elasticsearch so we don't have to wait for elasticsearch to scan for
     # them.
     install_elasticsearch_test_scripts
+    ESPLUGIN_COMMAND_USER=root install_and_check_plugin lang groovy
     start_elasticsearch_service
     run_elasticsearch_tests
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/60_systemd.bats b/qa/vagrant/src/test/resources/packaging/scripts/60_systemd.bats
index 76487fc..6558a38 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/60_systemd.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/60_systemd.bats
@@ -31,6 +31,7 @@
 # Load test utilities
 load packaging_test_utils
 load os_package
+load plugins
 
 # Cleans everything for the 1st execution
 setup() {
@@ -67,6 +68,7 @@ setup() {
     # starting Elasticsearch so we don't have to wait for elasticsearch to scan for
     # them.
     install_elasticsearch_test_scripts
+    ESPLUGIN_COMMAND_USER=root install_and_check_plugin lang groovy
     systemctl start elasticsearch.service
     wait_for_elasticsearch_status
     assert_file_exist "/var/run/elasticsearch/elasticsearch.pid"
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/70_sysv_initd.bats b/qa/vagrant/src/test/resources/packaging/scripts/70_sysv_initd.bats
index bb6e44c..1c5cce5 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/70_sysv_initd.bats
+++ b/qa/vagrant/src/test/resources/packaging/scripts/70_sysv_initd.bats
@@ -31,6 +31,7 @@
 # Load test utilities
 load packaging_test_utils
 load os_package
+load plugins
 
 # Cleans everything for the 1st execution
 setup() {
@@ -69,6 +70,7 @@ setup() {
     # Install scripts used to test script filters and search templates before
     # starting Elasticsearch so we don't have to wait for elasticsearch to scan for
     # them.
+    ESPLUGIN_COMMAND_USER=root install_and_check_plugin lang groovy
     install_elasticsearch_test_scripts
     service elasticsearch start
     wait_for_elasticsearch_status
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index 7f24b8a..c81af43 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -239,36 +239,13 @@ clean_before_test() {
 start_elasticsearch_service() {
     local desiredStatus=${1:-green}
 
-    if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
-        # su and the Elasticsearch init script work together to break bats.
-        # sudo isolates bats enough from the init script so everything continues
-        # to tick along
-        sudo -u elasticsearch /tmp/elasticsearch/bin/elasticsearch -d \
-            -p /tmp/elasticsearch/elasticsearch.pid
-    elif is_systemd; then
-        run systemctl daemon-reload
-        [ "$status" -eq 0 ]
-
-        run systemctl enable elasticsearch.service
-        [ "$status" -eq 0 ]
-
-        run systemctl is-enabled elasticsearch.service
-        [ "$status" -eq 0 ]
-
-        run systemctl start elasticsearch.service
-        [ "$status" -eq 0 ]
-
-    elif is_sysvinit; then
-        run service elasticsearch start
-        [ "$status" -eq 0 ]
-    fi
+    run_elasticsearch_service 0
 
     wait_for_elasticsearch_status $desiredStatus
 
     if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
         [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
-
         echo "Looking for elasticsearch pid...."
         ps $pid
     elif is_systemd; then
@@ -284,6 +261,64 @@ start_elasticsearch_service() {
     fi
 }
 
+# Start elasticsearch
+# $1 expected status code
+# $2 additional command line args
+run_elasticsearch_service() {
+    local expectedStatus=$1
+    local commandLineArgs=$2
+    # Set the CONF_DIR setting in case we start as a service
+    if [ ! -z "$CONF_DIR" ] ; then
+        if is_dpkg ; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/default/elasticsearch;
+        elif is_rpm; then
+            echo "CONF_DIR=$CONF_DIR" >> /etc/sysconfig/elasticsearch;
+        fi
+    fi
+
+    if [ -f "/tmp/elasticsearch/bin/elasticsearch" ]; then
+        if [ -z "$CONF_DIR" ]; then
+            local CONF_DIR=""
+        fi
+        # we must capture the exit code to compare so we don't want to start as background process in case we expect something other than 0
+        local background=""
+        local timeoutCommand=""
+        if [ "$expectedStatus" = 0 ]; then
+            background="-d"
+        else
+            timeoutCommand="timeout 60s "
+        fi
+        # su and the Elasticsearch init script work together to break bats.
+        # sudo isolates bats enough from the init script so everything continues
+        # to tick along
+        run sudo -u elasticsearch bash <<BASH
+# If jayatana is installed then we try to use it. Elasticsearch should ignore it even when we try.
+# If it doesn't ignore it then Elasticsearch will fail to start because of security errors.
+# This line is attempting to emulate the on login behavior of /usr/share/upstart/sessions/jayatana.conf
+[ -f /usr/share/java/jayatanaag.jar ] && export JAVA_TOOL_OPTIONS="-javaagent:/usr/share/java/jayatanaag.jar"
+# And now we can start Elasticsearch normally, in the background (-d) and with a pidfile (-p).
+$timeoutCommand/tmp/elasticsearch/bin/elasticsearch $background -p /tmp/elasticsearch/elasticsearch.pid -Des.path.conf=$CONF_DIR $commandLineArgs
+BASH
+        [ "$status" -eq "$expectedStatus" ]
+    elif is_systemd; then
+        run systemctl daemon-reload
+        [ "$status" -eq 0 ]
+
+        run systemctl enable elasticsearch.service
+        [ "$status" -eq 0 ]
+
+        run systemctl is-enabled elasticsearch.service
+        [ "$status" -eq 0 ]
+
+        run systemctl start elasticsearch.service
+        [ "$status" -eq "$expectedStatus" ]
+
+    elif is_sysvinit; then
+        run service elasticsearch start
+        [ "$status" -eq "$expectedStatus" ]
+    fi
+}
+
 stop_elasticsearch_service() {
     if [ -r "/tmp/elasticsearch/elasticsearch.pid" ]; then
         pid=$(cat /tmp/elasticsearch/elasticsearch.pid)
@@ -319,7 +354,7 @@ wait_for_elasticsearch_status() {
           if [ -e "$ESLOG/elasticsearch.log" ]; then
               cat "$ESLOG/elasticsearch.log"
           else
-              echo "The elasticsearch log doesn't exist. Maybe /vag/log/messages has something:"
+              echo "The elasticsearch log doesn't exist. Maybe /var/log/messages has something:"
               tail -n20 /var/log/messages
           fi
           false
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index ec2f177..eac395c 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -83,6 +83,35 @@ else
     }
 fi
 
+@test "[$GROUP] install jvm-example plugin with a custom CONFIG_FILE and check failure" {
+    local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath"
+    # this should fail because CONF_FILE is no longer supported
+    [ $status = 1 ]
+    CONF_FILE="$ESCONFIG/elasticsearch.yml" run sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" remove jvm-example
+    echo "status is $status"
+    [ $status = 1 ]
+}
+
+@test "[$GROUP] start elasticsearch with a custom CONFIG_FILE and check failure" {
+    local CONF_FILE="$ESCONFIG/elasticsearch.yml"
+
+    if is_dpkg; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/default/elasticsearch;
+    elif is_rpm; then
+        echo "CONF_FILE=$CONF_FILE" >> /etc/sysconfig/elasticsearch;
+    fi
+
+    run_elasticsearch_service 1 -Des.default.config="$CONF_FILE"
+
+    # remove settings again otherwise cleaning up before next testrun will fail
+    if is_dpkg ; then
+        sudo sed -i '/CONF_FILE/d' /etc/default/elasticsearch
+    elif is_rpm; then
+        sudo sed -i '/CONF_FILE/d' /etc/sysconfig/elasticsearch
+    fi
+}
+
 @test "[$GROUP] install jvm-example plugin with a custom path.plugins" {
     # Clean up after the last time this test was run
     rm -rf /tmp/plugins.*
@@ -111,6 +140,9 @@ fi
     move_config
 
     CONF_DIR="$ESCONFIG" install_jvm_example
+    CONF_DIR="$ESCONFIG" start_elasticsearch_service
+    diff  <(curl -s localhost:9200/_cat/configured_example | sed 's/ //g') <(echo "foo")
+    stop_elasticsearch_service
     CONF_DIR="$ESCONFIG" remove_jvm_example
 }
 
@@ -352,3 +384,43 @@ fi
 @test "[$GROUP] stop elasticsearch" {
     stop_elasticsearch_service
 }
+
+@test "[$GROUP] install jvm-example with different logging modes and check output" {
+    local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
+    sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" > /tmp/plugin-cli-output
+    local loglines=$(cat /tmp/plugin-cli-output | wc -l)
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+    # tar extraction does not create the plugins directory so the plugin tool will print an additional line that the directory will be created
+        [ "$loglines" -eq "7" ] || {
+            echo "Expected 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -eq "6" ] || {
+            echo "Expected 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
+    remove_jvm_example
+
+    local relativePath=${1:-$(readlink -m jvm-example-*.zip)}
+    sudo -E -u $ESPLUGIN_COMMAND_USER "$ESHOME/bin/plugin" install "file://$relativePath" -Des.logger.level=DEBUG > /tmp/plugin-cli-output
+    local loglines=$(cat /tmp/plugin-cli-output | wc -l)
+    if [ "$GROUP" == "TAR PLUGINS" ]; then
+        [ "$loglines" -gt "7" ] || {
+            echo "Expected more than 7 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    else
+        [ "$loglines" -gt "6" ] || {
+            echo "Expected more than 6 lines but the output was:"
+            cat /tmp/plugin-cli-output
+            false
+        }
+    fi
+    remove_jvm_example
+}
+
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
index 2af60c8..31e302c 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash
@@ -36,6 +36,8 @@ install_plugin() {
 
     assert_file_exist "$ESPLUGINS/$name"
     assert_file_exist "$ESPLUGINS/$name/plugin-descriptor.properties"
+    #check we did not accidentially create a log file as root as /usr/share/elasticsearch
+    assert_file_not_exist "/usr/share/elasticsearch/logs"
 
     # At some point installing or removing plugins caused elasticsearch's logs
     # to be owned by root. This is bad so we want to make sure it doesn't
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.health.json b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.health.json
index c361318..b622d01 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.health.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/cluster.health.json
@@ -7,7 +7,7 @@
       "paths": ["/_cluster/health", "/_cluster/health/{index}"],
       "parts": {
         "index": {
-          "type" : "string",
+          "type" : "list",
           "description" : "Limit the information returned to a specific index"
         }
       },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/delete_template.json b/rest-api-spec/src/main/resources/rest-api-spec/api/delete_template.json
index 9e92834..1dbc40a 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/delete_template.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/delete_template.json
@@ -8,7 +8,8 @@
       "parts": {
         "id": {
           "type" : "string",
-          "description" : "Template ID"
+          "description" : "Template ID",
+          "required" : true
         }
       },
       "params" : {
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.close.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.close.json
index 3a9bc59..4eaa930 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.close.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.close.json
@@ -7,9 +7,9 @@
       "paths": ["/{index}/_close"],
       "parts": {
         "index": {
-          "type" : "string",
+          "type" : "list",
           "required" : true,
-          "description" : "The name of the index"
+          "description" : "A comma separated list of indices to close"
         }
       },
       "params": {
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
index 14fae15..5c426f9 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get.json
@@ -13,7 +13,8 @@
         },
         "feature":{
           "type":"list",
-          "description":"A comma-separated list of features"
+          "description":"A comma-separated list of features",
+          "options": ["_settings", "_mappings", "_warmers", "_aliases"]
         }
       },
       "params":{
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_field_mapping.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_field_mapping.json
index 28d103a..3d5a629 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_field_mapping.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_field_mapping.json
@@ -3,8 +3,8 @@
     "documentation": "http://www.elastic.co/guide/en/elasticsearch/reference/master/indices-get-field-mapping.html",
     "methods": ["GET"],
     "url": {
-      "path": "/_mapping/field/{field}",
-      "paths": ["/_mapping/field/{field}", "/{index}/_mapping/field/{field}", "/_mapping/{type}/field/{field}", "/{index}/_mapping/{type}/field/{field}"],
+      "path": "/_mapping/field/{fields}",
+      "paths": ["/_mapping/field/{fields}", "/{index}/_mapping/field/{fields}", "/_mapping/{type}/field/{fields}", "/{index}/_mapping/{type}/field/{fields}"],
       "parts": {
         "index": {
           "type" : "list",
@@ -14,7 +14,7 @@
           "type" : "list",
           "description" : "A comma-separated list of document types"
         },
-        "field": {
+        "fields": {
           "type" : "list",
           "description" : "A comma-separated list of fields",
           "required" : true
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_template.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_template.json
index e948cad..e3a97ee 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_template.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.get_template.json
@@ -10,9 +10,9 @@
       ],
       "parts": {
         "name": {
-          "type": "string",
+          "type": "list",
           "required": false,
-          "description": "The name of the template"
+          "description": "The comma separated names of the index templates"
         }
       },
       "params": {
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.open.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.open.json
index 3c2cb1c..879ce5a 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.open.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.open.json
@@ -7,9 +7,9 @@
       "paths": ["/{index}/_open"],
       "parts": {
         "index": {
-          "type" : "string",
+          "type" : "list",
           "required" : true,
-          "description" : "The name of the index"
+          "description" : "A comma separated list of indices to open"
         }
       },
       "params": {
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
deleted file mode 100644
index 8ee11b0..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/35_external_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
deleted file mode 100644
index febb7a5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/36_external_gte_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   external_gte
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
deleted file mode 100644
index 393d16f..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/create/37_force_version.yaml
+++ /dev/null
@@ -1,33 +0,0 @@
----
-"External version":
-
- - do:
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - match:   { _version: 5}
-
- - do:
-      catch:             conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        5
-
- - do:
-      catch:              conflict
-      create:
-          index:          test_1
-          type:           test
-          id:             1
-          body:           { foo: bar }
-          version_type:   force
-          version:        6
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/10_basic.yaml
index 0b63631..7df0734 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/10_basic.yaml
@@ -18,7 +18,7 @@ setup:
 
   - do:
       indices.get_field_mapping:
-        field: text
+        fields: text
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
 
@@ -27,7 +27,7 @@ setup:
   - do:
       indices.get_field_mapping:
         index: test_index
-        field: text
+        fields: text
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
 
@@ -38,7 +38,7 @@ setup:
       indices.get_field_mapping:
         index: test_index
         type: test_type
-        field: text
+        fields: text
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
 
@@ -49,7 +49,7 @@ setup:
       indices.get_field_mapping:
         index: test_index
         type: test_type
-        field: [ text , text1 ]
+        fields: [ text , text1 ]
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
   - is_false: test_index.mappings.test_type.text1
@@ -61,19 +61,19 @@ setup:
       indices.get_field_mapping:
         index: test_index
         type: test_type
-        field: text
+        fields: text
         include_defaults: true
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
   - match: {test_index.mappings.test_type.text.mapping.text.analyzer: default}
 
 ---
-"Get field mapping should work without index specifying type and field": 
+"Get field mapping should work without index specifying type and fields":
 
   - do:
       indices.get_field_mapping:
         type: test_type
-        field: text
+        fields: text
 
   - match: {test_index.mappings.test_type.text.mapping.text.type:     string}
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/20_missing_field.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/20_missing_field.yaml
index 1eae257..d39fcc5 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/20_missing_field.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/20_missing_field.yaml
@@ -19,6 +19,6 @@
       indices.get_field_mapping:
         index: test_index
         type: test_type
-        field: not_existent
+        fields: not_existent
   
   - match: { '': {}}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/30_missing_type.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/30_missing_type.yaml
index 7c16d51..6f8e9d2 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/30_missing_type.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/30_missing_type.yaml
@@ -20,5 +20,5 @@
       indices.get_field_mapping:
         index: test_index
         type: not_test_type
-        field: text
+        fields: text
   
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/40_missing_index.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/40_missing_index.yaml
index ab2d985..7da516e 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/40_missing_index.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/40_missing_index.yaml
@@ -6,6 +6,6 @@
       indices.get_field_mapping:
         index: test_index
         type: type
-        field: field
+        fields: field
 
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/50_field_wildcards.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/50_field_wildcards.yaml
index cc53689..78af1b3 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/50_field_wildcards.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.get_field_mapping/50_field_wildcards.yaml
@@ -49,7 +49,7 @@ setup:
 
   - do:
       indices.get_field_mapping:
-        field: "*"
+        fields: "*"
 
   - match: {test_index.mappings.test_type.t1.full_name:     t1     }
   - match: {test_index.mappings.test_type.t2.full_name:     t2     }
@@ -63,7 +63,7 @@ setup:
   - do:
       indices.get_field_mapping:
         index: test_index
-        field: "t*"
+        fields: "t*"
 
   - match:  {test_index.mappings.test_type.t1.full_name:     t1       }
   - match:  {test_index.mappings.test_type.t2.full_name:     t2       }
@@ -75,7 +75,7 @@ setup:
   - do:
       indices.get_field_mapping:
         index: test_index
-        field: "*t1"
+        fields: "*t1"
   - match:  {test_index.mappings.test_type.t1.full_name:        t1       }
   - match:  {test_index.mappings.test_type.obj\.t1.full_name:   obj.t1   }
   - match:  {test_index.mappings.test_type.obj\.i_t1.full_name: obj.i_t1 }
@@ -87,7 +87,7 @@ setup:
   - do:
       indices.get_field_mapping:
         index: test_index
-        field: "obj.i_*"
+        fields: "obj.i_*"
   - match:  {test_index.mappings.test_type.obj\.i_t1.full_name: obj.i_t1 }
   - match:  {test_index.mappings.test_type.obj\.i_t3.full_name: obj.i_t3 }
   - length: {test_index.mappings.test_type: 2}
@@ -99,7 +99,7 @@ setup:
       indices.get_field_mapping:
         index: _all
         type: _all
-        field: "t*"
+        fields: "t*"
   - match:  {test_index.mappings.test_type.t1.full_name: t1 }
   - match:  {test_index.mappings.test_type.t2.full_name: t2 }
   - length: {test_index.mappings.test_type: 2}
@@ -114,7 +114,7 @@ setup:
       indices.get_field_mapping:
         index: '*'
         type: '*'
-        field: "t*"
+        fields: "t*"
   - match:  {test_index.mappings.test_type.t1.full_name: t1 }
   - match:  {test_index.mappings.test_type.t2.full_name: t2 }
   - length: {test_index.mappings.test_type: 2}
@@ -129,7 +129,7 @@ setup:
       indices.get_field_mapping:
         index: 'test_index,test_index_2'
         type: 'test_type,test_type_2'
-        field: "t*"
+        fields: "t*"
   - match:  {test_index.mappings.test_type.t1.full_name: t1 }
   - match:  {test_index.mappings.test_type.t2.full_name: t2 }
   - length: {test_index.mappings.test_type: 2}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
index 3f13b09..17c4806 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/update/30_internal_version.yaml
@@ -2,7 +2,7 @@
 "Internal version":
 
  - do:
-      catch:        conflict
+      catch:        missing
       update:
           index:    test_1
           type:     test
@@ -10,7 +10,14 @@
           version:  1
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
+
+ - do:
+      index:
+          index:    test_1
+          type:     test
+          id:       1
+          body:
+            doc:    { foo: baz }
 
  - do:
       catch:        conflict
@@ -18,7 +25,6 @@
           index:    test_1
           type:     test
           id:       1
-          version:  1
+          version:  2
           body:
             doc:    { foo: baz }
-            upsert: { foo: bar }
