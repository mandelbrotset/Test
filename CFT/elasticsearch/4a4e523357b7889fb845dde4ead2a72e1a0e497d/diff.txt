diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/PrecommitTasks.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/PrecommitTasks.groovy
index d4d4d08..1cdeec7 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/PrecommitTasks.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/PrecommitTasks.groovy
@@ -94,6 +94,9 @@ class PrecommitTasks {
         project.checkstyle {
             config = project.resources.text.fromFile(
                 PrecommitTasks.getResource('/checkstyle.xml'), 'UTF-8')
+            configProperties = [
+                suppressions: PrecommitTasks.getResource('/checkstyle_suppressions.xml')
+            ]
         }
         for (String taskName : ['checkstyleMain', 'checkstyleTest']) {
             Task task = project.tasks.findByName(taskName)
diff --git a/buildSrc/src/main/resources/checkstyle.xml b/buildSrc/src/main/resources/checkstyle.xml
index b44c649..4dd0534 100644
--- a/buildSrc/src/main/resources/checkstyle.xml
+++ b/buildSrc/src/main/resources/checkstyle.xml
@@ -6,6 +6,10 @@
 <module name="Checker">
   <property name="charset" value="UTF-8" />
 
+  <module name="SuppressionFilter">
+    <property name="file" value="${suppressions}" />
+  </module>
+
   <module name="TreeWalker">
     <!-- ~3500 violations
     <module name="LineLength">
diff --git a/buildSrc/src/main/resources/checkstyle_suppressions.xml b/buildSrc/src/main/resources/checkstyle_suppressions.xml
new file mode 100644
index 0000000..641c6ec
--- /dev/null
+++ b/buildSrc/src/main/resources/checkstyle_suppressions.xml
@@ -0,0 +1,10 @@
+<?xml version="1.0"?>
+<!DOCTYPE suppressions PUBLIC
+          "-//Puppy Crawl//DTD Suppressions 1.1//EN"
+          "http://www.puppycrawl.com/dtds/suppressions_1_1.dtd">
+
+<suppressions>
+  <!-- These files are generated by ANTLR so its silly to hold them to our rules. -->
+  <suppress files="org/elasticsearch/painless/PainlessLexer\.java" checks="." />
+  <suppress files="org/elasticsearch/painless/PainlessParser(|BaseVisitor|Visitor)\.java" checks="." />
+</suppressions>
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index e6dc7de..dbbe986 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch;
 
+import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -613,7 +614,8 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         RETRY_ON_REPLICA_EXCEPTION(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class, org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException::new, 136),
         TYPE_MISSING_EXCEPTION(org.elasticsearch.indices.TypeMissingException.class, org.elasticsearch.indices.TypeMissingException::new, 137),
         FAILED_TO_COMMIT_CLUSTER_STATE_EXCEPTION(org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class, org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException::new, 140),
-        QUERY_SHARD_EXCEPTION(org.elasticsearch.index.query.QueryShardException.class, org.elasticsearch.index.query.QueryShardException::new, 141);
+        QUERY_SHARD_EXCEPTION(org.elasticsearch.index.query.QueryShardException.class, org.elasticsearch.index.query.QueryShardException::new, 141),
+        NO_LONGER_PRIMARY_SHARD_EXCEPTION(ShardStateAction.NoLongerPrimaryShardException.class, ShardStateAction.NoLongerPrimaryShardException::new, 142);
 
         final Class<? extends ElasticsearchException> exceptionClass;
         final FunctionThatThrowsIOException<StreamInput, ? extends ElasticsearchException> constructor;
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index e558006..330ce5e 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -254,7 +254,11 @@ public class Version {
     public static final int V_1_7_3_ID = 1070399;
     public static final Version V_1_7_3 = new Version(V_1_7_3_ID, false, org.apache.lucene.util.Version.LUCENE_4_10_4);
     public static final int V_1_7_4_ID = 1070499;
-    public static final Version V_1_7_4 = new Version(V_1_7_4_ID, true, org.apache.lucene.util.Version.LUCENE_4_10_4);
+    public static final Version V_1_7_4 = new Version(V_1_7_4_ID, false, org.apache.lucene.util.Version.LUCENE_4_10_4);
+    public static final int V_1_7_5_ID = 1070599;
+    public static final Version V_1_7_5 = new Version(V_1_7_5_ID, false, org.apache.lucene.util.Version.LUCENE_4_10_4);
+    public static final int V_1_7_6_ID = 1070699;
+    public static final Version V_1_7_6 = new Version(V_1_7_6_ID, true, org.apache.lucene.util.Version.LUCENE_4_10_4);
 
     public static final int V_2_0_0_beta1_ID = 2000001;
     public static final Version V_2_0_0_beta1 = new Version(V_2_0_0_beta1_ID, false, org.apache.lucene.util.Version.LUCENE_5_2_1);
@@ -275,9 +279,13 @@ public class Version {
     public static final int V_2_1_1_ID = 2010199;
     public static final Version V_2_1_1 = new Version(V_2_1_1_ID, false, org.apache.lucene.util.Version.LUCENE_5_3_1);
     public static final int V_2_1_2_ID = 2010299;
-    public static final Version V_2_1_2 = new Version(V_2_1_2_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_1);
+    public static final Version V_2_1_2 = new Version(V_2_1_2_ID, false, org.apache.lucene.util.Version.LUCENE_5_3_1);
+    public static final int V_2_1_3_ID = 2010399;
+    public static final Version V_2_1_3 = new Version(V_2_1_3_ID, true, org.apache.lucene.util.Version.LUCENE_5_3_1);
     public static final int V_2_2_0_ID = 2020099;
-    public static final Version V_2_2_0 = new Version(V_2_2_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
+    public static final Version V_2_2_0 = new Version(V_2_2_0_ID, false, org.apache.lucene.util.Version.LUCENE_5_4_0);
+    public static final int V_2_2_1_ID = 2020199;
+    public static final Version V_2_2_1 = new Version(V_2_2_1_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
     public static final int V_2_3_0_ID = 2030099;
     public static final Version V_2_3_0 = new Version(V_2_3_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
     public static final int V_3_0_0_ID = 3000099;
@@ -299,8 +307,12 @@ public class Version {
                 return V_3_0_0;
             case V_2_3_0_ID:
                 return V_2_3_0;
+            case V_2_2_1_ID:
+                return V_2_2_1;
             case V_2_2_0_ID:
                 return V_2_2_0;
+            case V_2_1_3_ID:
+                return V_2_1_3;
             case V_2_1_2_ID:
                 return V_2_1_2;
             case V_2_1_1_ID:
@@ -321,6 +333,10 @@ public class Version {
                 return V_2_0_0_beta2;
             case V_2_0_0_beta1_ID:
                 return V_2_0_0_beta1;
+            case V_1_7_6_ID:
+                return V_1_7_6;
+            case V_1_7_5_ID:
+                return V_1_7_5;
             case V_1_7_4_ID:
                 return V_1_7_4;
             case V_1_7_3_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
index 4337d0e..c7c0822 100644
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
@@ -22,31 +22,24 @@ package org.elasticsearch.action.ingest;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.core.PipelineFactoryError;
-import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 
-public class SimulatePipelineResponse extends ActionResponse implements StatusToXContent {
+public class SimulatePipelineResponse extends ActionResponse implements ToXContent {
     private String pipelineId;
     private boolean verbose;
     private List<SimulateDocumentResult> results;
-    private PipelineFactoryError error;
 
     public SimulatePipelineResponse() {
 
     }
 
-    public SimulatePipelineResponse(PipelineFactoryError error) {
-        this.error = error;
-    }
-
     public SimulatePipelineResponse(String pipelineId, boolean verbose, List<SimulateDocumentResult> responses) {
         this.pipelineId = pipelineId;
         this.verbose = verbose;
@@ -65,69 +58,42 @@ public class SimulatePipelineResponse extends ActionResponse implements StatusTo
         return verbose;
     }
 
-    public boolean isError() {
-        return error != null;
-    }
-
-    @Override
-    public RestStatus status() {
-        if (isError()) {
-            return RestStatus.BAD_REQUEST;
-        }
-        return RestStatus.OK;
-    }
-
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeBoolean(isError());
-        if (isError()) {
-            error.writeTo(out);
-        } else {
-            out.writeString(pipelineId);
-            out.writeBoolean(verbose);
-            out.writeVInt(results.size());
-            for (SimulateDocumentResult response : results) {
-                response.writeTo(out);
-            }
+        out.writeString(pipelineId);
+        out.writeBoolean(verbose);
+        out.writeVInt(results.size());
+        for (SimulateDocumentResult response : results) {
+            response.writeTo(out);
         }
     }
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        boolean isError = in.readBoolean();
-        if (isError) {
-            error = new PipelineFactoryError();
-            error.readFrom(in);
-        } else {
-            this.pipelineId = in.readString();
-            boolean verbose = in.readBoolean();
-            int responsesLength = in.readVInt();
-            results = new ArrayList<>();
-            for (int i = 0; i < responsesLength; i++) {
-                SimulateDocumentResult<?> simulateDocumentResult;
-                if (verbose) {
-                    simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
-                } else {
-                    simulateDocumentResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(in);
-                }
-                results.add(simulateDocumentResult);
+        this.pipelineId = in.readString();
+        boolean verbose = in.readBoolean();
+        int responsesLength = in.readVInt();
+        results = new ArrayList<>();
+        for (int i = 0; i < responsesLength; i++) {
+            SimulateDocumentResult<?> simulateDocumentResult;
+            if (verbose) {
+                simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
+            } else {
+                simulateDocumentResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(in);
             }
+            results.add(simulateDocumentResult);
         }
     }
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (isError()) {
-            error.toXContent(builder, params);
-        } else {
-            builder.startArray(Fields.DOCUMENTS);
-            for (SimulateDocumentResult response : results) {
-                response.toXContent(builder, params);
-            }
-            builder.endArray();
+        builder.startArray(Fields.DOCUMENTS);
+        for (SimulateDocumentResult response : results) {
+            response.toXContent(builder, params);
         }
+        builder.endArray();
         return builder;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
index 3d65863..4f9a219 100644
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.ingest;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.HandledTransportAction;
@@ -27,8 +28,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.ingest.core.PipelineFactoryError;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -58,9 +57,6 @@ public class SimulatePipelineTransportAction extends HandledTransportAction<Simu
             } else {
                 simulateRequest = SimulatePipelineRequest.parse(source, request.isVerbose(), pipelineStore);
             }
-        } catch (ConfigurationPropertyException e) {
-            listener.onResponse(new SimulatePipelineResponse(new PipelineFactoryError(e)));
-            return;
         } catch (Exception e) {
             listener.onFailure(e);
             return;
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
index 2df9195..ced6085 100644
--- a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
@@ -22,12 +22,10 @@ package org.elasticsearch.action.ingest;
 import org.elasticsearch.action.support.master.AcknowledgedResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.ingest.core.PipelineFactoryError;
 
 import java.io.IOException;
 
 public class WritePipelineResponse extends AcknowledgedResponse {
-    private PipelineFactoryError error;
 
     WritePipelineResponse() {
 
@@ -35,36 +33,17 @@ public class WritePipelineResponse extends AcknowledgedResponse {
 
     public WritePipelineResponse(boolean acknowledged) {
         super(acknowledged);
-        if (!isAcknowledged()) {
-            error = new PipelineFactoryError("pipeline write is not acknowledged");
-        }
-    }
-
-    public WritePipelineResponse(PipelineFactoryError error) {
-        super(false);
-        this.error = error;
-    }
-
-    public PipelineFactoryError getError() {
-        return error;
     }
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         readAcknowledged(in);
-        if (!isAcknowledged()) {
-            error = new PipelineFactoryError();
-            error.readFrom(in);
-        }
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
         writeAcknowledged(out);
-        if (!isAcknowledged()) {
-            error.writeTo(out);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponseRestListener.java b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponseRestListener.java
deleted file mode 100644
index 1b2629c..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponseRestListener.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-
-import java.io.IOException;
-
-public class WritePipelineResponseRestListener extends AcknowledgedRestListener<WritePipelineResponse> {
-
-    public WritePipelineResponseRestListener(RestChannel channel) {
-        super(channel);
-    }
-
-    @Override
-    protected void addCustomFields(XContentBuilder builder, WritePipelineResponse response) throws IOException {
-        if (!response.isAcknowledged()) {
-            response.getError().toXContent(builder, null);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
index ed23017..4e6ec3c 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
@@ -55,6 +55,8 @@ public abstract class ReplicationRequest<Request extends ReplicationRequest<Requ
 
     private WriteConsistencyLevel consistencyLevel = WriteConsistencyLevel.DEFAULT;
 
+    private long routedBasedOnClusterVersion = 0;
+
     public ReplicationRequest() {
 
     }
@@ -141,6 +143,20 @@ public abstract class ReplicationRequest<Request extends ReplicationRequest<Requ
         return (Request) this;
     }
 
+    /**
+     * Sets the minimum version of the cluster state that is required on the next node before we redirect to another primary.
+     * Used to prevent redirect loops, see also {@link TransportReplicationAction.ReroutePhase#doRun()}
+     */
+    @SuppressWarnings("unchecked")
+    Request routedBasedOnClusterVersion(long routedBasedOnClusterVersion) {
+        this.routedBasedOnClusterVersion = routedBasedOnClusterVersion;
+        return (Request) this;
+    }
+
+    long routedBasedOnClusterVersion() {
+        return routedBasedOnClusterVersion;
+    }
+
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
@@ -161,6 +177,7 @@ public abstract class ReplicationRequest<Request extends ReplicationRequest<Requ
         consistencyLevel = WriteConsistencyLevel.fromId(in.readByte());
         timeout = TimeValue.readTimeValue(in);
         index = in.readString();
+        routedBasedOnClusterVersion = in.readVLong();
     }
 
     @Override
@@ -175,6 +192,7 @@ public abstract class ReplicationRequest<Request extends ReplicationRequest<Requ
         out.writeByte(consistencyLevel.id());
         timeout.writeTo(out);
         out.writeString(index);
+        out.writeVLong(routedBasedOnClusterVersion);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index 07e4322..7f29954 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -472,6 +472,15 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                 }
                 performAction(node, transportPrimaryAction, true);
             } else {
+                if (state.version() < request.routedBasedOnClusterVersion()) {
+                    logger.trace("failed to find primary [{}] for request [{}] despite sender thinking it would be here. Local cluster state version [{}]] is older than on sending node (version [{}]), scheduling a retry...", request.shardId(), request, state.version(), request.routedBasedOnClusterVersion());
+                    retryBecauseUnavailable(request.shardId(), "failed to find primary as current cluster state with version [" + state.version() + "] is stale (expected at least [" + request.routedBasedOnClusterVersion() + "]");
+                    return;
+                } else {
+                    // chasing the node with the active primary for a second hop requires that we are at least up-to-date with the current cluster state version
+                    // this prevents redirect loops between two nodes when a primary was relocated and the relocation target is not aware that it is the active primary shard already.
+                    request.routedBasedOnClusterVersion(state.version());
+                }
                 if (logger.isTraceEnabled()) {
                     logger.trace("send action [{}] on primary [{}] for request [{}] with cluster state version [{}] to [{}]", actionName, request.shardId(), request, state.version(), primary.currentNodeId());
                 }
@@ -769,16 +778,15 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         private final List<ShardRouting> shards;
         private final DiscoveryNodes nodes;
         private final boolean executeOnReplica;
-        private final String indexUUID;
         private final AtomicBoolean finished = new AtomicBoolean();
         private final AtomicInteger success = new AtomicInteger(1); // We already wrote into the primary shard
         private final ConcurrentMap<String, Throwable> shardReplicaFailures = ConcurrentCollections.newConcurrentMap();
         private final AtomicInteger pending;
         private final int totalShards;
-        private final Releasable indexShardReference;
+        private final IndexShardReference indexShardReference;
 
         public ReplicationPhase(ReplicaRequest replicaRequest, Response finalResponse, ShardId shardId,
-                                TransportChannel channel, Releasable indexShardReference) {
+                                TransportChannel channel, IndexShardReference indexShardReference) {
             this.replicaRequest = replicaRequest;
             this.channel = channel;
             this.finalResponse = finalResponse;
@@ -795,7 +803,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
             final IndexMetaData indexMetaData = state.getMetaData().index(shardId.getIndex());
             this.shards = (shardRoutingTable != null) ? shardRoutingTable.shards() : Collections.emptyList();
             this.executeOnReplica = (indexMetaData == null) || shouldExecuteReplication(indexMetaData.getSettings());
-            this.indexUUID = (indexMetaData != null) ? indexMetaData.getIndexUUID() : null;
             this.nodes = state.getNodes();
 
             if (shards.isEmpty()) {
@@ -931,22 +938,22 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                                 String message = String.format(Locale.ROOT, "failed to perform %s on replica on node %s", transportReplicaAction, node);
                                 logger.warn("[{}] {}", exp, shardId, message);
                                 shardStateAction.shardFailed(
-                                        shard,
-                                        indexUUID,
-                                        message,
-                                        exp,
-                                        new ShardStateAction.Listener() {
-                                            @Override
-                                            public void onSuccess() {
-                                                onReplicaFailure(nodeId, exp);
-                                            }
-
-                                            @Override
-                                            public void onFailure(Throwable t) {
-                                                // TODO: handle catastrophic non-channel failures
-                                                onReplicaFailure(nodeId, exp);
-                                            }
+                                    shard,
+                                    indexShardReference.routingEntry(),
+                                    message,
+                                    exp,
+                                    new ShardStateAction.Listener() {
+                                        @Override
+                                        public void onSuccess() {
+                                            onReplicaFailure(nodeId, exp);
+                                        }
+
+                                        @Override
+                                        public void onFailure(Throwable t) {
+                                            // TODO: handle catastrophic non-channel failures
+                                            onReplicaFailure(nodeId, exp);
                                         }
+                                    }
                                 );
                             }
                         }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index ed99f9b..5aa760d 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -314,7 +314,7 @@ final class Bootstrap {
             }
             ESLogger logger = Loggers.getLogger(Bootstrap.class);
             if (INSTANCE.node != null) {
-                logger = Loggers.getLogger(Bootstrap.class, INSTANCE.node.settings().get("node.name"));
+                logger = Loggers.getLogger(Bootstrap.class, INSTANCE.node.settings().get("name"));
             }
             // HACK, it sucks to do this, but we will run users out of disk space otherwise
             if (e instanceof CreationException) {
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
index c89ca8e..9cae3b8 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/BootstrapCLIParser.java
@@ -83,7 +83,9 @@ final class BootstrapCLIParser extends CliTool {
 
         @Override
         public ExitStatus execute(Settings settings, Environment env) throws Exception {
-            terminal.println("Version: %s, Build: %s/%s, JVM: %s", org.elasticsearch.Version.CURRENT, Build.CURRENT.shortHash(), Build.CURRENT.date(), JvmInfo.jvmInfo().version());
+            terminal.println("Version: " + org.elasticsearch.Version.CURRENT
+                    + ", Build: " + Build.CURRENT.shortHash() + "/" + Build.CURRENT.date()
+                    + ", JVM: " + JvmInfo.jvmInfo().version());
             return ExitStatus.OK_AND_EXIT;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
index 1e605b9..f9ee988 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
@@ -103,7 +103,6 @@ public class TransportClientNodesService extends AbstractComponent {
     public static final Setting<TimeValue> CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL = Setting.positiveTimeSetting("client.transport.nodes_sampler_interval", timeValueSeconds(5), false, Setting.Scope.CLUSTER);
     public static final Setting<TimeValue> CLIENT_TRANSPORT_PING_TIMEOUT = Setting.positiveTimeSetting("client.transport.ping_timeout", timeValueSeconds(5), false, Setting.Scope.CLUSTER);
     public static final Setting<Boolean> CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME = Setting.boolSetting("client.transport.ignore_cluster_name", false, false, Setting.Scope.CLUSTER);
-    public static final Setting<Boolean> CLIENT_TRANSPORT_SNIFF = Setting.boolSetting("client.transport.sniff", false, false, Setting.Scope.CLUSTER);
 
     @Inject
     public TransportClientNodesService(Settings settings, ClusterName clusterName, TransportService transportService,
@@ -122,7 +121,7 @@ public class TransportClientNodesService extends AbstractComponent {
             logger.debug("node_sampler_interval[" + nodesSamplerInterval + "]");
         }
 
-        if (CLIENT_TRANSPORT_SNIFF.get(this.settings)) {
+        if (this.settings.getAsBoolean("client.transport.sniff", false)) {
             this.nodesSampler = new SniffNodesSampler();
         } else {
             this.nodesSampler = new SimpleNodeSampler();
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java b/core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java
index e5d3f06..be9381a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterStateTaskExecutor.java
@@ -123,6 +123,11 @@ public interface ClusterStateTaskExecutor<T> {
             return this == SUCCESS;
         }
 
+        public Throwable getFailure() {
+            assert !isSuccess();
+            return failure;
+        }
+
         /**
          * Handle the execution result with the provided consumers
          * @param onSuccess handler to invoke on success
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
index 4aca9a4..fa70388 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.action.shard;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
@@ -28,8 +29,9 @@ import org.elasticsearch.cluster.ClusterStateTaskExecutor;
 import org.elasticsearch.cluster.ClusterStateTaskListener;
 import org.elasticsearch.cluster.MasterNodeChangePredicate;
 import org.elasticsearch.cluster.NotMasterException;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.cluster.routing.ShardRouting;
@@ -46,6 +48,7 @@ import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.discovery.Discovery;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.node.NodeClosedException;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.ConnectTransportException;
@@ -60,6 +63,7 @@ import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
@@ -125,17 +129,22 @@ public class ShardStateAction extends AbstractComponent {
         return ExceptionsHelper.unwrap(exp, MASTER_CHANNEL_EXCEPTIONS) != null;
     }
 
-    public void shardFailed(final ShardRouting shardRouting, final String indexUUID, final String message, @Nullable final Throwable failure, Listener listener) {
+    /**
+     * Send a shard failed request to the master node to update the
+     * cluster state.
+     *
+     * @param shardRouting       the shard to fail
+     * @param sourceShardRouting the source shard requesting the failure (must be the shard itself, or the primary shard)
+     * @param message            the reason for the failure
+     * @param failure            the underlying cause of the failure
+     * @param listener           callback upon completion of the request
+     */
+    public void shardFailed(final ShardRouting shardRouting, ShardRouting sourceShardRouting, final String message, @Nullable final Throwable failure, Listener listener) {
         ClusterStateObserver observer = new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext());
-        ShardRoutingEntry shardRoutingEntry = new ShardRoutingEntry(shardRouting, indexUUID, message, failure);
+        ShardRoutingEntry shardRoutingEntry = new ShardRoutingEntry(shardRouting, sourceShardRouting, message, failure);
         sendShardAction(SHARD_FAILED_ACTION_NAME, observer, shardRoutingEntry, listener);
     }
 
-    public void resendShardFailed(final ShardRouting shardRouting, final String indexUUID, final String message, @Nullable final Throwable failure, Listener listener) {
-        logger.trace("{} re-sending failed shard [{}], index UUID [{}], reason [{}]", shardRouting.shardId(), failure, shardRouting, indexUUID, message);
-        shardFailed(shardRouting, indexUUID, message, failure, listener);
-    }
-
     // visible for testing
     protected void waitForNewMasterAndRetry(String actionName, ClusterStateObserver observer, ShardRoutingEntry shardRoutingEntry, Listener listener) {
         observer.waitForNextChange(new ClusterStateObserver.Listener() {
@@ -231,15 +240,15 @@ public class ShardStateAction extends AbstractComponent {
 
             // partition tasks into those that correspond to shards
             // that exist versus do not exist
-            Map<Boolean, List<ShardRoutingEntry>> partition =
-                tasks.stream().collect(Collectors.partitioningBy(task -> shardExists(currentState, task)));
+            Map<ValidationResult, List<ShardRoutingEntry>> partition =
+                tasks.stream().collect(Collectors.groupingBy(task -> validateTask(currentState, task)));
 
             // tasks that correspond to non-existent shards are marked
             // as successful
-            batchResultBuilder.successes(partition.get(false));
+            batchResultBuilder.successes(partition.getOrDefault(ValidationResult.SHARD_MISSING, Collections.emptyList()));
 
             ClusterState maybeUpdatedState = currentState;
-            List<ShardRoutingEntry> tasksToFail = partition.get(true);
+            List<ShardRoutingEntry> tasksToFail = partition.getOrDefault(ValidationResult.VALID, Collections.emptyList());
             try {
                 List<FailedRerouteAllocation.FailedShard> failedShards =
                     tasksToFail
@@ -257,6 +266,15 @@ public class ShardStateAction extends AbstractComponent {
                 batchResultBuilder.failures(tasksToFail, t);
             }
 
+            partition
+                .getOrDefault(ValidationResult.SOURCE_INVALID, Collections.emptyList())
+                .forEach(task -> batchResultBuilder.failure(
+                    task,
+                    new NoLongerPrimaryShardException(
+                        task.getShardRouting().shardId(),
+                        "source shard [" + task.sourceShardRouting + "] is neither the local allocation nor the primary allocation")
+                ));
+
             return batchResultBuilder.build(maybeUpdatedState);
         }
 
@@ -265,17 +283,36 @@ public class ShardStateAction extends AbstractComponent {
             return allocationService.applyFailedShards(currentState, failedShards);
         }
 
-        private boolean shardExists(ClusterState currentState, ShardRoutingEntry task) {
+        private enum ValidationResult {
+            VALID,
+            SOURCE_INVALID,
+            SHARD_MISSING
+        }
+
+        private ValidationResult validateTask(ClusterState currentState, ShardRoutingEntry task) {
+
+            // non-local requests
+            if (!task.shardRouting.isSameAllocation(task.sourceShardRouting)) {
+                IndexShardRoutingTable indexShard = currentState.getRoutingTable().shardRoutingTableOrNull(task.shardRouting.shardId());
+                if (indexShard == null) {
+                    return ValidationResult.SOURCE_INVALID;
+                }
+                ShardRouting primaryShard = indexShard.primaryShard();
+                if (primaryShard == null || !primaryShard.isSameAllocation(task.sourceShardRouting)) {
+                    return ValidationResult.SOURCE_INVALID;
+                }
+            }
+
             RoutingNodes.RoutingNodeIterator routingNodeIterator =
                 currentState.getRoutingNodes().routingNodeIter(task.getShardRouting().currentNodeId());
             if (routingNodeIterator != null) {
                 for (ShardRouting maybe : routingNodeIterator) {
                     if (task.getShardRouting().isSameAllocation(maybe)) {
-                        return true;
+                        return ValidationResult.VALID;
                     }
                 }
             }
-            return false;
+            return ValidationResult.SHARD_MISSING;
         }
 
         @Override
@@ -291,9 +328,9 @@ public class ShardStateAction extends AbstractComponent {
         }
     }
 
-    public void shardStarted(final ShardRouting shardRouting, String indexUUID, final String message, Listener listener) {
+    public void shardStarted(final ShardRouting shardRouting, final String message, Listener listener) {
         ClusterStateObserver observer = new ClusterStateObserver(clusterService, null, logger, threadPool.getThreadContext());
-        ShardRoutingEntry shardRoutingEntry = new ShardRoutingEntry(shardRouting, indexUUID, message, null);
+        ShardRoutingEntry shardRoutingEntry = new ShardRoutingEntry(shardRouting, shardRouting, message, null);
         sendShardAction(SHARD_STARTED_ACTION_NAME, observer, shardRoutingEntry, listener);
     }
 
@@ -360,16 +397,16 @@ public class ShardStateAction extends AbstractComponent {
 
     public static class ShardRoutingEntry extends TransportRequest {
         ShardRouting shardRouting;
-        String indexUUID = IndexMetaData.INDEX_UUID_NA_VALUE;
+        ShardRouting sourceShardRouting;
         String message;
         Throwable failure;
 
         public ShardRoutingEntry() {
         }
 
-        ShardRoutingEntry(ShardRouting shardRouting, String indexUUID, String message, @Nullable Throwable failure) {
+        ShardRoutingEntry(ShardRouting shardRouting, ShardRouting sourceShardRouting, String message, @Nullable Throwable failure) {
             this.shardRouting = shardRouting;
-            this.indexUUID = indexUUID;
+            this.sourceShardRouting = sourceShardRouting;
             this.message = message;
             this.failure = failure;
         }
@@ -382,7 +419,7 @@ public class ShardStateAction extends AbstractComponent {
         public void readFrom(StreamInput in) throws IOException {
             super.readFrom(in);
             shardRouting = readShardRoutingEntry(in);
-            indexUUID = in.readString();
+            sourceShardRouting = readShardRoutingEntry(in);
             message = in.readString();
             failure = in.readThrowable();
         }
@@ -391,18 +428,25 @@ public class ShardStateAction extends AbstractComponent {
         public void writeTo(StreamOutput out) throws IOException {
             super.writeTo(out);
             shardRouting.writeTo(out);
-            out.writeString(indexUUID);
+            sourceShardRouting.writeTo(out);
             out.writeString(message);
             out.writeThrowable(failure);
         }
 
         @Override
         public String toString() {
-            return "" + shardRouting + ", indexUUID [" + indexUUID + "], message [" + message + "], failure [" + ExceptionsHelper.detailedMessage(failure) + "]";
+            return String.format(
+                Locale.ROOT,
+                "failed shard [%s], source shard [%s], message [%s], failure [%s]",
+                shardRouting,
+                sourceShardRouting,
+                message,
+                ExceptionsHelper.detailedMessage(failure));
         }
     }
 
     public interface Listener {
+
         default void onSuccess() {
         }
 
@@ -423,6 +467,20 @@ public class ShardStateAction extends AbstractComponent {
          */
         default void onFailure(final Throwable t) {
         }
+
+    }
+
+    public static class NoLongerPrimaryShardException extends ElasticsearchException {
+
+        public NoLongerPrimaryShardException(ShardId shardId, String msg) {
+            super(msg);
+            setShard(shardId);
+        }
+
+        public NoLongerPrimaryShardException(StreamInput in) throws IOException {
+            super(in);
+        }
+
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index 6d81556..58e3ed6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -43,6 +43,7 @@ import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
 import java.util.function.Predicate;
 
 /**
@@ -137,6 +138,13 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return shard;
     }
 
+    public IndexShardRoutingTable shardRoutingTableOrNull(ShardId shardId) {
+        return Optional
+            .ofNullable(index(shardId.getIndexName()))
+            .flatMap(irt -> Optional.ofNullable(irt.shard(shardId.getId())))
+            .orElse(null);
+    }
+
     public RoutingTable validateRaiseException(MetaData metaData) throws RoutingValidationException {
         RoutingTableValidation validation = validate(metaData);
         if (!validation.valid()) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index fa43c52..b592eeb 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -196,7 +196,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
         // note, we rely on the fact that its a new id each time we start, see FD and "kill -9" handling
         final String nodeId = DiscoveryService.generateNodeId(settings);
         final TransportAddress publishAddress = transportService.boundAddress().publishAddress();
-        DiscoveryNode localNode = new DiscoveryNode(settings.get("node.name"), nodeId, publishAddress, nodeAttributes, version);
+        DiscoveryNode localNode = new DiscoveryNode(settings.get("name"), nodeId, publishAddress, nodeAttributes, version);
         DiscoveryNodes.Builder nodeBuilder = DiscoveryNodes.builder().put(localNode).localNodeId(localNode.id());
         this.clusterState = ClusterState.builder(clusterState).nodes(nodeBuilder).blocks(initialBlocks).build();
         this.transportService.setLocalNode(localNode);
diff --git a/core/src/main/java/org/elasticsearch/common/cli/CheckFileCommand.java b/core/src/main/java/org/elasticsearch/common/cli/CheckFileCommand.java
index 9635ae5..95502f1 100644
--- a/core/src/main/java/org/elasticsearch/common/cli/CheckFileCommand.java
+++ b/core/src/main/java/org/elasticsearch/common/cli/CheckFileCommand.java
@@ -100,8 +100,9 @@ public abstract class CheckFileCommand extends CliTool.Command {
             Set<PosixFilePermission> permissionsBeforeWrite = entry.getValue();
             Set<PosixFilePermission> permissionsAfterWrite = Files.getPosixFilePermissions(entry.getKey());
             if (!permissionsBeforeWrite.equals(permissionsAfterWrite)) {
-                terminal.printWarn("The file permissions of [%s] have changed from [%s] to [%s]",
-                        entry.getKey(), PosixFilePermissions.toString(permissionsBeforeWrite), PosixFilePermissions.toString(permissionsAfterWrite));
+                terminal.printWarn("The file permissions of [" + entry.getKey() + "] have changed "
+                        + "from [" + PosixFilePermissions.toString(permissionsBeforeWrite) + "] "
+                        + "to [" + PosixFilePermissions.toString(permissionsAfterWrite) + "]");
                 terminal.printWarn("Please ensure that the user account running Elasticsearch has read access to this file!");
             }
         }
@@ -115,7 +116,7 @@ public abstract class CheckFileCommand extends CliTool.Command {
             String ownerBeforeWrite = entry.getValue();
             String ownerAfterWrite = Files.getOwner(entry.getKey()).getName();
             if (!ownerAfterWrite.equals(ownerBeforeWrite)) {
-                terminal.printWarn("WARN: Owner of file [%s] used to be [%s], but now is [%s]", entry.getKey(), ownerBeforeWrite, ownerAfterWrite);
+                terminal.printWarn("WARN: Owner of file [" + entry.getKey() + "] used to be [" + ownerBeforeWrite + "], but now is [" + ownerAfterWrite + "]");
             }
         }
 
@@ -128,7 +129,7 @@ public abstract class CheckFileCommand extends CliTool.Command {
             String groupBeforeWrite = entry.getValue();
             String groupAfterWrite = Files.readAttributes(entry.getKey(), PosixFileAttributes.class).group().getName();
             if (!groupAfterWrite.equals(groupBeforeWrite)) {
-                terminal.printWarn("WARN: Group of file [%s] used to be [%s], but now is [%s]", entry.getKey(), groupBeforeWrite, groupAfterWrite);
+                terminal.printWarn("WARN: Group of file [" + entry.getKey() + "] used to be [" + groupBeforeWrite + "], but now is [" + groupAfterWrite + "]");
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/cli/CliTool.java b/core/src/main/java/org/elasticsearch/common/cli/CliTool.java
index 7e954bc..17994eb 100644
--- a/core/src/main/java/org/elasticsearch/common/cli/CliTool.java
+++ b/core/src/main/java/org/elasticsearch/common/cli/CliTool.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.common.cli;
 
 import org.apache.commons.cli.AlreadySelectedException;
-import org.apache.commons.cli.AmbiguousOptionException;
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.CommandLineParser;
 import org.apache.commons.cli.DefaultParser;
@@ -31,7 +30,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 
-import java.io.IOException;
 import java.util.Locale;
 
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
@@ -127,7 +125,7 @@ public abstract class CliTool {
             String cmdName = args[0];
             cmd = config.cmd(cmdName);
             if (cmd == null) {
-                terminal.printError("unknown command [%s]. Use [-h] option to list available commands", cmdName);
+                terminal.printError("unknown command [" + cmdName + "]. Use [-h] option to list available commands");
                 return ExitStatus.USAGE;
             }
 
diff --git a/core/src/main/java/org/elasticsearch/common/cli/Terminal.java b/core/src/main/java/org/elasticsearch/common/cli/Terminal.java
index 7608c98..68229f6 100644
--- a/core/src/main/java/org/elasticsearch/common/cli/Terminal.java
+++ b/core/src/main/java/org/elasticsearch/common/cli/Terminal.java
@@ -89,37 +89,37 @@ public abstract class Terminal {
         println(Verbosity.NORMAL);
     }
 
-    public void println(String msg, Object... args) {
-        println(Verbosity.NORMAL, msg, args);
+    public void println(String msg) {
+        println(Verbosity.NORMAL, msg);
     }
 
-    public void print(String msg, Object... args) {
-        print(Verbosity.NORMAL, msg, args);
+    public void print(String msg) {
+        print(Verbosity.NORMAL, msg);
     }
 
     public void println(Verbosity verbosity) {
         println(verbosity, "");
     }
 
-    public void println(Verbosity verbosity, String msg, Object... args) {
-        print(verbosity, msg + System.lineSeparator(), args);
+    public void println(Verbosity verbosity, String msg) {
+        print(verbosity, msg + System.lineSeparator());
     }
 
-    public void print(Verbosity verbosity, String msg, Object... args) {
+    public void print(Verbosity verbosity, String msg) {
         if (this.verbosity.enabled(verbosity)) {
-            doPrint(msg, args);
+            doPrint(msg);
         }
     }
 
-    public void printError(String msg, Object... args) {
-        println(Verbosity.SILENT, "ERROR: " + msg, args);
+    public void printError(String msg) {
+        println(Verbosity.SILENT, "ERROR: " + msg);
     }
 
-    public void printWarn(String msg, Object... args) {
-        println(Verbosity.SILENT, "WARN: " + msg, args);
+    public void printWarn(String msg) {
+        println(Verbosity.SILENT, "WARN: " + msg);
     }
 
-    protected abstract void doPrint(String msg, Object... args);
+    protected abstract void doPrint(String msg);
 
     private static class ConsoleTerminal extends Terminal {
 
@@ -130,8 +130,8 @@ public abstract class Terminal {
         }
 
         @Override
-        public void doPrint(String msg, Object... args) {
-            console.printf(msg, args);
+        public void doPrint(String msg) {
+            console.printf("%s", msg);
             console.flush();
         }
 
@@ -157,13 +157,13 @@ public abstract class Terminal {
         private final PrintWriter printWriter = new PrintWriter(System.out);
 
         @Override
-        public void doPrint(String msg, Object... args) {
-            System.out.print(String.format(Locale.ROOT, msg, args));
+        public void doPrint(String msg) {
+            System.out.print(msg);
         }
 
         @Override
         public String readText(String text, Object... args) {
-            print(text, args);
+            print(text);
             BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
             try {
                 return reader.readLine();
diff --git a/core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java b/core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java
index d2fb9e3..6c2c19b 100644
--- a/core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java
+++ b/core/src/main/java/org/elasticsearch/common/component/AbstractComponent.java
@@ -50,7 +50,7 @@ public abstract class AbstractComponent {
      * Returns the nodes name from the settings or the empty string if not set.
      */
     public final String nodeName() {
-        return settings.get("node.name", "");
+        return settings.get("name", "");
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java b/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
index a10f94c..7c0a5ce 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/AbstractModule.java
@@ -23,7 +23,6 @@ import org.elasticsearch.common.inject.matcher.Matcher;
 import org.elasticsearch.common.inject.spi.Message;
 import org.elasticsearch.common.inject.spi.TypeConverter;
 import org.elasticsearch.common.inject.spi.TypeListener;
-import org.elasticsearch.common.settings.SettingsModule;
 
 import java.lang.annotation.Annotation;
 import java.util.Objects;
diff --git a/core/src/main/java/org/elasticsearch/common/logging/Loggers.java b/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
index 6aecca2..1447959 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/Loggers.java
@@ -99,7 +99,7 @@ public class Loggers {
                 prefixesList.add(addr.getHostName());
             }
         }
-        String name = settings.get("node.name");
+        String name = settings.get("name");
         if (name != null) {
             prefixesList.add(name);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
index 9b0998b..49d20d8 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
@@ -150,19 +150,16 @@ public class NetworkModule extends AbstractModule {
 
     public static final String TRANSPORT_TYPE_KEY = "transport.type";
     public static final String TRANSPORT_SERVICE_TYPE_KEY = "transport.service.type";
-    public static final String HTTP_TYPE_KEY = "http.type";
+
     public static final String LOCAL_TRANSPORT = "local";
     public static final String NETTY_TRANSPORT = "netty";
 
-    public static final Setting<String> HTTP_TYPE_SETTING = Setting.simpleString("http.type", false, Scope.CLUSTER);
+    public static final String HTTP_TYPE_KEY = "http.type";
     public static final Setting<Boolean> HTTP_ENABLED = Setting.boolSetting("http.enabled", true, false, Scope.CLUSTER);
-    public static final Setting<String> TRANSPORT_SERVICE_TYPE_SETTING = Setting.simpleString("transport.service.type", false, Scope.CLUSTER);
-    public static final Setting<String> TRANSPORT_TYPE_SETTING = Setting.simpleString("transport.type", false, Scope.CLUSTER);
-
-
 
     private static final List<Class<? extends RestHandler>> builtinRestHandlers = Arrays.asList(
         RestMainAction.class,
+
         RestNodesInfoAction.class,
         RestNodesStatsAction.class,
         RestNodesHotThreadsAction.class,
@@ -383,7 +380,7 @@ public class NetworkModule extends AbstractModule {
         } else {
             if (HTTP_ENABLED.get(settings)) {
                 bind(HttpServer.class).asEagerSingleton();
-                httpTransportTypes.bindType(binder(), settings, HTTP_TYPE_SETTING.getKey(), NETTY_TRANSPORT);
+                httpTransportTypes.bindType(binder(), settings, HTTP_TYPE_KEY, NETTY_TRANSPORT);
             }
             bind(RestController.class).asEagerSingleton();
             catHandlers.bind(binder());
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
index fa5b81c..616a732 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
@@ -75,7 +75,6 @@ import org.elasticsearch.monitor.os.OsService;
 import org.elasticsearch.monitor.process.ProcessService;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
-import org.elasticsearch.plugins.PluginsService;
 import org.elasticsearch.repositories.fs.FsRepository;
 import org.elasticsearch.repositories.uri.URLRepository;
 import org.elasticsearch.rest.BaseRestHandler;
@@ -114,9 +113,9 @@ public final class ClusterSettings extends AbstractScopedSettings {
         @Override
         public boolean hasChanged(Settings current, Settings previous) {
             return current.filter(loggerPredicate).getAsMap().equals(previous.filter(loggerPredicate).getAsMap()) == false;
-        }
+    }
 
-        @Override
+    @Override
         public Settings getValue(Settings current, Settings previous) {
             Settings.Builder builder = Settings.builder();
             builder.put(current.filter(loggerPredicate).getAsMap());
@@ -132,7 +131,7 @@ public final class ClusterSettings extends AbstractScopedSettings {
             return builder.build();
         }
 
-        @Override
+    @Override
         public void apply(Settings value, Settings current, Settings previous) {
             for (String key : value.getAsMap().keySet()) {
                 assert loggerPredicate.test(key);
@@ -143,243 +142,230 @@ public final class ClusterSettings extends AbstractScopedSettings {
                 } else {
                     ESLoggerFactory.getLogger(component).setLevel(value.get(key));
                 }
-            }
-        }
     }
-
-    ;
+    }
+    };
 
     public static Set<Setting<?>> BUILT_IN_CLUSTER_SETTINGS = Collections.unmodifiableSet(new HashSet<>(
-        Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
-            TransportClientNodesService.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL, // TODO these transport client settings are kind of odd here and should only be valid if we are a transport client
-            TransportClientNodesService.CLIENT_TRANSPORT_PING_TIMEOUT,
-            TransportClientNodesService.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME,
-            TransportClientNodesService.CLIENT_TRANSPORT_SNIFF,
-            AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
-            BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
-            BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
-            BalancedShardsAllocator.THRESHOLD_SETTING,
-            ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING,
-            ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING,
-            EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING,
-            EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING,
-            ZenDiscovery.REJOIN_ON_MASTER_GONE_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
-            FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
-            FsRepository.REPOSITORIES_CHUNK_SIZE_SETTING,
-            FsRepository.REPOSITORIES_COMPRESS_SETTING,
-            FsRepository.REPOSITORIES_LOCATION_SETTING,
-            IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
-            IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
-            IndicesQueryCache.INDICES_CACHE_QUERY_SIZE_SETTING,
-            IndicesQueryCache.INDICES_CACHE_QUERY_COUNT_SETTING,
-            IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
-            MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING,
-            MetaData.SETTING_READ_ONLY_SETTING,
-            RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING,
-            RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING,
-            RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING,
-            RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING,
-            RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING,
-            RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING,
-            ThreadPool.THREADPOOL_GROUP_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING,
-            ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING,
-            DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING,
-            InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING,
-            InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING,
-            SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING,
-            DestructiveOperations.REQUIRES_NAME_SETTING,
-            DiscoverySettings.PUBLISH_TIMEOUT_SETTING,
-            DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
-            DiscoverySettings.COMMIT_TIMEOUT_SETTING,
-            DiscoverySettings.NO_MASTER_BLOCK_SETTING,
-            GatewayService.EXPECTED_DATA_NODES_SETTING,
-            GatewayService.EXPECTED_MASTER_NODES_SETTING,
-            GatewayService.EXPECTED_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_DATA_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_NODES_SETTING,
-            GatewayService.RECOVER_AFTER_TIME_SETTING,
-            NetworkModule.HTTP_ENABLED,
-            NetworkModule.HTTP_TYPE_SETTING,
-            NetworkModule.TRANSPORT_SERVICE_TYPE_SETTING,
-            NetworkModule.TRANSPORT_TYPE_SETTING,
-            HttpTransportSettings.SETTING_CORS_ALLOW_CREDENTIALS,
-            HttpTransportSettings.SETTING_CORS_ENABLED,
-            HttpTransportSettings.SETTING_CORS_MAX_AGE,
-            HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
-            HttpTransportSettings.SETTING_PIPELINING,
-            HttpTransportSettings.SETTING_CORS_ALLOW_ORIGIN,
-            HttpTransportSettings.SETTING_HTTP_PORT,
-            HttpTransportSettings.SETTING_HTTP_PUBLISH_PORT,
-            HttpTransportSettings.SETTING_PIPELINING_MAX_EVENTS,
-            HttpTransportSettings.SETTING_HTTP_COMPRESSION,
-            HttpTransportSettings.SETTING_HTTP_COMPRESSION_LEVEL,
-            HttpTransportSettings.SETTING_CORS_ALLOW_METHODS,
-            HttpTransportSettings.SETTING_CORS_ALLOW_HEADERS,
-            HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
-            HttpTransportSettings.SETTING_HTTP_MAX_CONTENT_LENGTH,
-            HttpTransportSettings.SETTING_HTTP_MAX_CHUNK_SIZE,
-            HttpTransportSettings.SETTING_HTTP_MAX_HEADER_SIZE,
-            HttpTransportSettings.SETTING_HTTP_MAX_INITIAL_LINE_LENGTH,
-            HttpTransportSettings.SETTING_HTTP_RESET_COOKIES,
-            HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-            InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING,
-            SearchService.DEFAULT_SEARCH_TIMEOUT_SETTING,
-            ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING,
-            TransportService.TRACE_LOG_EXCLUDE_SETTING,
-            TransportService.TRACE_LOG_INCLUDE_SETTING,
-            TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING,
-            ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING,
-            InternalClusterService.CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING,
-            HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING,
-            HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING,
-            Transport.TRANSPORT_TCP_COMPRESS,
-            TransportSettings.TRANSPORT_PROFILES_SETTING,
-            TransportSettings.HOST,
-            TransportSettings.PUBLISH_HOST,
-            TransportSettings.BIND_HOST,
-            TransportSettings.PUBLISH_PORT,
-            TransportSettings.PORT,
-            NettyTransport.WORKER_COUNT,
-            NettyTransport.CONNECTIONS_PER_NODE_RECOVERY,
-            NettyTransport.CONNECTIONS_PER_NODE_BULK,
-            NettyTransport.CONNECTIONS_PER_NODE_REG,
-            NettyTransport.CONNECTIONS_PER_NODE_STATE,
-            NettyTransport.CONNECTIONS_PER_NODE_PING,
-            NettyTransport.PING_SCHEDULE,
-            NettyTransport.TCP_BLOCKING_CLIENT,
-            NettyTransport.TCP_CONNECT_TIMEOUT,
-            NettyTransport.NETTY_MAX_CUMULATION_BUFFER_CAPACITY,
-            NettyTransport.NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_SIZE,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_MIN,
-            NettyTransport.NETTY_RECEIVE_PREDICTOR_MAX,
-            NetworkService.NETWORK_SERVER,
-            NettyTransport.NETTY_BOSS_COUNT,
-            NettyTransport.TCP_NO_DELAY,
-            NettyTransport.TCP_KEEP_ALIVE,
-            NettyTransport.TCP_REUSE_ADDRESS,
-            NettyTransport.TCP_SEND_BUFFER_SIZE,
-            NettyTransport.TCP_RECEIVE_BUFFER_SIZE,
-            NettyTransport.TCP_BLOCKING_SERVER,
-            NetworkService.GLOBAL_NETWORK_HOST_SETTING,
-            NetworkService.GLOBAL_NETWORK_BINDHOST_SETTING,
-            NetworkService.GLOBAL_NETWORK_PUBLISHHOST_SETTING,
-            NetworkService.TcpSettings.TCP_NO_DELAY,
-            NetworkService.TcpSettings.TCP_KEEP_ALIVE,
-            NetworkService.TcpSettings.TCP_REUSE_ADDRESS,
-            NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE,
-            NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE,
-            NetworkService.TcpSettings.TCP_BLOCKING,
-            NetworkService.TcpSettings.TCP_BLOCKING_SERVER,
-            NetworkService.TcpSettings.TCP_BLOCKING_CLIENT,
-            NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT,
-            IndexSettings.QUERY_STRING_ANALYZE_WILDCARD,
-            IndexSettings.QUERY_STRING_ALLOW_LEADING_WILDCARD,
-            PrimaryShardAllocator.NODE_INITIAL_SHARDS_SETTING,
-            ScriptService.SCRIPT_CACHE_SIZE_SETTING,
-            ScriptService.SCRIPT_CACHE_EXPIRE_SETTING,
-            ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING,
-            IndicesFieldDataCache.INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING,
-            IndicesFieldDataCache.INDICES_FIELDDATA_CACHE_SIZE_KEY,
-            IndicesRequestCache.INDICES_CACHE_QUERY_SIZE,
-            IndicesRequestCache.INDICES_CACHE_QUERY_EXPIRE,
-            IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL,
-            HunspellService.HUNSPELL_LAZY_LOAD,
-            HunspellService.HUNSPELL_IGNORE_CASE,
-            HunspellService.HUNSPELL_DICTIONARY_OPTIONS,
-            IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT,
-            Environment.PATH_CONF_SETTING,
-            Environment.PATH_DATA_SETTING,
-            Environment.PATH_HOME_SETTING,
-            Environment.PATH_LOGS_SETTING,
-            Environment.PATH_PLUGINS_SETTING,
-            Environment.PATH_REPO_SETTING,
-            Environment.PATH_SCRIPTS_SETTING,
-            Environment.PATH_SHARED_DATA_SETTING,
-            Environment.PIDFILE_SETTING,
-            DiscoveryService.DISCOVERY_SEED_SETTING,
-            DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING,
-            DiscoveryModule.DISCOVERY_TYPE_SETTING,
-            DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING,
-            FaultDetection.PING_RETRIES_SETTING,
-            FaultDetection.PING_TIMEOUT_SETTING,
-            FaultDetection.REGISTER_CONNECTION_LISTENER_SETTING,
-            FaultDetection.PING_INTERVAL_SETTING,
-            FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING,
-            ZenDiscovery.PING_TIMEOUT_SETTING,
-            ZenDiscovery.JOIN_TIMEOUT_SETTING,
-            ZenDiscovery.JOIN_RETRY_ATTEMPTS_SETTING,
-            ZenDiscovery.JOIN_RETRY_DELAY_SETTING,
-            ZenDiscovery.MAX_PINGS_FROM_ANOTHER_MASTER_SETTING,
-            ZenDiscovery.SEND_LEAVE_REQUEST_SETTING,
-            ZenDiscovery.MASTER_ELECTION_FILTER_CLIENT_SETTING,
-            ZenDiscovery.MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING,
-            ZenDiscovery.MASTER_ELECTION_FILTER_DATA_SETTING,
-            UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING,
-            UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING,
-            SearchService.DEFAULT_KEEPALIVE_SETTING,
-            SearchService.KEEPALIVE_INTERVAL_SETTING,
-            Node.WRITE_PORTS_FIELD_SETTING,
-            Node.NODE_NAME_SETTING,
-            Node.NODE_CLIENT_SETTING,
-            Node.NODE_DATA_SETTING,
-            Node.NODE_MASTER_SETTING,
-            Node.NODE_LOCAL_SETTING,
-            Node.NODE_MODE_SETTING,
-            Node.NODE_INGEST_SETTING,
-            Node.NODE_ATTRIBUTES,
-            URLRepository.ALLOWED_URLS_SETTING,
-            URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
-            URLRepository.REPOSITORIES_URL_SETTING,
-            URLRepository.SUPPORTED_PROTOCOLS_SETTING,
-            TransportMasterNodeReadAction.FORCE_LOCAL_SETTING,
-            AutoCreateIndex.AUTO_CREATE_INDEX_SETTING,
-            BaseRestHandler.MULTI_ALLOW_EXPLICIT_INDEX,
-            ClusterName.CLUSTER_NAME_SETTING,
-            Client.CLIENT_TYPE_SETTING_S,
-            InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING,
-            ClusterModule.SHARDS_ALLOCATOR_TYPE_SETTING,
-            EsExecutors.PROCESSORS_SETTING,
-            ThreadContext.DEFAULT_HEADERS_SETTING,
-            ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING,
-            ESLoggerFactory.LOG_LEVEL_SETTING,
-            TribeService.BLOCKS_METADATA_SETTING,
-            TribeService.BLOCKS_WRITE_SETTING,
-            TribeService.BLOCKS_WRITE_INDICES_SETTING,
-            TribeService.BLOCKS_READ_INDICES_SETTING,
-            TribeService.BLOCKS_METADATA_INDICES_SETTING,
-            TribeService.ON_CONFLICT_SETTING,
-            TribeService.TRIBE_NAME_SETTING,
-            NodeEnvironment.MAX_LOCAL_STORAGE_NODES_SETTING,
-            NodeEnvironment.ENABLE_LUCENE_SEGMENT_INFOS_TRACE_SETTING,
-            NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH,
-            OsService.REFRESH_INTERVAL_SETTING,
-            ProcessService.REFRESH_INTERVAL_SETTING,
-            JvmService.REFRESH_INTERVAL_SETTING,
-            FsService.REFRESH_INTERVAL_SETTING,
-            JvmGcMonitorService.ENABLED_SETTING,
-            JvmGcMonitorService.REFRESH_INTERVAL_SETTING,
-            JvmGcMonitorService.GC_SETTING,
-            PageCacheRecycler.LIMIT_HEAP_SETTING,
-            PageCacheRecycler.WEIGHT_BYTES_SETTING,
-            PageCacheRecycler.WEIGHT_INT_SETTING,
-            PageCacheRecycler.WEIGHT_LONG_SETTING,
-            PageCacheRecycler.WEIGHT_OBJECTS_SETTING,
-            PageCacheRecycler.TYPE_SETTING,
-            PluginsService.MANDATORY_SETTING
-        )));
+            Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
+        TransportClientNodesService.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL, // TODO these transport client settings are kind of odd here and should only be valid if we are a transport client
+        TransportClientNodesService.CLIENT_TRANSPORT_PING_TIMEOUT,
+        TransportClientNodesService.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME,
+        AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
+        BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
+        BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
+        BalancedShardsAllocator.THRESHOLD_SETTING,
+        ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING,
+        ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING,
+        EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING,
+        EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING,
+        FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
+        FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
+        FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
+        FsRepository.REPOSITORIES_CHUNK_SIZE_SETTING,
+        FsRepository.REPOSITORIES_COMPRESS_SETTING,
+        FsRepository.REPOSITORIES_LOCATION_SETTING,
+        IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
+        IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
+                    IndicesQueryCache.INDICES_CACHE_QUERY_SIZE_SETTING,
+                    IndicesQueryCache.INDICES_CACHE_QUERY_COUNT_SETTING,
+        IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
+        MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING,
+        MetaData.SETTING_READ_ONLY_SETTING,
+        RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING,
+        RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING,
+        RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING,
+        RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING,
+        RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING,
+        RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING,
+        ThreadPool.THREADPOOL_GROUP_SETTING,
+        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING,
+        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING,
+        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_OUTGOING_RECOVERIES_SETTING,
+        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING,
+        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING,
+        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING,
+        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING,
+        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING,
+        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING,
+        InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING,
+        InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING,
+        SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING,
+        DestructiveOperations.REQUIRES_NAME_SETTING,
+        DiscoverySettings.PUBLISH_TIMEOUT_SETTING,
+        DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
+        DiscoverySettings.COMMIT_TIMEOUT_SETTING,
+        DiscoverySettings.NO_MASTER_BLOCK_SETTING,
+        GatewayService.EXPECTED_DATA_NODES_SETTING,
+        GatewayService.EXPECTED_MASTER_NODES_SETTING,
+        GatewayService.EXPECTED_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_DATA_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_TIME_SETTING,
+        NetworkModule.HTTP_ENABLED,
+        HttpTransportSettings.SETTING_CORS_ALLOW_CREDENTIALS,
+        HttpTransportSettings.SETTING_CORS_ENABLED,
+        HttpTransportSettings.SETTING_CORS_MAX_AGE,
+        HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
+        HttpTransportSettings.SETTING_PIPELINING,       
+        HttpTransportSettings.SETTING_CORS_ALLOW_ORIGIN,
+        HttpTransportSettings.SETTING_HTTP_PORT,
+        HttpTransportSettings.SETTING_HTTP_PUBLISH_PORT,
+        HttpTransportSettings.SETTING_PIPELINING_MAX_EVENTS,
+        HttpTransportSettings.SETTING_HTTP_COMPRESSION,
+        HttpTransportSettings.SETTING_HTTP_COMPRESSION_LEVEL,
+        HttpTransportSettings.SETTING_CORS_ALLOW_METHODS,
+        HttpTransportSettings.SETTING_CORS_ALLOW_HEADERS,
+        HttpTransportSettings.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
+        HttpTransportSettings.SETTING_HTTP_MAX_CONTENT_LENGTH,
+        HttpTransportSettings.SETTING_HTTP_MAX_CHUNK_SIZE,
+        HttpTransportSettings.SETTING_HTTP_MAX_HEADER_SIZE,
+        HttpTransportSettings.SETTING_HTTP_MAX_INITIAL_LINE_LENGTH,
+        HttpTransportSettings.SETTING_HTTP_RESET_COOKIES,
+        HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
+        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
+        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
+        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
+        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING,
+        InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING,
+        SearchService.DEFAULT_SEARCH_TIMEOUT_SETTING,
+        ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING,
+        TransportService.TRACE_LOG_EXCLUDE_SETTING,
+        TransportService.TRACE_LOG_INCLUDE_SETTING,
+        TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING,
+        ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING,
+        InternalClusterService.CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING,
+        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING,
+        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING,
+        Transport.TRANSPORT_TCP_COMPRESS,
+                    TransportSettings.TRANSPORT_PROFILES_SETTING,
+                    TransportSettings.HOST,
+                    TransportSettings.PUBLISH_HOST,
+                    TransportSettings.BIND_HOST,
+                    TransportSettings.PUBLISH_PORT,
+                    TransportSettings.PORT,
+                    NettyTransport.WORKER_COUNT,
+                    NettyTransport.CONNECTIONS_PER_NODE_RECOVERY,
+                    NettyTransport.CONNECTIONS_PER_NODE_BULK,
+                    NettyTransport.CONNECTIONS_PER_NODE_REG,
+                    NettyTransport.CONNECTIONS_PER_NODE_STATE,
+                    NettyTransport.CONNECTIONS_PER_NODE_PING,
+                    NettyTransport.PING_SCHEDULE,
+                    NettyTransport.TCP_BLOCKING_CLIENT,
+                    NettyTransport.TCP_CONNECT_TIMEOUT,
+                    NettyTransport.NETTY_MAX_CUMULATION_BUFFER_CAPACITY,
+                    NettyTransport.NETTY_MAX_COMPOSITE_BUFFER_COMPONENTS,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_SIZE,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_MIN,
+                    NettyTransport.NETTY_RECEIVE_PREDICTOR_MAX,
+                    NetworkService.NETWORK_SERVER,
+                    NettyTransport.NETTY_BOSS_COUNT,
+                    NettyTransport.TCP_NO_DELAY,
+                    NettyTransport.TCP_KEEP_ALIVE,
+                    NettyTransport.TCP_REUSE_ADDRESS,
+                    NettyTransport.TCP_SEND_BUFFER_SIZE,
+                    NettyTransport.TCP_RECEIVE_BUFFER_SIZE,
+                    NettyTransport.TCP_BLOCKING_SERVER,
+        NetworkService.GLOBAL_NETWORK_HOST_SETTING,
+        NetworkService.GLOBAL_NETWORK_BINDHOST_SETTING,
+        NetworkService.GLOBAL_NETWORK_PUBLISHHOST_SETTING,
+        NetworkService.TcpSettings.TCP_NO_DELAY,
+        NetworkService.TcpSettings.TCP_KEEP_ALIVE,
+        NetworkService.TcpSettings.TCP_REUSE_ADDRESS,
+        NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE,
+        NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE,
+        NetworkService.TcpSettings.TCP_BLOCKING,
+        NetworkService.TcpSettings.TCP_BLOCKING_SERVER,
+        NetworkService.TcpSettings.TCP_BLOCKING_CLIENT,
+        NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT,
+        IndexSettings.QUERY_STRING_ANALYZE_WILDCARD,
+        IndexSettings.QUERY_STRING_ALLOW_LEADING_WILDCARD,
+        PrimaryShardAllocator.NODE_INITIAL_SHARDS_SETTING,
+        ScriptService.SCRIPT_CACHE_SIZE_SETTING,
+        IndicesFieldDataCache.INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING,
+        IndicesFieldDataCache.INDICES_FIELDDATA_CACHE_SIZE_KEY,
+        IndicesRequestCache.INDICES_CACHE_QUERY_SIZE,
+        IndicesRequestCache.INDICES_CACHE_QUERY_EXPIRE,
+                    IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL,
+        HunspellService.HUNSPELL_LAZY_LOAD,
+        HunspellService.HUNSPELL_IGNORE_CASE,
+        HunspellService.HUNSPELL_DICTIONARY_OPTIONS,
+        IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT,
+        Environment.PATH_CONF_SETTING,
+        Environment.PATH_DATA_SETTING,
+        Environment.PATH_HOME_SETTING,
+        Environment.PATH_LOGS_SETTING,
+        Environment.PATH_PLUGINS_SETTING,
+        Environment.PATH_REPO_SETTING,
+        Environment.PATH_SCRIPTS_SETTING,
+        Environment.PATH_SHARED_DATA_SETTING,
+        Environment.PIDFILE_SETTING,
+        DiscoveryService.DISCOVERY_SEED_SETTING,
+        DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING,
+        DiscoveryModule.DISCOVERY_TYPE_SETTING,
+        DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING,
+        FaultDetection.PING_RETRIES_SETTING,
+        FaultDetection.PING_TIMEOUT_SETTING,
+        FaultDetection.REGISTER_CONNECTION_LISTENER_SETTING,
+        FaultDetection.PING_INTERVAL_SETTING,
+        FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING,
+        ZenDiscovery.PING_TIMEOUT_SETTING,
+        ZenDiscovery.JOIN_TIMEOUT_SETTING,
+        ZenDiscovery.JOIN_RETRY_ATTEMPTS_SETTING,
+        ZenDiscovery.JOIN_RETRY_DELAY_SETTING,
+        ZenDiscovery.MAX_PINGS_FROM_ANOTHER_MASTER_SETTING,
+        ZenDiscovery.SEND_LEAVE_REQUEST_SETTING,
+        ZenDiscovery.MASTER_ELECTION_FILTER_CLIENT_SETTING,
+        ZenDiscovery.MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING,
+        ZenDiscovery.MASTER_ELECTION_FILTER_DATA_SETTING,
+        UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING,
+        UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING,
+        SearchService.DEFAULT_KEEPALIVE_SETTING,
+        SearchService.KEEPALIVE_INTERVAL_SETTING,
+        Node.WRITE_PORTS_FIELD_SETTING,
+                    Node.NODE_CLIENT_SETTING,
+                    Node.NODE_DATA_SETTING,
+                    Node.NODE_MASTER_SETTING,
+                    Node.NODE_LOCAL_SETTING,
+                    Node.NODE_MODE_SETTING,
+                    Node.NODE_INGEST_SETTING,
+        URLRepository.ALLOWED_URLS_SETTING,
+        URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
+        URLRepository.REPOSITORIES_URL_SETTING,
+                    URLRepository.SUPPORTED_PROTOCOLS_SETTING,
+                    TransportMasterNodeReadAction.FORCE_LOCAL_SETTING,
+                    AutoCreateIndex.AUTO_CREATE_INDEX_SETTING,
+                    BaseRestHandler.MULTI_ALLOW_EXPLICIT_INDEX,
+                    ClusterName.CLUSTER_NAME_SETTING,
+                    Client.CLIENT_TYPE_SETTING_S,
+                    InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING,
+                    ClusterModule.SHARDS_ALLOCATOR_TYPE_SETTING,
+                    EsExecutors.PROCESSORS_SETTING,
+                    ThreadContext.DEFAULT_HEADERS_SETTING,
+                    ESLoggerFactory.LOG_DEFAULT_LEVEL_SETTING,
+                    ESLoggerFactory.LOG_LEVEL_SETTING,
+                    TribeService.BLOCKS_METADATA_SETTING,
+                    TribeService.BLOCKS_WRITE_SETTING,
+                    TribeService.BLOCKS_WRITE_INDICES_SETTING,
+                    TribeService.BLOCKS_READ_INDICES_SETTING,
+                    TribeService.BLOCKS_METADATA_INDICES_SETTING,
+                    TribeService.ON_CONFLICT_SETTING,
+                    NodeEnvironment.MAX_LOCAL_STORAGE_NODES_SETTING,
+                    NodeEnvironment.ENABLE_LUCENE_SEGMENT_INFOS_TRACE_SETTING,
+                    NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH,
+                    OsService.REFRESH_INTERVAL_SETTING,
+                    ProcessService.REFRESH_INTERVAL_SETTING,
+                    JvmService.REFRESH_INTERVAL_SETTING,
+                    FsService.REFRESH_INTERVAL_SETTING,
+                    JvmGcMonitorService.ENABLED_SETTING,
+                    JvmGcMonitorService.REFRESH_INTERVAL_SETTING,
+                    JvmGcMonitorService.GC_SETTING,
+                    PageCacheRecycler.LIMIT_HEAP_SETTING,
+                    PageCacheRecycler.WEIGHT_BYTES_SETTING,
+                    PageCacheRecycler.WEIGHT_INT_SETTING,
+                    PageCacheRecycler.WEIGHT_LONG_SETTING,
+                    PageCacheRecycler.WEIGHT_OBJECTS_SETTING,
+                    PageCacheRecycler.TYPE_SETTING
+                )));
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java b/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
index 5d2f4d3..f95cc1f 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.common.settings;
 
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.tribe.TribeService;
 
 import java.util.HashMap;
 import java.util.HashSet;
@@ -38,8 +37,6 @@ public class SettingsModule extends AbstractModule {
     private final SettingsFilter settingsFilter;
     private final Map<String, Setting<?>> clusterSettings = new HashMap<>();
     private final Map<String, Setting<?>> indexSettings = new HashMap<>();
-    private static final Predicate<String> TRIBE_CLIENT_NODE_SETTINGS_PREDICATE =  (s) -> s.startsWith("tribe.") && TribeService.TRIBE_SETTING_KEYS.contains(s) == false;
-
 
     public SettingsModule(Settings settings, SettingsFilter settingsFilter) {
         this.settings = settings;
@@ -58,8 +55,12 @@ public class SettingsModule extends AbstractModule {
         final ClusterSettings clusterSettings = new ClusterSettings(settings, new HashSet<>(this.clusterSettings.values()));
         // by now we are fully configured, lets check node level settings for unregistered index settings
         indexScopedSettings.validate(settings.filter(IndexScopedSettings.INDEX_SETTINGS_KEY_PREDICATE));
-        final Predicate<String> acceptOnlyClusterSettings = TRIBE_CLIENT_NODE_SETTINGS_PREDICATE.or(IndexScopedSettings.INDEX_SETTINGS_KEY_PREDICATE).negate();
-        clusterSettings.validate(settings.filter(acceptOnlyClusterSettings));
+        Predicate<String> noIndexSettingPredicate = IndexScopedSettings.INDEX_SETTINGS_KEY_PREDICATE.negate();
+        Predicate<String> noTribePredicate = (s) -> s.startsWith("tribe.") == false;
+        for (Map.Entry<String, String> entry : settings.filter(noTribePredicate.and(noIndexSettingPredicate)).getAsMap().entrySet()) {
+            validateClusterSetting(clusterSettings, entry.getKey(), settings);
+        }
+
         validateTribeSettings(settings, clusterSettings);
         bind(Settings.class).toInstance(settings);
         bind(SettingsFilter.class).toInstance(settingsFilter);
@@ -86,16 +87,24 @@ public class SettingsModule extends AbstractModule {
     }
 
     public void validateTribeSettings(Settings settings, ClusterSettings clusterSettings) {
-        Map<String, Settings> groups = settings.filter(TRIBE_CLIENT_NODE_SETTINGS_PREDICATE).getGroups("tribe.", true);
+        Map<String, Settings> groups = settings.getGroups("tribe.", true);
         for (Map.Entry<String, Settings>  tribeSettings : groups.entrySet()) {
-            Settings thisTribesSettings = tribeSettings.getValue();
-            for (Map.Entry<String, String> entry : thisTribesSettings.getAsMap().entrySet()) {
-                try {
-                    clusterSettings.validate(entry.getKey(), thisTribesSettings);
-                } catch (IllegalArgumentException ex) {
-                    throw new IllegalArgumentException("tribe." + tribeSettings.getKey() +" validation failed: "+ ex.getMessage(), ex);
-                }
+            for (Map.Entry<String, String> entry : tribeSettings.getValue().getAsMap().entrySet()) {
+                validateClusterSetting(clusterSettings, entry.getKey(), tribeSettings.getValue());
             }
         }
     }
+
+    private final void validateClusterSetting(ClusterSettings clusterSettings, String key, Settings settings) {
+        // we can't call this method yet since we have not all node level settings registered.
+        // yet we can validate the ones we have registered to not have invalid values. this is better than nothing
+        // and progress over perfection and we fail as soon as possible.
+        // clusterSettings.validate(settings.filter(IndexScopedSettings.INDEX_SETTINGS_KEY_PREDICATE.negate()));
+        if (clusterSettings.get(key) != null) {
+            clusterSettings.validate(key, settings);
+        } else if (AbstractScopedSettings.isValidKey(key) == false) {
+            throw new IllegalArgumentException("illegal settings key: [" + key + "]");
+        }
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java
index 10b1412..723d7df 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java
@@ -90,7 +90,7 @@ public class EsExecutors {
     }
 
     public static String threadName(Settings settings, String namePrefix) {
-        String name = settings.get("node.name");
+        String name = settings.get("name");
         if (name == null) {
             name = "elasticsearch";
         } else {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index 55eaf78..ffa29c8 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -89,17 +89,16 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
  */
 public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implements Discovery, PingContextProvider {
 
-    public final static Setting<Boolean> REJOIN_ON_MASTER_GONE_SETTING = Setting.boolSetting("discovery.zen.rejoin_on_master_gone", true, true, Setting.Scope.CLUSTER);
     public final static Setting<TimeValue> PING_TIMEOUT_SETTING = Setting.positiveTimeSetting("discovery.zen.ping_timeout", timeValueSeconds(3), false, Setting.Scope.CLUSTER);
     public final static Setting<TimeValue> JOIN_TIMEOUT_SETTING = Setting.timeSetting("discovery.zen.join_timeout",
-        settings -> TimeValue.timeValueMillis(PING_TIMEOUT_SETTING.get(settings).millis() * 20).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
+            settings -> TimeValue.timeValueMillis(PING_TIMEOUT_SETTING.get(settings).millis() * 20).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
     public final static Setting<Integer> JOIN_RETRY_ATTEMPTS_SETTING = Setting.intSetting("discovery.zen.join_retry_attempts", 3, 1, false, Setting.Scope.CLUSTER);
     public final static Setting<TimeValue> JOIN_RETRY_DELAY_SETTING = Setting.positiveTimeSetting("discovery.zen.join_retry_delay", TimeValue.timeValueMillis(100), false, Setting.Scope.CLUSTER);
     public final static Setting<Integer> MAX_PINGS_FROM_ANOTHER_MASTER_SETTING = Setting.intSetting("discovery.zen.max_pings_from_another_master", 3, 1, false, Setting.Scope.CLUSTER);
     public final static Setting<Boolean> SEND_LEAVE_REQUEST_SETTING = Setting.boolSetting("discovery.zen.send_leave_request", true, false, Setting.Scope.CLUSTER);
     public final static Setting<Boolean> MASTER_ELECTION_FILTER_CLIENT_SETTING = Setting.boolSetting("discovery.zen.master_election.filter_client", true, false, Setting.Scope.CLUSTER);
     public final static Setting<TimeValue> MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING = Setting.timeSetting("discovery.zen.master_election.wait_for_joins_timeout",
-        settings -> TimeValue.timeValueMillis(JOIN_TIMEOUT_SETTING.get(settings).millis() / 2).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
+            settings -> TimeValue.timeValueMillis(JOIN_TIMEOUT_SETTING.get(settings).millis() / 2).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
     public final static Setting<Boolean> MASTER_ELECTION_FILTER_DATA_SETTING = Setting.boolSetting("discovery.zen.master_election.filter_data", false, false, Setting.Scope.CLUSTER);
 
     public static final String DISCOVERY_REJOIN_ACTION_NAME = "internal:discovery/zen/rejoin";
@@ -142,8 +141,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
 
     private final AtomicBoolean initialStateSent = new AtomicBoolean();
 
-    private volatile boolean rejoinOnMasterGone;
-
     /** counts the time this node has joined the cluster or have elected it self as master */
     private final AtomicLong clusterJoinsCounter = new AtomicLong();
 
@@ -177,7 +174,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.masterElectionFilterClientNodes = MASTER_ELECTION_FILTER_CLIENT_SETTING.get(settings);
         this.masterElectionFilterDataNodes = MASTER_ELECTION_FILTER_DATA_SETTING.get(settings);
         this.masterElectionWaitForJoinsTimeout = MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING.get(settings);
-        this.rejoinOnMasterGone = REJOIN_ON_MASTER_GONE_SETTING.get(settings);
 
         logger.debug("using ping_timeout [{}], join.timeout [{}], master_election.filter_client [{}], master_election.filter_data [{}]", this.pingTimeout, joinTimeout, masterElectionFilterClientNodes, masterElectionFilterDataNodes);
 
@@ -188,7 +184,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                 throw new IllegalArgumentException("cannot set " + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey() + " to more than the current master nodes count [" + masterNodes + "]");
             }
         });
-        clusterSettings.addSettingsUpdateConsumer(REJOIN_ON_MASTER_GONE_SETTING, this::setRejoingOnMasterGone);
 
         this.masterFD = new MasterFaultDetection(settings, threadPool, transportService, clusterName, clusterService);
         this.masterFD.addListener(new MasterNodeFailureListener());
@@ -323,10 +318,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         return clusterJoinsCounter.get() > 0;
     }
 
-    private void setRejoingOnMasterGone(boolean rejoin) {
-        this.rejoinOnMasterGone = rejoin;
-    }
-
     /** end of {@link org.elasticsearch.discovery.zen.ping.PingContextProvider } implementation */
 
 
@@ -670,35 +661,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
                 // flush any pending cluster states from old master, so it will not be set as master again
                 publishClusterState.pendingStatesQueue().failAllStatesAndClear(new ElasticsearchException("master left [{}]", reason));
 
-                if (rejoinOnMasterGone) {
-                    return rejoin(ClusterState.builder(currentState).nodes(discoveryNodes).build(), "master left (reason = " + reason + ")");
-                }
-
-                if (!electMaster.hasEnoughMasterNodes(discoveryNodes)) {
-                    return rejoin(ClusterState.builder(currentState).nodes(discoveryNodes).build(), "not enough master nodes after master left (reason = " + reason + ")");
-                }
-
-                final DiscoveryNode electedMaster = electMaster.electMaster(discoveryNodes); // elect master
-                final DiscoveryNode localNode = currentState.nodes().localNode();
-                if (localNode.equals(electedMaster)) {
-                    masterFD.stop("got elected as new master since master left (reason = " + reason + ")");
-                    discoveryNodes = DiscoveryNodes.builder(discoveryNodes).masterNodeId(localNode.id()).build();
-                    ClusterState newState = ClusterState.builder(currentState).nodes(discoveryNodes).build();
-                    nodesFD.updateNodesAndPing(newState);
-                    return newState;
-
-                } else {
-                    nodesFD.stop();
-                    if (electedMaster != null) {
-                        discoveryNodes = DiscoveryNodes.builder(discoveryNodes).masterNodeId(electedMaster.id()).build();
-                        masterFD.restart(electedMaster, "possible elected master since master left (reason = " + reason + ")");
-                        return ClusterState.builder(currentState)
-                                .nodes(discoveryNodes)
-                                .build();
-                    } else {
-                        return rejoin(ClusterState.builder(currentState).nodes(discoveryNodes).build(), "master_left and no other node elected to become master");
-                    }
-                }
+                return rejoin(ClusterState.builder(currentState).nodes(discoveryNodes).build(), "master left (reason = " + reason + ")");
             }
 
             @Override
@@ -857,7 +820,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
             // Sanity check: maybe we don't end up here, because serialization may have failed.
             if (node.getVersion().before(minimumNodeJoinVersion)) {
                 callback.onFailure(
-                    new IllegalStateException("Can't handle join request from a node with a version [" + node.getVersion() + "] that is lower than the minimum compatible version [" + minimumNodeJoinVersion.minimumCompatibilityVersion() + "]")
+                        new IllegalStateException("Can't handle join request from a node with a version [" + node.getVersion() + "] that is lower than the minimum compatible version [" + minimumNodeJoinVersion.minimumCompatibilityVersion() + "]")
                 );
                 return;
             }
@@ -1109,10 +1072,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         }
     }
 
-    boolean isRejoinOnMasterGone() {
-        return rejoinOnMasterGone;
-    }
-
     public static class RejoinClusterRequest extends TransportRequest {
 
         private String fromNodeId;
diff --git a/core/src/main/java/org/elasticsearch/index/IndexSettings.java b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
index d6a4e90..3919584 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexSettings.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexSettings.java
@@ -182,7 +182,7 @@ public final class IndexSettings {
         this.index = indexMetaData.getIndex();
         version = Version.indexCreated(settings);
         logger = Loggers.getLogger(getClass(), settings, index);
-        nodeName = settings.get("node.name", "");
+        nodeName = settings.get("name", "");
         this.indexMetaData = indexMetaData;
         numberOfShards = settings.getAsInt(IndexMetaData.SETTING_NUMBER_OF_SHARDS, null);
         isShadowReplicaIndex = IndexMetaData.isIndexUsingShadowReplicas(settings);
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index 7fc12eb..98bbd5f 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -39,13 +39,11 @@ import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.Callback;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.index.IndexService;
@@ -93,26 +91,12 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
     private static final ShardStateAction.Listener SHARD_STATE_ACTION_LISTENER = new ShardStateAction.Listener() {};
 
-    // a map of mappings type we have seen per index due to cluster state
-    // we need this so we won't remove types automatically created as part of the indexing process
-    private final ConcurrentMap<Tuple<String, String>, Boolean> seenMappings = ConcurrentCollections.newConcurrentMap();
-
     // a list of shards that failed during recovery
     // we keep track of these shards in order to prevent repeated recovery of these shards on each cluster state update
-    private final ConcurrentMap<ShardId, FailedShard> failedShards = ConcurrentCollections.newConcurrentMap();
+    private final ConcurrentMap<ShardId, ShardRouting> failedShards = ConcurrentCollections.newConcurrentMap();
     private final RestoreService restoreService;
     private final RepositoriesService repositoriesService;
 
-    static class FailedShard {
-        public final long version;
-        public final long timestamp;
-
-        FailedShard(long version) {
-            this.version = version;
-            this.timestamp = System.currentTimeMillis();
-        }
-    }
-
     private final Object mutex = new Object();
     private final FailedShardHandler failedShardHandler = new FailedShardHandler();
 
@@ -322,7 +306,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                 try {
                     indicesService.createIndex(nodeServicesProvider, indexMetaData, buildInIndexListener);
                 } catch (Throwable e) {
-                    sendFailShard(shard, indexMetaData.getIndexUUID(), "failed to create index", e);
+                    sendFailShard(shard, "failed to create index", e);
                 }
             }
         }
@@ -387,7 +371,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                 // so this failure typically means wrong node level configuration or something similar
                 for (IndexShard indexShard : indexService) {
                     ShardRouting shardRouting = indexShard.routingEntry();
-                    failAndRemoveShard(shardRouting, indexService.indexUUID(), indexService, true, "failed to update mappings", t);
+                    failAndRemoveShard(shardRouting, indexService, true, "failed to update mappings", t);
                 }
             }
         }
@@ -436,6 +420,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             failedShards.clear();
             return;
         }
+
         DiscoveryNodes nodes = event.state().nodes();
 
         for (final ShardRouting shardRouting : routingNode) {
@@ -455,12 +440,13 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             if (!indexService.hasShard(shardId) && shardRouting.started()) {
                 if (failedShards.containsKey(shardRouting.shardId())) {
                     if (nodes.masterNode() != null) {
-                        shardStateAction.resendShardFailed(shardRouting, indexMetaData.getIndexUUID(),
-                                "master " + nodes.masterNode() + " marked shard as started, but shard has previous failed. resending shard failure.", null, SHARD_STATE_ACTION_LISTENER);
+                        String message = "master " + nodes.masterNode() + " marked shard as started, but shard has previous failed. resending shard failure";
+                        logger.trace("[{}] re-sending failed shard [{}], reason [{}]", shardRouting.shardId(), shardRouting, message);
+                        shardStateAction.shardFailed(shardRouting, shardRouting, message, null, SHARD_STATE_ACTION_LISTENER);
                     }
                 } else {
                     // the master thinks we are started, but we don't have this shard at all, mark it as failed
-                    sendFailShard(shardRouting, indexMetaData.getIndexUUID(), "master [" + nodes.masterNode() + "] marked shard as started, but shard has not been created, mark shard as failed", null);
+                    sendFailShard(shardRouting, "master [" + nodes.masterNode() + "] marked shard as started, but shard has not been created, mark shard as failed", null);
                 }
                 continue;
             }
@@ -495,7 +481,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                     try {
                         indexShard.updateRoutingEntry(shardRouting, event.state().blocks().disableStatePersistence() == false);
                     } catch (Throwable e) {
-                        failAndRemoveShard(shardRouting, indexService.indexUUID(), indexService, true, "failed updating shard routing entry", e);
+                        failAndRemoveShard(shardRouting, indexService, true, "failed updating shard routing entry", e);
                     }
                 }
             }
@@ -507,40 +493,29 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
     }
 
     private void cleanFailedShards(final ClusterChangedEvent event) {
-        RoutingTable routingTable = event.state().routingTable();
         RoutingNodes.RoutingNodeIterator routingNode = event.state().getRoutingNodes().routingNodeIter(event.state().nodes().localNodeId());
         if (routingNode == null) {
             failedShards.clear();
             return;
         }
-        DiscoveryNodes nodes = event.state().nodes();
-        long now = System.currentTimeMillis();
-        String localNodeId = nodes.localNodeId();
-        Iterator<Map.Entry<ShardId, FailedShard>> iterator = failedShards.entrySet().iterator();
-        shards:
-        while (iterator.hasNext()) {
-            Map.Entry<ShardId, FailedShard> entry = iterator.next();
-            FailedShard failedShard = entry.getValue();
-            IndexRoutingTable indexRoutingTable = routingTable.index(entry.getKey().getIndex());
-            if (indexRoutingTable != null) {
-                IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(entry.getKey().id());
-                if (shardRoutingTable != null) {
-                    for (ShardRouting shardRouting : shardRoutingTable.assignedShards()) {
-                        if (localNodeId.equals(shardRouting.currentNodeId())) {
-                            // we have a timeout here just to make sure we don't have dangled failed shards for some reason
-                            // its just another safely layer
-                            if (shardRouting.version() == failedShard.version && ((now - failedShard.timestamp) < TimeValue.timeValueMinutes(60).millis())) {
-                                // It's the same failed shard - keep it if it hasn't timed out
-                                continue shards;
-                            } else {
-                                // Different version or expired, remove it
-                                break;
-                            }
-                        }
-                    }
-                }
+        RoutingTable routingTable = event.state().routingTable();
+        for (Iterator<Map.Entry<ShardId, ShardRouting>> iterator = failedShards.entrySet().iterator(); iterator.hasNext(); ) {
+            Map.Entry<ShardId, ShardRouting> entry = iterator.next();
+            ShardId failedShardId = entry.getKey();
+            ShardRouting failedShardRouting = entry.getValue();
+            IndexRoutingTable indexRoutingTable = routingTable.index(failedShardId.getIndex());
+            if (indexRoutingTable == null) {
+                iterator.remove();
+                continue;
+            }
+            IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(failedShardId.id());
+            if (shardRoutingTable == null) {
+                iterator.remove();
+                continue;
+            }
+            if (shardRoutingTable.assignedShards().stream().noneMatch(shr -> shr.isSameAllocation(failedShardRouting))) {
+                iterator.remove();
             }
-            iterator.remove();
         }
     }
 
@@ -565,7 +540,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                             indexShard.shardId(), indexShard.state(), nodes.masterNode());
                 }
                 if (nodes.masterNode() != null) {
-                    shardStateAction.shardStarted(shardRouting, indexMetaData.getIndexUUID(),
+                    shardStateAction.shardStarted(shardRouting,
                         "master " + nodes.masterNode() + " marked shard as initializing, but shard state is [" + indexShard.state() + "], mark shard as started",
                         SHARD_STATE_ACTION_LISTENER);
                 }
@@ -592,8 +567,9 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
         if (!indexService.hasShard(shardId)) {
             if (failedShards.containsKey(shardRouting.shardId())) {
                 if (nodes.masterNode() != null) {
-                    shardStateAction.resendShardFailed(shardRouting, indexMetaData.getIndexUUID(),
-                            "master " + nodes.masterNode() + " marked shard as initializing, but shard is marked as failed, resend shard failure", null, SHARD_STATE_ACTION_LISTENER);
+                    String message = "master " + nodes.masterNode() + " marked shard as initializing, but shard is marked as failed, resend shard failure";
+                    logger.trace("[{}] re-sending failed shard [{}], reason [{}]", shardRouting.shardId(), shardRouting, message);
+                    shardStateAction.shardFailed(shardRouting, shardRouting, message, null, SHARD_STATE_ACTION_LISTENER);
                 }
                 return;
             }
@@ -606,7 +582,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             } catch (IndexShardAlreadyExistsException e) {
                 // ignore this, the method call can happen several times
             } catch (Throwable e) {
-                failAndRemoveShard(shardRouting, indexService.indexUUID(), indexService, true, "failed to create shard", e);
+                failAndRemoveShard(shardRouting, indexService, true, "failed to create shard", e);
                 return;
             }
         }
@@ -648,7 +624,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             threadPool.generic().execute(() -> {
                 try {
                     if (indexShard.recoverFromStore(nodes.localNode())) {
-                        shardStateAction.shardStarted(shardRouting, indexMetaData.getIndexUUID(), "after recovery from store", SHARD_STATE_ACTION_LISTENER);
+                        shardStateAction.shardStarted(shardRouting, "after recovery from store", SHARD_STATE_ACTION_LISTENER);
                     }
                 } catch (Throwable t) {
                     handleRecoveryFailure(indexService, shardRouting, true, t);
@@ -666,7 +642,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                     final IndexShardRepository indexShardRepository = repositoriesService.indexShardRepository(restoreSource.snapshotId().getRepository());
                     if (indexShard.restoreFromRepository(indexShardRepository, nodes.localNode())) {
                         restoreService.indexShardRestoreCompleted(restoreSource.snapshotId(), sId);
-                        shardStateAction.shardStarted(shardRouting, indexMetaData.getIndexUUID(), "after recovery from repository", SHARD_STATE_ACTION_LISTENER);
+                        shardStateAction.shardStarted(shardRouting, "after recovery from repository", SHARD_STATE_ACTION_LISTENER);
                     }
                 } catch (Throwable first) {
                     try {
@@ -736,7 +712,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
         @Override
         public void onRecoveryDone(RecoveryState state) {
-            shardStateAction.shardStarted(shardRouting, indexMetaData.getIndexUUID(), "after recovery (replica) from node [" + state.getSourceNode() + "]", SHARD_STATE_ACTION_LISTENER);
+            shardStateAction.shardStarted(shardRouting, "after recovery (replica) from node [" + state.getSourceNode() + "]", SHARD_STATE_ACTION_LISTENER);
         }
 
         @Override
@@ -747,7 +723,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
     private void handleRecoveryFailure(IndexService indexService, ShardRouting shardRouting, boolean sendShardFailure, Throwable failure) {
         synchronized (mutex) {
-            failAndRemoveShard(shardRouting, indexService.indexUUID(), indexService, sendShardFailure, "failed recovery", failure);
+            failAndRemoveShard(shardRouting, indexService, sendShardFailure, "failed recovery", failure);
         }
     }
 
@@ -768,7 +744,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
 
     }
 
-    private void failAndRemoveShard(ShardRouting shardRouting, String indexUUID, @Nullable IndexService indexService, boolean sendShardFailure, String message, @Nullable Throwable failure) {
+    private void failAndRemoveShard(ShardRouting shardRouting, @Nullable IndexService indexService, boolean sendShardFailure, String message, @Nullable Throwable failure) {
         if (indexService != null && indexService.hasShard(shardRouting.getId())) {
             // if the indexService is null we can't remove the shard, that's fine since we might have a failure
             // when the index is remove and then we already removed the index service for that shard...
@@ -781,15 +757,15 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             }
         }
         if (sendShardFailure) {
-            sendFailShard(shardRouting, indexUUID, message, failure);
+            sendFailShard(shardRouting, message, failure);
         }
     }
 
-    private void sendFailShard(ShardRouting shardRouting, String indexUUID, String message, @Nullable Throwable failure) {
+    private void sendFailShard(ShardRouting shardRouting, String message, @Nullable Throwable failure) {
         try {
             logger.warn("[{}] marking and sending shard failed due to [{}]", failure, shardRouting.shardId(), message);
-            failedShards.put(shardRouting.shardId(), new FailedShard(shardRouting.version()));
-            shardStateAction.shardFailed(shardRouting, indexUUID, message, failure, SHARD_STATE_ACTION_LISTENER);
+            failedShards.put(shardRouting.shardId(), shardRouting);
+            shardStateAction.shardFailed(shardRouting, shardRouting, message, failure, SHARD_STATE_ACTION_LISTENER);
         } catch (Throwable e1) {
             logger.warn("[{}][{}] failed to mark shard as failed (because of [{}])", e1, shardRouting.getIndexName(), shardRouting.getId(), message);
         }
@@ -802,7 +778,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
             final ShardRouting shardRouting = shardFailure.routing;
             threadPool.generic().execute(() -> {
                 synchronized (mutex) {
-                    failAndRemoveShard(shardRouting, shardFailure.indexUUID, indexService, true, "shard failure, reason [" + shardFailure.reason + "]", shardFailure.cause);
+                    failAndRemoveShard(shardRouting, indexService, true, "shard failure, reason [" + shardFailure.reason + "]", shardFailure.cause);
                 }
             });
         }
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
index 21128a9..e2d6819 100644
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
+++ b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.ingest;
 
 import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ResourceNotFoundException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ingest.DeletePipelineRequest;
@@ -36,10 +37,8 @@ import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.PipelineFactoryError;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 import org.elasticsearch.script.ScriptService;
 
 import java.io.Closeable;
@@ -104,8 +103,10 @@ public class PipelineStore extends AbstractComponent implements Closeable, Clust
         for (PipelineConfiguration pipeline : ingestMetadata.getPipelines().values()) {
             try {
                 pipelines.put(pipeline.getId(), factory.create(pipeline.getId(), pipeline.getConfigAsMap(), processorFactoryRegistry));
+            } catch (ElasticsearchParseException e) {
+                throw e;
             } catch (Exception e) {
-                throw new RuntimeException(e);
+                throw new ElasticsearchParseException("Error updating pipeline with id [" + pipeline.getId() + "]", e);
             }
         }
         this.pipelines = Collections.unmodifiableMap(pipelines);
@@ -154,9 +155,10 @@ public class PipelineStore extends AbstractComponent implements Closeable, Clust
     public void put(ClusterService clusterService, PutPipelineRequest request, ActionListener<WritePipelineResponse> listener) {
         // validates the pipeline and processor configuration before submitting a cluster update task:
         Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.getSource(), false).v2();
-        WritePipelineResponse response = validatePipelineResponse(request.getId(), pipelineConfig);
-        if (response != null) {
-            listener.onResponse(response);
+        try {
+            factory.create(request.getId(), pipelineConfig, processorFactoryRegistry);
+        } catch(Exception e) {
+            listener.onFailure(e);
             return;
         }
         clusterService.submitStateUpdateTask("put-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<WritePipelineResponse>(request, listener) {
@@ -234,16 +236,4 @@ public class PipelineStore extends AbstractComponent implements Closeable, Clust
         }
         return result;
     }
-
-    WritePipelineResponse validatePipelineResponse(String id, Map<String, Object> config) {
-        try {
-            factory.create(id, config, processorFactoryRegistry);
-            return null;
-        } catch (ConfigurationPropertyException e) {
-            return new WritePipelineResponse(new PipelineFactoryError(e));
-        } catch (Exception e) {
-            return new WritePipelineResponse(new PipelineFactoryError(e.getMessage()));
-        }
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
index 69adc0f..bd3fd8c 100644
--- a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
+++ b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
@@ -19,7 +19,8 @@
 
 package org.elasticsearch.ingest.core;
 
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.ElasticsearchParseException;
 
 import java.util.List;
 import java.util.Map;
@@ -32,7 +33,7 @@ public final class ConfigurationUtils {
     /**
      * Returns and removes the specified optional property from the specified configuration map.
      *
-     * If the property value isn't of type string a {@link ConfigurationPropertyException} is thrown.
+     * If the property value isn't of type string a {@link ElasticsearchParseException} is thrown.
      */
     public static String readOptionalStringProperty(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
@@ -42,8 +43,8 @@ public final class ConfigurationUtils {
     /**
      * Returns and removes the specified property from the specified configuration map.
      *
-     * If the property value isn't of type string an {@link ConfigurationPropertyException} is thrown.
-     * If the property is missing an {@link ConfigurationPropertyException} is thrown
+     * If the property value isn't of type string an {@link ElasticsearchParseException} is thrown.
+     * If the property is missing an {@link ElasticsearchParseException} is thrown
      */
     public static String readStringProperty(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         return readStringProperty(processorType, processorTag, configuration, propertyName, null);
@@ -52,15 +53,15 @@ public final class ConfigurationUtils {
     /**
      * Returns and removes the specified property from the specified configuration map.
      *
-     * If the property value isn't of type string a {@link ConfigurationPropertyException} is thrown.
-     * If the property is missing and no default value has been specified a {@link ConfigurationPropertyException} is thrown
+     * If the property value isn't of type string a {@link ElasticsearchParseException} is thrown.
+     * If the property is missing and no default value has been specified a {@link ElasticsearchParseException} is thrown
      */
     public static String readStringProperty(String processorType, String processorTag, Map<String, Object> configuration, String propertyName, String defaultValue) {
         Object value = configuration.remove(propertyName);
         if (value == null && defaultValue != null) {
             return defaultValue;
         } else if (value == null) {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "required property is missing");
+            throw newConfigurationException(processorType, processorTag, propertyName, "required property is missing");
         }
         return readString(processorType, processorTag, propertyName, value);
     }
@@ -72,13 +73,13 @@ public final class ConfigurationUtils {
         if (value instanceof String) {
             return (String) value;
         }
-        throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "property isn't a string, but of type [" + value.getClass().getName() + "]");
+        throw newConfigurationException(processorType, processorTag, propertyName, "property isn't a string, but of type [" + value.getClass().getName() + "]");
     }
 
     /**
      * Returns and removes the specified property of type list from the specified configuration map.
      *
-     * If the property value isn't of type list an {@link ConfigurationPropertyException} is thrown.
+     * If the property value isn't of type list an {@link ElasticsearchParseException} is thrown.
      */
     public static <T> List<T> readOptionalList(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
@@ -91,13 +92,13 @@ public final class ConfigurationUtils {
     /**
      * Returns and removes the specified property of type list from the specified configuration map.
      *
-     * If the property value isn't of type list an {@link ConfigurationPropertyException} is thrown.
-     * If the property is missing an {@link ConfigurationPropertyException} is thrown
+     * If the property value isn't of type list an {@link ElasticsearchParseException} is thrown.
+     * If the property is missing an {@link ElasticsearchParseException} is thrown
      */
     public static <T> List<T> readList(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
         if (value == null) {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "required property is missing");
+            throw newConfigurationException(processorType, processorTag, propertyName, "required property is missing");
         }
 
         return readList(processorType, processorTag, propertyName, value);
@@ -109,20 +110,20 @@ public final class ConfigurationUtils {
             List<T> stringList = (List<T>) value;
             return stringList;
         } else {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "property isn't a list, but of type [" + value.getClass().getName() + "]");
+            throw newConfigurationException(processorType, processorTag, propertyName, "property isn't a list, but of type [" + value.getClass().getName() + "]");
         }
     }
 
     /**
      * Returns and removes the specified property of type map from the specified configuration map.
      *
-     * If the property value isn't of type map an {@link ConfigurationPropertyException} is thrown.
-     * If the property is missing an {@link ConfigurationPropertyException} is thrown
+     * If the property value isn't of type map an {@link ElasticsearchParseException} is thrown.
+     * If the property is missing an {@link ElasticsearchParseException} is thrown
      */
     public static <T> Map<String, T> readMap(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
         if (value == null) {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "required property is missing");
+            throw newConfigurationException(processorType, processorTag, propertyName, "required property is missing");
         }
 
         return readMap(processorType, processorTag, propertyName, value);
@@ -131,7 +132,7 @@ public final class ConfigurationUtils {
     /**
      * Returns and removes the specified property of type map from the specified configuration map.
      *
-     * If the property value isn't of type map an {@link ConfigurationPropertyException} is thrown.
+     * If the property value isn't of type map an {@link ElasticsearchParseException} is thrown.
      */
     public static <T> Map<String, T> readOptionalMap(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
@@ -148,7 +149,7 @@ public final class ConfigurationUtils {
             Map<String, T> map = (Map<String, T>) value;
             return map;
         } else {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "property isn't a map, but of type [" + value.getClass().getName() + "]");
+            throw newConfigurationException(processorType, processorTag, propertyName, "property isn't a map, but of type [" + value.getClass().getName() + "]");
         }
     }
 
@@ -158,8 +159,23 @@ public final class ConfigurationUtils {
     public static Object readObject(String processorType, String processorTag, Map<String, Object> configuration, String propertyName) {
         Object value = configuration.remove(propertyName);
         if (value == null) {
-            throw new ConfigurationPropertyException(processorType, processorTag, propertyName, "required property is missing");
+            throw newConfigurationException(processorType, processorTag, propertyName, "required property is missing");
         }
         return value;
     }
+
+    public static ElasticsearchParseException newConfigurationException(String processorType, String processorTag, String propertyName, String reason) {
+        ElasticsearchParseException exception = new ElasticsearchParseException("[" + propertyName + "] " + reason);
+
+        if (processorType != null) {
+            exception.addHeader("processor_type", processorType);
+        }
+        if (processorTag != null) {
+            exception.addHeader("processor_tag", processorTag);
+        }
+        if (propertyName != null) {
+            exception.addHeader("property_name", propertyName);
+        }
+        return exception;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
index 5c654fb..1c560fa 100644
--- a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
+++ b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.ingest.core;
 
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
+import org.elasticsearch.ElasticsearchParseException;
 
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -27,6 +27,7 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
+
 /**
  * A pipeline is a list of {@link Processor} instances grouped under a unique id.
  */
@@ -84,20 +85,20 @@ public final class Pipeline {
 
     public final static class Factory {
 
-        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws ConfigurationPropertyException {
+        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws Exception {
             String description = ConfigurationUtils.readOptionalStringProperty(null, null, config, DESCRIPTION_KEY);
             List<Map<String, Map<String, Object>>> processorConfigs = ConfigurationUtils.readList(null, null, config, PROCESSORS_KEY);
             List<Processor> processors = readProcessorConfigs(processorConfigs, processorRegistry);
             List<Map<String, Map<String, Object>>> onFailureProcessorConfigs = ConfigurationUtils.readOptionalList(null, null, config, ON_FAILURE_KEY);
             List<Processor> onFailureProcessors = readProcessorConfigs(onFailureProcessorConfigs, processorRegistry);
             if (config.isEmpty() == false) {
-                throw new ConfigurationPropertyException("pipeline [" + id + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
+                throw new ElasticsearchParseException("pipeline [" + id + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
             }
             CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.unmodifiableList(processors), Collections.unmodifiableList(onFailureProcessors));
             return new Pipeline(id, description, compoundProcessor);
         }
 
-        private List<Processor> readProcessorConfigs(List<Map<String, Map<String, Object>>> processorConfigs, Map<String, Processor.Factory> processorRegistry) throws ConfigurationPropertyException {
+        private List<Processor> readProcessorConfigs(List<Map<String, Map<String, Object>>> processorConfigs, Map<String, Processor.Factory> processorRegistry) throws Exception {
             List<Processor> processors = new ArrayList<>();
             if (processorConfigs != null) {
                 for (Map<String, Map<String, Object>> processorConfigWithKey : processorConfigs) {
@@ -110,28 +111,22 @@ public final class Pipeline {
             return processors;
         }
 
-        private Processor readProcessor(Map<String, Processor.Factory> processorRegistry, String type, Map<String, Object> config) throws ConfigurationPropertyException {
+        private Processor readProcessor(Map<String, Processor.Factory> processorRegistry, String type, Map<String, Object> config) throws Exception {
             Processor.Factory factory = processorRegistry.get(type);
             if (factory != null) {
                 List<Map<String, Map<String, Object>>> onFailureProcessorConfigs = ConfigurationUtils.readOptionalList(null, null, config, ON_FAILURE_KEY);
                 List<Processor> onFailureProcessors = readProcessorConfigs(onFailureProcessorConfigs, processorRegistry);
                 Processor processor;
-                try {
-                    processor = factory.create(config);
-                } catch (ConfigurationPropertyException e) {
-                    throw e;
-                } catch (Exception e) {
-                    throw new ConfigurationPropertyException(e.getMessage());
-                }
+                processor = factory.create(config);
                 if (!config.isEmpty()) {
-                    throw new ConfigurationPropertyException("processor [" + type + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
+                    throw new ElasticsearchParseException("processor [" + type + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
                 }
                 if (onFailureProcessors.isEmpty()) {
                     return processor;
                 }
                 return new CompoundProcessor(Collections.singletonList(processor), onFailureProcessors);
             }
-            throw new ConfigurationPropertyException("No processor type exists with name [" + type + "]");
+            throw new ElasticsearchParseException("No processor type exists with name [" + type + "]");
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryError.java b/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryError.java
deleted file mode 100644
index b987e1e..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryError.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
-
-import java.io.IOException;
-
-public class PipelineFactoryError implements Streamable, ToXContent {
-    private String reason;
-    private String processorType;
-    private String processorTag;
-    private String processorPropertyName;
-
-    public PipelineFactoryError() {
-
-    }
-
-    public PipelineFactoryError(ConfigurationPropertyException e) {
-        this.reason = e.getMessage();
-        this.processorType = e.getProcessorType();
-        this.processorTag = e.getProcessorTag();
-        this.processorPropertyName = e.getPropertyName();
-    }
-
-    public PipelineFactoryError(String reason) {
-        this.reason = "Constructing Pipeline failed:" + reason;
-    }
-
-    public String getReason() {
-        return reason;
-    }
-
-    public String getProcessorTag() {
-        return processorTag;
-    }
-
-    public String getProcessorPropertyName() {
-        return processorPropertyName;
-    }
-
-    public String getProcessorType() {
-        return processorType;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        reason = in.readString();
-        processorType = in.readOptionalString();
-        processorTag = in.readOptionalString();
-        processorPropertyName = in.readOptionalString();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(reason);
-        out.writeOptionalString(processorType);
-        out.writeOptionalString(processorTag);
-        out.writeOptionalString(processorPropertyName);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject("error");
-        builder.field("type", processorType);
-        builder.field("tag", processorTag);
-        builder.field("reason", reason);
-        builder.field("property_name", processorPropertyName);
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryResult.java b/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryResult.java
deleted file mode 100644
index ab28498..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/PipelineFactoryResult.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-public class PipelineFactoryResult {
-    private final Pipeline pipeline;
-    private final PipelineFactoryError error;
-
-    public PipelineFactoryResult(Pipeline pipeline) {
-        this.pipeline = pipeline;
-        this.error = null;
-    }
-
-    public PipelineFactoryResult(PipelineFactoryError error) {
-        this.error = error;
-        this.pipeline = null;
-    }
-
-    public Pipeline getPipeline() {
-        return pipeline;
-    }
-
-    public PipelineFactoryError getError() {
-        return error;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
index 2804998..8cdff87 100644
--- a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
@@ -17,11 +17,8 @@
  * under the License.
  */
 
-
 package org.elasticsearch.ingest.core;
 
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
-
 import java.util.Map;
 
 /**
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/ConfigurationPropertyException.java b/core/src/main/java/org/elasticsearch/ingest/processor/ConfigurationPropertyException.java
deleted file mode 100644
index dbc35c9..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/ConfigurationPropertyException.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-/**
- * Exception class thrown by processor factories.
- */
-public class ConfigurationPropertyException extends RuntimeException {
-    private String processorType;
-    private String processorTag;
-    private String propertyName;
-
-    public ConfigurationPropertyException(String processorType, String processorTag, String propertyName, String message) {
-        super("[" + propertyName + "] " + message);
-        this.processorTag = processorTag;
-        this.processorType = processorType;
-        this.propertyName = propertyName;
-    }
-
-    public ConfigurationPropertyException(String errorMessage) {
-        super(errorMessage);
-    }
-
-    public String getPropertyName() {
-        return propertyName;
-    }
-
-    public String getProcessorType() {
-        return processorType;
-    }
-
-    public String getProcessorTag() {
-        return processorTag;
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
index 213e3ec..7cc8590 100644
--- a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
@@ -29,6 +30,8 @@ import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 
+import static org.elasticsearch.ingest.core.ConfigurationUtils.newConfigurationException;
+
 /**
  * Processor that converts fields content to a different type. Supported types are: integer, float, boolean and string.
  * Throws exception if the field is not there or the conversion fails.
@@ -80,11 +83,11 @@ public class ConvertProcessor extends AbstractProcessor {
 
         public abstract Object convert(Object value);
 
-        public static Type fromString(String type) {
+        public static Type fromString(String processorTag, String propertyName, String type) {
             try {
                 return Type.valueOf(type.toUpperCase(Locale.ROOT));
             } catch(IllegalArgumentException e) {
-                throw new IllegalArgumentException("type [" + type + "] not supported, cannot convert field.", e);
+                throw newConfigurationException(TYPE, processorTag, propertyName, "type [" + type + "] not supported, cannot convert field.");
             }
         }
     }
@@ -138,7 +141,8 @@ public class ConvertProcessor extends AbstractProcessor {
         @Override
         public ConvertProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
             String field = ConfigurationUtils.readStringProperty(TYPE, processorTag, config, "field");
-            Type convertType = Type.fromString(ConfigurationUtils.readStringProperty(TYPE, processorTag, config, "type"));
+            String typeProperty = ConfigurationUtils.readStringProperty(TYPE, processorTag, config, "type");
+            Type convertType = Type.fromString(processorTag, "type", typeProperty);
             return new ConvertProcessor(processorTag, field, convertType);
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
deleted file mode 100644
index 62063a4..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Processor that replaces dots in document field names with a
- * specified separator.
- */
-public class DeDotProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "dedot";
-    static final String DEFAULT_SEPARATOR = "_";
-
-    private final String separator;
-
-    DeDotProcessor(String tag, String separator) {
-        super(tag);
-        this.separator = separator;
-    }
-
-    public String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        deDot(document.getSourceAndMetadata());
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    /**
-     * Recursively iterates through Maps and Lists in search of map entries with
-     * keys containing dots. The dots in these fields are replaced with {@link #separator}.
-     *
-     * @param obj The current object in context to be checked for dots in its fields.
-     */
-    private void deDot(Object obj) {
-        if (obj instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> doc = (Map) obj;
-            Iterator<Map.Entry<String, Object>> it = doc.entrySet().iterator();
-            Map<String, Object> deDottedFields = new HashMap<>();
-            while (it.hasNext()) {
-                Map.Entry<String, Object> entry = it.next();
-                deDot(entry.getValue());
-                String fieldName = entry.getKey();
-                if (fieldName.contains(".")) {
-                    String deDottedFieldName = fieldName.replaceAll("\\.", separator);
-                    deDottedFields.put(deDottedFieldName, entry.getValue());
-                    it.remove();
-                }
-            }
-            doc.putAll(deDottedFields);
-        } else if (obj instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List) obj;
-            for (Object value : list) {
-                deDot(value);
-            }
-        }
-    }
-
-    public static class Factory extends AbstractProcessorFactory<DeDotProcessor> {
-
-        @Override
-        public DeDotProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String separator = ConfigurationUtils.readOptionalStringProperty(TYPE, processorTag, config, "separator");
-            if (separator == null) {
-                separator = DEFAULT_SEPARATOR;
-            }
-            return new DeDotProcessor(processorTag, separator);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 49952cf..542039f 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -133,11 +133,6 @@ public class Node implements Closeable {
     public static final Setting<Boolean> NODE_LOCAL_SETTING = Setting.boolSetting("node.local", false, false, Setting.Scope.CLUSTER);
     public static final Setting<String> NODE_MODE_SETTING = new Setting<>("node.mode", "network", Function.identity(), false, Setting.Scope.CLUSTER);
     public static final Setting<Boolean> NODE_INGEST_SETTING = Setting.boolSetting("node.ingest", true, false, Setting.Scope.CLUSTER);
-    public static final Setting<String> NODE_NAME_SETTING = Setting.simpleString("node.name", false, Setting.Scope.CLUSTER);
-    // this sucks that folks can mistype client etc and get away with it.
-    // TODO: we should move this to node.attribute.${name} = ${value} instead.
-    public static final Setting<Settings> NODE_ATTRIBUTES = Setting.groupSetting("node.", false, Setting.Scope.CLUSTER);
-
 
     private static final String CLIENT_TYPE = "node";
     private final Lifecycle lifecycle = new Lifecycle();
@@ -161,7 +156,7 @@ public class Node implements Closeable {
             .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build();
         tmpSettings = TribeService.processSettings(tmpSettings);
 
-        ESLogger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(tmpSettings));
+        ESLogger logger = Loggers.getLogger(Node.class, tmpSettings.get("name"));
         logger.info("version[{}], pid[{}], build[{}/{}]", version, JvmInfo.jvmInfo().pid(), Build.CURRENT.shortHash(), Build.CURRENT.date());
 
         logger.info("initializing ...");
@@ -202,8 +197,7 @@ public class Node implements Closeable {
             modules.add(new EnvironmentModule(environment));
             modules.add(new NodeModule(this, monitorService));
             modules.add(new NetworkModule(networkService, settings, false, namedWriteableRegistry));
-            ScriptModule scriptModule = new ScriptModule();
-            modules.add(scriptModule);
+            modules.add(new ScriptModule(settingsModule));
             modules.add(new NodeEnvironmentModule(nodeEnvironment));
             modules.add(new ClusterNameModule(this.settings));
             modules.add(new ThreadPoolModule(threadPool));
@@ -221,7 +215,7 @@ public class Node implements Closeable {
             modules.add(new AnalysisModule(environment));
 
             pluginsService.processModules(modules);
-            scriptModule.prepareSettings(settingsModule);
+
             injector = modules.createInjector();
 
             client = injector.getInstance(Client.class);
@@ -268,7 +262,7 @@ public class Node implements Closeable {
             return this;
         }
 
-        ESLogger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings));
+        ESLogger logger = Loggers.getLogger(Node.class, settings.get("name"));
         logger.info("starting ...");
         // hack around dependency injection problem (for now...)
         injector.getInstance(Discovery.class).setRoutingService(injector.getInstance(RoutingService.class));
@@ -322,7 +316,7 @@ public class Node implements Closeable {
         if (!lifecycle.moveToStopped()) {
             return this;
         }
-        ESLogger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings));
+        ESLogger logger = Loggers.getLogger(Node.class, settings.get("name"));
         logger.info("stopping ...");
 
         injector.getInstance(TribeService.class).stop();
@@ -369,7 +363,7 @@ public class Node implements Closeable {
             return;
         }
 
-        ESLogger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings));
+        ESLogger logger = Loggers.getLogger(Node.class, settings.get("name"));
         logger.info("closing ...");
         List<Closeable> toClose = new ArrayList<>();
         StopWatch stopWatch = new StopWatch("node_close");
diff --git a/core/src/main/java/org/elasticsearch/node/NodeModule.java b/core/src/main/java/org/elasticsearch/node/NodeModule.java
index 442dc72..365e260 100644
--- a/core/src/main/java/org/elasticsearch/node/NodeModule.java
+++ b/core/src/main/java/org/elasticsearch/node/NodeModule.java
@@ -28,7 +28,6 @@ import org.elasticsearch.ingest.core.TemplateService;
 import org.elasticsearch.ingest.processor.AppendProcessor;
 import org.elasticsearch.ingest.processor.ConvertProcessor;
 import org.elasticsearch.ingest.processor.DateProcessor;
-import org.elasticsearch.ingest.processor.DeDotProcessor;
 import org.elasticsearch.ingest.processor.FailProcessor;
 import org.elasticsearch.ingest.processor.GsubProcessor;
 import org.elasticsearch.ingest.processor.JoinProcessor;
@@ -75,7 +74,6 @@ public class NodeModule extends AbstractModule {
         registerProcessor(ConvertProcessor.TYPE, (templateService) -> new ConvertProcessor.Factory());
         registerProcessor(GsubProcessor.TYPE, (templateService) -> new GsubProcessor.Factory());
         registerProcessor(FailProcessor.TYPE, FailProcessor.Factory::new);
-        registerProcessor(DeDotProcessor.TYPE, (templateService) -> new DeDotProcessor.Factory());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 06fab2d..34b6d07 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -154,6 +154,14 @@ public class InternalSettingsPreparer {
         }
         output.replacePropertyPlaceholders();
 
+        // check if name is set in settings, if not look for system property and set it
+        if (output.get("name") == null) {
+            String name = System.getProperty("name");
+            if (name != null) {
+                output.put("name", name);
+            }
+        }
+
         // put the cluster name
         if (output.get(ClusterName.CLUSTER_NAME_SETTING.getKey()) == null) {
             output.put(ClusterName.CLUSTER_NAME_SETTING.getKey(), ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY));
@@ -162,10 +170,12 @@ public class InternalSettingsPreparer {
         replacePromptPlaceholders(output, terminal);
         // all settings placeholders have been resolved. resolve the value for the name setting by checking for name,
         // then looking for node.name, and finally generate one if needed
-        String name = output.get("node.name");
-        if (name == null || name.isEmpty()) {
-            name = randomNodeName(configDir);
-            output.put("node.name", name);
+        if (output.get("name") == null) {
+            String name = output.get("node.name");
+            if (name == null || name.isEmpty()) {
+                name = randomNodeName(configDir);
+            }
+            output.put("name", name);
         }
     }
 
@@ -237,8 +247,8 @@ public class InternalSettingsPreparer {
         }
 
         if (secret) {
-            return new String(terminal.readSecret("Enter value for [%s]: ", key));
+            return new String(terminal.readSecret("Enter value for [" + key + "]: ", key));
         }
-        return terminal.readText("Enter value for [%s]: ", key);
+        return terminal.readText("Enter value for [" + key + "]: ", key);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java b/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
index 5600d40..f9300d8 100644
--- a/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
+++ b/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java
@@ -19,6 +19,18 @@
 
 package org.elasticsearch.plugins;
 
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.Build;
+import org.elasticsearch.Version;
+import org.elasticsearch.bootstrap.JarHell;
+import org.elasticsearch.common.cli.CliTool;
+import org.elasticsearch.common.cli.Terminal;
+import org.elasticsearch.common.cli.UserError;
+import org.elasticsearch.common.hash.MessageDigests;
+import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.InputStream;
@@ -42,18 +54,6 @@ import java.util.Set;
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipInputStream;
 
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.Build;
-import org.elasticsearch.Version;
-import org.elasticsearch.bootstrap.JarHell;
-import org.elasticsearch.common.cli.CliTool;
-import org.elasticsearch.common.cli.Terminal;
-import org.elasticsearch.common.cli.UserError;
-import org.elasticsearch.common.hash.MessageDigests;
-import org.elasticsearch.common.io.FileSystemUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.cli.Terminal.Verbosity.VERBOSE;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
@@ -133,7 +133,7 @@ class InstallPluginCommand extends CliTool.Command {
 
         // TODO: remove this leniency!! is it needed anymore?
         if (Files.exists(env.pluginsFile()) == false) {
-            terminal.println("Plugins directory [%s] does not exist. Creating...", env.pluginsFile());
+            terminal.println("Plugins directory [" + env.pluginsFile() + "] does not exist. Creating...");
             Files.createDirectory(env.pluginsFile());
         }
 
@@ -160,9 +160,9 @@ class InstallPluginCommand extends CliTool.Command {
             return downloadZipAndChecksum(url, tmpDir);
         }
 
-        // now try as maven coordinates, a valid URL would only have a single colon
+        // now try as maven coordinates, a valid URL would only have a colon and slash
         String[] coordinates = pluginId.split(":");
-        if (coordinates.length == 3) {
+        if (coordinates.length == 3 && pluginId.contains("/") == false) {
             String mavenUrl = String.format(Locale.ROOT, "https://repo1.maven.org/maven2/%1$s/%2$s/%3$s/%2$s-%3$s.zip",
                 coordinates[0].replace(".", "/") /* groupId */, coordinates[1] /* artifactId */, coordinates[2] /* version */);
             terminal.println("-> Downloading " + pluginId + " from maven central");
@@ -242,7 +242,7 @@ class InstallPluginCommand extends CliTool.Command {
     private PluginInfo verify(Path pluginRoot, Environment env) throws Exception {
         // read and validate the plugin descriptor
         PluginInfo info = PluginInfo.readFromProperties(pluginRoot);
-        terminal.println(VERBOSE, "%s", info);
+        terminal.println(VERBOSE, info.toString());
 
         // don't let luser install plugin as a module...
         // they might be unavoidably in maven central and are packaged up the same way)
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginSecurity.java b/core/src/main/java/org/elasticsearch/plugins/PluginSecurity.java
index 4fd039c..9bbafa6 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginSecurity.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginSecurity.java
@@ -87,7 +87,7 @@ class PluginSecurity {
         terminal.println(Verbosity.NORMAL, "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@");
         // print all permissions:
         for (Permission permission : requested) {
-            terminal.println(Verbosity.NORMAL, "* %s", formatPermission(permission));
+            terminal.println(Verbosity.NORMAL, "* " + formatPermission(permission));
         }
         terminal.println(Verbosity.NORMAL, "See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html");
         terminal.println(Verbosity.NORMAL, "for descriptions of what these permissions allow and the associated risks.");
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index ac3e616..4e61185 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -36,7 +36,6 @@ import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexModule;
 
@@ -57,7 +56,6 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.function.Function;
 
 import static org.elasticsearch.common.io.FileSystemUtils.isAccessibleDirectory;
 
@@ -71,7 +69,6 @@ public class PluginsService extends AbstractComponent {
      */
     private final List<Tuple<PluginInfo, Plugin>> plugins;
     private final PluginsAndModules info;
-    public static final Setting<List<String>> MANDATORY_SETTING = Setting.listSetting("plugin.mandatory", Collections.emptyList(), Function.identity(), false, Setting.Scope.CLUSTER);
 
     private final Map<Plugin, List<OnModuleReference>> onModuleReferences;
 
@@ -146,8 +143,8 @@ public class PluginsService extends AbstractComponent {
         }
 
         // Checking expected plugins
-        List<String> mandatoryPlugins = MANDATORY_SETTING.get(settings);
-        if (mandatoryPlugins.isEmpty() == false) {
+        String[] mandatoryPlugins = settings.getAsArray("plugin.mandatory", null);
+        if (mandatoryPlugins != null) {
             Set<String> missingPlugins = new HashSet<>();
             for (String mandatoryPlugin : mandatoryPlugins) {
                 if (!pluginsNames.contains(mandatoryPlugin) && !missingPlugins.contains(mandatoryPlugin)) {
diff --git a/core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java b/core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java
index d8fd0b8..8ce1056 100644
--- a/core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java
+++ b/core/src/main/java/org/elasticsearch/plugins/RemovePluginCommand.java
@@ -19,12 +19,6 @@
 
 package org.elasticsearch.plugins;
 
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardCopyOption;
-import java.util.ArrayList;
-import java.util.List;
-
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.CliTool;
@@ -33,6 +27,12 @@ import org.elasticsearch.common.cli.UserError;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardCopyOption;
+import java.util.ArrayList;
+import java.util.List;
+
 import static org.elasticsearch.common.cli.Terminal.Verbosity.VERBOSE;
 
 /**
@@ -63,10 +63,10 @@ class RemovePluginCommand extends CliTool.Command {
                 throw new UserError(CliTool.ExitStatus.IO_ERROR, "Bin dir for " + pluginName + " is not a directory");
             }
             pluginPaths.add(pluginBinDir);
-            terminal.println(VERBOSE, "Removing: %s", pluginBinDir);
+            terminal.println(VERBOSE, "Removing: " + pluginBinDir);
         }
 
-        terminal.println(VERBOSE, "Removing: %s", pluginDir);
+        terminal.println(VERBOSE, "Removing: " + pluginDir);
         Path tmpPluginDir = env.pluginsFile().resolve(".removing-" + pluginName);
         Files.move(pluginDir, tmpPluginDir, StandardCopyOption.ATOMIC_MOVE);
         pluginPaths.add(tmpPluginDir);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
index badccbb..a96ed3d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.rest.action.ingest;
 
 import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
-import org.elasticsearch.action.ingest.WritePipelineResponseRestListener;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -34,7 +30,6 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
 import org.elasticsearch.rest.action.support.RestActions;
 
-import java.io.IOException;
 
 public class RestPutPipelineAction extends BaseRestHandler {
 
@@ -49,7 +44,7 @@ public class RestPutPipelineAction extends BaseRestHandler {
         PutPipelineRequest request = new PutPipelineRequest(restRequest.param("id"), RestActions.getRestContent(restRequest));
         request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
         request.timeout(restRequest.paramAsTime("timeout", request.timeout()));
-        client.admin().cluster().putPipeline(request, new WritePipelineResponseRestListener(channel));
+        client.admin().cluster().putPipeline(request, new AcknowledgedRestListener<>(channel));
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
index 94f80a9..fc2e834 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
@@ -47,6 +47,6 @@ public class RestSimulatePipelineAction extends BaseRestHandler {
         SimulatePipelineRequest request = new SimulatePipelineRequest(RestActions.getRestContent(restRequest));
         request.setId(restRequest.param("id"));
         request.setVerbose(restRequest.paramAsBoolean("verbose", false));
-        client.admin().cluster().simulatePipeline(request, new RestStatusToXContentListener<>(channel));
+        client.admin().cluster().simulatePipeline(request, new RestToXContentListener<>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java b/core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java
index 5504878..aaf0906 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java
@@ -27,7 +27,6 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.BytesRestResponse;
 import org.elasticsearch.rest.RestChannel;
@@ -77,8 +76,9 @@ public class RestMainAction extends BaseRestHandler {
         }
 
         builder.startObject();
-        assert settings.get("node.name") != null;
-        builder.field("name", Node.NODE_NAME_SETTING.get(settings));
+        if (settings.get("name") != null) {
+            builder.field("name", settings.get("name"));
+        }
         builder.field("cluster_name", clusterName.value());
         builder.startObject("version")
                 .field("number", version.number())
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModule.java b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
index 11bc2b5..1ccc7eb 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModule.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
@@ -37,6 +37,8 @@ import java.util.Objects;
  */
 public class ScriptModule extends AbstractModule {
 
+    private final SettingsModule settingsModule;
+
     private final List<ScriptEngineRegistry.ScriptEngineRegistration> scriptEngineRegistrations = new ArrayList<>();
 
     {
@@ -47,6 +49,9 @@ public class ScriptModule extends AbstractModule {
 
     private final List<ScriptContext.Plugin> customScriptContexts = new ArrayList<>();
 
+    public ScriptModule(SettingsModule settingsModule) {
+        this.settingsModule = settingsModule;
+    }
 
     public void addScriptEngine(ScriptEngineRegistry.ScriptEngineRegistration scriptEngineRegistration) {
         Objects.requireNonNull(scriptEngineRegistration);
@@ -65,21 +70,6 @@ public class ScriptModule extends AbstractModule {
         customScriptContexts.add(scriptContext);
     }
 
-    /**
-     * This method is called after all modules have been processed but before we actually validate all settings. This allwos the
-     * script extensions to add all their settings.
-     */
-    public void prepareSettings(SettingsModule settingsModule) {
-        ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customScriptContexts);
-        ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(scriptEngineRegistrations);
-        ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-
-        scriptSettings.getScriptTypeSettings().forEach(settingsModule::registerSetting);
-        scriptSettings.getScriptContextSettings().forEach(settingsModule::registerSetting);
-        scriptSettings.getScriptLanguageSettings().forEach(settingsModule::registerSetting);
-        settingsModule.registerSetting(scriptSettings.getDefaultScriptLanguageSetting());
-    }
-
     @Override
     protected void configure() {
         MapBinder<String, NativeScriptFactory> scriptsBinder
@@ -95,11 +85,16 @@ public class ScriptModule extends AbstractModule {
             multibinder.addBinding().to(scriptEngineRegistration.getScriptEngineService()).asEagerSingleton();
         }
 
-
         ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customScriptContexts);
         ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(scriptEngineRegistrations);
+
         ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
 
+        scriptSettings.getScriptTypeSettings().forEach(settingsModule::registerSetting);
+        scriptSettings.getScriptContextSettings().forEach(settingsModule::registerSetting);
+        scriptSettings.getScriptLanguageSettings().forEach(settingsModule::registerSetting);
+        settingsModule.registerSetting(scriptSettings.getDefaultScriptLanguageSetting());
+
         bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
         bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
         bind(ScriptSettings.class).toInstance(scriptSettings);
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index c7f4392..378a849 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -40,7 +40,6 @@ import org.elasticsearch.common.util.concurrent.XRejectedExecutionHandler;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.node.Node;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -207,13 +206,13 @@ public class ThreadPool extends AbstractComponent implements Closeable {
     private final ThreadContext threadContext;
 
     public ThreadPool(String name) {
-        this(Settings.builder().put(Node.NODE_NAME_SETTING.getKey(), name).build());
+        this(Settings.builder().put("name", name).build());
     }
 
     public ThreadPool(Settings settings) {
         super(settings);
 
-        assert Node.NODE_NAME_SETTING.exists(settings) : "ThreadPool's settings should contain a name";
+        assert settings.get("name") != null : "ThreadPool's settings should contain a name";
         threadContext = new ThreadContext(settings);
         Map<String, Settings> groupSettings = THREADPOOL_GROUP_SETTING.get(settings).getAsGroups();
         validate(groupSettings);
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index af2e3fe..88c4dc7 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -46,7 +46,6 @@ import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
-import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.env.Environment;
@@ -62,7 +61,6 @@ import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.function.Function;
-import java.util.function.Predicate;
 
 import static java.util.Collections.unmodifiableMap;
 
@@ -121,7 +119,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
     }
 
     // internal settings only
-    public static final Setting<String> TRIBE_NAME_SETTING = Setting.simpleString("tribe.name", false, Setting.Scope.CLUSTER);
+    private static final Setting<String> TRIBE_NAME_SETTING = Setting.simpleString("tribe.name", false, Setting.Scope.CLUSTER);
     private final ClusterService clusterService;
     private final String[] blockIndicesWrite;
     private final String[] blockIndicesRead;
@@ -129,17 +127,11 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
     private static final String ON_CONFLICT_ANY = "any", ON_CONFLICT_DROP = "drop", ON_CONFLICT_PREFER = "prefer_";
 
     public static final Setting<String> ON_CONFLICT_SETTING = new Setting<>("tribe.on_conflict", ON_CONFLICT_ANY, (s) -> {
-        switch (s) {
-            case ON_CONFLICT_ANY:
-            case ON_CONFLICT_DROP:
-                return s;
-            default:
-                if (s.startsWith(ON_CONFLICT_PREFER) && s.length() > ON_CONFLICT_PREFER.length()) {
-                    return s;
-                }
-                throw new IllegalArgumentException(
-                    "Invalid value for [tribe.on_conflict] must be either [any, drop or start with prefer_] but was: [" + s + "]");
+        if (ON_CONFLICT_ANY.equals(s) || ON_CONFLICT_DROP.equals(s) || s.startsWith(ON_CONFLICT_PREFER)) {
+            return s;
         }
+        throw new IllegalArgumentException(
+                "Invalid value for [tribe.on_conflict] must be either [any, drop or start with prefer_] but was: " + s);
     }, false, Setting.Scope.CLUSTER);
 
     public static final Setting<Boolean> BLOCKS_METADATA_SETTING = Setting.boolSetting("tribe.blocks.metadata", false, false,
@@ -153,9 +145,6 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
     public static final Setting<List<String>> BLOCKS_METADATA_INDICES_SETTING = Setting.listSetting("tribe.blocks.metadata.indices",
             Collections.emptyList(), Function.identity(), false, Setting.Scope.CLUSTER);
 
-    public static final Set<String> TRIBE_SETTING_KEYS = Sets.newHashSet(TRIBE_NAME_SETTING.getKey(), ON_CONFLICT_SETTING.getKey(),
-        BLOCKS_METADATA_INDICES_SETTING.getKey(), BLOCKS_METADATA_SETTING.getKey(), BLOCKS_READ_INDICES_SETTING.getKey(), BLOCKS_WRITE_INDICES_SETTING.getKey(), BLOCKS_WRITE_SETTING.getKey());
-
     private final String onConflict;
     private final Set<String> droppedIndices = ConcurrentCollections.newConcurrentSet();
 
@@ -170,7 +159,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
         nodesSettings.remove("on_conflict"); // remove prefix settings that don't indicate a client
         for (Map.Entry<String, Settings> entry : nodesSettings.entrySet()) {
             Settings.Builder sb = Settings.builder().put(entry.getValue());
-            sb.put("node.name", settings.get("node.name") + "/" + entry.getKey());
+            sb.put("name", settings.get("name") + "/" + entry.getKey());
             sb.put(Environment.PATH_HOME_SETTING.getKey(), Environment.PATH_HOME_SETTING.get(settings)); // pass through ES home dir
             if (Environment.PATH_CONF_SETTING.exists(settings)) {
                 sb.put(Environment.PATH_CONF_SETTING.getKey(), Environment.PATH_CONF_SETTING.get(settings));
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 50764ee..0b693ae 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -24,6 +24,7 @@ import org.elasticsearch.action.TimestampParsingException;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.client.AbstractClientHeadersTestCase;
+import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -591,7 +592,14 @@ public class ExceptionSerializationTests extends ESTestCase {
         assertEquals("foo", e.getHeader("foo").get(0));
         assertEquals("bar", e.getHeader("foo").get(1));
         assertSame(status, e.status());
+    }
 
+    public void testNoLongerPrimaryShardException() throws IOException {
+        ShardId shardId = new ShardId(new Index(randomAsciiOfLength(4), randomAsciiOfLength(4)), randomIntBetween(0, Integer.MAX_VALUE));
+        String msg = randomAsciiOfLength(4);
+        ShardStateAction.NoLongerPrimaryShardException ex = serialize(new ShardStateAction.NoLongerPrimaryShardException(shardId, msg));
+        assertEquals(shardId, ex.getShardId());
+        assertEquals(msg, ex.getMessage());
     }
 
     public static class UnknownHeaderException extends ElasticsearchException {
@@ -776,6 +784,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(139, null);
         ids.put(140, org.elasticsearch.discovery.Discovery.FailedToCommitClusterStateException.class);
         ids.put(141, org.elasticsearch.index.query.QueryShardException.class);
+        ids.put(142, ShardStateAction.NoLongerPrimaryShardException.class);
 
         Map<Class<? extends ElasticsearchException>, Integer> reverse = new HashMap<>();
         for (Map.Entry<Integer, Class<? extends ElasticsearchException>> entry : ids.entrySet()) {
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
index 89fd404..fbb9320 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TasksIT.java
@@ -66,7 +66,7 @@ public class TasksIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
             .put(super.nodeSettings(nodeOrdinal))
-            .put(MockTaskManager.USE_MOCK_TASK_MANAGER_SETTING.getKey(), true)
+            .put(MockTaskManager.USE_MOCK_TASK_MANAGER, true)
             .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java
index fe74b53..4e1c082 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/node/tasks/TransportTasksActionTests.java
@@ -118,7 +118,7 @@ public class TransportTasksActionTests extends ESTestCase {
                 threadPool){
                 @Override
                 protected TaskManager createTaskManager() {
-                    if (MockTaskManager.USE_MOCK_TASK_MANAGER_SETTING.get(settings)) {
+                    if (settings.getAsBoolean(MockTaskManager.USE_MOCK_TASK_MANAGER, false)) {
                         return new MockTaskManager(settings);
                     } else {
                         return super.createTaskManager();
@@ -659,7 +659,7 @@ public class TransportTasksActionTests extends ESTestCase {
     }
 
     public void testFailedTasksCount() throws ExecutionException, InterruptedException, IOException {
-        Settings settings = Settings.builder().put(MockTaskManager.USE_MOCK_TASK_MANAGER_SETTING.getKey(), true).build();
+        Settings settings = Settings.builder().put(MockTaskManager.USE_MOCK_TASK_MANAGER, true).build();
         setupTestNodes(settings);
         connectNodes(testNodes);
         TestNodesAction[] actions = new TestNodesAction[nodesCount];
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestIT.java b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestIT.java
index d5316ea..118560b 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestIT.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreRequestIT.java
@@ -222,7 +222,7 @@ public class IndicesShardStoreRequestIT extends ESIntegTestCase {
 
         @Override
         public boolean test(Settings settings) {
-            return nodesWithShard.contains(settings.get("node.name"));
+            return nodesWithShard.contains(settings.get("name"));
         }
 
         private Set<String> findNodesWithShard(String index) {
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
index cf1cab2..d58b9bf 100644
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
@@ -54,7 +54,7 @@ public class SimulateExecutionServiceTests extends ESTestCase {
     public void setup() {
         threadPool = new ThreadPool(
                 Settings.builder()
-                        .put("node.name", getClass().getName())
+                        .put("name", getClass().getName())
                         .build()
         );
         executionService = new SimulateExecutionService(threadPool);
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/WritePipelineResponseTests.java b/core/src/test/java/org/elasticsearch/action/ingest/WritePipelineResponseTests.java
index 8eb3f4e..3f252c3 100644
--- a/core/src/test/java/org/elasticsearch/action/ingest/WritePipelineResponseTests.java
+++ b/core/src/test/java/org/elasticsearch/action/ingest/WritePipelineResponseTests.java
@@ -21,13 +21,11 @@ package org.elasticsearch.action.ingest;
 
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.PipelineFactoryError;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
 
 import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.nullValue;
 
 public class WritePipelineResponseTests extends ESTestCase {
 
@@ -45,17 +43,13 @@ public class WritePipelineResponseTests extends ESTestCase {
     }
 
     public void testSerializationWithError() throws IOException {
-        PipelineFactoryError error = new PipelineFactoryError("error");
-        WritePipelineResponse response = new WritePipelineResponse(error);
+        WritePipelineResponse response = new WritePipelineResponse();
         BytesStreamOutput out = new BytesStreamOutput();
         response.writeTo(out);
         StreamInput streamInput = StreamInput.wrap(out.bytes());
         WritePipelineResponse otherResponse = new WritePipelineResponse();
         otherResponse.readFrom(streamInput);
 
-        assertThat(otherResponse.getError().getReason(), equalTo(response.getError().getReason()));
-        assertThat(otherResponse.getError().getProcessorType(), equalTo(response.getError().getProcessorType()));
-        assertThat(otherResponse.getError().getProcessorTag(), equalTo(response.getError().getProcessorTag()));
-        assertThat(otherResponse.getError().getProcessorPropertyName(), equalTo(response.getError().getProcessorPropertyName()));
+        assertThat(otherResponse.isAcknowledged(), equalTo(response.isAcknowledged()));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
index 4542be2..2e4e3cb 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
@@ -42,6 +42,8 @@ import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
 import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.allocation.AllocationService;
+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -53,6 +55,7 @@ import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.shard.ShardNotFoundException;
 import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.test.ESAllocationTestCase;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.cluster.TestClusterService;
 import org.elasticsearch.test.transport.CapturingTransport;
@@ -67,6 +70,7 @@ import org.junit.BeforeClass;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -205,6 +209,56 @@ public class TransportReplicationActionTests extends ESTestCase {
         assertIndexShardCounter(1);
     }
 
+    /**
+     * When relocating a primary shard, there is a cluster state update at the end of relocation where the active primary is switched from
+     * the relocation source to the relocation target. If relocation source receives and processes this cluster state
+     * before the relocation target, there is a time span where relocation source believes active primary to be on
+     * relocation target and relocation target believes active primary to be on relocation source. This results in replication
+     * requests being sent back and forth.
+     *
+     * This test checks that replication request is not routed back from relocation target to relocation source in case of
+     * stale index routing table on relocation target.
+     */
+    public void testNoRerouteOnStaleClusterState() throws InterruptedException, ExecutionException {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, "_na_", 0);
+        ClusterState state = state(index, true, ShardRoutingState.RELOCATING);
+        String relocationTargetNode = state.getRoutingTable().shardRoutingTable(shardId).primaryShard().relocatingNodeId();
+        state = ClusterState.builder(state).nodes(DiscoveryNodes.builder(state.nodes()).localNodeId(relocationTargetNode)).build();
+        clusterService.setState(state);
+        logger.debug("--> relocation ongoing state:\n{}", clusterService.state().prettyPrint());
+
+        Request request = new Request(shardId).timeout("1ms").routedBasedOnClusterVersion(clusterService.state().version() + 1);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(null, request, listener);
+        reroutePhase.run();
+        assertListenerThrows("cluster state too old didn't cause a timeout", listener, UnavailableShardsException.class);
+
+        request = new Request(shardId).routedBasedOnClusterVersion(clusterService.state().version() + 1);
+        listener = new PlainActionFuture<>();
+        reroutePhase = action.new ReroutePhase(null, request, listener);
+        reroutePhase.run();
+        assertFalse("cluster state too old didn't cause a retry", listener.isDone());
+
+        // finish relocation
+        ShardRouting relocationTarget = clusterService.state().getRoutingTable().shardRoutingTable(shardId).shardsWithState(ShardRoutingState.INITIALIZING).get(0);
+        AllocationService allocationService = ESAllocationTestCase.createAllocationService();
+        RoutingAllocation.Result result = allocationService.applyStartedShards(state, Arrays.asList(relocationTarget));
+        ClusterState updatedState = ClusterState.builder(clusterService.state()).routingResult(result).build();
+
+        clusterService.setState(updatedState);
+        logger.debug("--> relocation complete state:\n{}", clusterService.state().prettyPrint());
+
+        IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());
+        final String primaryNodeId = shardRoutingTable.primaryShard().currentNodeId();
+        final List<CapturingTransport.CapturedRequest> capturedRequests =
+            transport.getCapturedRequestsByTargetNodeAndClear().get(primaryNodeId);
+        assertThat(capturedRequests, notNullValue());
+        assertThat(capturedRequests.size(), equalTo(1));
+        assertThat(capturedRequests.get(0).action, equalTo("testAction[p]"));
+        assertIndexShardCounter(1);
+    }
+
     public void testUnknownIndexOrShardOnReroute() throws InterruptedException {
         final String index = "test";
         // no replicas in oder to skip the replication part
@@ -496,7 +550,7 @@ public class TransportReplicationActionTests extends ESTestCase {
             }
         }
 
-        runReplicateTest(shardRoutingTable, assignedReplicas, totalShards);
+        runReplicateTest(state, shardRoutingTable, assignedReplicas, totalShards);
     }
 
     public void testReplicationWithShadowIndex() throws ExecutionException, InterruptedException {
@@ -527,18 +581,22 @@ public class TransportReplicationActionTests extends ESTestCase {
                 totalShards++;
             }
         }
-        runReplicateTest(shardRoutingTable, assignedReplicas, totalShards);
+        runReplicateTest(state, shardRoutingTable, assignedReplicas, totalShards);
     }
 
 
-    protected void runReplicateTest(IndexShardRoutingTable shardRoutingTable, int assignedReplicas, int totalShards) throws InterruptedException, ExecutionException {
+    protected void runReplicateTest(ClusterState state, IndexShardRoutingTable shardRoutingTable, int assignedReplicas, int totalShards) throws InterruptedException, ExecutionException {
         final ShardIterator shardIt = shardRoutingTable.shardsIt();
         final ShardId shardId = shardIt.shardId();
         final Request request = new Request(shardId);
         final PlainActionFuture<Response> listener = new PlainActionFuture<>();
         logger.debug("expecting [{}] assigned replicas, [{}] total shards. using state: \n{}", assignedReplicas, totalShards, clusterService.state().prettyPrint());
 
-        Releasable reference = getOrCreateIndexShardOperationsCounter();
+        TransportReplicationAction.IndexShardReference reference = getOrCreateIndexShardOperationsCounter();
+
+        ShardRouting primaryShard = state.getRoutingTable().shardRoutingTable(shardId).primaryShard();
+        indexShardRouting.set(primaryShard);
+
         assertIndexShardCounter(2);
         // TODO: set a default timeout
         TransportReplicationAction<Request, Request, Response>.ReplicationPhase replicationPhase =
@@ -701,6 +759,8 @@ public class TransportReplicationActionTests extends ESTestCase {
         // one replica to make sure replication is attempted
         clusterService.setState(state(index, true,
                 ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        ShardRouting primaryShard = clusterService.state().routingTable().shardRoutingTable(shardId).primaryShard();
+        indexShardRouting.set(primaryShard);
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Request request = new Request(shardId).timeout("100ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index b5b1f95..d23e11e 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -279,6 +279,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
         }
     }
 
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/16373")
     public void testOldIndexes() throws Exception {
         setupCluster();
 
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
index 3b18211..25a1e70 100644
--- a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
+++ b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
@@ -78,7 +78,7 @@ public abstract class AbstractClientHeadersTestCase extends ESTestCase {
         Settings settings = Settings.builder()
                 .put(HEADER_SETTINGS)
                 .put("path.home", createTempDir().toString())
-                .put("node.name", "test-" + getTestName())
+                .put("name", "test-" + getTestName())
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         threadPool = new ThreadPool(settings);
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
index 9d75983..dcb5ac4 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
@@ -53,7 +53,7 @@ public class TransportClientRetryIT extends ESIntegTestCase {
         }
 
         Settings.Builder builder = settingsBuilder().put("client.transport.nodes_sampler_interval", "1s")
-                .put("node.name", "transport_client_retry_test")
+                .put("name", "transport_client_retry_test")
                 .put(Node.NODE_MODE_SETTING.getKey(), internalCluster().getNodeMode())
                 .put(ClusterName.CLUSTER_NAME_SETTING.getKey(), internalCluster().getClusterName())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING.getKey(), true)
diff --git a/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java b/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java
index 4e8d1d9..3ad8d50 100644
--- a/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardFailedClusterStateTaskExecutorTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.action.shard;
 
+import com.carrotsearch.hppc.cursors.ObjectCursor;
 import org.apache.lucene.index.CorruptIndexException;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
@@ -28,6 +29,7 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.GroupShardsIterator;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardIterator;
@@ -38,6 +40,9 @@ import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.index.Index;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Before;
 
@@ -45,12 +50,15 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
+import java.util.Optional;
+import java.util.Set;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.instanceOf;
 import static org.hamcrest.CoreMatchers.not;
 
 public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCase {
@@ -119,9 +127,25 @@ public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCa
         tasks.addAll(failingTasks);
         tasks.addAll(nonExistentTasks);
         ClusterStateTaskExecutor.BatchResult<ShardStateAction.ShardRoutingEntry> result = failingExecutor.execute(currentState, tasks);
-        Map<ShardStateAction.ShardRoutingEntry, Boolean> taskResultMap =
-            failingTasks.stream().collect(Collectors.toMap(Function.identity(), task -> false));
-        taskResultMap.putAll(nonExistentTasks.stream().collect(Collectors.toMap(Function.identity(), task -> true)));
+        Map<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> taskResultMap =
+            failingTasks.stream().collect(Collectors.toMap(Function.identity(), task -> ClusterStateTaskExecutor.TaskResult.failure(new RuntimeException("simulated applyFailedShards failure"))));
+        taskResultMap.putAll(nonExistentTasks.stream().collect(Collectors.toMap(Function.identity(), task -> ClusterStateTaskExecutor.TaskResult.success())));
+        assertTaskResults(taskResultMap, result, currentState, false);
+    }
+
+    public void testIllegalShardFailureRequests() throws Exception {
+        String reason = "test illegal shard failure requests";
+        ClusterState currentState = createClusterStateWithStartedShards(reason);
+        List<ShardStateAction.ShardRoutingEntry> failingTasks = createExistingShards(currentState, reason);
+        List<ShardStateAction.ShardRoutingEntry> tasks = new ArrayList<>();
+        for (ShardStateAction.ShardRoutingEntry failingTask : failingTasks) {
+            tasks.add(new ShardStateAction.ShardRoutingEntry(failingTask.getShardRouting(), randomInvalidSourceShard(currentState, failingTask.getShardRouting()), failingTask.message, failingTask.failure));
+        }
+        Map<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> taskResultMap =
+            tasks.stream().collect(Collectors.toMap(
+                Function.identity(),
+                task -> ClusterStateTaskExecutor.TaskResult.failure(new ShardStateAction.NoLongerPrimaryShardException(task.getShardRouting().shardId(), "source shard [" + task.sourceShardRouting + "] is neither the local allocation nor the primary allocation"))));
+        ClusterStateTaskExecutor.BatchResult<ShardStateAction.ShardRoutingEntry> result = executor.execute(currentState, tasks);
         assertTaskResults(taskResultMap, result, currentState, false);
     }
 
@@ -156,17 +180,22 @@ public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCa
         for (int i = 0; i < numberOfTasks; i++) {
             shardsToFail.add(randomFrom(failures));
         }
-        return toTasks(shardsToFail, indexUUID, reason);
+        return toTasks(currentState, shardsToFail, indexUUID, reason);
     }
 
     private List<ShardStateAction.ShardRoutingEntry> createNonExistentShards(ClusterState currentState, String reason) {
         // add shards from a non-existent index
-        MetaData nonExistentMetaData =
-            MetaData.builder()
-                .put(IndexMetaData.builder("non-existent").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(numberOfReplicas))
-                .build();
-        RoutingTable routingTable = RoutingTable.builder().addAsNew(nonExistentMetaData.index("non-existent")).build();
-        String nonExistentIndexUUID = nonExistentMetaData.index("non-existent").getIndexUUID();
+        String nonExistentIndexUUID = "non-existent";
+        Index index = new Index("non-existent", nonExistentIndexUUID);
+        List<String> nodeIds = new ArrayList<>();
+        for (ObjectCursor<String> nodeId : currentState.nodes().getNodes().keys()) {
+            nodeIds.add(nodeId.toString());
+        }
+        List<ShardRouting> nonExistentShards = new ArrayList<>();
+        nonExistentShards.add(nonExistentShardRouting(index, nodeIds, true));
+        for (int i = 0; i < numberOfReplicas; i++) {
+            nonExistentShards.add(nonExistentShardRouting(index, nodeIds, false));
+        }
 
         List<ShardStateAction.ShardRoutingEntry> existingShards = createExistingShards(currentState, reason);
         List<ShardStateAction.ShardRoutingEntry> shardsWithMismatchedAllocationIds = new ArrayList<>();
@@ -174,28 +203,32 @@ public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCa
             ShardRouting sr = existingShard.getShardRouting();
             ShardRouting nonExistentShardRouting =
                 TestShardRouting.newShardRouting(sr.index(), sr.id(), sr.currentNodeId(), sr.relocatingNodeId(), sr.restoreSource(), sr.primary(), sr.state(), sr.version());
-            shardsWithMismatchedAllocationIds.add(new ShardStateAction.ShardRoutingEntry(nonExistentShardRouting, existingShard.indexUUID, existingShard.message, existingShard.failure));
+            shardsWithMismatchedAllocationIds.add(new ShardStateAction.ShardRoutingEntry(nonExistentShardRouting, nonExistentShardRouting, existingShard.message, existingShard.failure));
         }
 
         List<ShardStateAction.ShardRoutingEntry> tasks = new ArrayList<>();
-        tasks.addAll(toTasks(routingTable.allShards(), nonExistentIndexUUID, reason));
+        nonExistentShards.forEach(shard -> tasks.add(new ShardStateAction.ShardRoutingEntry(shard, shard, reason, new CorruptIndexException("simulated", nonExistentIndexUUID))));
         tasks.addAll(shardsWithMismatchedAllocationIds);
         return tasks;
     }
 
+    private ShardRouting nonExistentShardRouting(Index index, List<String> nodeIds, boolean primary) {
+        return TestShardRouting.newShardRouting(index, 0, randomFrom(nodeIds), primary, randomFrom(ShardRoutingState.INITIALIZING, ShardRoutingState.RELOCATING, ShardRoutingState.STARTED), randomIntBetween(1, 8));
+    }
+
     private static void assertTasksSuccessful(
         List<ShardStateAction.ShardRoutingEntry> tasks,
         ClusterStateTaskExecutor.BatchResult<ShardStateAction.ShardRoutingEntry> result,
         ClusterState clusterState,
         boolean clusterStateChanged
     ) {
-        Map<ShardStateAction.ShardRoutingEntry, Boolean> taskResultMap =
-            tasks.stream().collect(Collectors.toMap(Function.identity(), task -> true));
+        Map<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> taskResultMap =
+            tasks.stream().collect(Collectors.toMap(Function.identity(), task -> ClusterStateTaskExecutor.TaskResult.success()));
         assertTaskResults(taskResultMap, result, clusterState, clusterStateChanged);
     }
 
     private static void assertTaskResults(
-        Map<ShardStateAction.ShardRoutingEntry, Boolean> taskResultMap,
+        Map<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> taskResultMap,
         ClusterStateTaskExecutor.BatchResult<ShardStateAction.ShardRoutingEntry> result,
         ClusterState clusterState,
         boolean clusterStateChanged
@@ -203,24 +236,29 @@ public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCa
         // there should be as many task results as tasks
         assertEquals(taskResultMap.size(), result.executionResults.size());
 
-        for (Map.Entry<ShardStateAction.ShardRoutingEntry, Boolean> entry : taskResultMap.entrySet()) {
+        for (Map.Entry<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> entry : taskResultMap.entrySet()) {
             // every task should have a corresponding task result
             assertTrue(result.executionResults.containsKey(entry.getKey()));
 
             // the task results are as expected
-            assertEquals(entry.getValue(), result.executionResults.get(entry.getKey()).isSuccess());
+            assertEquals(entry.getValue().isSuccess(), result.executionResults.get(entry.getKey()).isSuccess());
         }
 
-        // every shard that we requested to be successfully failed is
-        // gone
         List<ShardRouting> shards = clusterState.getRoutingTable().allShards();
-        for (Map.Entry<ShardStateAction.ShardRoutingEntry, Boolean> entry : taskResultMap.entrySet()) {
-            if (entry.getValue()) {
+        for (Map.Entry<ShardStateAction.ShardRoutingEntry, ClusterStateTaskExecutor.TaskResult> entry : taskResultMap.entrySet()) {
+            if (entry.getValue().isSuccess()) {
+                // the shard was successfully failed and so should not
+                // be in the routing table
                 for (ShardRouting shard : shards) {
                     if (entry.getKey().getShardRouting().allocationId() != null) {
                         assertThat(shard.allocationId(), not(equalTo(entry.getKey().getShardRouting().allocationId())));
                     }
                 }
+            } else {
+                // check we saw the expected failure
+                ClusterStateTaskExecutor.TaskResult actualResult = result.executionResults.get(entry.getKey());
+                assertThat(actualResult.getFailure(), instanceOf(entry.getValue().getFailure().getClass()));
+                assertThat(actualResult.getFailure().getMessage(), equalTo(entry.getValue().getFailure().getMessage()));
             }
         }
 
@@ -231,11 +269,49 @@ public class ShardFailedClusterStateTaskExecutorTests extends ESAllocationTestCa
         }
     }
 
-    private static List<ShardStateAction.ShardRoutingEntry> toTasks(List<ShardRouting> shards, String indexUUID, String message) {
+    private static List<ShardStateAction.ShardRoutingEntry> toTasks(ClusterState currentState, List<ShardRouting> shards, String indexUUID, String message) {
         return shards
             .stream()
-            .map(shard -> new ShardStateAction.ShardRoutingEntry(shard, indexUUID, message, new CorruptIndexException("simulated", indexUUID)))
+            .map(shard -> new ShardStateAction.ShardRoutingEntry(shard, randomValidSourceShard(currentState, shard), message, new CorruptIndexException("simulated", indexUUID)))
             .collect(Collectors.toList());
     }
 
+    private static ShardRouting randomValidSourceShard(ClusterState currentState, ShardRouting shardRouting) {
+        // for the request node ID to be valid, either the request is
+        // from the node the shard is assigned to, or the request is
+        // from the node holding the primary shard
+        if (randomBoolean()) {
+            // request from local node
+            return shardRouting;
+        } else {
+            // request from primary node unless in the case of
+            // non-existent shards there is not one and we fallback to
+            // the local node
+            ShardRouting primaryNodeId = primaryShard(currentState, shardRouting);
+            return primaryNodeId != null ? primaryNodeId : shardRouting;
+        }
+    }
+
+    private static ShardRouting randomInvalidSourceShard(ClusterState currentState, ShardRouting shardRouting) {
+        ShardRouting primaryShard = primaryShard(currentState, shardRouting);
+        Set<ShardRouting> shards =
+            currentState
+                .routingTable()
+                .allShards()
+                .stream()
+                .filter(shard -> !shard.isSameAllocation(shardRouting))
+                .filter(shard -> !shard.isSameAllocation(primaryShard))
+                .collect(Collectors.toSet());
+        if (!shards.isEmpty()) {
+            return randomSubsetOf(1, shards.toArray(new ShardRouting[0])).get(0);
+        } else {
+            return
+                TestShardRouting.newShardRouting(shardRouting.index(), shardRouting.id(), DiscoveryService.generateNodeId(Settings.EMPTY), randomBoolean(), randomFrom(ShardRoutingState.values()), shardRouting.version());
+        }
+    }
+
+    private static ShardRouting primaryShard(ClusterState currentState, ShardRouting shardRouting) {
+        IndexShardRoutingTable indexShard = currentState.getRoutingTable().shardRoutingTableOrNull(shardRouting.shardId());
+        return indexShard == null ? null : indexShard.primaryShard();
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java b/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java
index b6e69b2..62f32e2 100644
--- a/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/action/shard/ShardStateActionTests.java
@@ -30,7 +30,9 @@ import org.elasticsearch.cluster.routing.IndexRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.ShardsIterator;
+import org.elasticsearch.cluster.routing.TestShardRouting;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.Discovery;
@@ -128,13 +130,11 @@ public class ShardStateActionTests extends ESTestCase {
 
         clusterService.setState(ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5)));
 
-        String indexUUID = clusterService.state().metaData().index(index).getIndexUUID();
-
         AtomicBoolean success = new AtomicBoolean();
         CountDownLatch latch = new CountDownLatch(1);
 
         ShardRouting shardRouting = getRandomShardRouting(index);
-        shardStateAction.shardFailed(shardRouting, indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+        shardStateAction.shardFailed(shardRouting, shardRouting, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 success.set(true);
@@ -174,15 +174,14 @@ public class ShardStateActionTests extends ESTestCase {
         noMasterBuilder.masterNodeId(null);
         clusterService.setState(ClusterState.builder(clusterService.state()).nodes(noMasterBuilder));
 
-        String indexUUID = clusterService.state().metaData().index(index).getIndexUUID();
-
         CountDownLatch latch = new CountDownLatch(1);
         AtomicInteger retries = new AtomicInteger();
         AtomicBoolean success = new AtomicBoolean();
 
         setUpMasterRetryVerification(1, retries, latch, requestId -> {});
 
-        shardStateAction.shardFailed(getRandomShardRouting(index), indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+        ShardRouting failedShard = getRandomShardRouting(index);
+        shardStateAction.shardFailed(failedShard, failedShard, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 success.set(true);
@@ -208,8 +207,6 @@ public class ShardStateActionTests extends ESTestCase {
 
         clusterService.setState(ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5)));
 
-        String indexUUID = clusterService.state().metaData().index(index).getIndexUUID();
-
         CountDownLatch latch = new CountDownLatch(1);
         AtomicInteger retries = new AtomicInteger();
         AtomicBoolean success = new AtomicBoolean();
@@ -232,7 +229,8 @@ public class ShardStateActionTests extends ESTestCase {
         final int numberOfRetries = randomIntBetween(1, 256);
         setUpMasterRetryVerification(numberOfRetries, retries, latch, retryLoop);
 
-        shardStateAction.shardFailed(getRandomShardRouting(index), indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+        ShardRouting failedShard = getRandomShardRouting(index);
+        shardStateAction.shardFailed(failedShard, failedShard, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 success.set(true);
@@ -265,11 +263,10 @@ public class ShardStateActionTests extends ESTestCase {
 
         clusterService.setState(ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5)));
 
-        String indexUUID = clusterService.state().metaData().index(index).getIndexUUID();
-
         AtomicBoolean failure = new AtomicBoolean();
 
-        shardStateAction.shardFailed(getRandomShardRouting(index), indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+        ShardRouting failedShard = getRandomShardRouting(index);
+        shardStateAction.shardFailed(failedShard, failedShard, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 failure.set(false);
@@ -295,15 +292,13 @@ public class ShardStateActionTests extends ESTestCase {
 
         clusterService.setState(ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5)));
 
-        String indexUUID = clusterService.state().metaData().index(index).getIndexUUID();
-
         AtomicBoolean success = new AtomicBoolean();
         CountDownLatch latch = new CountDownLatch(1);
 
         ShardRouting failedShard = getRandomShardRouting(index);
         RoutingTable routingTable = RoutingTable.builder(clusterService.state().getRoutingTable()).remove(index).build();
         clusterService.setState(ClusterState.builder(clusterService.state()).routingTable(routingTable));
-        shardStateAction.shardFailed(failedShard, indexUUID, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+        shardStateAction.shardFailed(failedShard, failedShard, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 success.set(true);
@@ -325,6 +320,44 @@ public class ShardStateActionTests extends ESTestCase {
         assertTrue(success.get());
     }
 
+    public void testNoLongerPrimaryShardException() throws InterruptedException {
+        final String index = "test";
+
+        clusterService.setState(ClusterStateCreationUtils.stateWithActivePrimary(index, true, randomInt(5)));
+
+        ShardRouting failedShard = getRandomShardRouting(index);
+
+        String nodeId = randomFrom(clusterService.state().nodes().nodes().keys().toArray(String.class));
+
+        AtomicReference<Throwable> failure = new AtomicReference<>();
+        CountDownLatch latch = new CountDownLatch(1);
+
+        ShardRouting sourceFailedShard = TestShardRouting.newShardRouting(failedShard.index(), failedShard.id(), nodeId, randomBoolean(), randomFrom(ShardRoutingState.values()), failedShard.version());
+        shardStateAction.shardFailed(failedShard, sourceFailedShard, "test", getSimulatedFailure(), new ShardStateAction.Listener() {
+            @Override
+            public void onSuccess() {
+                failure.set(null);
+                latch.countDown();
+            }
+
+            @Override
+            public void onFailure(Throwable t) {
+                failure.set(t);
+                latch.countDown();
+            }
+        });
+
+        ShardStateAction.NoLongerPrimaryShardException catastrophicError =
+            new ShardStateAction.NoLongerPrimaryShardException(failedShard.shardId(), "source shard [" + sourceFailedShard + " is neither the local allocation nor the primary allocation");
+        CapturingTransport.CapturedRequest[] capturedRequests = transport.getCapturedRequestsAndClear();
+        transport.handleRemoteError(capturedRequests[0].requestId, catastrophicError);
+
+        latch.await();
+        assertNotNull(failure.get());
+        assertThat(failure.get(), instanceOf(ShardStateAction.NoLongerPrimaryShardException.class));
+        assertThat(failure.get().getMessage(), equalTo(catastrophicError.getMessage()));
+    }
+
     private ShardRouting getRandomShardRouting(String index) {
         IndexRoutingTable indexRoutingTable = clusterService.state().routingTable().index(index);
         ShardsIterator shardsIterator = indexRoutingTable.randomAllActiveShardsIt();
diff --git a/core/src/test/java/org/elasticsearch/common/cli/TerminalTests.java b/core/src/test/java/org/elasticsearch/common/cli/TerminalTests.java
index 498dd26..3f5562f 100644
--- a/core/src/test/java/org/elasticsearch/common/cli/TerminalTests.java
+++ b/core/src/test/java/org/elasticsearch/common/cli/TerminalTests.java
@@ -19,9 +19,6 @@
 
 package org.elasticsearch.common.cli;
 
-import java.nio.file.NoSuchFileException;
-import java.util.List;
-
 import static org.hamcrest.Matchers.hasItem;
 import static org.hamcrest.Matchers.hasSize;
 
@@ -46,6 +43,11 @@ public class TerminalTests extends CliToolTestCase {
         assertPrinted(terminal, Terminal.Verbosity.VERBOSE, "text");
     }
 
+    public void testEscaping() throws Exception {
+        CaptureOutputTerminal terminal = new CaptureOutputTerminal(Terminal.Verbosity.NORMAL);
+        assertPrinted(terminal, Terminal.Verbosity.NORMAL, "This message contains percent like %20n");
+    }
+
     private void assertPrinted(CaptureOutputTerminal logTerminal, Terminal.Verbosity verbosity, String text) {
         logTerminal.print(verbosity, text);
         assertThat(logTerminal.getTerminalOutput(), hasSize(1));
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
index 4090daa..0fb9b1a 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
@@ -127,7 +127,7 @@ public class NetworkModuleTests extends ModuleTestCase {
     }
 
     public void testRegisterHttpTransport() {
-        Settings settings = Settings.builder().put(NetworkModule.HTTP_TYPE_SETTING.getKey(), "custom").build();
+        Settings settings = Settings.builder().put(NetworkModule.HTTP_TYPE_KEY, "custom").build();
         NetworkModule module = new NetworkModule(new NetworkService(settings), settings, false, null);
         module.registerHttpTransport("custom", FakeHttpTransport.class);
         assertBinding(module, HttpServerTransport.class, FakeHttpTransport.class);
diff --git a/core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java b/core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java
index e60e404..290eec0 100644
--- a/core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/common/settings/SettingsModuleTests.java
@@ -74,40 +74,11 @@ public class SettingsModuleTests extends ModuleTestCase {
                 assertInstanceBinding(module, Settings.class, (s) -> s == settings);
                 fail();
             } catch (IllegalArgumentException ex) {
-                assertEquals("tribe.t1 validation failed: Failed to parse value [[2.0]] for setting [cluster.routing.allocation.balance.shard]", ex.getMessage());
-            }
-        }
-    }
-
-    public void testSpecialTribeSetting() {
-        {
-            Settings settings = Settings.builder().put("tribe.blocks.write", "false").build();
-            SettingsModule module = new SettingsModule(settings, new SettingsFilter(Settings.EMPTY));
-            assertInstanceBinding(module, Settings.class, (s) -> s == settings);
-        }
-        {
-            Settings settings = Settings.builder().put("tribe.blocks.write", "BOOM").build();
-            SettingsModule module = new SettingsModule(settings, new SettingsFilter(Settings.EMPTY));
-            try {
-                assertInstanceBinding(module, Settings.class, (s) -> s == settings);
-                fail();
-            } catch (IllegalArgumentException ex) {
-                assertEquals("Failed to parse value [BOOM] cannot be parsed to boolean [ true/1/on/yes OR false/0/off/no ]", ex.getMessage());
-            }
-        }
-        {
-            Settings settings = Settings.builder().put("tribe.blocks.wtf", "BOOM").build();
-            SettingsModule module = new SettingsModule(settings, new SettingsFilter(Settings.EMPTY));
-            try {
-                assertInstanceBinding(module, Settings.class, (s) -> s == settings);
-                fail();
-            } catch (IllegalArgumentException ex) {
-                assertEquals("tribe.blocks validation failed: unknown setting [wtf]", ex.getMessage());
+                assertEquals("Failed to parse value [[2.0]] for setting [cluster.routing.allocation.balance.shard]", ex.getMessage());
             }
         }
     }
 
-
     public void testLoggerSettings() {
         {
             Settings settings = Settings.builder().put("logger._root", "TRACE").put("logger.transport", "INFO").build();
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index c282f3e..739e07d 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -56,7 +56,6 @@ import org.elasticsearch.discovery.zen.ping.ZenPing;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
 import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
-import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.store.IndicesStoreIntegrationIT;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -905,7 +904,6 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         ShardRouting failedShard =
             randomFrom(clusterService().state().getRoutingNodes().node(nonMasterNodeId).shardsWithState(ShardRoutingState.STARTED));
         ShardStateAction service = internalCluster().getInstance(ShardStateAction.class, nonMasterNode);
-        String indexUUID = clusterService().state().metaData().index("test").getIndexUUID();
         CountDownLatch latch = new CountDownLatch(1);
         AtomicBoolean success = new AtomicBoolean();
 
@@ -913,7 +911,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         NetworkPartition networkPartition = addRandomIsolation(isolatedNode);
         networkPartition.startDisrupting();
 
-        service.shardFailed(failedShard, indexUUID, "simulated", new CorruptIndexException("simulated", (String) null), new ShardStateAction.Listener() {
+        service.shardFailed(failedShard, failedShard, "simulated", new CorruptIndexException("simulated", (String) null), new ShardStateAction.Listener() {
             @Override
             public void onSuccess() {
                 success.set(true);
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
index 6c564a9..ee92945 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
@@ -80,20 +80,6 @@ import static org.hamcrest.Matchers.sameInstance;
 @ESIntegTestCase.SuppressLocalMode
 @TestLogging("_root:DEBUG")
 public class ZenDiscoveryIT extends ESIntegTestCase {
-    public void testChangeRejoinOnMasterOptionIsDynamic() throws Exception {
-        Settings nodeSettings = Settings.settingsBuilder()
-                .put("discovery.type", "zen") // <-- To override the local setting if set externally
-                .build();
-        String nodeName = internalCluster().startNode(nodeSettings);
-        ZenDiscovery zenDiscovery = (ZenDiscovery) internalCluster().getInstance(Discovery.class, nodeName);
-        assertThat(zenDiscovery.isRejoinOnMasterGone(), is(true));
-
-        client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(ZenDiscovery.REJOIN_ON_MASTER_GONE_SETTING.getKey(), false))
-                .get();
-
-        assertThat(zenDiscovery.isRejoinOnMasterGone(), is(false));
-    }
 
     public void testNoShardRelocationsOccurWhenElectedMasterNodeFails() throws Exception {
         Settings defaultSettings = Settings.builder()
diff --git a/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
index 5146c7d..2fce6e4 100644
--- a/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
@@ -43,6 +43,7 @@ public class RecoveryBackwardsCompatibilityIT extends ESBackcompatTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
+                .put("action.admin.cluster.node.shutdown.delay", "10ms")
                 .put("gateway.recover_after_nodes", 2).build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
index dd398bd..a08a072 100644
--- a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
@@ -331,6 +331,7 @@ public class RecoveryFromGatewayIT extends ESIntegTestCase {
 
     public void testReusePeerRecovery() throws Exception {
         final Settings settings = settingsBuilder()
+                .put("action.admin.cluster.node.shutdown.delay", "10ms")
                 .put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false)
                 .put("gateway.recover_after_nodes", 4)
                 .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_INCOMING_RECOVERIES_SETTING.getKey(), 4)
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index ac27217..9f2b33b 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -186,7 +186,7 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
         // we have to prefer CURRENT since with the range of versions we support it's rather unlikely to get the current actually.
         Version version = randomBoolean() ? Version.CURRENT : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
         Settings settings = Settings.settingsBuilder()
-                .put("node.name", AbstractQueryTestCase.class.toString())
+                .put("name", AbstractQueryTestCase.class.toString())
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
                 .build();
@@ -204,35 +204,6 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                 new Class[]{Client.class},
                 clientInvocationHandler);
         namedWriteableRegistry = new NamedWriteableRegistry();
-        ScriptModule scriptModule = new ScriptModule() {
-            @Override
-            protected void configure() {
-                Settings settings = Settings.builder()
-                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
-                    // no file watching, so we don't need a ResourceWatcherService
-                    .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-                    .build();
-                MockScriptEngine mockScriptEngine = new MockScriptEngine();
-                Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
-                multibinder.addBinding().toInstance(mockScriptEngine);
-                Set<ScriptEngineService> engines = new HashSet<>();
-                engines.add(mockScriptEngine);
-                List<ScriptContext.Plugin> customContexts = new ArrayList<>();
-                ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections.singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
-                bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
-                ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
-                bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
-                ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-                bind(ScriptSettings.class).toInstance(scriptSettings);
-                try {
-                    ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null, scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-                    bind(ScriptService.class).toInstance(scriptService);
-                } catch(IOException e) {
-                    throw new IllegalStateException("error while binding ScriptService", e);
-                }
-            }
-        };
-        scriptModule.prepareSettings(settingsModule);
         injector = new ModulesBuilder().add(
                 new EnvironmentModule(new Environment(settings)),
                 settingsModule,
@@ -244,7 +215,34 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                         bindMapperExtension();
                     }
                 },
-                scriptModule,
+                new ScriptModule(settingsModule) {
+                    @Override
+                    protected void configure() {
+                        Settings settings = Settings.builder()
+                                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
+                                // no file watching, so we don't need a ResourceWatcherService
+                                .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
+                                .build();
+                        MockScriptEngine mockScriptEngine = new MockScriptEngine();
+                        Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
+                        multibinder.addBinding().toInstance(mockScriptEngine);
+                        Set<ScriptEngineService> engines = new HashSet<>();
+                        engines.add(mockScriptEngine);
+                        List<ScriptContext.Plugin> customContexts = new ArrayList<>();
+                        ScriptEngineRegistry scriptEngineRegistry = new ScriptEngineRegistry(Collections.singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MockScriptEngine.class, MockScriptEngine.TYPES)));
+                        bind(ScriptEngineRegistry.class).toInstance(scriptEngineRegistry);
+                        ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(customContexts);
+                        bind(ScriptContextRegistry.class).toInstance(scriptContextRegistry);
+                        ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
+                        bind(ScriptSettings.class).toInstance(scriptSettings);
+                        try {
+                            ScriptService scriptService = new ScriptService(settings, new Environment(settings), engines, null, scriptEngineRegistry, scriptContextRegistry, scriptSettings);
+                            bind(ScriptService.class).toInstance(scriptService);
+                        } catch(IOException e) {
+                            throw new IllegalStateException("error while binding ScriptService", e);
+                        }
+                    }
+                },
                 new IndexSettingsModule(index, indexSettings),
                 new SearchModule(settings, namedWriteableRegistry) {
                     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index bedda8d..a77e75d 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -816,7 +816,6 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         }
     }
 
-    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/16364")
     public void testStressRelocated() throws Exception {
         assertAcked(client().admin().indices().prepareCreate("test").setSettings(
             Settings.builder().put("index.number_of_shards", 1).put("index.number_of_replicas", 0)
@@ -827,14 +826,14 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         final IndexShard shard = test.getShardOrNull(0);
         final int numThreads = randomIntBetween(2, 4);
         Thread[] indexThreads = new Thread[numThreads];
-        CountDownLatch somePrimaryOperationLockAcquired = new CountDownLatch(1);
+        CountDownLatch allPrimaryOperationLocksAcquired = new CountDownLatch(numThreads);
         CyclicBarrier barrier = new CyclicBarrier(numThreads + 1);
         for (int i = 0; i < indexThreads.length; i++) {
             indexThreads[i] = new Thread() {
                 @Override
                 public void run() {
                     try (Releasable operationLock = shard.acquirePrimaryOperationLock()) {
-                        somePrimaryOperationLockAcquired.countDown();
+                        allPrimaryOperationLocksAcquired.countDown();
                         barrier.await();
                     } catch (InterruptedException | BrokenBarrierException e) {
                         throw new RuntimeException(e);
@@ -848,8 +847,8 @@ public class IndexShardTests extends ESSingleNodeTestCase {
             shard.relocated("simulated recovery");
             relocated.set(true);
         });
-        // ensure we wait for at least one primary operation lock to be acquired
-        somePrimaryOperationLockAcquired.await();
+        // ensure we wait for all primary operation locks to be acquired
+        allPrimaryOperationLocksAcquired.await();
         // start recovery thread
         recoveryThread.start();
         assertThat(relocated.get(), equalTo(false));
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
index bcbe41d..e5fcba2 100644
--- a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
+++ b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.ingest;
 
+import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
@@ -38,11 +40,13 @@ import org.elasticsearch.ingest.core.IngestDocument;
 import org.elasticsearch.node.NodeModule;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.transport.RemoteTransportException;
 
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -201,23 +205,7 @@ public class IngestClientIT extends ESIntegTestCase {
         assertThat(getResponse.pipelines().size(), equalTo(0));
     }
 
-    public void testPutWithPipelineError() throws Exception {
-        BytesReference source = jsonBuilder().startObject()
-                .field("description", "my_pipeline")
-                .startArray("processors")
-                .startObject()
-                .startObject("not_found")
-                .endObject()
-                .endObject()
-                .endArray()
-                .endObject().bytes();
-        PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
-        WritePipelineResponse response = client().admin().cluster().putPipeline(putPipelineRequest).get();
-        assertThat(response.isAcknowledged(), equalTo(false));
-        assertThat(response.getError().getReason(), equalTo("No processor type exists with name [not_found]"));
-    }
-
-    public void testPutWithProcessorFactoryError() throws Exception {
+    public void testPutWithPipelineFactoryError() throws Exception {
         BytesReference source = jsonBuilder().startObject()
                 .field("description", "my_pipeline")
                 .startArray("processors")
@@ -229,9 +217,13 @@ public class IngestClientIT extends ESIntegTestCase {
                 .endArray()
                 .endObject().bytes();
         PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
-        WritePipelineResponse response = client().admin().cluster().putPipeline(putPipelineRequest).get();
-        assertThat(response.isAcknowledged(), equalTo(false));
-        assertThat(response.getError().getReason(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
+        try {
+            client().admin().cluster().putPipeline(putPipelineRequest).get();
+        } catch (ExecutionException e) {
+            ElasticsearchParseException ex = (ElasticsearchParseException) ExceptionsHelper.unwrap(e, ElasticsearchParseException.class);
+            assertNotNull(ex);
+            assertThat(ex.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
+        }
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
index a75a84f..bdf1f7d 100644
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.ingest;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ResourceNotFoundException;
 import org.elasticsearch.action.ingest.DeletePipelineRequest;
 import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -103,42 +103,21 @@ public class PipelineStoreTests extends ESTestCase {
     }
 
     public void testPutWithErrorResponse() {
+        String id = "_id";
+        Pipeline pipeline = store.get(id);
+        assertThat(pipeline, nullValue());
+        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build();
 
-    }
-
-    public void testConstructPipelineResponseSuccess() {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("field", "foo");
-        processorConfig.put("value", "bar");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("set", processorConfig)));
-        WritePipelineResponse response = store.validatePipelineResponse("test_id", pipelineConfig);
-        assertThat(response, nullValue());
-    }
-
-    public void testConstructPipelineResponseMissingProcessorsFieldException() {
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        WritePipelineResponse response = store.validatePipelineResponse("test_id", pipelineConfig);
-        assertThat(response.getError().getProcessorType(), is(nullValue()));
-        assertThat(response.getError().getProcessorTag(), is(nullValue()));
-        assertThat(response.getError().getProcessorPropertyName(), equalTo("processors"));
-        assertThat(response.getError().getReason(), equalTo("[processors] required property is missing"));
-    }
-
-    public void testConstructPipelineResponseConfigurationException() {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("field", "foo");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put("description", "_description");
-        pipelineConfig.put("processors", Collections.singletonList(Collections.singletonMap("set", processorConfig)));
-        WritePipelineResponse response = store.validatePipelineResponse("test_id", pipelineConfig);
-
-        assertThat(response.getError().getProcessorTag(), nullValue());
-        assertThat(response.getError().getProcessorType(), equalTo("set"));
-        assertThat(response.getError().getProcessorPropertyName(), equalTo("value"));
-        assertThat(response.getError().getReason(), equalTo("[value] required property is missing"));
+        PutPipelineRequest putRequest = new PutPipelineRequest(id, new BytesArray("{\"description\": \"empty processors\"}"));
+        clusterState = store.innerPut(putRequest, clusterState);
+        try {
+            store.innerUpdatePipelines(clusterState);
+            fail("should fail");
+        } catch (ElasticsearchParseException e) {
+            assertThat(e.getMessage(), equalTo("[processors] required property is missing"));
+        }
+        pipeline = store.get(id);
+        assertThat(pipeline, nullValue());
     }
 
     public void testDelete() {
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
index 954a03c..722f14e 100644
--- a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.ingest.core;
 
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Before;
 
@@ -58,7 +58,7 @@ public class ConfigurationUtilsTests extends ESTestCase {
     public void testReadStringPropertyInvalidType() {
         try {
             ConfigurationUtils.readStringProperty(null, null, config, "arr");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[arr] property isn't a string, but of type [java.util.Arrays$ArrayList]"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
index 746ac2f..04f887e 100644
--- a/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
@@ -19,8 +19,8 @@
 
 package org.elasticsearch.ingest.core;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.Arrays;
@@ -59,7 +59,7 @@ public class PipelineFactoryTests extends ESTestCase {
         try {
             factory.create("_id", pipelineConfig, Collections.emptyMap());
             fail("should fail, missing required [processors] field");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[processors] required property is missing"));
         }
     }
@@ -91,7 +91,7 @@ public class PipelineFactoryTests extends ESTestCase {
         Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
         try {
             factory.create("_id", pipelineConfig, processorRegistry);
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
index c4c13a6..7350e3d 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.TestTemplateService;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Before;
 
@@ -65,7 +65,7 @@ public class AppendProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -76,7 +76,7 @@ public class AppendProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[value] required property is missing"));
         }
     }
@@ -88,7 +88,7 @@ public class AppendProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[value] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
index a07cec5..831e874 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -28,6 +29,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
 
 public class ConvertProcessorFactoryTests extends ESTestCase {
 
@@ -54,8 +56,11 @@ public class ConvertProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("type [" + type + "] not supported, cannot convert field."));
+        } catch (ElasticsearchParseException e) {
+            assertThat(e.getMessage(), Matchers.equalTo("[type] type [" + type + "] not supported, cannot convert field."));
+            assertThat(e.getHeader("processor_type").get(0), equalTo(ConvertProcessor.TYPE));
+            assertThat(e.getHeader("property_name").get(0), equalTo("type"));
+            assertThat(e.getHeader("processor_tag"), nullValue());
         }
     }
 
@@ -67,7 +72,7 @@ public class ConvertProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), Matchers.equalTo("[field] required property is missing"));
         }
     }
@@ -79,7 +84,7 @@ public class ConvertProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), Matchers.equalTo("[type] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
index 1139f19..7ea1de1 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -64,7 +65,7 @@ public class DateProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("processor creation should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), containsString("[match_field] required property is missing"));
         }
     }
@@ -80,7 +81,7 @@ public class DateProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("processor creation should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), containsString("[match_formats] required property is missing"));
         }
     }
@@ -170,7 +171,7 @@ public class DateProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("processor creation should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), containsString("[match_formats] property isn't a list, but of type [java.lang.String]"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
deleted file mode 100644
index 63eee56..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class DeDotProcessorFactoryTests extends ESTestCase {
-
-    private DeDotProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new DeDotProcessor.Factory();
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "_");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        DeDotProcessor deDotProcessor = factory.create(config);
-        assertThat(deDotProcessor.getSeparator(), equalTo("_"));
-        assertThat(deDotProcessor.getTag(), equalTo(processorTag));
-    }
-
-    public void testCreateMissingSeparatorField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        DeDotProcessor deDotProcessor = factory.create(config);
-        assertThat(deDotProcessor.getSeparator(), equalTo(DeDotProcessor.DEFAULT_SEPARATOR));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
deleted file mode 100644
index a0c87d7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class DeDotProcessorTests extends ESTestCase {
-
-    public void testSimple() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        source.put("a.b", "hello world!");
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        String separator = randomUnicodeOfCodepointLengthBetween(1, 10);
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getSourceAndMetadata().get("a" + separator + "b" ), equalTo("hello world!"));
-    }
-
-    public void testSimpleMap() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        Map<String, Object> subField = new HashMap<>();
-        subField.put("b.c", "hello world!");
-        source.put("a", subField);
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
-        processor.execute(ingestDocument);
-
-        IngestDocument expectedDocument = new IngestDocument(
-            Collections.singletonMap("a", Collections.singletonMap("b_c", "hello world!")),
-            Collections.emptyMap());
-        assertThat(ingestDocument, equalTo(expectedDocument));
-    }
-
-    public void testSimpleList() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        Map<String, Object> subField = new HashMap<>();
-        subField.put("b.c", "hello world!");
-        source.put("a", Arrays.asList(subField));
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
-        processor.execute(ingestDocument);
-
-        IngestDocument expectedDocument = new IngestDocument(
-            Collections.singletonMap("a",
-                Collections.singletonList(Collections.singletonMap("b_c", "hello world!"))),
-            Collections.emptyMap());
-        assertThat(ingestDocument, equalTo(expectedDocument));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
index 661a638..0d88710 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.TestTemplateService;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
@@ -55,7 +56,7 @@ public class FailProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[message] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
index bce0330..2440ff6 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -53,7 +54,7 @@ public class GsubProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -66,7 +67,7 @@ public class GsubProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[pattern] required property is missing"));
         }
     }
@@ -79,7 +80,7 @@ public class GsubProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[replacement] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
index 51eb989..c374b8a 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -50,7 +51,7 @@ public class JoinProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -62,7 +63,7 @@ public class JoinProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[separator] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
index 32eefa0..09d676b 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -47,7 +48,7 @@ public class LowercaseProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
index 5b03d28..1b9d881 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.TestTemplateService;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
@@ -55,7 +56,7 @@ public class RemoveProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
index ea6284f..85fc3e7 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -50,7 +51,7 @@ public class RenameProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -62,7 +63,7 @@ public class RenameProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[to] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
index 1c3cf15..2db2dcd 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.TestTemplateService;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
@@ -58,7 +59,7 @@ public class SetProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -69,7 +70,7 @@ public class SetProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[value] required property is missing"));
         }
     }
@@ -81,7 +82,7 @@ public class SetProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[value] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
index 3bd2f95..70fca6f 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -50,7 +51,7 @@ public class SplitProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
@@ -62,7 +63,7 @@ public class SplitProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[separator] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
index 8012893..1e74b78 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -47,7 +48,7 @@ public class TrimProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
index 914909f..40e14b5 100644
--- a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.ingest.processor;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.Processor;
 import org.elasticsearch.test.ESTestCase;
@@ -47,7 +48,7 @@ public class UppercaseProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("factory create should have failed");
-        } catch(ConfigurationPropertyException e) {
+        } catch(ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/monitor/jvm/JvmGcMonitorServiceSettingsTests.java b/core/src/test/java/org/elasticsearch/monitor/jvm/JvmGcMonitorServiceSettingsTests.java
index 29f4974..2c24896 100644
--- a/core/src/test/java/org/elasticsearch/monitor/jvm/JvmGcMonitorServiceSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/monitor/jvm/JvmGcMonitorServiceSettingsTests.java
@@ -62,9 +62,9 @@ public class JvmGcMonitorServiceSettingsTests extends ESTestCase {
     public void testMissingSetting() throws InterruptedException {
         String collector = randomAsciiOfLength(5);
         Set<AbstractMap.SimpleEntry<String, String>> entries = new HashSet<>();
-        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".warn", randomTimeValue()));
-        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".info", randomTimeValue()));
-        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".debug", randomTimeValue()));
+        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".warn", randomPositiveTimeValue()));
+        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".info", randomPositiveTimeValue()));
+        entries.add(new AbstractMap.SimpleEntry<>("monitor.jvm.gc.collector." + collector + ".debug", randomPositiveTimeValue()));
         Settings.Builder builder = Settings.builder();
 
         // drop a random setting or two
diff --git a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
index 0f8ee84..204dcef 100644
--- a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
+++ b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
@@ -59,13 +59,13 @@ public class InternalSettingsPreparerTests extends ESTestCase {
 
     public void testEmptySettings() {
         Settings settings = InternalSettingsPreparer.prepareSettings(Settings.EMPTY);
-        assertNotNull(settings.get("node.name")); // a name was set
+        assertNotNull(settings.get("name")); // a name was set
         assertNotNull(settings.get(ClusterName.CLUSTER_NAME_SETTING.getKey())); // a cluster name was set
         int size = settings.names().size();
 
         Environment env = InternalSettingsPreparer.prepareEnvironment(baseEnvSettings, null);
         settings = env.settings();
-        assertNotNull(settings.get("node.name")); // a name was set
+        assertNotNull(settings.get("name")); // a name was set
         assertNotNull(settings.get(ClusterName.CLUSTER_NAME_SETTING.getKey())); // a cluster name was set
         assertEquals(settings.toString(), size + 1 /* path.home is in the base settings */, settings.names().size());
         String home = Environment.PATH_HOME_SETTING.get(baseEnvSettings);
diff --git a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
index 7561b9b..144aedb 100644
--- a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
@@ -47,12 +47,11 @@ import static org.hamcrest.Matchers.notNullValue;
 public class NativeScriptTests extends ESTestCase {
     public void testNativeScript() throws InterruptedException {
         Settings settings = Settings.settingsBuilder()
-                .put("node.name", "testNativeScript")
+                .put("name", "testNativeScript")
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         SettingsModule settingsModule = new SettingsModule(settings, new SettingsFilter(settings));
-        ScriptModule scriptModule = new ScriptModule();
-        scriptModule.prepareSettings(settingsModule);
+        ScriptModule scriptModule = new ScriptModule(settingsModule);
         scriptModule.registerScript("my", MyNativeScriptFactory.class);
         Injector injector = new ModulesBuilder().add(
                 new EnvironmentModule(new Environment(settings)),
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index f240960..b729692 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -23,7 +23,6 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramInterval;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -34,8 +33,6 @@ import org.junit.After;
 import org.junit.Before;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Collections;
 import java.util.List;
 import java.util.concurrent.ExecutionException;
 
@@ -61,11 +58,6 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
     }
 
     @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return Collections.singleton(AssertingLocalTransport.TestPlugin.class);
-    }
-
-    @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index b214a17..bb969b9 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -83,7 +83,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
     @BeforeClass
     public static void init() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("node.name", SearchSourceBuilderTests.class.toString())
+                .put("name", SearchSourceBuilderTests.class.toString())
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         namedWriteableRegistry = new NamedWriteableRegistry();
diff --git a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
index 7e9bd14..c6e9320 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
@@ -92,7 +92,7 @@ public abstract class AbstractSnapshotIntegTestCase extends ESIntegTestCase {
     }
 
     public static void stopNode(final String node) throws IOException {
-        internalCluster().stopRandomNode(settings -> settings.get("node.name").equals(node));
+        internalCluster().stopRandomNode(settings -> settings.get("name").equals(node));
     }
 
     public void waitForBlock(String node, String repository, TimeValue timeout) throws InterruptedException {
diff --git a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
index 30dd0bd..c30954f 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java
@@ -97,7 +97,7 @@ public class ThreadPoolSerializationTests extends ESTestCase {
     }
 
     public void testThatNegativeSettingAllowsToStart() throws InterruptedException {
-        Settings settings = settingsBuilder().put("node.name", "index").put("threadpool.index.queue_size", "-1").build();
+        Settings settings = settingsBuilder().put("name", "index").put("threadpool.index.queue_size", "-1").build();
         ThreadPool threadPool = new ThreadPool(settings);
         assertThat(threadPool.info("index").getQueueSize(), is(nullValue()));
         terminate(threadPool);
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index 67a3f6f..01fae30 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -55,7 +55,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         ThreadPool threadPool = null;
         try {
             threadPool = new ThreadPool(settingsBuilder()
-                    .put("node.name", "testCorrectThreadPoolTypePermittedInSettings")
+                    .put("name", "testCorrectThreadPoolTypePermittedInSettings")
                     .put("threadpool." + threadPoolName + ".type", correctThreadPoolType.getType())
                     .build());
             ThreadPool.Info info = info(threadPool, threadPoolName);
@@ -78,7 +78,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         try {
             threadPool = new ThreadPool(
                     settingsBuilder()
-                            .put("node.name", "testThreadPoolCanNotOverrideThreadPoolType")
+                            .put("name", "testThreadPoolCanNotOverrideThreadPoolType")
                             .put("threadpool." + threadPoolName + ".type", incorrectThreadPoolType.getType())
                             .build());
             terminate(threadPool);
@@ -102,7 +102,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
 
                 // try to create a too-big (maxSize+1) thread pool
                 threadPool = new ThreadPool(settingsBuilder()
-                                               .put("node.name", "testIndexingThreadPoolsMaxSize")
+                                               .put("name", "testIndexingThreadPoolsMaxSize")
                                                .put("threadpool." + name + ".size", maxSize+1)
                                                .build());
 
@@ -143,7 +143,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         ThreadPool.ThreadPoolType validThreadPoolType = ThreadPool.THREAD_POOL_TYPES.get(threadPoolName);
         ThreadPool threadPool = null;
         try {
-            threadPool = new ThreadPool(settingsBuilder().put("node.name", "testUpdateSettingsCanNotChangeThreadPoolType").build());
+            threadPool = new ThreadPool(settingsBuilder().put("name", "testUpdateSettingsCanNotChangeThreadPoolType").build());
             ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
 
@@ -168,7 +168,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         ThreadPool threadPool = null;
         try {
             Settings nodeSettings = Settings.settingsBuilder()
-                    .put("node.name", "testCachedExecutorType").build();
+                    .put("name", "testCachedExecutorType").build();
             threadPool = new ThreadPool(nodeSettings);
             ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
@@ -227,7 +227,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
 
         try {
             Settings nodeSettings = Settings.settingsBuilder()
-                    .put("node.name", "testFixedExecutorType").build();
+                    .put("name", "testFixedExecutorType").build();
             threadPool = new ThreadPool(nodeSettings);
             ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
@@ -287,7 +287,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         try {
             Settings nodeSettings = settingsBuilder()
                     .put("threadpool." + threadPoolName + ".size", 10)
-                    .put("node.name", "testScalingExecutorType").build();
+                    .put("name", "testScalingExecutorType").build();
             threadPool = new ThreadPool(nodeSettings);
             ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
@@ -325,7 +325,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         try {
             Settings nodeSettings = Settings.settingsBuilder()
                     .put("threadpool." + threadPoolName + ".queue_size", 1000)
-                    .put("node.name", "testShutdownNowInterrupts").build();
+                    .put("name", "testShutdownNowInterrupts").build();
             threadPool = new ThreadPool(nodeSettings);
             ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
@@ -362,7 +362,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
                     .put("threadpool.my_pool2.type", "fixed")
                     .put("threadpool.my_pool2.size", "1")
                     .put("threadpool.my_pool2.queue_size", "1")
-                    .put("node.name", "testCustomThreadPool").build();
+                    .put("name", "testCustomThreadPool").build();
             threadPool = new ThreadPool(nodeSettings);
             ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
             threadPool.setClusterSettings(clusterSettings);
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
index 4688dae..19254c1 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -556,7 +556,7 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
         });
         final CountDownLatch latch = new CountDownLatch(1);
         TransportFuture<StringMessageResponse> res = serviceB.submitRequest(nodeA, "sayHelloTimeoutDelayedResponse",
-                new StringMessageRequest("300ms"), TransportRequestOptions.builder().withTimeout(100).build(), new BaseTransportResponseHandler<StringMessageResponse>() {
+                new StringMessageRequest("2m"), TransportRequestOptions.builder().withTimeout(100).build(), new BaseTransportResponseHandler<StringMessageResponse>() {
                     @Override
                     public StringMessageResponse newInstance() {
                         return new StringMessageResponse();
diff --git a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
index 9de8de6..def9a11 100644
--- a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
@@ -51,7 +51,7 @@ import static org.hamcrest.Matchers.is;
 public class NettySizeHeaderFrameDecoderTests extends ESTestCase {
 
     private final Settings settings = settingsBuilder()
-            .put("node.name", "NettySizeHeaderFrameDecoderTests")
+            .put("name", "foo")
             .put(TransportSettings.BIND_HOST.getKey(), "127.0.0.1")
             .put(TransportSettings.PORT.getKey(), "0")
             .build();
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index 55a79ff..ae4555b 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -143,6 +143,7 @@ public class TribeIT extends ESIntegTestCase {
                 .put("tribe.t1.cluster.name", internalCluster().getClusterName())
                 .put("tribe.t2.cluster.name", cluster2.getClusterName())
                 .put("tribe.blocks.write", false)
+                .put("tribe.blocks.read", false)
                 .put(settings)
                 .put(tribe1Defaults.build())
                 .put(tribe2Defaults.build())
diff --git a/core/src/test/resources/indices/bwc/index-2.1.2.zip b/core/src/test/resources/indices/bwc/index-2.1.2.zip
new file mode 100644
index 0000000..739c104
Binary files /dev/null and b/core/src/test/resources/indices/bwc/index-2.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/index-2.2.0.zip b/core/src/test/resources/indices/bwc/index-2.2.0.zip
new file mode 100644
index 0000000..f9011ff
Binary files /dev/null and b/core/src/test/resources/indices/bwc/index-2.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/repo-2.1.2.zip b/core/src/test/resources/indices/bwc/repo-2.1.2.zip
new file mode 100644
index 0000000..a89507f
Binary files /dev/null and b/core/src/test/resources/indices/bwc/repo-2.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/repo-2.2.0.zip b/core/src/test/resources/indices/bwc/repo-2.2.0.zip
new file mode 100644
index 0000000..90f8282
Binary files /dev/null and b/core/src/test/resources/indices/bwc/repo-2.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.4.zip
new file mode 100644
index 0000000..a47ff4f
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.5.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.5.zip
new file mode 100644
index 0000000..2262529
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.4.zip
new file mode 100644
index 0000000..86be302
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.5.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.5.zip
new file mode 100644
index 0000000..46bada2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.5.zip differ
diff --git a/docs/reference/aggregations/pipeline.asciidoc b/docs/reference/aggregations/pipeline.asciidoc
index e4cdae5..540f4c2 100644
--- a/docs/reference/aggregations/pipeline.asciidoc
+++ b/docs/reference/aggregations/pipeline.asciidoc
@@ -22,7 +22,7 @@ parameter to indicate the paths to the required metrics. The syntax for defining
 
 Pipeline aggregations cannot have sub-aggregations but depending on the type it can reference another pipeline in the `buckets_path`
 allowing pipeline aggregations to be chained.  For example, you can chain together two derivatives to calculate the second derivative
-(e.g. a derivative of a derivative).
+(i.e. a derivative of a derivative).
 
 NOTE: Because pipeline aggregations only add to the output, when chaining pipeline aggregations the output of each pipeline aggregation 
 will be included in the final output.
diff --git a/docs/reference/ingest/ingest.asciidoc b/docs/reference/ingest/ingest.asciidoc
index dfe377e..e9226e7 100644
--- a/docs/reference/ingest/ingest.asciidoc
+++ b/docs/reference/ingest/ingest.asciidoc
@@ -534,27 +534,6 @@ to the requester.
 }
 --------------------------------------------------
 
-==== DeDot Processor
-The DeDot Processor is used to remove dots (".") from field names and
-replace them with a specific `separator` string.
-
-[[dedot-options]]
-.DeDot Options
-[options="header"]
-|======
-| Name         | Required  | Default  | Description
-| `separator`  | yes       | "_"      | The string to replace dots with in all field names
-|======
-
-[source,js]
---------------------------------------------------
-{
-  "dedot": {
-    "separator": "_"
-  }
-}
---------------------------------------------------
-
 
 === Accessing data in pipelines
 
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index 60f80b0..5e840ac 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -126,6 +126,10 @@ to prevent clashes with the watcher plugin
 * `watcher.interval.medium` is now `resource.reload.interval.medium`
 * `watcher.interval.high` is now `resource.reload.interval.high`
 
+==== index.gateway setting renamed
+
+* `index.gateway.local.sync` is now `index.translog.sync_interval`
+
 ==== Hunspell dictionary configuration
 
 The parameter `indices.analysis.hunspell.dictionary.location` has been
diff --git a/docs/reference/migration/migrate_2_2.asciidoc b/docs/reference/migration/migrate_2_2.asciidoc
index efb063f..39c059e 100644
--- a/docs/reference/migration/migrate_2_2.asciidoc
+++ b/docs/reference/migration/migrate_2_2.asciidoc
@@ -48,3 +48,8 @@ Proxy settings have been deprecated and renamed:
 If you are using proxy settings, update your settings as deprecated ones will
 be removed in next major version.
 
+[float]
+=== Multicast plugin deprecated
+
+The `discovery-multicast` plugin has been deprecated in 2.2.0 and has
+been removed in 3.0.0.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index ec06320..0ea68ec 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -166,7 +166,7 @@ with `_parent` field mapping created before version `2.0.0`. The data of these i
 
 The format of the join between parent and child documents have changed with the `2.0.0` release. The old
 format can't read from version `3.0.0` and onwards. The new format allows for a much more efficient and
-scalable join between parent and child documents and the join data structures are stored on on disk
+scalable join between parent and child documents and the join data structures are stored on disk
 data structures as opposed as before the join data structures were stored in the jvm heap space.
 
 ==== `score_type` has been removed
@@ -195,22 +195,6 @@ the parent id under the `_parent` key.
 [[breaking_30_settings_changes]]
 === Settings changes
 
-From Elasticsearch 3.0 on all settings are validated before they are applied. Node level and default index
-level settings are validated on node startup, dynamic cluster and index setting are validated before they are updated/added
-to the cluster state. Every setting must be a _known_ setting or in other words all settings must be registered with the
-node or transport client they are used with. This implies that plugins that define custom settings must register all of their
-settings during pluging loading on the `SettingsModule#registerSettings(Setting)` method.
-
-==== Node settings
-
-The `name` settings has been remove and is replaced with `node.name`. Usage of `-Dname=some_node_name` is not supported
-anymore. Scripts must be updated accordingly.
-
-==== Transport Settings
-
-All settings with a `netty` infix have been replaced by their already existing `transport` synonyms. For instance `transport.netty.bind_host` is
-now not supported anymore and should be replaced by the superseeding setting `transport.bind_host`.
-
 ==== Analysis settings
 
 The `index.analysis.analyzer.default_index` analyzer is not supported anymore.
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
index 54800ac..9ccccad 100644
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
+++ b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
@@ -59,7 +59,7 @@ public class IngestGrokPlugin extends Plugin {
         nodeModule.registerProcessor(GrokProcessor.TYPE, (templateService) -> new GrokProcessor.Factory(builtinPatterns));
     }
 
-    static Map<String, String> loadBuiltinPatterns() throws IOException {
+    public static Map<String, String> loadBuiltinPatterns() throws IOException {
         Map<String, String> builtinPatterns = new HashMap<>();
         for (String pattern : PATTERN_NAMES) {
             try(InputStream is = IngestGrokPlugin.class.getResourceAsStream("/patterns/" + pattern)) {
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
index 1c36e26..db98090 100644
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
+++ b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
@@ -19,9 +19,8 @@
 
 package org.elasticsearch.ingest.grok;
 
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.Collections;
@@ -54,7 +53,7 @@ public class GrokProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("should fail");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[field] required property is missing"));
 
         }
@@ -67,7 +66,7 @@ public class GrokProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("should fail");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[pattern] required property is missing"));
         }
 
diff --git a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
index 06e04f4..4246d34 100644
--- a/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
+++ b/modules/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
@@ -41,6 +41,7 @@ public class IndexedExpressionTests extends ESIntegTestCase {
         Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
         builder.put("script.engine.expression.indexed.update", "false");
         builder.put("script.engine.expression.indexed.search", "false");
+        builder.put("script.engine.expression.indexed.mapping", "false");
         return builder.build();
     }
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
index c4445a9..65b83a9 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
@@ -64,6 +64,9 @@ public class IndexedScriptTests extends ESIntegTestCase {
         builder.put("script.engine.groovy.indexed.search", "true");
         builder.put("script.engine.groovy.indexed.aggs", "true");
         builder.put("script.engine.groovy.inline.aggs", "false");
+        builder.put("script.engine.expression.indexed.update", "false");
+        builder.put("script.engine.expression.indexed.search", "false");
+        builder.put("script.engine.expression.indexed.mapping", "false");
         return builder.build();
     }
 
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
index a9c75b3..6611872 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
@@ -91,7 +91,7 @@ public class TemplateQueryParserTests extends ESTestCase {
         Settings settings = Settings.settingsBuilder()
                 .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(Environment.PATH_CONF_SETTING.getKey(), this.getDataPath("config"))
-                .put("node.name", getClass().getName())
+                .put("name", getClass().getName())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         final Client proxy = (Client) Proxy.newProxyInstance(
@@ -102,8 +102,7 @@ public class TemplateQueryParserTests extends ESTestCase {
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings("test", settings);
         Index index = idxSettings.getIndex();
         SettingsModule settingsModule = new SettingsModule(settings, new SettingsFilter(settings));
-        ScriptModule scriptModule = new ScriptModule();
-        scriptModule.prepareSettings(settingsModule);
+        ScriptModule scriptModule = new ScriptModule(settingsModule);
         // TODO: make this use a mock engine instead of mustache and it will no longer be messy!
         scriptModule.addScriptEngine(new ScriptEngineRegistry.ScriptEngineRegistration(MustacheScriptEngineService.class, MustacheScriptEngineService.TYPES));
         settingsModule.registerSetting(InternalSettingsPlugin.VERSION_CREATED);
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
index 9bee3b8..418bd12 100644
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
@@ -71,11 +71,6 @@ public class AzureDiscoveryPlugin extends Plugin {
 
     public void onModule(SettingsModule settingsModule) {
         settingsModule.registerSetting(AzureComputeService.Discovery.REFRESH_SETTING);
-        settingsModule.registerSetting(AzureComputeService.Management.KEYSTORE_PASSWORD_SETTING);
-        settingsModule.registerSetting(AzureComputeService.Management.KEYSTORE_PATH_SETTING);
-        settingsModule.registerSetting(AzureComputeService.Management.KEYSTORE_TYPE_SETTING);
-        settingsModule.registerSetting(AzureComputeService.Management.SUBSCRIPTION_ID_SETTING);
-        settingsModule.registerSetting(AzureComputeService.Management.SERVICE_NAME_SETTING);
         settingsModule.registerSetting(AzureComputeService.Discovery.HOST_TYPE_SETTING);
     }
 }
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
index dbcdbbc..0f51d82 100644
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
+++ b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
@@ -29,14 +29,13 @@ import com.maxmind.geoip2.record.Country;
 import com.maxmind.geoip2.record.Location;
 import com.maxmind.geoip2.record.Subdivision;
 import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.common.network.InetAddresses;
 import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.ingest.core.AbstractProcessor;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
 import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -52,6 +51,7 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
+import static org.elasticsearch.ingest.core.ConfigurationUtils.newConfigurationException;
 import static org.elasticsearch.ingest.core.ConfigurationUtils.readOptionalList;
 import static org.elasticsearch.ingest.core.ConfigurationUtils.readStringProperty;
 
@@ -94,7 +94,7 @@ public final class GeoIpProcessor extends AbstractProcessor {
                 }
                 break;
             default:
-                throw new IllegalStateException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]");
+                throw new ElasticsearchParseException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]", new IllegalStateException());
         }
         ingestDocument.setFieldValue(targetField, geoData);
     }
@@ -240,7 +240,7 @@ public final class GeoIpProcessor extends AbstractProcessor {
                     try {
                         fields.add(Field.parse(fieldName));
                     } catch (Exception e) {
-                        throw new ConfigurationPropertyException(TYPE, processorTag, "fields", "illegal field option [" + fieldName + "]. valid values are [" + Arrays.toString(Field.values()) +"]");
+                        throw newConfigurationException(TYPE, processorTag, "fields", "illegal field option [" + fieldName + "]. valid values are [" + Arrays.toString(Field.values()) + "]");
                     }
                 }
             } else {
@@ -249,7 +249,7 @@ public final class GeoIpProcessor extends AbstractProcessor {
 
             DatabaseReader databaseReader = databaseReaders.get(databaseFile);
             if (databaseReader == null) {
-                throw new ConfigurationPropertyException(TYPE, processorTag, "database_file", "database file [" + databaseFile + "] doesn't exist");
+                throw newConfigurationException(TYPE, processorTag, "database_file", "database file [" + databaseFile + "] doesn't exist");
             }
             return new GeoIpProcessor(processorTag, ipField, databaseReader, targetField, fields);
         }
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
index f92cb7b..570b1e2 100644
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
+++ b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
@@ -53,7 +53,7 @@ public class IngestGeoIpPlugin extends Plugin {
         nodeModule.registerProcessor(GeoIpProcessor.TYPE, (templateService) -> new GeoIpProcessor.Factory(databaseReaders));
     }
 
-    static Map<String, DatabaseReader> loadDatabaseReaders(Path geoIpConfigDirectory) throws IOException {
+    public static Map<String, DatabaseReader> loadDatabaseReaders(Path geoIpConfigDirectory) throws IOException {
         if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) {
             throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory  + "] containing databases doesn't exist");
         }
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
index 410f6e3..13143a0 100644
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
+++ b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
@@ -20,9 +20,8 @@
 package org.elasticsearch.ingest.geoip;
 
 import com.maxmind.geoip2.DatabaseReader;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.ConfigurationPropertyException;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.StreamsUtils;
 import org.junit.AfterClass;
@@ -113,7 +112,7 @@ public class GeoIpProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("Exception expected");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[database_file] database file [does-not-exist.mmdb] doesn't exist"));
         }
     }
@@ -146,7 +145,7 @@ public class GeoIpProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("exception expected");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[fields] illegal field option [invalid]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]"));
         }
 
@@ -156,7 +155,7 @@ public class GeoIpProcessorFactoryTests extends ESTestCase {
         try {
             factory.create(config);
             fail("exception expected");
-        } catch (ConfigurationPropertyException e) {
+        } catch (ElasticsearchParseException e) {
             assertThat(e.getMessage(), equalTo("[fields] property isn't a list, but of type [java.lang.String]"));
         }
     }
diff --git a/plugins/lang-painless/src/main/antlr/PainlessLexer.g4 b/plugins/lang-painless/src/main/antlr/PainlessLexer.g4
index 11cd97c..866bbd7 100644
--- a/plugins/lang-painless/src/main/antlr/PainlessLexer.g4
+++ b/plugins/lang-painless/src/main/antlr/PainlessLexer.g4
@@ -96,7 +96,6 @@ AOR:    '|=';
 ALSH:   '<<=';
 ARSH:   '>>=';
 AUSH:   '>>>=';
-ACAT:   '..=';
 
 OCTAL: '0' [0-7]+ [lL]?;
 HEX: '0' [xX] [0-9a-fA-F]+ [lL]?;
diff --git a/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessLexer.java b/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessLexer.java
index 3a01626..a7cf506 100644
--- a/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessLexer.java
+++ b/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessLexer.java
@@ -30,9 +30,8 @@ class PainlessLexer extends Lexer {
     LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43,
     BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51,
     AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59,
-    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67,
-    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75,
-    EXTID=76;
+    ALSH=60, ARSH=61, AUSH=62, OCTAL=63, HEX=64, INTEGER=65, DECIMAL=66, STRING=67,
+    CHAR=68, TRUE=69, FALSE=70, NULL=71, TYPE=72, ID=73, EXTINTEGER=74, EXTID=75;
   public static final int EXT = 1;
   public static String[] modeNames = {
     "DEFAULT_MODE", "EXT"
@@ -45,9 +44,9 @@ class PainlessLexer extends Lexer {
     "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT",
     "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND",
     "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL",
-    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT",
-    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE",
-    "NULL", "TYPE", "GENERIC", "ID", "EXTINTEGER", "EXTID"
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "OCTAL",
+    "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", "NULL",
+    "TYPE", "GENERIC", "ID", "EXTINTEGER", "EXTID"
   };
 
   private static final String[] _LITERAL_NAMES = {
@@ -57,8 +56,8 @@ class PainlessLexer extends Lexer {
     "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'",
     "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'",
     "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='",
-    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null,
-    null, null, null, null, null, "'true'", "'false'", "'null'"
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", null, null,
+    null, null, null, null, "'true'", "'false'", "'null'"
   };
   private static final String[] _SYMBOLIC_NAMES = {
     null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP",
@@ -67,9 +66,9 @@ class PainlessLexer extends Lexer {
     "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT",
     "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND",
     "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL",
-    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT",
-    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE",
-    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "OCTAL",
+    "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", "NULL",
+    "TYPE", "ID", "EXTINTEGER", "EXTID"
   };
   public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
 
@@ -135,13 +134,13 @@ class PainlessLexer extends Lexer {
   @Override
   public void action(RuleContext _localctx, int ruleIndex, int actionIndex) {
     switch (ruleIndex) {
-    case 67:
+    case 66:
       STRING_action((RuleContext)_localctx, actionIndex);
       break;
-    case 68:
+    case 67:
       CHAR_action((RuleContext)_localctx, actionIndex);
       break;
-    case 72:
+    case 71:
       TYPE_action((RuleContext)_localctx, actionIndex);
       break;
     }
@@ -170,7 +169,7 @@ class PainlessLexer extends Lexer {
   @Override
   public boolean sempred(RuleContext _localctx, int ruleIndex, int predIndex) {
     switch (ruleIndex) {
-    case 72:
+    case 71:
       return TYPE_sempred((RuleContext)_localctx, predIndex);
     }
     return true;
@@ -184,7 +183,7 @@ class PainlessLexer extends Lexer {
   }
 
   public static final String _serializedATN =
-    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\2N\u0236\b\1\b\1\4"+
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\2M\u0230\b\1\b\1\4"+
     "\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n"+
     "\4\13\t\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22"+
     "\t\22\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31"+
@@ -193,195 +192,193 @@ class PainlessLexer extends Lexer {
     "+\4,\t,\4-\t-\4.\t.\4/\t/\4\60\t\60\4\61\t\61\4\62\t\62\4\63\t\63\4\64"+
     "\t\64\4\65\t\65\4\66\t\66\4\67\t\67\48\t8\49\t9\4:\t:\4;\t;\4<\t<\4=\t"+
     "=\4>\t>\4?\t?\4@\t@\4A\tA\4B\tB\4C\tC\4D\tD\4E\tE\4F\tF\4G\tG\4H\tH\4"+
-    "I\tI\4J\tJ\4K\tK\4L\tL\4M\tM\4N\tN\3\2\6\2\u00a0\n\2\r\2\16\2\u00a1\3"+
-    "\2\3\2\3\3\3\3\3\3\3\3\7\3\u00aa\n\3\f\3\16\3\u00ad\13\3\3\3\3\3\3\3\3"+
-    "\3\3\3\7\3\u00b4\n\3\f\3\16\3\u00b7\13\3\3\3\3\3\5\3\u00bb\n\3\3\3\3\3"+
-    "\3\4\3\4\3\5\3\5\3\6\3\6\3\7\3\7\3\b\3\b\3\t\3\t\3\n\3\n\3\n\3\n\3\13"+
-    "\3\13\3\f\3\f\3\r\3\r\3\r\3\16\3\16\3\16\3\16\3\16\3\17\3\17\3\17\3\17"+
-    "\3\17\3\17\3\20\3\20\3\20\3\21\3\21\3\21\3\21\3\22\3\22\3\22\3\22\3\22"+
-    "\3\22\3\22\3\22\3\22\3\23\3\23\3\23\3\23\3\23\3\23\3\24\3\24\3\24\3\24"+
-    "\3\24\3\24\3\24\3\25\3\25\3\25\3\25\3\26\3\26\3\26\3\26\3\27\3\27\3\27"+
-    "\3\27\3\27\3\27\3\30\3\30\3\30\3\30\3\30\3\30\3\31\3\31\3\32\3\32\3\33"+
-    "\3\33\3\34\3\34\3\35\3\35\3\36\3\36\3\37\3\37\3 \3 \3 \3!\3!\3!\3\"\3"+
-    "\"\3\"\3\"\3#\3#\3$\3$\3$\3%\3%\3&\3&\3&\3\'\3\'\3\'\3(\3(\3(\3(\3)\3"+
-    ")\3)\3*\3*\3*\3*\3+\3+\3,\3,\3-\3-\3.\3.\3.\3/\3/\3/\3\60\3\60\3\61\3"+
-    "\61\3\62\3\62\3\62\3\63\3\63\3\63\3\64\3\64\3\65\3\65\3\65\3\66\3\66\3"+
-    "\66\3\67\3\67\3\67\38\38\38\39\39\39\3:\3:\3:\3;\3;\3;\3<\3<\3<\3=\3="+
-    "\3=\3=\3>\3>\3>\3>\3?\3?\3?\3?\3?\3@\3@\3@\3@\3A\3A\6A\u0185\nA\rA\16"+
-    "A\u0186\3A\5A\u018a\nA\3B\3B\3B\6B\u018f\nB\rB\16B\u0190\3B\5B\u0194\n"+
-    "B\3C\3C\3C\7C\u0199\nC\fC\16C\u019c\13C\5C\u019e\nC\3C\5C\u01a1\nC\3D"+
-    "\3D\3D\7D\u01a6\nD\fD\16D\u01a9\13D\5D\u01ab\nD\3D\3D\7D\u01af\nD\fD\16"+
-    "D\u01b2\13D\3D\3D\5D\u01b6\nD\3D\6D\u01b9\nD\rD\16D\u01ba\5D\u01bd\nD"+
-    "\3D\5D\u01c0\nD\3E\3E\3E\3E\3E\3E\7E\u01c8\nE\fE\16E\u01cb\13E\3E\3E\3"+
-    "E\3F\3F\3F\3F\3F\3G\3G\3G\3G\3G\3H\3H\3H\3H\3H\3H\3I\3I\3I\3I\3I\3J\3"+
-    "J\5J\u01e7\nJ\3J\3J\3J\3K\7K\u01ed\nK\fK\16K\u01f0\13K\3K\3K\7K\u01f4"+
-    "\nK\fK\16K\u01f7\13K\3K\3K\5K\u01fb\nK\3K\7K\u01fe\nK\fK\16K\u0201\13"+
-    "K\3K\3K\7K\u0205\nK\fK\16K\u0208\13K\3K\3K\5K\u020c\nK\3K\7K\u020f\nK"+
-    "\fK\16K\u0212\13K\7K\u0214\nK\fK\16K\u0217\13K\3K\3K\3L\3L\7L\u021d\n"+
-    "L\fL\16L\u0220\13L\3M\3M\3M\7M\u0225\nM\fM\16M\u0228\13M\5M\u022a\nM\3"+
-    "M\3M\3N\3N\7N\u0230\nN\fN\16N\u0233\13N\3N\3N\5\u00ab\u00b5\u01c9\2O\4"+
-    "\3\6\4\b\5\n\6\f\7\16\b\20\t\22\n\24\13\26\f\30\r\32\16\34\17\36\20 \21"+
-    "\"\22$\23&\24(\25*\26,\27.\30\60\31\62\32\64\33\66\348\35:\36<\37> @!"+
-    "B\"D#F$H%J&L\'N(P)R*T+V,X-Z.\\/^\60`\61b\62d\63f\64h\65j\66l\67n8p9r:"+
-    "t;v<x=z>|?~@\u0080A\u0082B\u0084C\u0086D\u0088E\u008aF\u008cG\u008eH\u0090"+
-    "I\u0092J\u0094K\u0096\2\u0098L\u009aM\u009cN\4\2\3\21\5\2\13\f\17\17\""+
-    "\"\4\2\f\f\17\17\3\2\629\4\2NNnn\4\2ZZzz\5\2\62;CHch\3\2\63;\3\2\62;\b"+
-    "\2FFHHNNffhhnn\4\2GGgg\4\2--//\4\2HHhh\4\2$$^^\5\2C\\aac|\6\2\62;C\\a"+
-    "ac|\u0255\2\4\3\2\2\2\2\6\3\2\2\2\2\b\3\2\2\2\2\n\3\2\2\2\2\f\3\2\2\2"+
-    "\2\16\3\2\2\2\2\20\3\2\2\2\2\22\3\2\2\2\2\24\3\2\2\2\2\26\3\2\2\2\2\30"+
-    "\3\2\2\2\2\32\3\2\2\2\2\34\3\2\2\2\2\36\3\2\2\2\2 \3\2\2\2\2\"\3\2\2\2"+
-    "\2$\3\2\2\2\2&\3\2\2\2\2(\3\2\2\2\2*\3\2\2\2\2,\3\2\2\2\2.\3\2\2\2\2\60"+
-    "\3\2\2\2\2\62\3\2\2\2\2\64\3\2\2\2\2\66\3\2\2\2\28\3\2\2\2\2:\3\2\2\2"+
-    "\2<\3\2\2\2\2>\3\2\2\2\2@\3\2\2\2\2B\3\2\2\2\2D\3\2\2\2\2F\3\2\2\2\2H"+
-    "\3\2\2\2\2J\3\2\2\2\2L\3\2\2\2\2N\3\2\2\2\2P\3\2\2\2\2R\3\2\2\2\2T\3\2"+
-    "\2\2\2V\3\2\2\2\2X\3\2\2\2\2Z\3\2\2\2\2\\\3\2\2\2\2^\3\2\2\2\2`\3\2\2"+
-    "\2\2b\3\2\2\2\2d\3\2\2\2\2f\3\2\2\2\2h\3\2\2\2\2j\3\2\2\2\2l\3\2\2\2\2"+
-    "n\3\2\2\2\2p\3\2\2\2\2r\3\2\2\2\2t\3\2\2\2\2v\3\2\2\2\2x\3\2\2\2\2z\3"+
-    "\2\2\2\2|\3\2\2\2\2~\3\2\2\2\2\u0080\3\2\2\2\2\u0082\3\2\2\2\2\u0084\3"+
-    "\2\2\2\2\u0086\3\2\2\2\2\u0088\3\2\2\2\2\u008a\3\2\2\2\2\u008c\3\2\2\2"+
-    "\2\u008e\3\2\2\2\2\u0090\3\2\2\2\2\u0092\3\2\2\2\2\u0094\3\2\2\2\2\u0098"+
-    "\3\2\2\2\3\u009a\3\2\2\2\3\u009c\3\2\2\2\4\u009f\3\2\2\2\6\u00ba\3\2\2"+
-    "\2\b\u00be\3\2\2\2\n\u00c0\3\2\2\2\f\u00c2\3\2\2\2\16\u00c4\3\2\2\2\20"+
-    "\u00c6\3\2\2\2\22\u00c8\3\2\2\2\24\u00ca\3\2\2\2\26\u00ce\3\2\2\2\30\u00d0"+
-    "\3\2\2\2\32\u00d2\3\2\2\2\34\u00d5\3\2\2\2\36\u00da\3\2\2\2 \u00e0\3\2"+
-    "\2\2\"\u00e3\3\2\2\2$\u00e7\3\2\2\2&\u00f0\3\2\2\2(\u00f6\3\2\2\2*\u00fd"+
-    "\3\2\2\2,\u0101\3\2\2\2.\u0105\3\2\2\2\60\u010b\3\2\2\2\62\u0111\3\2\2"+
-    "\2\64\u0113\3\2\2\2\66\u0115\3\2\2\28\u0117\3\2\2\2:\u0119\3\2\2\2<\u011b"+
-    "\3\2\2\2>\u011d\3\2\2\2@\u011f\3\2\2\2B\u0122\3\2\2\2D\u0125\3\2\2\2F"+
-    "\u0129\3\2\2\2H\u012b\3\2\2\2J\u012e\3\2\2\2L\u0130\3\2\2\2N\u0133\3\2"+
-    "\2\2P\u0136\3\2\2\2R\u013a\3\2\2\2T\u013d\3\2\2\2V\u0141\3\2\2\2X\u0143"+
-    "\3\2\2\2Z\u0145\3\2\2\2\\\u0147\3\2\2\2^\u014a\3\2\2\2`\u014d\3\2\2\2"+
-    "b\u014f\3\2\2\2d\u0151\3\2\2\2f\u0154\3\2\2\2h\u0157\3\2\2\2j\u0159\3"+
-    "\2\2\2l\u015c\3\2\2\2n\u015f\3\2\2\2p\u0162\3\2\2\2r\u0165\3\2\2\2t\u0168"+
-    "\3\2\2\2v\u016b\3\2\2\2x\u016e\3\2\2\2z\u0171\3\2\2\2|\u0175\3\2\2\2~"+
-    "\u0179\3\2\2\2\u0080\u017e\3\2\2\2\u0082\u0182\3\2\2\2\u0084\u018b\3\2"+
-    "\2\2\u0086\u019d\3\2\2\2\u0088\u01aa\3\2\2\2\u008a\u01c1\3\2\2\2\u008c"+
-    "\u01cf\3\2\2\2\u008e\u01d4\3\2\2\2\u0090\u01d9\3\2\2\2\u0092\u01df\3\2"+
-    "\2\2\u0094\u01e4\3\2\2\2\u0096\u01ee\3\2\2\2\u0098\u021a\3\2\2\2\u009a"+
-    "\u0229\3\2\2\2\u009c\u022d\3\2\2\2\u009e\u00a0\t\2\2\2\u009f\u009e\3\2"+
-    "\2\2\u00a0\u00a1\3\2\2\2\u00a1\u009f\3\2\2\2\u00a1\u00a2\3\2\2\2\u00a2"+
-    "\u00a3\3\2\2\2\u00a3\u00a4\b\2\2\2\u00a4\5\3\2\2\2\u00a5\u00a6\7\61\2"+
-    "\2\u00a6\u00a7\7\61\2\2\u00a7\u00ab\3\2\2\2\u00a8\u00aa\13\2\2\2\u00a9"+
-    "\u00a8\3\2\2\2\u00aa\u00ad\3\2\2\2\u00ab\u00ac\3\2\2\2\u00ab\u00a9\3\2"+
-    "\2\2\u00ac\u00ae\3\2\2\2\u00ad\u00ab\3\2\2\2\u00ae\u00bb\t\3\2\2\u00af"+
-    "\u00b0\7\61\2\2\u00b0\u00b1\7,\2\2\u00b1\u00b5\3\2\2\2\u00b2\u00b4\13"+
-    "\2\2\2\u00b3\u00b2\3\2\2\2\u00b4\u00b7\3\2\2\2\u00b5\u00b6\3\2\2\2\u00b5"+
-    "\u00b3\3\2\2\2\u00b6\u00b8\3\2\2\2\u00b7\u00b5\3\2\2\2\u00b8\u00b9\7,"+
-    "\2\2\u00b9\u00bb\7\61\2\2\u00ba\u00a5\3\2\2\2\u00ba\u00af\3\2\2\2\u00bb"+
-    "\u00bc\3\2\2\2\u00bc\u00bd\b\3\2\2\u00bd\7\3\2\2\2\u00be\u00bf\7}\2\2"+
-    "\u00bf\t\3\2\2\2\u00c0\u00c1\7\177\2\2\u00c1\13\3\2\2\2\u00c2\u00c3\7"+
-    "]\2\2\u00c3\r\3\2\2\2\u00c4\u00c5\7_\2\2\u00c5\17\3\2\2\2\u00c6\u00c7"+
-    "\7*\2\2\u00c7\21\3\2\2\2\u00c8\u00c9\7+\2\2\u00c9\23\3\2\2\2\u00ca\u00cb"+
-    "\7\60\2\2\u00cb\u00cc\3\2\2\2\u00cc\u00cd\b\n\3\2\u00cd\25\3\2\2\2\u00ce"+
-    "\u00cf\7.\2\2\u00cf\27\3\2\2\2\u00d0\u00d1\7=\2\2\u00d1\31\3\2\2\2\u00d2"+
-    "\u00d3\7k\2\2\u00d3\u00d4\7h\2\2\u00d4\33\3\2\2\2\u00d5\u00d6\7g\2\2\u00d6"+
-    "\u00d7\7n\2\2\u00d7\u00d8\7u\2\2\u00d8\u00d9\7g\2\2\u00d9\35\3\2\2\2\u00da"+
-    "\u00db\7y\2\2\u00db\u00dc\7j\2\2\u00dc\u00dd\7k\2\2\u00dd\u00de\7n\2\2"+
-    "\u00de\u00df\7g\2\2\u00df\37\3\2\2\2\u00e0\u00e1\7f\2\2\u00e1\u00e2\7"+
-    "q\2\2\u00e2!\3\2\2\2\u00e3\u00e4\7h\2\2\u00e4\u00e5\7q\2\2\u00e5\u00e6"+
-    "\7t\2\2\u00e6#\3\2\2\2\u00e7\u00e8\7e\2\2\u00e8\u00e9\7q\2\2\u00e9\u00ea"+
-    "\7p\2\2\u00ea\u00eb\7v\2\2\u00eb\u00ec\7k\2\2\u00ec\u00ed\7p\2\2\u00ed"+
-    "\u00ee\7w\2\2\u00ee\u00ef\7g\2\2\u00ef%\3\2\2\2\u00f0\u00f1\7d\2\2\u00f1"+
-    "\u00f2\7t\2\2\u00f2\u00f3\7g\2\2\u00f3\u00f4\7c\2\2\u00f4\u00f5\7m\2\2"+
-    "\u00f5\'\3\2\2\2\u00f6\u00f7\7t\2\2\u00f7\u00f8\7g\2\2\u00f8\u00f9\7v"+
-    "\2\2\u00f9\u00fa\7w\2\2\u00fa\u00fb\7t\2\2\u00fb\u00fc\7p\2\2\u00fc)\3"+
-    "\2\2\2\u00fd\u00fe\7p\2\2\u00fe\u00ff\7g\2\2\u00ff\u0100\7y\2\2\u0100"+
-    "+\3\2\2\2\u0101\u0102\7v\2\2\u0102\u0103\7t\2\2\u0103\u0104\7{\2\2\u0104"+
-    "-\3\2\2\2\u0105\u0106\7e\2\2\u0106\u0107\7c\2\2\u0107\u0108\7v\2\2\u0108"+
-    "\u0109\7e\2\2\u0109\u010a\7j\2\2\u010a/\3\2\2\2\u010b\u010c\7v\2\2\u010c"+
-    "\u010d\7j\2\2\u010d\u010e\7t\2\2\u010e\u010f\7q\2\2\u010f\u0110\7y\2\2"+
-    "\u0110\61\3\2\2\2\u0111\u0112\7#\2\2\u0112\63\3\2\2\2\u0113\u0114\7\u0080"+
-    "\2\2\u0114\65\3\2\2\2\u0115\u0116\7,\2\2\u0116\67\3\2\2\2\u0117\u0118"+
-    "\7\61\2\2\u01189\3\2\2\2\u0119\u011a\7\'\2\2\u011a;\3\2\2\2\u011b\u011c"+
-    "\7-\2\2\u011c=\3\2\2\2\u011d\u011e\7/\2\2\u011e?\3\2\2\2\u011f\u0120\7"+
-    ">\2\2\u0120\u0121\7>\2\2\u0121A\3\2\2\2\u0122\u0123\7@\2\2\u0123\u0124"+
-    "\7@\2\2\u0124C\3\2\2\2\u0125\u0126\7@\2\2\u0126\u0127\7@\2\2\u0127\u0128"+
-    "\7@\2\2\u0128E\3\2\2\2\u0129\u012a\7>\2\2\u012aG\3\2\2\2\u012b\u012c\7"+
-    ">\2\2\u012c\u012d\7?\2\2\u012dI\3\2\2\2\u012e\u012f\7@\2\2\u012fK\3\2"+
-    "\2\2\u0130\u0131\7@\2\2\u0131\u0132\7?\2\2\u0132M\3\2\2\2\u0133\u0134"+
-    "\7?\2\2\u0134\u0135\7?\2\2\u0135O\3\2\2\2\u0136\u0137\7?\2\2\u0137\u0138"+
-    "\7?\2\2\u0138\u0139\7?\2\2\u0139Q\3\2\2\2\u013a\u013b\7#\2\2\u013b\u013c"+
-    "\7?\2\2\u013cS\3\2\2\2\u013d\u013e\7#\2\2\u013e\u013f\7?\2\2\u013f\u0140"+
-    "\7?\2\2\u0140U\3\2\2\2\u0141\u0142\7(\2\2\u0142W\3\2\2\2\u0143\u0144\7"+
-    "`\2\2\u0144Y\3\2\2\2\u0145\u0146\7~\2\2\u0146[\3\2\2\2\u0147\u0148\7("+
-    "\2\2\u0148\u0149\7(\2\2\u0149]\3\2\2\2\u014a\u014b\7~\2\2\u014b\u014c"+
-    "\7~\2\2\u014c_\3\2\2\2\u014d\u014e\7A\2\2\u014ea\3\2\2\2\u014f\u0150\7"+
-    "<\2\2\u0150c\3\2\2\2\u0151\u0152\7-\2\2\u0152\u0153\7-\2\2\u0153e\3\2"+
-    "\2\2\u0154\u0155\7/\2\2\u0155\u0156\7/\2\2\u0156g\3\2\2\2\u0157\u0158"+
-    "\7?\2\2\u0158i\3\2\2\2\u0159\u015a\7-\2\2\u015a\u015b\7?\2\2\u015bk\3"+
-    "\2\2\2\u015c\u015d\7/\2\2\u015d\u015e\7?\2\2\u015em\3\2\2\2\u015f\u0160"+
-    "\7,\2\2\u0160\u0161\7?\2\2\u0161o\3\2\2\2\u0162\u0163\7\61\2\2\u0163\u0164"+
-    "\7?\2\2\u0164q\3\2\2\2\u0165\u0166\7\'\2\2\u0166\u0167\7?\2\2\u0167s\3"+
-    "\2\2\2\u0168\u0169\7(\2\2\u0169\u016a\7?\2\2\u016au\3\2\2\2\u016b\u016c"+
-    "\7`\2\2\u016c\u016d\7?\2\2\u016dw\3\2\2\2\u016e\u016f\7~\2\2\u016f\u0170"+
-    "\7?\2\2\u0170y\3\2\2\2\u0171\u0172\7>\2\2\u0172\u0173\7>\2\2\u0173\u0174"+
-    "\7?\2\2\u0174{\3\2\2\2\u0175\u0176\7@\2\2\u0176\u0177\7@\2\2\u0177\u0178"+
-    "\7?\2\2\u0178}\3\2\2\2\u0179\u017a\7@\2\2\u017a\u017b\7@\2\2\u017b\u017c"+
-    "\7@\2\2\u017c\u017d\7?\2\2\u017d\177\3\2\2\2\u017e\u017f\7\60\2\2\u017f"+
-    "\u0180\7\60\2\2\u0180\u0181\7?\2\2\u0181\u0081\3\2\2\2\u0182\u0184\7\62"+
-    "\2\2\u0183\u0185\t\4\2\2\u0184\u0183\3\2\2\2\u0185\u0186\3\2\2\2\u0186"+
-    "\u0184\3\2\2\2\u0186\u0187\3\2\2\2\u0187\u0189\3\2\2\2\u0188\u018a\t\5"+
-    "\2\2\u0189\u0188\3\2\2\2\u0189\u018a\3\2\2\2\u018a\u0083\3\2\2\2\u018b"+
-    "\u018c\7\62\2\2\u018c\u018e\t\6\2\2\u018d\u018f\t\7\2\2\u018e\u018d\3"+
-    "\2\2\2\u018f\u0190\3\2\2\2\u0190\u018e\3\2\2\2\u0190\u0191\3\2\2\2\u0191"+
-    "\u0193\3\2\2\2\u0192\u0194\t\5\2\2\u0193\u0192\3\2\2\2\u0193\u0194\3\2"+
-    "\2\2\u0194\u0085\3\2\2\2\u0195\u019e\7\62\2\2\u0196\u019a\t\b\2\2\u0197"+
-    "\u0199\t\t\2\2\u0198\u0197\3\2\2\2\u0199\u019c\3\2\2\2\u019a\u0198\3\2"+
-    "\2\2\u019a\u019b\3\2\2\2\u019b\u019e\3\2\2\2\u019c\u019a\3\2\2\2\u019d"+
-    "\u0195\3\2\2\2\u019d\u0196\3\2\2\2\u019e\u01a0\3\2\2\2\u019f\u01a1\t\n"+
-    "\2\2\u01a0\u019f\3\2\2\2\u01a0\u01a1\3\2\2\2\u01a1\u0087\3\2\2\2\u01a2"+
-    "\u01ab\7\62\2\2\u01a3\u01a7\t\b\2\2\u01a4\u01a6\t\t\2\2\u01a5\u01a4\3"+
-    "\2\2\2\u01a6\u01a9\3\2\2\2\u01a7\u01a5\3\2\2\2\u01a7\u01a8\3\2\2\2\u01a8"+
-    "\u01ab\3\2\2\2\u01a9\u01a7\3\2\2\2\u01aa\u01a2\3\2\2\2\u01aa\u01a3\3\2"+
-    "\2\2\u01ab\u01ac\3\2\2\2\u01ac\u01b0\5\24\n\2\u01ad\u01af\t\t\2\2\u01ae"+
-    "\u01ad\3\2\2\2\u01af\u01b2\3\2\2\2\u01b0\u01ae\3\2\2\2\u01b0\u01b1\3\2"+
-    "\2\2\u01b1\u01bc\3\2\2\2\u01b2\u01b0\3\2\2\2\u01b3\u01b5\t\13\2\2\u01b4"+
-    "\u01b6\t\f\2\2\u01b5\u01b4\3\2\2\2\u01b5\u01b6\3\2\2\2\u01b6\u01b8\3\2"+
-    "\2\2\u01b7\u01b9\t\t\2\2\u01b8\u01b7\3\2\2\2\u01b9\u01ba\3\2\2\2\u01ba"+
-    "\u01b8\3\2\2\2\u01ba\u01bb\3\2\2\2\u01bb\u01bd\3\2\2\2\u01bc\u01b3\3\2"+
-    "\2\2\u01bc\u01bd\3\2\2\2\u01bd\u01bf\3\2\2\2\u01be\u01c0\t\r\2\2\u01bf"+
-    "\u01be\3\2\2\2\u01bf\u01c0\3\2\2\2\u01c0\u0089\3\2\2\2\u01c1\u01c9\7$"+
-    "\2\2\u01c2\u01c3\7^\2\2\u01c3\u01c8\7$\2\2\u01c4\u01c5\7^\2\2\u01c5\u01c8"+
-    "\7^\2\2\u01c6\u01c8\n\16\2\2\u01c7\u01c2\3\2\2\2\u01c7\u01c4\3\2\2\2\u01c7"+
-    "\u01c6\3\2\2\2\u01c8\u01cb\3\2\2\2\u01c9\u01ca\3\2\2\2\u01c9\u01c7\3\2"+
-    "\2\2\u01ca\u01cc\3\2\2\2\u01cb\u01c9\3\2\2\2\u01cc\u01cd\7$\2\2\u01cd"+
-    "\u01ce\bE\4\2\u01ce\u008b\3\2\2\2\u01cf\u01d0\7)\2\2\u01d0\u01d1\13\2"+
-    "\2\2\u01d1\u01d2\7)\2\2\u01d2\u01d3\bF\5\2\u01d3\u008d\3\2\2\2\u01d4\u01d5"+
-    "\7v\2\2\u01d5\u01d6\7t\2\2\u01d6\u01d7\7w\2\2\u01d7\u01d8\7g\2\2\u01d8"+
-    "\u008f\3\2\2\2\u01d9\u01da\7h\2\2\u01da\u01db\7c\2\2\u01db\u01dc\7n\2"+
-    "\2\u01dc\u01dd\7u\2\2\u01dd\u01de\7g\2\2\u01de\u0091\3\2\2\2\u01df\u01e0"+
-    "\7p\2\2\u01e0\u01e1\7w\2\2\u01e1\u01e2\7n\2\2\u01e2\u01e3\7n\2\2\u01e3"+
-    "\u0093\3\2\2\2\u01e4\u01e6\5\u0098L\2\u01e5\u01e7\5\u0096K\2\u01e6\u01e5"+
-    "\3\2\2\2\u01e6\u01e7\3\2\2\2\u01e7\u01e8\3\2\2\2\u01e8\u01e9\6J\2\2\u01e9"+
-    "\u01ea\bJ\6\2\u01ea\u0095\3\2\2\2\u01eb\u01ed\7\"\2\2\u01ec\u01eb\3\2"+
-    "\2\2\u01ed\u01f0\3\2\2\2\u01ee\u01ec\3\2\2\2\u01ee\u01ef\3\2\2\2\u01ef"+
-    "\u01f1\3\2\2\2\u01f0\u01ee\3\2\2\2\u01f1\u01f5\7>\2\2\u01f2\u01f4\7\""+
-    "\2\2\u01f3\u01f2\3\2\2\2\u01f4\u01f7\3\2\2\2\u01f5\u01f3\3\2\2\2\u01f5"+
-    "\u01f6\3\2\2\2\u01f6\u01f8\3\2\2\2\u01f7\u01f5\3\2\2\2\u01f8\u01fa\5\u0098"+
-    "L\2\u01f9\u01fb\5\u0096K\2\u01fa\u01f9\3\2\2\2\u01fa\u01fb\3\2\2\2\u01fb"+
-    "\u01ff\3\2\2\2\u01fc\u01fe\7\"\2\2\u01fd\u01fc\3\2\2\2\u01fe\u0201\3\2"+
-    "\2\2\u01ff\u01fd\3\2\2\2\u01ff\u0200\3\2\2\2\u0200\u0215\3\2\2\2\u0201"+
-    "\u01ff\3\2\2\2\u0202\u0206\5\26\13\2\u0203\u0205\7\"\2\2\u0204\u0203\3"+
-    "\2\2\2\u0205\u0208\3\2\2\2\u0206\u0204\3\2\2\2\u0206\u0207\3\2\2\2\u0207"+
-    "\u0209\3\2\2\2\u0208\u0206\3\2\2\2\u0209\u020b\5\u0098L\2\u020a\u020c"+
-    "\5\u0096K\2\u020b\u020a\3\2\2\2\u020b\u020c\3\2\2\2\u020c\u0210\3\2\2"+
-    "\2\u020d\u020f\7\"\2\2\u020e\u020d\3\2\2\2\u020f\u0212\3\2\2\2\u0210\u020e"+
-    "\3\2\2\2\u0210\u0211\3\2\2\2\u0211\u0214\3\2\2\2\u0212\u0210\3\2\2\2\u0213"+
-    "\u0202\3\2\2\2\u0214\u0217\3\2\2\2\u0215\u0213\3\2\2\2\u0215\u0216\3\2"+
-    "\2\2\u0216\u0218\3\2\2\2\u0217\u0215\3\2\2\2\u0218\u0219\7@\2\2\u0219"+
-    "\u0097\3\2\2\2\u021a\u021e\t\17\2\2\u021b\u021d\t\20\2\2\u021c\u021b\3"+
-    "\2\2\2\u021d\u0220\3\2\2\2\u021e\u021c\3\2\2\2\u021e\u021f\3\2\2\2\u021f"+
-    "\u0099\3\2\2\2\u0220\u021e\3\2\2\2\u0221\u022a\7\62\2\2\u0222\u0226\t"+
-    "\b\2\2\u0223\u0225\t\t\2\2\u0224\u0223\3\2\2\2\u0225\u0228\3\2\2\2\u0226"+
-    "\u0224\3\2\2\2\u0226\u0227\3\2\2\2\u0227\u022a\3\2\2\2\u0228\u0226\3\2"+
-    "\2\2\u0229\u0221\3\2\2\2\u0229\u0222\3\2\2\2\u022a\u022b\3\2\2\2\u022b"+
-    "\u022c\bM\7\2\u022c\u009b\3\2\2\2\u022d\u0231\t\17\2\2\u022e\u0230\t\20"+
-    "\2\2\u022f\u022e\3\2\2\2\u0230\u0233\3\2\2\2\u0231\u022f\3\2\2\2\u0231"+
-    "\u0232\3\2\2\2\u0232\u0234\3\2\2\2\u0233\u0231\3\2\2\2\u0234\u0235\bN"+
-    "\7\2\u0235\u009d\3\2\2\2%\2\3\u00a1\u00ab\u00b5\u00ba\u0186\u0189\u0190"+
-    "\u0193\u019a\u019d\u01a0\u01a7\u01aa\u01b0\u01b5\u01ba\u01bc\u01bf\u01c7"+
-    "\u01c9\u01e6\u01ee\u01f5\u01fa\u01ff\u0206\u020b\u0210\u0215\u021e\u0226"+
-    "\u0229\u0231\b\b\2\2\4\3\2\3E\2\3F\3\3J\4\4\2\2";
+    "I\tI\4J\tJ\4K\tK\4L\tL\4M\tM\3\2\6\2\u009e\n\2\r\2\16\2\u009f\3\2\3\2"+
+    "\3\3\3\3\3\3\3\3\7\3\u00a8\n\3\f\3\16\3\u00ab\13\3\3\3\3\3\3\3\3\3\3\3"+
+    "\7\3\u00b2\n\3\f\3\16\3\u00b5\13\3\3\3\3\3\5\3\u00b9\n\3\3\3\3\3\3\4\3"+
+    "\4\3\5\3\5\3\6\3\6\3\7\3\7\3\b\3\b\3\t\3\t\3\n\3\n\3\n\3\n\3\13\3\13\3"+
+    "\f\3\f\3\r\3\r\3\r\3\16\3\16\3\16\3\16\3\16\3\17\3\17\3\17\3\17\3\17\3"+
+    "\17\3\20\3\20\3\20\3\21\3\21\3\21\3\21\3\22\3\22\3\22\3\22\3\22\3\22\3"+
+    "\22\3\22\3\22\3\23\3\23\3\23\3\23\3\23\3\23\3\24\3\24\3\24\3\24\3\24\3"+
+    "\24\3\24\3\25\3\25\3\25\3\25\3\26\3\26\3\26\3\26\3\27\3\27\3\27\3\27\3"+
+    "\27\3\27\3\30\3\30\3\30\3\30\3\30\3\30\3\31\3\31\3\32\3\32\3\33\3\33\3"+
+    "\34\3\34\3\35\3\35\3\36\3\36\3\37\3\37\3 \3 \3 \3!\3!\3!\3\"\3\"\3\"\3"+
+    "\"\3#\3#\3$\3$\3$\3%\3%\3&\3&\3&\3\'\3\'\3\'\3(\3(\3(\3(\3)\3)\3)\3*\3"+
+    "*\3*\3*\3+\3+\3,\3,\3-\3-\3.\3.\3.\3/\3/\3/\3\60\3\60\3\61\3\61\3\62\3"+
+    "\62\3\62\3\63\3\63\3\63\3\64\3\64\3\65\3\65\3\65\3\66\3\66\3\66\3\67\3"+
+    "\67\3\67\38\38\38\39\39\39\3:\3:\3:\3;\3;\3;\3<\3<\3<\3=\3=\3=\3=\3>\3"+
+    ">\3>\3>\3?\3?\3?\3?\3?\3@\3@\6@\u017f\n@\r@\16@\u0180\3@\5@\u0184\n@\3"+
+    "A\3A\3A\6A\u0189\nA\rA\16A\u018a\3A\5A\u018e\nA\3B\3B\3B\7B\u0193\nB\f"+
+    "B\16B\u0196\13B\5B\u0198\nB\3B\5B\u019b\nB\3C\3C\3C\7C\u01a0\nC\fC\16"+
+    "C\u01a3\13C\5C\u01a5\nC\3C\3C\7C\u01a9\nC\fC\16C\u01ac\13C\3C\3C\5C\u01b0"+
+    "\nC\3C\6C\u01b3\nC\rC\16C\u01b4\5C\u01b7\nC\3C\5C\u01ba\nC\3D\3D\3D\3"+
+    "D\3D\3D\7D\u01c2\nD\fD\16D\u01c5\13D\3D\3D\3D\3E\3E\3E\3E\3E\3F\3F\3F"+
+    "\3F\3F\3G\3G\3G\3G\3G\3G\3H\3H\3H\3H\3H\3I\3I\5I\u01e1\nI\3I\3I\3I\3J"+
+    "\7J\u01e7\nJ\fJ\16J\u01ea\13J\3J\3J\7J\u01ee\nJ\fJ\16J\u01f1\13J\3J\3"+
+    "J\5J\u01f5\nJ\3J\7J\u01f8\nJ\fJ\16J\u01fb\13J\3J\3J\7J\u01ff\nJ\fJ\16"+
+    "J\u0202\13J\3J\3J\5J\u0206\nJ\3J\7J\u0209\nJ\fJ\16J\u020c\13J\7J\u020e"+
+    "\nJ\fJ\16J\u0211\13J\3J\3J\3K\3K\7K\u0217\nK\fK\16K\u021a\13K\3L\3L\3"+
+    "L\7L\u021f\nL\fL\16L\u0222\13L\5L\u0224\nL\3L\3L\3M\3M\7M\u022a\nM\fM"+
+    "\16M\u022d\13M\3M\3M\5\u00a9\u00b3\u01c3\2N\4\3\6\4\b\5\n\6\f\7\16\b\20"+
+    "\t\22\n\24\13\26\f\30\r\32\16\34\17\36\20 \21\"\22$\23&\24(\25*\26,\27"+
+    ".\30\60\31\62\32\64\33\66\348\35:\36<\37> @!B\"D#F$H%J&L\'N(P)R*T+V,X"+
+    "-Z.\\/^\60`\61b\62d\63f\64h\65j\66l\67n8p9r:t;v<x=z>|?~@\u0080A\u0082"+
+    "B\u0084C\u0086D\u0088E\u008aF\u008cG\u008eH\u0090I\u0092J\u0094\2\u0096"+
+    "K\u0098L\u009aM\4\2\3\21\5\2\13\f\17\17\"\"\4\2\f\f\17\17\3\2\629\4\2"+
+    "NNnn\4\2ZZzz\5\2\62;CHch\3\2\63;\3\2\62;\b\2FFHHNNffhhnn\4\2GGgg\4\2-"+
+    "-//\4\2HHhh\4\2$$^^\5\2C\\aac|\6\2\62;C\\aac|\u024f\2\4\3\2\2\2\2\6\3"+
+    "\2\2\2\2\b\3\2\2\2\2\n\3\2\2\2\2\f\3\2\2\2\2\16\3\2\2\2\2\20\3\2\2\2\2"+
+    "\22\3\2\2\2\2\24\3\2\2\2\2\26\3\2\2\2\2\30\3\2\2\2\2\32\3\2\2\2\2\34\3"+
+    "\2\2\2\2\36\3\2\2\2\2 \3\2\2\2\2\"\3\2\2\2\2$\3\2\2\2\2&\3\2\2\2\2(\3"+
+    "\2\2\2\2*\3\2\2\2\2,\3\2\2\2\2.\3\2\2\2\2\60\3\2\2\2\2\62\3\2\2\2\2\64"+
+    "\3\2\2\2\2\66\3\2\2\2\28\3\2\2\2\2:\3\2\2\2\2<\3\2\2\2\2>\3\2\2\2\2@\3"+
+    "\2\2\2\2B\3\2\2\2\2D\3\2\2\2\2F\3\2\2\2\2H\3\2\2\2\2J\3\2\2\2\2L\3\2\2"+
+    "\2\2N\3\2\2\2\2P\3\2\2\2\2R\3\2\2\2\2T\3\2\2\2\2V\3\2\2\2\2X\3\2\2\2\2"+
+    "Z\3\2\2\2\2\\\3\2\2\2\2^\3\2\2\2\2`\3\2\2\2\2b\3\2\2\2\2d\3\2\2\2\2f\3"+
+    "\2\2\2\2h\3\2\2\2\2j\3\2\2\2\2l\3\2\2\2\2n\3\2\2\2\2p\3\2\2\2\2r\3\2\2"+
+    "\2\2t\3\2\2\2\2v\3\2\2\2\2x\3\2\2\2\2z\3\2\2\2\2|\3\2\2\2\2~\3\2\2\2\2"+
+    "\u0080\3\2\2\2\2\u0082\3\2\2\2\2\u0084\3\2\2\2\2\u0086\3\2\2\2\2\u0088"+
+    "\3\2\2\2\2\u008a\3\2\2\2\2\u008c\3\2\2\2\2\u008e\3\2\2\2\2\u0090\3\2\2"+
+    "\2\2\u0092\3\2\2\2\2\u0096\3\2\2\2\3\u0098\3\2\2\2\3\u009a\3\2\2\2\4\u009d"+
+    "\3\2\2\2\6\u00b8\3\2\2\2\b\u00bc\3\2\2\2\n\u00be\3\2\2\2\f\u00c0\3\2\2"+
+    "\2\16\u00c2\3\2\2\2\20\u00c4\3\2\2\2\22\u00c6\3\2\2\2\24\u00c8\3\2\2\2"+
+    "\26\u00cc\3\2\2\2\30\u00ce\3\2\2\2\32\u00d0\3\2\2\2\34\u00d3\3\2\2\2\36"+
+    "\u00d8\3\2\2\2 \u00de\3\2\2\2\"\u00e1\3\2\2\2$\u00e5\3\2\2\2&\u00ee\3"+
+    "\2\2\2(\u00f4\3\2\2\2*\u00fb\3\2\2\2,\u00ff\3\2\2\2.\u0103\3\2\2\2\60"+
+    "\u0109\3\2\2\2\62\u010f\3\2\2\2\64\u0111\3\2\2\2\66\u0113\3\2\2\28\u0115"+
+    "\3\2\2\2:\u0117\3\2\2\2<\u0119\3\2\2\2>\u011b\3\2\2\2@\u011d\3\2\2\2B"+
+    "\u0120\3\2\2\2D\u0123\3\2\2\2F\u0127\3\2\2\2H\u0129\3\2\2\2J\u012c\3\2"+
+    "\2\2L\u012e\3\2\2\2N\u0131\3\2\2\2P\u0134\3\2\2\2R\u0138\3\2\2\2T\u013b"+
+    "\3\2\2\2V\u013f\3\2\2\2X\u0141\3\2\2\2Z\u0143\3\2\2\2\\\u0145\3\2\2\2"+
+    "^\u0148\3\2\2\2`\u014b\3\2\2\2b\u014d\3\2\2\2d\u014f\3\2\2\2f\u0152\3"+
+    "\2\2\2h\u0155\3\2\2\2j\u0157\3\2\2\2l\u015a\3\2\2\2n\u015d\3\2\2\2p\u0160"+
+    "\3\2\2\2r\u0163\3\2\2\2t\u0166\3\2\2\2v\u0169\3\2\2\2x\u016c\3\2\2\2z"+
+    "\u016f\3\2\2\2|\u0173\3\2\2\2~\u0177\3\2\2\2\u0080\u017c\3\2\2\2\u0082"+
+    "\u0185\3\2\2\2\u0084\u0197\3\2\2\2\u0086\u01a4\3\2\2\2\u0088\u01bb\3\2"+
+    "\2\2\u008a\u01c9\3\2\2\2\u008c\u01ce\3\2\2\2\u008e\u01d3\3\2\2\2\u0090"+
+    "\u01d9\3\2\2\2\u0092\u01de\3\2\2\2\u0094\u01e8\3\2\2\2\u0096\u0214\3\2"+
+    "\2\2\u0098\u0223\3\2\2\2\u009a\u0227\3\2\2\2\u009c\u009e\t\2\2\2\u009d"+
+    "\u009c\3\2\2\2\u009e\u009f\3\2\2\2\u009f\u009d\3\2\2\2\u009f\u00a0\3\2"+
+    "\2\2\u00a0\u00a1\3\2\2\2\u00a1\u00a2\b\2\2\2\u00a2\5\3\2\2\2\u00a3\u00a4"+
+    "\7\61\2\2\u00a4\u00a5\7\61\2\2\u00a5\u00a9\3\2\2\2\u00a6\u00a8\13\2\2"+
+    "\2\u00a7\u00a6\3\2\2\2\u00a8\u00ab\3\2\2\2\u00a9\u00aa\3\2\2\2\u00a9\u00a7"+
+    "\3\2\2\2\u00aa\u00ac\3\2\2\2\u00ab\u00a9\3\2\2\2\u00ac\u00b9\t\3\2\2\u00ad"+
+    "\u00ae\7\61\2\2\u00ae\u00af\7,\2\2\u00af\u00b3\3\2\2\2\u00b0\u00b2\13"+
+    "\2\2\2\u00b1\u00b0\3\2\2\2\u00b2\u00b5\3\2\2\2\u00b3\u00b4\3\2\2\2\u00b3"+
+    "\u00b1\3\2\2\2\u00b4\u00b6\3\2\2\2\u00b5\u00b3\3\2\2\2\u00b6\u00b7\7,"+
+    "\2\2\u00b7\u00b9\7\61\2\2\u00b8\u00a3\3\2\2\2\u00b8\u00ad\3\2\2\2\u00b9"+
+    "\u00ba\3\2\2\2\u00ba\u00bb\b\3\2\2\u00bb\7\3\2\2\2\u00bc\u00bd\7}\2\2"+
+    "\u00bd\t\3\2\2\2\u00be\u00bf\7\177\2\2\u00bf\13\3\2\2\2\u00c0\u00c1\7"+
+    "]\2\2\u00c1\r\3\2\2\2\u00c2\u00c3\7_\2\2\u00c3\17\3\2\2\2\u00c4\u00c5"+
+    "\7*\2\2\u00c5\21\3\2\2\2\u00c6\u00c7\7+\2\2\u00c7\23\3\2\2\2\u00c8\u00c9"+
+    "\7\60\2\2\u00c9\u00ca\3\2\2\2\u00ca\u00cb\b\n\3\2\u00cb\25\3\2\2\2\u00cc"+
+    "\u00cd\7.\2\2\u00cd\27\3\2\2\2\u00ce\u00cf\7=\2\2\u00cf\31\3\2\2\2\u00d0"+
+    "\u00d1\7k\2\2\u00d1\u00d2\7h\2\2\u00d2\33\3\2\2\2\u00d3\u00d4\7g\2\2\u00d4"+
+    "\u00d5\7n\2\2\u00d5\u00d6\7u\2\2\u00d6\u00d7\7g\2\2\u00d7\35\3\2\2\2\u00d8"+
+    "\u00d9\7y\2\2\u00d9\u00da\7j\2\2\u00da\u00db\7k\2\2\u00db\u00dc\7n\2\2"+
+    "\u00dc\u00dd\7g\2\2\u00dd\37\3\2\2\2\u00de\u00df\7f\2\2\u00df\u00e0\7"+
+    "q\2\2\u00e0!\3\2\2\2\u00e1\u00e2\7h\2\2\u00e2\u00e3\7q\2\2\u00e3\u00e4"+
+    "\7t\2\2\u00e4#\3\2\2\2\u00e5\u00e6\7e\2\2\u00e6\u00e7\7q\2\2\u00e7\u00e8"+
+    "\7p\2\2\u00e8\u00e9\7v\2\2\u00e9\u00ea\7k\2\2\u00ea\u00eb\7p\2\2\u00eb"+
+    "\u00ec\7w\2\2\u00ec\u00ed\7g\2\2\u00ed%\3\2\2\2\u00ee\u00ef\7d\2\2\u00ef"+
+    "\u00f0\7t\2\2\u00f0\u00f1\7g\2\2\u00f1\u00f2\7c\2\2\u00f2\u00f3\7m\2\2"+
+    "\u00f3\'\3\2\2\2\u00f4\u00f5\7t\2\2\u00f5\u00f6\7g\2\2\u00f6\u00f7\7v"+
+    "\2\2\u00f7\u00f8\7w\2\2\u00f8\u00f9\7t\2\2\u00f9\u00fa\7p\2\2\u00fa)\3"+
+    "\2\2\2\u00fb\u00fc\7p\2\2\u00fc\u00fd\7g\2\2\u00fd\u00fe\7y\2\2\u00fe"+
+    "+\3\2\2\2\u00ff\u0100\7v\2\2\u0100\u0101\7t\2\2\u0101\u0102\7{\2\2\u0102"+
+    "-\3\2\2\2\u0103\u0104\7e\2\2\u0104\u0105\7c\2\2\u0105\u0106\7v\2\2\u0106"+
+    "\u0107\7e\2\2\u0107\u0108\7j\2\2\u0108/\3\2\2\2\u0109\u010a\7v\2\2\u010a"+
+    "\u010b\7j\2\2\u010b\u010c\7t\2\2\u010c\u010d\7q\2\2\u010d\u010e\7y\2\2"+
+    "\u010e\61\3\2\2\2\u010f\u0110\7#\2\2\u0110\63\3\2\2\2\u0111\u0112\7\u0080"+
+    "\2\2\u0112\65\3\2\2\2\u0113\u0114\7,\2\2\u0114\67\3\2\2\2\u0115\u0116"+
+    "\7\61\2\2\u01169\3\2\2\2\u0117\u0118\7\'\2\2\u0118;\3\2\2\2\u0119\u011a"+
+    "\7-\2\2\u011a=\3\2\2\2\u011b\u011c\7/\2\2\u011c?\3\2\2\2\u011d\u011e\7"+
+    ">\2\2\u011e\u011f\7>\2\2\u011fA\3\2\2\2\u0120\u0121\7@\2\2\u0121\u0122"+
+    "\7@\2\2\u0122C\3\2\2\2\u0123\u0124\7@\2\2\u0124\u0125\7@\2\2\u0125\u0126"+
+    "\7@\2\2\u0126E\3\2\2\2\u0127\u0128\7>\2\2\u0128G\3\2\2\2\u0129\u012a\7"+
+    ">\2\2\u012a\u012b\7?\2\2\u012bI\3\2\2\2\u012c\u012d\7@\2\2\u012dK\3\2"+
+    "\2\2\u012e\u012f\7@\2\2\u012f\u0130\7?\2\2\u0130M\3\2\2\2\u0131\u0132"+
+    "\7?\2\2\u0132\u0133\7?\2\2\u0133O\3\2\2\2\u0134\u0135\7?\2\2\u0135\u0136"+
+    "\7?\2\2\u0136\u0137\7?\2\2\u0137Q\3\2\2\2\u0138\u0139\7#\2\2\u0139\u013a"+
+    "\7?\2\2\u013aS\3\2\2\2\u013b\u013c\7#\2\2\u013c\u013d\7?\2\2\u013d\u013e"+
+    "\7?\2\2\u013eU\3\2\2\2\u013f\u0140\7(\2\2\u0140W\3\2\2\2\u0141\u0142\7"+
+    "`\2\2\u0142Y\3\2\2\2\u0143\u0144\7~\2\2\u0144[\3\2\2\2\u0145\u0146\7("+
+    "\2\2\u0146\u0147\7(\2\2\u0147]\3\2\2\2\u0148\u0149\7~\2\2\u0149\u014a"+
+    "\7~\2\2\u014a_\3\2\2\2\u014b\u014c\7A\2\2\u014ca\3\2\2\2\u014d\u014e\7"+
+    "<\2\2\u014ec\3\2\2\2\u014f\u0150\7-\2\2\u0150\u0151\7-\2\2\u0151e\3\2"+
+    "\2\2\u0152\u0153\7/\2\2\u0153\u0154\7/\2\2\u0154g\3\2\2\2\u0155\u0156"+
+    "\7?\2\2\u0156i\3\2\2\2\u0157\u0158\7-\2\2\u0158\u0159\7?\2\2\u0159k\3"+
+    "\2\2\2\u015a\u015b\7/\2\2\u015b\u015c\7?\2\2\u015cm\3\2\2\2\u015d\u015e"+
+    "\7,\2\2\u015e\u015f\7?\2\2\u015fo\3\2\2\2\u0160\u0161\7\61\2\2\u0161\u0162"+
+    "\7?\2\2\u0162q\3\2\2\2\u0163\u0164\7\'\2\2\u0164\u0165\7?\2\2\u0165s\3"+
+    "\2\2\2\u0166\u0167\7(\2\2\u0167\u0168\7?\2\2\u0168u\3\2\2\2\u0169\u016a"+
+    "\7`\2\2\u016a\u016b\7?\2\2\u016bw\3\2\2\2\u016c\u016d\7~\2\2\u016d\u016e"+
+    "\7?\2\2\u016ey\3\2\2\2\u016f\u0170\7>\2\2\u0170\u0171\7>\2\2\u0171\u0172"+
+    "\7?\2\2\u0172{\3\2\2\2\u0173\u0174\7@\2\2\u0174\u0175\7@\2\2\u0175\u0176"+
+    "\7?\2\2\u0176}\3\2\2\2\u0177\u0178\7@\2\2\u0178\u0179\7@\2\2\u0179\u017a"+
+    "\7@\2\2\u017a\u017b\7?\2\2\u017b\177\3\2\2\2\u017c\u017e\7\62\2\2\u017d"+
+    "\u017f\t\4\2\2\u017e\u017d\3\2\2\2\u017f\u0180\3\2\2\2\u0180\u017e\3\2"+
+    "\2\2\u0180\u0181\3\2\2\2\u0181\u0183\3\2\2\2\u0182\u0184\t\5\2\2\u0183"+
+    "\u0182\3\2\2\2\u0183\u0184\3\2\2\2\u0184\u0081\3\2\2\2\u0185\u0186\7\62"+
+    "\2\2\u0186\u0188\t\6\2\2\u0187\u0189\t\7\2\2\u0188\u0187\3\2\2\2\u0189"+
+    "\u018a\3\2\2\2\u018a\u0188\3\2\2\2\u018a\u018b\3\2\2\2\u018b\u018d\3\2"+
+    "\2\2\u018c\u018e\t\5\2\2\u018d\u018c\3\2\2\2\u018d\u018e\3\2\2\2\u018e"+
+    "\u0083\3\2\2\2\u018f\u0198\7\62\2\2\u0190\u0194\t\b\2\2\u0191\u0193\t"+
+    "\t\2\2\u0192\u0191\3\2\2\2\u0193\u0196\3\2\2\2\u0194\u0192\3\2\2\2\u0194"+
+    "\u0195\3\2\2\2\u0195\u0198\3\2\2\2\u0196\u0194\3\2\2\2\u0197\u018f\3\2"+
+    "\2\2\u0197\u0190\3\2\2\2\u0198\u019a\3\2\2\2\u0199\u019b\t\n\2\2\u019a"+
+    "\u0199\3\2\2\2\u019a\u019b\3\2\2\2\u019b\u0085\3\2\2\2\u019c\u01a5\7\62"+
+    "\2\2\u019d\u01a1\t\b\2\2\u019e\u01a0\t\t\2\2\u019f\u019e\3\2\2\2\u01a0"+
+    "\u01a3\3\2\2\2\u01a1\u019f\3\2\2\2\u01a1\u01a2\3\2\2\2\u01a2\u01a5\3\2"+
+    "\2\2\u01a3\u01a1\3\2\2\2\u01a4\u019c\3\2\2\2\u01a4\u019d\3\2\2\2\u01a5"+
+    "\u01a6\3\2\2\2\u01a6\u01aa\5\24\n\2\u01a7\u01a9\t\t\2\2\u01a8\u01a7\3"+
+    "\2\2\2\u01a9\u01ac\3\2\2\2\u01aa\u01a8\3\2\2\2\u01aa\u01ab\3\2\2\2\u01ab"+
+    "\u01b6\3\2\2\2\u01ac\u01aa\3\2\2\2\u01ad\u01af\t\13\2\2\u01ae\u01b0\t"+
+    "\f\2\2\u01af\u01ae\3\2\2\2\u01af\u01b0\3\2\2\2\u01b0\u01b2\3\2\2\2\u01b1"+
+    "\u01b3\t\t\2\2\u01b2\u01b1\3\2\2\2\u01b3\u01b4\3\2\2\2\u01b4\u01b2\3\2"+
+    "\2\2\u01b4\u01b5\3\2\2\2\u01b5\u01b7\3\2\2\2\u01b6\u01ad\3\2\2\2\u01b6"+
+    "\u01b7\3\2\2\2\u01b7\u01b9\3\2\2\2\u01b8\u01ba\t\r\2\2\u01b9\u01b8\3\2"+
+    "\2\2\u01b9\u01ba\3\2\2\2\u01ba\u0087\3\2\2\2\u01bb\u01c3\7$\2\2\u01bc"+
+    "\u01bd\7^\2\2\u01bd\u01c2\7$\2\2\u01be\u01bf\7^\2\2\u01bf\u01c2\7^\2\2"+
+    "\u01c0\u01c2\n\16\2\2\u01c1\u01bc\3\2\2\2\u01c1\u01be\3\2\2\2\u01c1\u01c0"+
+    "\3\2\2\2\u01c2\u01c5\3\2\2\2\u01c3\u01c4\3\2\2\2\u01c3\u01c1\3\2\2\2\u01c4"+
+    "\u01c6\3\2\2\2\u01c5\u01c3\3\2\2\2\u01c6\u01c7\7$\2\2\u01c7\u01c8\bD\4"+
+    "\2\u01c8\u0089\3\2\2\2\u01c9\u01ca\7)\2\2\u01ca\u01cb\13\2\2\2\u01cb\u01cc"+
+    "\7)\2\2\u01cc\u01cd\bE\5\2\u01cd\u008b\3\2\2\2\u01ce\u01cf\7v\2\2\u01cf"+
+    "\u01d0\7t\2\2\u01d0\u01d1\7w\2\2\u01d1\u01d2\7g\2\2\u01d2\u008d\3\2\2"+
+    "\2\u01d3\u01d4\7h\2\2\u01d4\u01d5\7c\2\2\u01d5\u01d6\7n\2\2\u01d6\u01d7"+
+    "\7u\2\2\u01d7\u01d8\7g\2\2\u01d8\u008f\3\2\2\2\u01d9\u01da\7p\2\2\u01da"+
+    "\u01db\7w\2\2\u01db\u01dc\7n\2\2\u01dc\u01dd\7n\2\2\u01dd\u0091\3\2\2"+
+    "\2\u01de\u01e0\5\u0096K\2\u01df\u01e1\5\u0094J\2\u01e0\u01df\3\2\2\2\u01e0"+
+    "\u01e1\3\2\2\2\u01e1\u01e2\3\2\2\2\u01e2\u01e3\6I\2\2\u01e3\u01e4\bI\6"+
+    "\2\u01e4\u0093\3\2\2\2\u01e5\u01e7\7\"\2\2\u01e6\u01e5\3\2\2\2\u01e7\u01ea"+
+    "\3\2\2\2\u01e8\u01e6\3\2\2\2\u01e8\u01e9\3\2\2\2\u01e9\u01eb\3\2\2\2\u01ea"+
+    "\u01e8\3\2\2\2\u01eb\u01ef\7>\2\2\u01ec\u01ee\7\"\2\2\u01ed\u01ec\3\2"+
+    "\2\2\u01ee\u01f1\3\2\2\2\u01ef\u01ed\3\2\2\2\u01ef\u01f0\3\2\2\2\u01f0"+
+    "\u01f2\3\2\2\2\u01f1\u01ef\3\2\2\2\u01f2\u01f4\5\u0096K\2\u01f3\u01f5"+
+    "\5\u0094J\2\u01f4\u01f3\3\2\2\2\u01f4\u01f5\3\2\2\2\u01f5\u01f9\3\2\2"+
+    "\2\u01f6\u01f8\7\"\2\2\u01f7\u01f6\3\2\2\2\u01f8\u01fb\3\2\2\2\u01f9\u01f7"+
+    "\3\2\2\2\u01f9\u01fa\3\2\2\2\u01fa\u020f\3\2\2\2\u01fb\u01f9\3\2\2\2\u01fc"+
+    "\u0200\5\26\13\2\u01fd\u01ff\7\"\2\2\u01fe\u01fd\3\2\2\2\u01ff\u0202\3"+
+    "\2\2\2\u0200\u01fe\3\2\2\2\u0200\u0201\3\2\2\2\u0201\u0203\3\2\2\2\u0202"+
+    "\u0200\3\2\2\2\u0203\u0205\5\u0096K\2\u0204\u0206\5\u0094J\2\u0205\u0204"+
+    "\3\2\2\2\u0205\u0206\3\2\2\2\u0206\u020a\3\2\2\2\u0207\u0209\7\"\2\2\u0208"+
+    "\u0207\3\2\2\2\u0209\u020c\3\2\2\2\u020a\u0208\3\2\2\2\u020a\u020b\3\2"+
+    "\2\2\u020b\u020e\3\2\2\2\u020c\u020a\3\2\2\2\u020d\u01fc\3\2\2\2\u020e"+
+    "\u0211\3\2\2\2\u020f\u020d\3\2\2\2\u020f\u0210\3\2\2\2\u0210\u0212\3\2"+
+    "\2\2\u0211\u020f\3\2\2\2\u0212\u0213\7@\2\2\u0213\u0095\3\2\2\2\u0214"+
+    "\u0218\t\17\2\2\u0215\u0217\t\20\2\2\u0216\u0215\3\2\2\2\u0217\u021a\3"+
+    "\2\2\2\u0218\u0216\3\2\2\2\u0218\u0219\3\2\2\2\u0219\u0097\3\2\2\2\u021a"+
+    "\u0218\3\2\2\2\u021b\u0224\7\62\2\2\u021c\u0220\t\b\2\2\u021d\u021f\t"+
+    "\t\2\2\u021e\u021d\3\2\2\2\u021f\u0222\3\2\2\2\u0220\u021e\3\2\2\2\u0220"+
+    "\u0221\3\2\2\2\u0221\u0224\3\2\2\2\u0222\u0220\3\2\2\2\u0223\u021b\3\2"+
+    "\2\2\u0223\u021c\3\2\2\2\u0224\u0225\3\2\2\2\u0225\u0226\bL\7\2\u0226"+
+    "\u0099\3\2\2\2\u0227\u022b\t\17\2\2\u0228\u022a\t\20\2\2\u0229\u0228\3"+
+    "\2\2\2\u022a\u022d\3\2\2\2\u022b\u0229\3\2\2\2\u022b\u022c\3\2\2\2\u022c"+
+    "\u022e\3\2\2\2\u022d\u022b\3\2\2\2\u022e\u022f\bM\7\2\u022f\u009b\3\2"+
+    "\2\2%\2\3\u009f\u00a9\u00b3\u00b8\u0180\u0183\u018a\u018d\u0194\u0197"+
+    "\u019a\u01a1\u01a4\u01aa\u01af\u01b4\u01b6\u01b9\u01c1\u01c3\u01e0\u01e8"+
+    "\u01ef\u01f4\u01f9\u0200\u0205\u020a\u020f\u0218\u0220\u0223\u022b\b\b"+
+    "\2\2\4\3\2\3D\2\3E\3\3I\4\4\2\2";
   public static final ATN _ATN =
     new ATNDeserializer().deserialize(_serializedATN.toCharArray());
   static {
diff --git a/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessParser.java b/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessParser.java
index e7b331d..53c6eb3 100644
--- a/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessParser.java
+++ b/plugins/lang-painless/src/main/java/org/elasticsearch/painless/PainlessParser.java
@@ -36,9 +36,8 @@ class PainlessParser extends Parser {
     LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43,
     BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51,
     AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59,
-    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67,
-    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75,
-    EXTID=76;
+    ALSH=60, ARSH=61, AUSH=62, OCTAL=63, HEX=64, INTEGER=65, DECIMAL=66, STRING=67,
+    CHAR=68, TRUE=69, FALSE=70, NULL=71, TYPE=72, ID=73, EXTINTEGER=74, EXTID=75;
   public static final int
     RULE_source = 0, RULE_statement = 1, RULE_block = 2, RULE_empty = 3, RULE_emptyscope = 4,
     RULE_initializer = 5, RULE_afterthought = 6, RULE_declaration = 7, RULE_decltype = 8,
@@ -60,8 +59,8 @@ class PainlessParser extends Parser {
     "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'",
     "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'",
     "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='",
-    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null,
-    null, null, null, null, null, "'true'", "'false'", "'null'"
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", null, null,
+    null, null, null, null, "'true'", "'false'", "'null'"
   };
   private static final String[] _SYMBOLIC_NAMES = {
     null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP",
@@ -70,9 +69,9 @@ class PainlessParser extends Parser {
     "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT",
     "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND",
     "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL",
-    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT",
-    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE",
-    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "OCTAL",
+    "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", "NULL",
+    "TYPE", "ID", "EXTINTEGER", "EXTID"
   };
   public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
 
@@ -162,7 +161,7 @@ class PainlessParser extends Parser {
         setState(53);
         _errHandler.sync(this);
         _la = _input.LA(1);
-      } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
+      } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
       setState(55);
       match(EOF);
       }
@@ -469,7 +468,7 @@ class PainlessParser extends Parser {
         match(LP);
         setState(86);
         _la = _input.LA(1);
-        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
           {
           setState(85);
           initializer();
@@ -480,7 +479,7 @@ class PainlessParser extends Parser {
         match(SEMICOLON);
         setState(90);
         _la = _input.LA(1);
-        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
           {
           setState(89);
           expression(0);
@@ -491,7 +490,7 @@ class PainlessParser extends Parser {
         match(SEMICOLON);
         setState(94);
         _la = _input.LA(1);
-        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
           {
           setState(93);
           afterthought();
@@ -731,7 +730,7 @@ class PainlessParser extends Parser {
           setState(140);
           _errHandler.sync(this);
           _la = _input.LA(1);
-        } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
+        } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
         setState(142);
         match(RBRACK);
         }
@@ -1540,7 +1539,7 @@ class PainlessParser extends Parser {
         _prevctx = _localctx;
         setState(208);
         _la = _input.LA(1);
-        if ( !(((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)))) != 0)) ) {
+        if ( !(((((_la - 63)) & ~0x3f) == 0 && ((1L << (_la - 63)) & ((1L << (OCTAL - 63)) | (1L << (HEX - 63)) | (1L << (INTEGER - 63)) | (1L << (DECIMAL - 63)))) != 0)) ) {
         _errHandler.recoverInline(this);
         } else {
           consume();
@@ -2665,7 +2664,7 @@ class PainlessParser extends Parser {
       match(LP);
       setState(361);
       _la = _input.LA(1);
-      if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+      if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR) | (1L << OCTAL))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
         {
         setState(353);
         expression(0);
@@ -2781,7 +2780,7 @@ class PainlessParser extends Parser {
   }
 
   public static final String _serializedATN =
-    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3N\u0172\4\2\t\2\4"+
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3M\u0172\4\2\t\2\4"+
     "\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t"+
     "\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22"+
     "\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31\t\31"+
@@ -2809,8 +2808,8 @@ class PainlessParser extends Parser {
     "\27\5\27\u015a\n\27\5\27\u015c\n\27\3\30\3\30\3\30\5\30\u0161\n\30\3\31"+
     "\3\31\3\31\3\31\7\31\u0167\n\31\f\31\16\31\u016a\13\31\5\31\u016c\n\31"+
     "\3\31\3\31\3\32\3\32\3\32\2\3\30\33\2\4\6\b\n\f\16\20\22\24\26\30\32\34"+
-    "\36 \"$&(*,.\60\62\2\f\4\2\32\33\37 \3\2\65@\3\2BE\3\2\34\36\3\2\37 \3"+
-    "\2!#\3\2$\'\3\2(+\3\2MN\3\2\63\64\u01b2\2\65\3\2\2\2\4\u0088\3\2\2\2\6"+
+    "\36 \"$&(*,.\60\62\2\f\4\2\32\33\37 \3\2\65@\3\2AD\3\2\34\36\3\2\37 \3"+
+    "\2!#\3\2$\'\3\2(+\3\2LM\3\2\63\64\u01b2\2\65\3\2\2\2\4\u0088\3\2\2\2\6"+
     "\u0093\3\2\2\2\b\u0097\3\2\2\2\n\u0099\3\2\2\2\f\u009e\3\2\2\2\16\u00a0"+
     "\3\2\2\2\20\u00a2\3\2\2\2\22\u00ab\3\2\2\2\24\u00b3\3\2\2\2\26\u00b8\3"+
     "\2\2\2\30\u00de\3\2\2\2\32\u010f\3\2\2\2\34\u0111\3\2\2\2\36\u011f\3\2"+
@@ -2846,20 +2845,20 @@ class PainlessParser extends Parser {
     "\r\3\2\2\2\u00a0\u00a1\5\30\r\2\u00a1\17\3\2\2\2\u00a2\u00a3\5\22\n\2"+
     "\u00a3\u00a8\5\24\13\2\u00a4\u00a5\7\f\2\2\u00a5\u00a7\5\24\13\2\u00a6"+
     "\u00a4\3\2\2\2\u00a7\u00aa\3\2\2\2\u00a8\u00a6\3\2\2\2\u00a8\u00a9\3\2"+
-    "\2\2\u00a9\21\3\2\2\2\u00aa\u00a8\3\2\2\2\u00ab\u00b0\7K\2\2\u00ac\u00ad"+
+    "\2\2\u00a9\21\3\2\2\2\u00aa\u00a8\3\2\2\2\u00ab\u00b0\7J\2\2\u00ac\u00ad"+
     "\7\7\2\2\u00ad\u00af\7\b\2\2\u00ae\u00ac\3\2\2\2\u00af\u00b2\3\2\2\2\u00b0"+
     "\u00ae\3\2\2\2\u00b0\u00b1\3\2\2\2\u00b1\23\3\2\2\2\u00b2\u00b0\3\2\2"+
-    "\2\u00b3\u00b6\7L\2\2\u00b4\u00b5\7\65\2\2\u00b5\u00b7\5\30\r\2\u00b6"+
+    "\2\u00b3\u00b6\7K\2\2\u00b4\u00b5\7\65\2\2\u00b5\u00b7\5\30\r\2\u00b6"+
     "\u00b4\3\2\2\2\u00b6\u00b7\3\2\2\2\u00b7\25\3\2\2\2\u00b8\u00b9\7\30\2"+
-    "\2\u00b9\u00ba\7\t\2\2\u00ba\u00bb\7K\2\2\u00bb\u00bc\7L\2\2\u00bc\u00bd"+
+    "\2\u00b9\u00ba\7\t\2\2\u00ba\u00bb\7J\2\2\u00bb\u00bc\7K\2\2\u00bc\u00bd"+
     "\3\2\2\2\u00bd\u00c0\7\n\2\2\u00be\u00c1\5\6\4\2\u00bf\u00c1\5\n\6\2\u00c0"+
     "\u00be\3\2\2\2\u00c0\u00bf\3\2\2\2\u00c1\27\3\2\2\2\u00c2\u00c3\b\r\1"+
     "\2\u00c3\u00c4\t\2\2\2\u00c4\u00df\5\30\r\20\u00c5\u00c6\7\t\2\2\u00c6"+
     "\u00c7\5\22\n\2\u00c7\u00c8\7\n\2\2\u00c8\u00c9\5\30\r\17\u00c9\u00df"+
     "\3\2\2\2\u00ca\u00cb\5\32\16\2\u00cb\u00cc\t\3\2\2\u00cc\u00cd\5\30\r"+
     "\3\u00cd\u00df\3\2\2\2\u00ce\u00cf\7\t\2\2\u00cf\u00d0\5\30\r\2\u00d0"+
-    "\u00d1\7\n\2\2\u00d1\u00df\3\2\2\2\u00d2\u00df\t\4\2\2\u00d3\u00df\7G"+
-    "\2\2\u00d4\u00df\7H\2\2\u00d5\u00df\7I\2\2\u00d6\u00df\7J\2\2\u00d7\u00d8"+
+    "\u00d1\7\n\2\2\u00d1\u00df\3\2\2\2\u00d2\u00df\t\4\2\2\u00d3\u00df\7F"+
+    "\2\2\u00d4\u00df\7G\2\2\u00d5\u00df\7H\2\2\u00d6\u00df\7I\2\2\u00d7\u00d8"+
     "\5\32\16\2\u00d8\u00d9\5\62\32\2\u00d9\u00df\3\2\2\2\u00da\u00db\5\62"+
     "\32\2\u00db\u00dc\5\32\16\2\u00dc\u00df\3\2\2\2\u00dd\u00df\5\32\16\2"+
     "\u00de\u00c2\3\2\2\2\u00de\u00c5\3\2\2\2\u00de\u00ca\3\2\2\2\u00de\u00ce"+
@@ -2899,21 +2898,21 @@ class PainlessParser extends Parser {
     "\u0130\5 \21\2\u012f\u012d\3\2\2\2\u012f\u012e\3\2\2\2\u012f\u0130\3\2"+
     "\2\2\u0130!\3\2\2\2\u0131\u0134\7\13\2\2\u0132\u0135\5&\24\2\u0133\u0135"+
     "\5*\26\2\u0134\u0132\3\2\2\2\u0134\u0133\3\2\2\2\u0135#\3\2\2\2\u0136"+
-    "\u0137\7K\2\2\u0137\u0138\5\"\22\2\u0138%\3\2\2\2\u0139\u013a\7N\2\2\u013a"+
+    "\u0137\7J\2\2\u0137\u0138\5\"\22\2\u0138%\3\2\2\2\u0139\u013a\7M\2\2\u013a"+
     "\u013d\5\60\31\2\u013b\u013e\5\"\22\2\u013c\u013e\5 \21\2\u013d\u013b"+
     "\3\2\2\2\u013d\u013c\3\2\2\2\u013d\u013e\3\2\2\2\u013e\'\3\2\2\2\u013f"+
-    "\u0142\7L\2\2\u0140\u0143\5\"\22\2\u0141\u0143\5 \21\2\u0142\u0140\3\2"+
+    "\u0142\7K\2\2\u0140\u0143\5\"\22\2\u0141\u0143\5 \21\2\u0142\u0140\3\2"+
     "\2\2\u0142\u0141\3\2\2\2\u0142\u0143\3\2\2\2\u0143)\3\2\2\2\u0144\u0147"+
     "\t\n\2\2\u0145\u0148\5\"\22\2\u0146\u0148\5 \21\2\u0147\u0145\3\2\2\2"+
     "\u0147\u0146\3\2\2\2\u0147\u0148\3\2\2\2\u0148+\3\2\2\2\u0149\u014a\7"+
-    "\26\2\2\u014a\u015b\7K\2\2\u014b\u014e\5\60\31\2\u014c\u014f\5\"\22\2"+
+    "\26\2\2\u014a\u015b\7J\2\2\u014b\u014e\5\60\31\2\u014c\u014f\5\"\22\2"+
     "\u014d\u014f\5 \21\2\u014e\u014c\3\2\2\2\u014e\u014d\3\2\2\2\u014e\u014f"+
     "\3\2\2\2\u014f\u015c\3\2\2\2\u0150\u0151\7\7\2\2\u0151\u0152\5\30\r\2"+
     "\u0152\u0153\7\b\2\2\u0153\u0155\3\2\2\2\u0154\u0150\3\2\2\2\u0155\u0156"+
     "\3\2\2\2\u0156\u0154\3\2\2\2\u0156\u0157\3\2\2\2\u0157\u0159\3\2\2\2\u0158"+
     "\u015a\5\"\22\2\u0159\u0158\3\2\2\2\u0159\u015a\3\2\2\2\u015a\u015c\3"+
     "\2\2\2\u015b\u014b\3\2\2\2\u015b\u0154\3\2\2\2\u015c-\3\2\2\2\u015d\u0160"+
-    "\7F\2\2\u015e\u0161\5\"\22\2\u015f\u0161\5 \21\2\u0160\u015e\3\2\2\2\u0160"+
+    "\7E\2\2\u015e\u0161\5\"\22\2\u015f\u0161\5 \21\2\u0160\u015e\3\2\2\2\u0160"+
     "\u015f\3\2\2\2\u0160\u0161\3\2\2\2\u0161/\3\2\2\2\u0162\u016b\7\t\2\2"+
     "\u0163\u0168\5\30\r\2\u0164\u0165\7\f\2\2\u0165\u0167\5\30\r\2\u0166\u0164"+
     "\3\2\2\2\u0167\u016a\3\2\2\2\u0168\u0166\3\2\2\2\u0168\u0169\3\2\2\2\u0169"+
diff --git a/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTest.java b/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTest.java
deleted file mode 100644
index b4807bb..0000000
--- a/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTest.java
+++ /dev/null
@@ -1,178 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.painless;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class NoSemiColonTest extends ScriptTestCase {
-
-    public void testIfStatement() {
-        assertEquals(1, exec("int x = 5 if (x == 5) return 1 return 0"));
-        assertEquals(0, exec("int x = 4 if (x == 5) return 1 else return 0"));
-        assertEquals(2, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 2 else return 0"));
-        assertEquals(1, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 1 else return 0"));
-
-        assertEquals(3, exec(
-                "int x = 5\n" +
-                        "if (x == 5) {\n" +
-                        "    int y = 2\n" +
-                        "    \n" +
-                        "    if (y == 2) {\n" +
-                        "        x = 3\n" +
-                        "    }\n" +
-                        "    \n" +
-                        "}\n" +
-                        "\n" +
-                        "return x\n"));
-    }
-
-    public void testWhileStatement() {
-
-        assertEquals("aaaaaa", exec("String c = \"a\" int x while (x < 5) { c ..= \"a\" ++x } return c"));
-
-        Object value = exec(
-                " byte[][] b = new byte[5][5]       \n" +
-                " byte x = 0, y                     \n" +
-                "                                   \n" +
-                " while (x < 5) {                   \n" +
-                "     y = 0                         \n" +
-                "                                   \n" +
-                "     while (y < 5) {               \n" +
-                "         b[x][y] = (byte)(x*y)     \n" +
-                "         ++y                       \n" +
-                "     }                             \n" +
-                "                                   \n" +
-                "     ++x                           \n" +
-                " }                                 \n" +
-                "                                   \n" +
-                " return b                          \n");
-
-        byte[][] b = (byte[][])value;
-
-        for (byte x = 0; x < 5; ++x) {
-            for (byte y = 0; y < 5; ++y) {
-                assertEquals(x*y, b[x][y]);
-            }
-        }
-    }
-
-    public void testDoWhileStatement() {
-        assertEquals("aaaaaa", exec("String c = \"a\" int x do { c ..= \"a\" ++x } while (x < 5) return c"));
-
-        Object value = exec(
-                " long[][] l = new long[5][5]     \n" +
-                " long x = 0, y                   \n" +
-                "                                 \n" +
-                " do {                            \n" +
-                "     y = 0                       \n" +
-                "                                 \n" +
-                "     do {                        \n" +
-                "         l[(int)x][(int)y] = x*y \n" +
-                "         ++y                     \n" +
-                "     } while (y < 5)             \n" +
-                "                                 \n" +
-                "     ++x                         \n" +
-                " } while (x < 5)                 \n" +
-                "                                 \n" +
-                " return l                        \n");
-
-        long[][] l = (long[][])value;
-
-        for (long x = 0; x < 5; ++x) {
-            for (long y = 0; y < 5; ++y) {
-                assertEquals(x*y, l[(int)x][(int)y]);
-            }
-        }
-    }
-
-    public void testForStatement() {
-        assertEquals("aaaaaa", exec("String c = \"a\" for (int x = 0; x < 5; ++x) c ..= \"a\" return c"));
-
-        Object value = exec(
-                " int[][] i = new int[5][5]         \n" +
-                " for (int x = 0; x < 5; ++x) {     \n" +
-                "     for (int y = 0; y < 5; ++y) { \n" +
-                "         i[x][y] = x*y             \n" +
-                "     }                             \n" +
-                " }                                 \n" +
-                "                                   \n" +
-                " return i                          \n");
-
-        int[][] i = (int[][])value;
-
-        for (int x = 0; x < 5; ++x) {
-            for (int y = 0; y < 5; ++y) {
-                assertEquals(x*y, i[x][y]);
-            }
-        }
-    }
-
-    public void testDeclarationStatement() {
-        assertEquals((byte)2, exec("byte a = 2 return a"));
-        assertEquals((short)2, exec("short a = 2 return a"));
-        assertEquals((char)2, exec("char a = 2 return a"));
-        assertEquals(2, exec("int a = 2 return a"));
-        assertEquals(2L, exec("long a = 2 return a"));
-        assertEquals(2F, exec("float a = 2 return a"));
-        assertEquals(2.0, exec("double a = 2 return a"));
-        assertEquals(false, exec("boolean a = false return a"));
-        assertEquals("string", exec("String a = \"string\" return a"));
-        assertEquals(HashMap.class, exec("Map<String, Object> a = new HashMap<String, Object>() return a").getClass());
-
-        assertEquals(byte[].class, exec("byte[] a = new byte[1] return a").getClass());
-        assertEquals(short[].class, exec("short[] a = new short[1] return a").getClass());
-        assertEquals(char[].class, exec("char[] a = new char[1] return a").getClass());
-        assertEquals(int[].class, exec("int[] a = new int[1] return a").getClass());
-        assertEquals(long[].class, exec("long[] a = new long[1] return a").getClass());
-        assertEquals(float[].class, exec("float[] a = new float[1] return a").getClass());
-        assertEquals(double[].class, exec("double[] a = new double[1] return a").getClass());
-        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1] return a").getClass());
-        assertEquals(String[].class, exec("String[] a = new String[1] return a").getClass());
-        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1] return a").getClass());
-
-        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2] return a").getClass());
-        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3] return a").getClass());
-        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4] return a").getClass());
-        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5] return a").getClass());
-        assertEquals(long[][].class, exec("long[][] a = new long[1][2] return a").getClass());
-        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3] return a").getClass());
-        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4] return a").getClass());
-        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5] return a").getClass());
-        assertEquals(String[][].class, exec("String[][] a = new String[1][2] return a").getClass());
-        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3] return a").getClass());
-    }
-
-    public void testContinueStatement() {
-        assertEquals(9, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 1) continue ++y } return y"));
-    }
-
-    public void testBreakStatement() {
-        assertEquals(4, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 5) break ++y } return y"));
-    }
-
-    public void testReturnStatement() {
-        assertEquals(10, exec("return 10"));
-        assertEquals(5, exec("int x = 5 return x"));
-        assertEquals(4, exec("int[] x = new int[2] x[1] = 4 return x[1]"));
-        assertEquals(5, ((short[])exec("short[] s = new short[3] s[1] = 5 return s"))[1]);
-        assertEquals(10, ((Map)exec("Map<String,Object> s = new HashMap< String,Object>() s.put(\"x\", 10) return s")).get("x"));
-    }
-}
diff --git a/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTests.java b/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTests.java
new file mode 100644
index 0000000..e9c399e
--- /dev/null
+++ b/plugins/lang-painless/src/test/java/org/elasticsearch/painless/NoSemiColonTests.java
@@ -0,0 +1,178 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.painless;
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class NoSemiColonTests extends ScriptTestCase {
+
+    public void testIfStatement() {
+        assertEquals(1, exec("int x = 5 if (x == 5) return 1 return 0"));
+        assertEquals(0, exec("int x = 4 if (x == 5) return 1 else return 0"));
+        assertEquals(2, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 2 else return 0"));
+        assertEquals(1, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 1 else return 0"));
+
+        assertEquals(3, exec(
+                "int x = 5\n" +
+                        "if (x == 5) {\n" +
+                        "    int y = 2\n" +
+                        "    \n" +
+                        "    if (y == 2) {\n" +
+                        "        x = 3\n" +
+                        "    }\n" +
+                        "    \n" +
+                        "}\n" +
+                        "\n" +
+                        "return x\n"));
+    }
+
+    public void testWhileStatement() {
+
+        assertEquals("aaaaaa", exec("String c = \"a\" int x while (x < 5) { ++x c += \"a\" } return c"));
+
+        Object value = exec(
+                " byte[][] b = new byte[5][5]       \n" +
+                " byte x = 0, y                     \n" +
+                "                                   \n" +
+                " while (x < 5) {                   \n" +
+                "     y = 0                         \n" +
+                "                                   \n" +
+                "     while (y < 5) {               \n" +
+                "         b[x][y] = (byte)(x*y)     \n" +
+                "         ++y                       \n" +
+                "     }                             \n" +
+                "                                   \n" +
+                "     ++x                           \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return b                          \n");
+
+        byte[][] b = (byte[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDoWhileStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" int x do { c += \"a\"; ++x } while (x < 5) return c"));
+
+        Object value = exec(
+                " long[][] l = new long[5][5]      \n" +
+                " long x = 0, y                    \n" +
+                "                                  \n" +
+                " do {                             \n" +
+                "     y = 0                        \n" +
+                "                                  \n" +
+                "     do {                         \n" +
+                "         l[(int)x][(int)y] = x*y; \n" +
+                "         ++y                      \n" +
+                "     } while (y < 5)              \n" +
+                "                                  \n" +
+                "     ++x                          \n" +
+                " } while (x < 5)                  \n" +
+                "                                  \n" +
+                " return l                         \n");
+
+        long[][] l = (long[][])value;
+
+        for (long x = 0; x < 5; ++x) {
+            for (long y = 0; y < 5; ++y) {
+                assertEquals(x*y, l[(int)x][(int)y]);
+            }
+        }
+    }
+
+    public void testForStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" for (int x = 0; x < 5; ++x) c += \"a\" return c"));
+
+        Object value = exec(
+                " int[][] i = new int[5][5]         \n" +
+                " for (int x = 0; x < 5; ++x) {     \n" +
+                "     for (int y = 0; y < 5; ++y) { \n" +
+                "         i[x][y] = x*y             \n" +
+                "     }                             \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return i                          \n");
+
+        int[][] i = (int[][])value;
+
+        for (int x = 0; x < 5; ++x) {
+            for (int y = 0; y < 5; ++y) {
+                assertEquals(x*y, i[x][y]);
+            }
+        }
+    }
+
+    public void testDeclarationStatement() {
+        assertEquals((byte)2, exec("byte a = 2 return a"));
+        assertEquals((short)2, exec("short a = 2 return a"));
+        assertEquals((char)2, exec("char a = 2 return a"));
+        assertEquals(2, exec("int a = 2 return a"));
+        assertEquals(2L, exec("long a = 2 return a"));
+        assertEquals(2F, exec("float a = 2 return a"));
+        assertEquals(2.0, exec("double a = 2 return a"));
+        assertEquals(false, exec("boolean a = false return a"));
+        assertEquals("string", exec("String a = \"string\" return a"));
+        assertEquals(HashMap.class, exec("Map<String, Object> a = new HashMap<String, Object>() return a").getClass());
+
+        assertEquals(byte[].class, exec("byte[] a = new byte[1] return a").getClass());
+        assertEquals(short[].class, exec("short[] a = new short[1] return a").getClass());
+        assertEquals(char[].class, exec("char[] a = new char[1] return a").getClass());
+        assertEquals(int[].class, exec("int[] a = new int[1] return a").getClass());
+        assertEquals(long[].class, exec("long[] a = new long[1] return a").getClass());
+        assertEquals(float[].class, exec("float[] a = new float[1] return a").getClass());
+        assertEquals(double[].class, exec("double[] a = new double[1] return a").getClass());
+        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1] return a").getClass());
+        assertEquals(String[].class, exec("String[] a = new String[1] return a").getClass());
+        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1] return a").getClass());
+
+        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2] return a").getClass());
+        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3] return a").getClass());
+        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4] return a").getClass());
+        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5] return a").getClass());
+        assertEquals(long[][].class, exec("long[][] a = new long[1][2] return a").getClass());
+        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3] return a").getClass());
+        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4] return a").getClass());
+        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5] return a").getClass());
+        assertEquals(String[][].class, exec("String[][] a = new String[1][2] return a").getClass());
+        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3] return a").getClass());
+    }
+
+    public void testContinueStatement() {
+        assertEquals(9, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 1) continue ++y } return y"));
+    }
+
+    public void testBreakStatement() {
+        assertEquals(4, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 5) break ++y } return y"));
+    }
+
+    public void testReturnStatement() {
+        assertEquals(10, exec("return 10"));
+        assertEquals(5, exec("int x = 5 return x"));
+        assertEquals(4, exec("int[] x = new int[2] x[1] = 4 return x[1]"));
+        assertEquals(5, ((short[])exec("short[] s = new short[3] s[1] = 5 return s"))[1]);
+        assertEquals(10, ((Map)exec("Map<String,Object> s = new HashMap< String,Object>() s.put(\"x\", 10) return s")).get("x"));
+    }
+}
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
index c94b5e0..03c6e65 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
@@ -36,7 +36,6 @@ import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.ParseContext;
 
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.file.Files;
@@ -119,7 +118,7 @@ public class StandaloneRunner extends CliTool {
 
             terminal.println("## Extracted text");
             terminal.println("--------------------- BEGIN -----------------------");
-            terminal.println("%s", doc.get("file.content"));
+            terminal.println(doc.get("file.content"));
             terminal.println("---------------------- END ------------------------");
             terminal.println("## Metadata");
             printMetadataContent(doc, AttachmentMapper.FieldNames.AUTHOR);
@@ -135,7 +134,7 @@ public class StandaloneRunner extends CliTool {
         }
 
         private void printMetadataContent(ParseContext.Document doc, String field) {
-            terminal.println("- %s: %s", field, doc.get(docMapper.mappers().getMapper("file." + field).fieldType().name()));
+            terminal.println("- " + field + ":" + doc.get(docMapper.mappers().getMapper("file." + field).fieldType().name()));
         }
 
         public static byte[] copyToBytes(Path path) throws IOException {
diff --git a/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java b/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java
index 0c593e2..e32c105 100644
--- a/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java
+++ b/plugins/repository-azure/src/main/java/org/elasticsearch/plugin/repository/azure/AzureRepositoryPlugin.java
@@ -20,12 +20,10 @@
 package org.elasticsearch.plugin.repository.azure;
 
 import org.elasticsearch.cloud.azure.AzureRepositoryModule;
-import org.elasticsearch.cloud.azure.storage.AzureStorageService;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.repositories.RepositoriesModule;
@@ -66,13 +64,4 @@ public class AzureRepositoryPlugin extends Plugin {
         logger.debug("registering repository type [{}]", AzureRepository.TYPE);
         module.registerRepository(AzureRepository.TYPE, AzureRepository.class, BlobStoreIndexShardRepository.class);
     }
-
-    public void onModule(SettingsModule module) {
-        module.registerSetting(AzureStorageService.Storage.ACCOUNT_SETTING);
-        module.registerSetting(AzureStorageService.Storage.COMPRESS_SETTING);
-        module.registerSetting(AzureStorageService.Storage.CONTAINER_SETTING);
-        module.registerSetting(AzureStorageService.Storage.BASE_PATH_SETTING);
-        module.registerSetting(AzureStorageService.Storage.CHUNK_SIZE_SETTING);
-        module.registerSetting(AzureStorageService.Storage.LOCATION_MODE_SETTING);
-    }
 }
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java
new file mode 100644
index 0000000..61410c6
--- /dev/null
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.node.internal;
+
+import org.elasticsearch.common.SuppressForbidden;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+
+@SuppressForbidden(reason = "modifies system properties intentionally")
+public class EvilInternalSettingsPreparerTests extends ESTestCase {
+
+    Map<String, String> savedProperties = new HashMap<>();
+    Settings baseEnvSettings;
+
+    @Before
+    public void saveSettingsSystemProperties() {
+        // clear out any properties the settings preparer may look for
+        savedProperties.clear();
+        for (Object propObj : System.getProperties().keySet()) {
+            String property = (String)propObj;
+            // NOTE: these prefixes are prefixes of the defaults, so both are handled here
+            for (String prefix : InternalSettingsPreparer.PROPERTY_PREFIXES) {
+                if (property.startsWith(prefix)) {
+                    savedProperties.put(property, System.getProperty(property));
+                }
+            }
+        }
+        String name = System.getProperty("name");
+        if (name != null) {
+            savedProperties.put("name", name);
+        }
+        for (String property : savedProperties.keySet()) {
+            System.clearProperty(property);
+        }
+    }
+
+    @After
+    public void restoreSettingsSystemProperties() {
+        for (Map.Entry<String, String> property : savedProperties.entrySet()) {
+            System.setProperty(property.getKey(), property.getValue());
+        }
+    }
+
+    @Before
+    public void createBaseEnvSettings() {
+        baseEnvSettings = settingsBuilder()
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
+            .build();
+    }
+
+    @After
+    public void clearBaseEnvSettings() {
+        baseEnvSettings = null;
+    }
+
+    public void testIgnoreSystemProperties() {
+        try {
+            System.setProperty("es.node.zone", "foo");
+            Settings settings = settingsBuilder()
+                .put("node.zone", "bar")
+                .put(baseEnvSettings)
+                .build();
+            Environment env = InternalSettingsPreparer.prepareEnvironment(settings, null);
+            // Should use setting from the system property
+            assertThat(env.settings().get("node.zone"), equalTo("foo"));
+
+            settings = settingsBuilder()
+                .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING.getKey(), true)
+                .put("node.zone", "bar")
+                .put(baseEnvSettings)
+                .build();
+            env = InternalSettingsPreparer.prepareEnvironment(settings, null);
+            // Should use setting from the system property
+            assertThat(env.settings().get("node.zone"), equalTo("bar"));
+        } finally {
+            System.clearProperty("es.node.zone");
+        }
+    }
+
+    public void testNameSettingsPreference() {
+        try {
+            System.setProperty("name", "sys-prop-name");
+            // Test system property overrides node.name
+            Settings settings = settingsBuilder()
+                .put("node.name", "node-name")
+                .put(baseEnvSettings)
+                .build();
+            Environment env = InternalSettingsPreparer.prepareEnvironment(settings, null);
+            assertThat(env.settings().get("name"), equalTo("sys-prop-name"));
+
+            // test name in settings overrides sys prop and node.name
+            settings = settingsBuilder()
+                .put("name", "name-in-settings")
+                .put("node.name", "node-name")
+                .put(baseEnvSettings)
+                .build();
+            env = InternalSettingsPreparer.prepareEnvironment(settings, null);
+            assertThat(env.settings().get("name"), equalTo("name-in-settings"));
+
+            // test only node.name in settings
+            System.clearProperty("name");
+            settings = settingsBuilder()
+                .put("node.name", "node-name")
+                .put(baseEnvSettings)
+                .build();
+            env = InternalSettingsPreparer.prepareEnvironment(settings, null);
+            assertThat(env.settings().get("name"), equalTo("node-name"));
+
+            // test no name at all results in name being set
+            env = InternalSettingsPreparer.prepareEnvironment(baseEnvSettings, null);
+            assertThat(env.settings().get("name"), not("name-in-settings"));
+            assertThat(env.settings().get("name"), not("sys-prop-name"));
+            assertThat(env.settings().get("name"), not("node-name"));
+            assertThat(env.settings().get("name"), notNullValue());
+        } finally {
+            System.clearProperty("name");
+        }
+    }
+}
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java
index b61ba3c..727728f 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/InstallPluginCommandTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.plugins;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
 import java.net.URL;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.DirectoryStream;
@@ -205,6 +206,14 @@ public class InstallPluginCommandTests extends ESTestCase {
         assertPlugin("fake", pluginDir, env);
     }
 
+    public void testMalformedUrlNotMaven() throws Exception {
+        // has two colons, so it appears similar to maven coordinates
+        MalformedURLException e = expectThrows(MalformedURLException.class, () -> {
+            installPlugin("://host:1234", createEnv());
+        });
+        assertTrue(e.getMessage(), e.getMessage().contains("no protocol"));
+    }
+
     public void testPluginsDirMissing() throws Exception {
         Environment env = createEnv();
         Files.delete(env.pluginsFile());
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
index 157a7b7..ca9f5aa 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
@@ -65,14 +65,14 @@ public class TribeUnitTests extends ESTestCase {
             Settings.builder()
                 .put(baseSettings)
                 .put("cluster.name", "tribe1")
-                .put("node.name", "tribe1_node")
+                .put("name", "tribe1_node")
                 .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), random().nextLong())
                 .build()).start();
         tribe2 = new TribeClientNode(
             Settings.builder()
                 .put(baseSettings)
                 .put("cluster.name", "tribe2")
-                .put("node.name", "tribe2_node")
+                .put("name", "tribe2_node")
                 .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), random().nextLong())
                 .build()).start();
     }
diff --git a/qa/ingest-disabled/build.gradle b/qa/ingest-disabled/build.gradle
deleted file mode 100644
index ca71697..0000000
--- a/qa/ingest-disabled/build.gradle
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-integTest {
-    cluster {
-        systemProperty 'es.node.ingest', 'false'
-    }
-}
diff --git a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java b/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
deleted file mode 100644
index e162807..0000000
--- a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestDisabledIT extends ESRestTestCase {
-
-    public IngestDisabledIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml b/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
deleted file mode 100644
index 01d6740..0000000
--- a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
----
-"Test ingest CRUD APIS work fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { pipelines.0.id: "my_pipeline" }
-  - match: { pipelines.0.config.description: "_description" }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { acknowledged: true }
-
----
-"Test ingest simulate API works fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test index api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
----
-"Test bulk api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      bulk:
-        pipeline: "my_pipeline_1"
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-          - f1: v2
-
----
-"Test bulk api that contains a single index call with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      bulk:
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-              pipeline: my_pipeline_1
-          - f1: v2
-
diff --git a/qa/ingest-with-mustache/build.gradle b/qa/ingest-with-mustache/build.gradle
deleted file mode 100644
index e5ca482..0000000
--- a/qa/ingest-with-mustache/build.gradle
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-dependencies {
-    testCompile project(path: ':modules:lang-mustache', configuration: 'runtime')
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
deleted file mode 100644
index 49e9964..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptEngineRegistry;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptSettings;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-
-public abstract class AbstractMustacheTests extends ESTestCase {
-
-    protected TemplateService templateService;
-
-    @Before
-    public void init() throws Exception {
-        Settings settings = Settings.builder()
-            .put("path.home", createTempDir())
-            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
-            .build();
-        MustacheScriptEngineService mustache = new MustacheScriptEngineService(settings);
-        ScriptEngineRegistry scriptEngineRegistry =
-            new ScriptEngineRegistry(Collections.singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MustacheScriptEngineService.class, MustacheScriptEngineService.TYPES)));
-        ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(Collections.emptyList());
-        ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
-        ScriptService scriptService =
-            new ScriptService(settings, new Environment(settings), Collections.singleton(mustache), null, scriptEngineRegistry, scriptContextRegistry, scriptSettings);
-        templateService = new InternalTemplateService(scriptService);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
deleted file mode 100644
index f27a8e4..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class IngestDocumentMustacheIT extends AbstractMustacheTests {
-
-    public void testAccessMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("foo", "bar");
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 bar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 bar"));
-    }
-
-    public void testAccessMapMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("bar", "hello bar");
-        innerObject.put("baz", "hello baz");
-        innerObject.put("qux", Collections.singletonMap("fubar", "hello qux and fubar"));
-        document.put("foo", innerObject);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo.bar}} {{foo.baz}} {{foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 hello bar hello baz hello qux and fubar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo.bar}} {{_source.foo.baz}} {{_source.foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 hello bar hello baz hello qux and fubar"));
-    }
-
-    public void testAccessListMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("list1", Arrays.asList("foo", "bar", null));
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-        document.put("list2", list);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{list1.0}} {{list2.0}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 foo {field=value}"));
-    }
-
-    public void testAccessIngestMetadataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("ingest_timestamp"), ValueSource.wrap("{{_ingest.timestamp}} and {{_source._ingest.timestamp}}", templateService));
-        assertThat(ingestDocument.getFieldValue("ingest_timestamp", String.class), equalTo(ingestDocument.getIngestMetadata().get("timestamp") + " and bogus_timestamp"));
-    }
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
deleted file mode 100644
index e94765a..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.processor.RemoveProcessor;
-import org.hamcrest.CoreMatchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IngestMustacheRemoveProcessorIT extends AbstractMustacheTests {
-
-    public void testRemoveProcessorMustacheExpression() throws Exception {
-        RemoveProcessor.Factory factory = new RemoveProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field{{var}}");
-        RemoveProcessor processor = factory.create(config);
-        assertThat(processor.getField().execute(Collections.singletonMap("var", "_value")), CoreMatchers.equalTo("field_value"));
-    }
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
deleted file mode 100644
index 6846679..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IngestMustacheSetProcessorIT extends AbstractMustacheTests {
-
-    public void testExpression() throws Exception {
-        SetProcessor processor = createSetProcessor("_index", "text {{var}}");
-        assertThat(processor.getValue(), instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(processor.getValue().copyAndResolve(Collections.singletonMap("var", "_value")), equalTo("text _value"));
-    }
-
-    public void testSetMetadataWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value {{field}}");
-        IngestDocument ingestDocument = createIngestDocument(Collections.singletonMap("field", "value"));
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value value"));
-    }
-
-    public void testSetWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.INDEX, IngestDocument.MetaData.TYPE, IngestDocument.MetaData.ID);
-        Processor processor = createSetProcessor("field{{_type}}", "_value {{" + randomMetaData.getFieldName() + "}}");
-        IngestDocument ingestDocument = createIngestDocument(new HashMap<>());
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("field_type", String.class), Matchers.equalTo("_value " + ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class)));
-    }
-
-    private SetProcessor createSetProcessor(String fieldName, Object fieldValue) throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", fieldName);
-        config.put("value", fieldValue);
-        return factory.create(config);
-    }
-
-    private IngestDocument createIngestDocument(Map<String, Object> source) {
-        return new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
deleted file mode 100644
index 1d1579f..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class TemplateServiceIT extends AbstractMustacheTests {
-
-    public void testTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("fielda", "value1");
-        model.put("fieldb", Collections.singletonMap("fieldc", "value3"));
-
-        TemplateService.Template template = templateService.compile("{{fielda}}/{{fieldb}}/{{fieldb.fieldc}}");
-        assertThat(template.execute(model), equalTo("value1/{fieldc=value3}/value3"));
-    }
-
-    public void testWrongTemplateUsage() {
-        Map<String, Object> model = Collections.emptyMap();
-        TemplateService.Template template = templateService.compile("value");
-        assertThat(template.execute(model), equalTo("value"));
-
-        template = templateService.compile("value {{");
-        assertThat(template.execute(model), equalTo("value {{"));
-        template = templateService.compile("value {{abc");
-        assertThat(template.execute(model), equalTo("value {{abc"));
-        template = templateService.compile("value }}");
-        assertThat(template.execute(model), equalTo("value }}"));
-        template = templateService.compile("value }} {{");
-        assertThat(template.execute(model), equalTo("value }} {{"));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
deleted file mode 100644
index 18085b94..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ValueSourceMustacheIT extends AbstractMustacheTests {
-
-    public void testValueSourceWithTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("field1", "value1");
-        model.put("field2", Collections.singletonMap("field3", "value3"));
-
-        ValueSource valueSource = ValueSource.wrap("{{field1}}/{{field2}}/{{field2.field3}}", templateService);
-        assertThat(valueSource, instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(valueSource.copyAndResolve(model), equalTo("value1/{field3=value3}/value3"));
-
-        valueSource = ValueSource.wrap(Arrays.asList("_value", "{{field1}}"), templateService);
-        assertThat(valueSource, instanceOf(ValueSource.ListValue.class));
-        List<String> result = (List<String>) valueSource.copyAndResolve(model);
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0), equalTo("_value"));
-        assertThat(result.get(1), equalTo("value1"));
-
-        Map<String, Object> map = new HashMap<>();
-        map.put("field1", "{{field1}}");
-        map.put("field2", Collections.singletonMap("field3", "{{field2.field3}}"));
-        map.put("field4", "_value");
-        valueSource = ValueSource.wrap(map, templateService);
-        assertThat(valueSource, instanceOf(ValueSource.MapValue.class));
-        Map<String, Object> resultMap = (Map<String, Object>) valueSource.copyAndResolve(model);
-        assertThat(resultMap.size(), equalTo(3));
-        assertThat(resultMap.get("field1"), equalTo("value1"));
-        assertThat(((Map) resultMap.get("field2")).size(), equalTo(1));
-        assertThat(((Map) resultMap.get("field2")).get("field3"), equalTo("value3"));
-        assertThat(resultMap.get("field4"), equalTo("_value"));
-    }
-
-    public void testAccessSourceViaTemplate() {
-        IngestDocument ingestDocument = new IngestDocument("marvel", "type", "id", null, null, null, null, new HashMap<>());
-        assertThat(ingestDocument.hasField("marvel"), is(false));
-        ingestDocument.setFieldValue(templateService.compile("{{_index}}"), ValueSource.wrap("{{_index}}", templateService));
-        assertThat(ingestDocument.getFieldValue("marvel", String.class), equalTo("marvel"));
-        ingestDocument.removeField(templateService.compile("{{marvel}}"));
-        assertThat(ingestDocument.hasField("index"), is(false));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
deleted file mode 100644
index 73f64d4..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestWithMustacheIT extends ESRestTestCase {
-
-    public IngestWithMustacheIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml b/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
deleted file mode 100644
index 491e1da..0000000
--- a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
+++ /dev/null
@@ -1,221 +0,0 @@
----
-"Test metadata templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "index_type_id",
-                  "value": "{{_index}}/{{_type}}/{{_id}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{_index}}", "{{_type}}", "{{_id}}"]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.index_type_id: "test/test/1" }
-  - match: { _source.metadata: ["test", "test", "1"] }
-
----
-"Test templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field4",
-                  "value": "{{field1}}/{{field2}}/{{field3}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{field1}}", "{{field2}}", "{{field3}}"]
-                }
-              }
-
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_2"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "{{field1}}",
-                  "value": "value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_3"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "field" : "{{field_to_remove}}"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          metadata: "0",
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 5 }
-  - match: { _source.field1: "1" }
-  - match: { _source.field2: "2" }
-  - match: { _source.field3: "3" }
-  - match: { _source.field4: "1/2/3" }
-  - match: { _source.metadata: ["0","1","2","3"] }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_2"
-        body: {
-          field1: "field2"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.field1: "field2" }
-  - match: { _source.field2: "value" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_3"
-        body: {
-          field_to_remove: "field2",
-          field2: "2",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field_to_remove: "field2" }
-
----
-"Test on_failure metadata context templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_handled_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "tag" : "first_processor",
-                  "field" : "field_to_remove",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "error",
-                        "value" : "processor {{ _ingest.on_failure_processor_tag }} [{{ _ingest.on_failure_processor_type }}]: {{ _ingest.on_failure_message }}"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_handled_pipeline"
-        body: {
-          do_nothing: "foo",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.do_nothing: "foo" }
-  - match: { _source.error: "processor first_processor [remove]: field [field_to_remove] not present as part of path [field_to_remove]" }
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
index e9a6ee0..97e328f 100644
--- a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -77,7 +77,7 @@ public abstract class ESSmokeClientTestCase extends LuceneTestCase {
 
     private static Client startClient(Path tempDir, TransportAddress... transportAddresses) {
         Settings clientSettings = Settings.settingsBuilder()
-                .put("node.name", "qa_smoke_client_" + counter.getAndIncrement())
+                .put("name", "qa_smoke_client_" + counter.getAndIncrement())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING.getKey(), true) // prevents any settings to be replaced by system properties.
                 .put("client.transport.ignore_cluster_name", true)
                 .put(Environment.PATH_HOME_SETTING.getKey(), tempDir)
diff --git a/qa/smoke-test-ingest-disabled/build.gradle b/qa/smoke-test-ingest-disabled/build.gradle
new file mode 100644
index 0000000..ca71697
--- /dev/null
+++ b/qa/smoke-test-ingest-disabled/build.gradle
@@ -0,0 +1,26 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+apply plugin: 'elasticsearch.rest-test'
+
+integTest {
+    cluster {
+        systemProperty 'es.node.ingest', 'false'
+    }
+}
diff --git a/qa/smoke-test-ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java b/qa/smoke-test-ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
new file mode 100644
index 0000000..e162807
--- /dev/null
+++ b/qa/smoke-test-ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class IngestDisabledIT extends ESRestTestCase {
+
+    public IngestDisabledIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+
+}
diff --git a/qa/smoke-test-ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml b/qa/smoke-test-ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
new file mode 100644
index 0000000..01d6740
--- /dev/null
+++ b/qa/smoke-test-ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
@@ -0,0 +1,122 @@
+---
+"Test ingest CRUD APIS work fine when node.ingest is set to false":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value": "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.get_pipeline:
+        id: "my_pipeline"
+  - match: { pipelines.0.id: "my_pipeline" }
+  - match: { pipelines.0.config.description: "_description" }
+
+  - do:
+      ingest.delete_pipeline:
+        id: "my_pipeline"
+  - match: { acknowledged: true }
+
+---
+"Test ingest simulate API works fine when node.ingest is set to false":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value" : "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.simulate:
+        id: "my_pipeline"
+        body: >
+          {
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - match: { docs.0.doc._source.foo: "bar" }
+  - match: { docs.0.doc._source.field2: "_value" }
+  - length: { docs.0.doc._ingest: 1 }
+  - is_true: docs.0.doc._ingest.timestamp
+
+---
+"Test index api with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {
+          field1: "1",
+          field2: "2",
+          field3: "3"
+        }
+
+---
+"Test bulk api with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      bulk:
+        pipeline: "my_pipeline_1"
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+          - f1: v2
+
+---
+"Test bulk api that contains a single index call with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      bulk:
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+              pipeline: my_pipeline_1
+          - f1: v2
+
diff --git a/qa/smoke-test-ingest-with-all-dependencies/build.gradle b/qa/smoke-test-ingest-with-all-dependencies/build.gradle
new file mode 100644
index 0000000..118e36d
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/build.gradle
@@ -0,0 +1,26 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+apply plugin: 'elasticsearch.rest-test'
+
+dependencies {
+    testCompile project(path: ':modules:ingest-grok', configuration: 'runtime')
+    testCompile project(path: ':plugins:ingest-geoip', configuration: 'runtime')
+    testCompile project(path: ':modules:lang-mustache', configuration: 'runtime')
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
new file mode 100644
index 0000000..49e9964
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.script.ScriptContextRegistry;
+import org.elasticsearch.script.ScriptEngineRegistry;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptSettings;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+
+public abstract class AbstractMustacheTests extends ESTestCase {
+
+    protected TemplateService templateService;
+
+    @Before
+    public void init() throws Exception {
+        Settings settings = Settings.builder()
+            .put("path.home", createTempDir())
+            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING.getKey(), false)
+            .build();
+        MustacheScriptEngineService mustache = new MustacheScriptEngineService(settings);
+        ScriptEngineRegistry scriptEngineRegistry =
+            new ScriptEngineRegistry(Collections.singletonList(new ScriptEngineRegistry.ScriptEngineRegistration(MustacheScriptEngineService.class, MustacheScriptEngineService.TYPES)));
+        ScriptContextRegistry scriptContextRegistry = new ScriptContextRegistry(Collections.emptyList());
+        ScriptSettings scriptSettings = new ScriptSettings(scriptEngineRegistry, scriptContextRegistry);
+        ScriptService scriptService =
+            new ScriptService(settings, new Environment(settings), Collections.singleton(mustache), null, scriptEngineRegistry, scriptContextRegistry, scriptSettings);
+        templateService = new InternalTemplateService(scriptService);
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/CombineProcessorsTests.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/CombineProcessorsTests.java
new file mode 100644
index 0000000..0245233
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/CombineProcessorsTests.java
@@ -0,0 +1,209 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import com.maxmind.geoip2.DatabaseReader;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.collect.HppcMaps;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.support.XContentMapValues;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.geoip.GeoIpProcessor;
+import org.elasticsearch.ingest.geoip.IngestGeoIpPlugin;
+import org.elasticsearch.ingest.grok.GrokProcessor;
+import org.elasticsearch.ingest.grok.IngestGrokPlugin;
+import org.elasticsearch.ingest.processor.AppendProcessor;
+import org.elasticsearch.ingest.processor.ConvertProcessor;
+import org.elasticsearch.ingest.processor.DateProcessor;
+import org.elasticsearch.ingest.processor.LowercaseProcessor;
+import org.elasticsearch.ingest.processor.RemoveProcessor;
+import org.elasticsearch.ingest.processor.RenameProcessor;
+import org.elasticsearch.ingest.processor.SplitProcessor;
+import org.elasticsearch.ingest.processor.TrimProcessor;
+import org.elasticsearch.ingest.processor.UppercaseProcessor;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.StreamsUtils;
+
+import java.io.ByteArrayInputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+
+public class CombineProcessorsTests extends ESTestCase {
+
+    private static final String LOG = "70.193.17.92 - - [08/Sep/2014:02:54:42 +0000] \"GET /presentations/logstash-scale11x/images/ahhh___rage_face_by_samusmmx-d5g5zap.png HTTP/1.1\" 200 175208 \"http://mobile.rivals.com/board_posts.asp?SID=880&mid=198829575&fid=2208&tid=198829575&Team=&TeamId=&SiteId=\" \"Mozilla/5.0 (Linux; Android 4.2.2; VS980 4G Build/JDQ39B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.135 Mobile Safari/537.36\"";
+
+    public void testLogging() throws Exception {
+        Path configDir = createTempDir();
+        Path geoIpConfigDir = configDir.resolve("ingest-geoip");
+        Files.createDirectories(geoIpConfigDir);
+        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-City.mmdb")), geoIpConfigDir.resolve("GeoLite2-City.mmdb"));
+        Map<String, DatabaseReader> databaseReaders = IngestGeoIpPlugin.loadDatabaseReaders(geoIpConfigDir);
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "log");
+        config.put("pattern", "%{COMBINEDAPACHELOG}");
+        Processor processor1 = new GrokProcessor.Factory(IngestGrokPlugin.loadBuiltinPatterns()).doCreate(null, config);
+        config = new HashMap<>();
+        config.put("field", "response");
+        config.put("type", "integer");
+        Processor processor2 = new ConvertProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("field", "bytes");
+        config.put("type", "integer");
+        Processor processor3 = new ConvertProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("match_field", "timestamp");
+        config.put("target_field", "timestamp");
+        config.put("match_formats", Arrays.asList("dd/MMM/YYYY:HH:mm:ss Z"));
+        Processor processor4 = new DateProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("source_field", "clientip");
+        Processor processor5 = new GeoIpProcessor.Factory(databaseReaders).create(config);
+
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor1, processor2, processor3, processor4, processor5));
+
+        Map<String, Object> source = new HashMap<>();
+        source.put("log", LOG);
+        IngestDocument document = new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
+        pipeline.execute(document);
+
+        assertThat(document.getSourceAndMetadata().size(), equalTo(17));
+        assertThat(document.getSourceAndMetadata().get("request"), equalTo("/presentations/logstash-scale11x/images/ahhh___rage_face_by_samusmmx-d5g5zap.png"));
+        assertThat(document.getSourceAndMetadata().get("agent"), equalTo("\"Mozilla/5.0 (Linux; Android 4.2.2; VS980 4G Build/JDQ39B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.135 Mobile Safari/537.36\""));
+        assertThat(document.getSourceAndMetadata().get("auth"), equalTo("-"));
+        assertThat(document.getSourceAndMetadata().get("ident"), equalTo("-"));
+        assertThat(document.getSourceAndMetadata().get("verb"), equalTo("GET"));
+        assertThat(document.getSourceAndMetadata().get("referrer"), equalTo("\"http://mobile.rivals.com/board_posts.asp?SID=880&mid=198829575&fid=2208&tid=198829575&Team=&TeamId=&SiteId=\""));
+        assertThat(document.getSourceAndMetadata().get("response"), equalTo(200));
+        assertThat(document.getSourceAndMetadata().get("bytes"), equalTo(175208));
+        assertThat(document.getSourceAndMetadata().get("clientip"), equalTo("70.193.17.92"));
+        assertThat(document.getSourceAndMetadata().get("httpversion"), equalTo("1.1"));
+        assertThat(document.getSourceAndMetadata().get("rawrequest"), nullValue());
+        assertThat(document.getSourceAndMetadata().get("timestamp"), equalTo("2014-09-08T02:54:42.000Z"));
+        Map<String, Object> geoInfo = (Map<String, Object>) document.getSourceAndMetadata().get("geoip");
+        assertThat(geoInfo.size(), equalTo(5));
+        assertThat(geoInfo.get("continent_name"), equalTo("North America"));
+        assertThat(geoInfo.get("city_name"), equalTo("Charlotte"));
+        assertThat(geoInfo.get("country_iso_code"), equalTo("US"));
+        assertThat(geoInfo.get("region_name"), equalTo("North Carolina"));
+        assertThat(geoInfo.get("location"), notNullValue());
+    }
+
+    private static final String PERSON = "{\n" +
+        "    \"age\": 33,\n" +
+        "    \"eyeColor\": \"brown\",\n" +
+        "    \"name\": \"Miranda Goodwin\",\n" +
+        "    \"gender\": \"male\",\n" +
+        "    \"company\": \"ATGEN\",\n" +
+        "    \"email\": \"mirandagoodwin@atgen.com\",\n" +
+        "    \"phone\": \"+1 (914) 489-3656\",\n" +
+        "    \"address\": \"713 Bartlett Place, Accoville, Puerto Rico, 9221\",\n" +
+        "    \"registered\": \"2014-11-23T08:34:21 -01:00\",\n" +
+        "    \"tags\": [\n" +
+        "      \"ex\",\n" +
+        "      \"do\",\n" +
+        "      \"occaecat\",\n" +
+        "      \"reprehenderit\",\n" +
+        "      \"anim\",\n" +
+        "      \"laboris\",\n" +
+        "      \"cillum\"\n" +
+        "    ],\n" +
+        "    \"friends\": [\n" +
+        "      {\n" +
+        "        \"id\": 0,\n" +
+        "        \"name\": \"Wendi Odonnell\"\n" +
+        "      },\n" +
+        "      {\n" +
+        "        \"id\": 1,\n" +
+        "        \"name\": \"Mayra Boyd\"\n" +
+        "      },\n" +
+        "      {\n" +
+        "        \"id\": 2,\n" +
+        "        \"name\": \"Lee Gonzalez\"\n" +
+        "      }\n" +
+        "    ]\n" +
+        "  }";
+
+    @SuppressWarnings("unchecked")
+    public void testMutate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        // TODO: when we add foreach processor we should delete all friends.id fields
+        config.put("field", "friends.0.id");
+        RemoveProcessor processor1 = new RemoveProcessor.Factory(TestTemplateService.instance()).create(config);
+        config = new HashMap<>();
+        config.put("field", "tags");
+        config.put("value", "new_value");
+        AppendProcessor processor2 = new AppendProcessor.Factory(TestTemplateService.instance()).create(config);
+        config = new HashMap<>();
+        config.put("field", "address");
+        config.put("separator", ",");
+        SplitProcessor processor3 = new SplitProcessor.Factory().create(config);
+        config = new HashMap<>();
+        // TODO: when we add foreach processor, then change the test to trim all address values
+        config.put("field", "address.1");
+        TrimProcessor processor4 = new TrimProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("field", "company");
+        LowercaseProcessor processor5 = new LowercaseProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("field", "gender");
+        UppercaseProcessor processor6 = new UppercaseProcessor.Factory().create(config);
+        config = new HashMap<>();
+        config.put("field", "eyeColor");
+        config.put("to", "eye_color");
+        RenameProcessor processor7 = new RenameProcessor.Factory().create(config);
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(
+            processor1, processor2, processor3, processor4, processor5, processor6, processor7
+        ));
+
+        Map<String, Object> source = XContentHelper.createParser(new BytesArray(PERSON)).map();
+        IngestDocument document = new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
+        pipeline.execute(document);
+
+        assertThat(((List<Map<String, Object>>) document.getSourceAndMetadata().get("friends")).get(0).get("id"), nullValue());
+        assertThat(((List<Map<String, Object>>) document.getSourceAndMetadata().get("friends")).get(1).get("id"), equalTo(1));
+        assertThat(((List<Map<String, Object>>) document.getSourceAndMetadata().get("friends")).get(2).get("id"), equalTo(2));
+        assertThat(document.getFieldValue("tags.7", String.class), equalTo("new_value"));
+
+        List<String> addressDetails = document.getFieldValue("address", List.class);
+        assertThat(addressDetails.size(), equalTo(4));
+        assertThat(addressDetails.get(0), equalTo("713 Bartlett Place"));
+        assertThat(addressDetails.get(1), equalTo("Accoville"));
+        assertThat(addressDetails.get(2), equalTo(" Puerto Rico"));
+        assertThat(addressDetails.get(3), equalTo(" 9221"));
+
+        assertThat(document.getSourceAndMetadata().get("company"), equalTo("atgen"));
+        assertThat(document.getSourceAndMetadata().get("gender"), equalTo("MALE"));
+        assertThat(document.getSourceAndMetadata().get("eye_color"), equalTo("brown"));
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
new file mode 100644
index 0000000..f27a8e4
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class IngestDocumentMustacheIT extends AbstractMustacheTests {
+
+    public void testAccessMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        document.put("foo", "bar");
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 bar"));
+
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 bar"));
+    }
+
+    public void testAccessMapMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        Map<String, Object> innerObject = new HashMap<>();
+        innerObject.put("bar", "hello bar");
+        innerObject.put("baz", "hello baz");
+        innerObject.put("qux", Collections.singletonMap("fubar", "hello qux and fubar"));
+        document.put("foo", innerObject);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo.bar}} {{foo.baz}} {{foo.qux.fubar}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 hello bar hello baz hello qux and fubar"));
+
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo.bar}} {{_source.foo.baz}} {{_source.foo.qux.fubar}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 hello bar hello baz hello qux and fubar"));
+    }
+
+    public void testAccessListMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        document.put("list1", Arrays.asList("foo", "bar", null));
+        List<Map<String, Object>> list = new ArrayList<>();
+        Map<String, Object> value = new HashMap<>();
+        value.put("field", "value");
+        list.add(value);
+        list.add(null);
+        document.put("list2", list);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{list1.0}} {{list2.0}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 foo {field=value}"));
+    }
+
+    public void testAccessIngestMetadataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        Map<String, Object> ingestMap = new HashMap<>();
+        ingestMap.put("timestamp", "bogus_timestamp");
+        document.put("_ingest", ingestMap);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("ingest_timestamp"), ValueSource.wrap("{{_ingest.timestamp}} and {{_source._ingest.timestamp}}", templateService));
+        assertThat(ingestDocument.getFieldValue("ingest_timestamp", String.class), equalTo(ingestDocument.getIngestMetadata().get("timestamp") + " and bogus_timestamp"));
+    }
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
new file mode 100644
index 0000000..e94765a
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.processor.RemoveProcessor;
+import org.hamcrest.CoreMatchers;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+public class IngestMustacheRemoveProcessorIT extends AbstractMustacheTests {
+
+    public void testRemoveProcessorMustacheExpression() throws Exception {
+        RemoveProcessor.Factory factory = new RemoveProcessor.Factory(templateService);
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field{{var}}");
+        RemoveProcessor processor = factory.create(config);
+        assertThat(processor.getField().execute(Collections.singletonMap("var", "_value")), CoreMatchers.equalTo("field_value"));
+    }
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
new file mode 100644
index 0000000..6846679
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.processor.SetProcessor;
+import org.hamcrest.Matchers;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.instanceOf;
+
+public class IngestMustacheSetProcessorIT extends AbstractMustacheTests {
+
+    public void testExpression() throws Exception {
+        SetProcessor processor = createSetProcessor("_index", "text {{var}}");
+        assertThat(processor.getValue(), instanceOf(ValueSource.TemplatedValue.class));
+        assertThat(processor.getValue().copyAndResolve(Collections.singletonMap("var", "_value")), equalTo("text _value"));
+    }
+
+    public void testSetMetadataWithTemplates() throws Exception {
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
+        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value {{field}}");
+        IngestDocument ingestDocument = createIngestDocument(Collections.singletonMap("field", "value"));
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value value"));
+    }
+
+    public void testSetWithTemplates() throws Exception {
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.INDEX, IngestDocument.MetaData.TYPE, IngestDocument.MetaData.ID);
+        Processor processor = createSetProcessor("field{{_type}}", "_value {{" + randomMetaData.getFieldName() + "}}");
+        IngestDocument ingestDocument = createIngestDocument(new HashMap<>());
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("field_type", String.class), Matchers.equalTo("_value " + ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class)));
+    }
+
+    private SetProcessor createSetProcessor(String fieldName, Object fieldValue) throws Exception {
+        SetProcessor.Factory factory = new SetProcessor.Factory(templateService);
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", fieldName);
+        config.put("value", fieldValue);
+        return factory.create(config);
+    }
+
+    private IngestDocument createIngestDocument(Map<String, Object> source) {
+        return new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
new file mode 100644
index 0000000..1d1579f
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.TemplateService;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class TemplateServiceIT extends AbstractMustacheTests {
+
+    public void testTemplates() {
+        Map<String, Object> model = new HashMap<>();
+        model.put("fielda", "value1");
+        model.put("fieldb", Collections.singletonMap("fieldc", "value3"));
+
+        TemplateService.Template template = templateService.compile("{{fielda}}/{{fieldb}}/{{fieldb.fieldc}}");
+        assertThat(template.execute(model), equalTo("value1/{fieldc=value3}/value3"));
+    }
+
+    public void testWrongTemplateUsage() {
+        Map<String, Object> model = Collections.emptyMap();
+        TemplateService.Template template = templateService.compile("value");
+        assertThat(template.execute(model), equalTo("value"));
+
+        template = templateService.compile("value {{");
+        assertThat(template.execute(model), equalTo("value {{"));
+        template = templateService.compile("value {{abc");
+        assertThat(template.execute(model), equalTo("value {{abc"));
+        template = templateService.compile("value }}");
+        assertThat(template.execute(model), equalTo("value }}"));
+        template = templateService.compile("value }} {{");
+        assertThat(template.execute(model), equalTo("value }} {{"));
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
new file mode 100644
index 0000000..18085b94
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.is;
+
+public class ValueSourceMustacheIT extends AbstractMustacheTests {
+
+    public void testValueSourceWithTemplates() {
+        Map<String, Object> model = new HashMap<>();
+        model.put("field1", "value1");
+        model.put("field2", Collections.singletonMap("field3", "value3"));
+
+        ValueSource valueSource = ValueSource.wrap("{{field1}}/{{field2}}/{{field2.field3}}", templateService);
+        assertThat(valueSource, instanceOf(ValueSource.TemplatedValue.class));
+        assertThat(valueSource.copyAndResolve(model), equalTo("value1/{field3=value3}/value3"));
+
+        valueSource = ValueSource.wrap(Arrays.asList("_value", "{{field1}}"), templateService);
+        assertThat(valueSource, instanceOf(ValueSource.ListValue.class));
+        List<String> result = (List<String>) valueSource.copyAndResolve(model);
+        assertThat(result.size(), equalTo(2));
+        assertThat(result.get(0), equalTo("_value"));
+        assertThat(result.get(1), equalTo("value1"));
+
+        Map<String, Object> map = new HashMap<>();
+        map.put("field1", "{{field1}}");
+        map.put("field2", Collections.singletonMap("field3", "{{field2.field3}}"));
+        map.put("field4", "_value");
+        valueSource = ValueSource.wrap(map, templateService);
+        assertThat(valueSource, instanceOf(ValueSource.MapValue.class));
+        Map<String, Object> resultMap = (Map<String, Object>) valueSource.copyAndResolve(model);
+        assertThat(resultMap.size(), equalTo(3));
+        assertThat(resultMap.get("field1"), equalTo("value1"));
+        assertThat(((Map) resultMap.get("field2")).size(), equalTo(1));
+        assertThat(((Map) resultMap.get("field2")).get("field3"), equalTo("value3"));
+        assertThat(resultMap.get("field4"), equalTo("_value"));
+    }
+
+    public void testAccessSourceViaTemplate() {
+        IngestDocument ingestDocument = new IngestDocument("marvel", "type", "id", null, null, null, null, new HashMap<>());
+        assertThat(ingestDocument.hasField("marvel"), is(false));
+        ingestDocument.setFieldValue(templateService.compile("{{_index}}"), ValueSource.wrap("{{_index}}", templateService));
+        assertThat(ingestDocument.getFieldValue("marvel", String.class), equalTo("marvel"));
+        ingestDocument.removeField(templateService.compile("{{marvel}}"));
+        assertThat(ingestDocument.hasField("index"), is(false));
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
new file mode 100644
index 0000000..73f64d4
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class IngestWithMustacheIT extends ESRestTestCase {
+
+    public IngestWithMustacheIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+
+}
diff --git a/qa/smoke-test-ingest-with-all-dependencies/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml b/qa/smoke-test-ingest-with-all-dependencies/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
new file mode 100644
index 0000000..491e1da
--- /dev/null
+++ b/qa/smoke-test-ingest-with-all-dependencies/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
@@ -0,0 +1,221 @@
+---
+"Test metadata templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_1"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "index_type_id",
+                  "value": "{{_index}}/{{_type}}/{{_id}}"
+                }
+              },
+              {
+                "append" : {
+                  "field" : "metadata",
+                  "value": ["{{_index}}", "{{_type}}", "{{_id}}"]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.index_type_id: "test/test/1" }
+  - match: { _source.metadata: ["test", "test", "1"] }
+
+---
+"Test templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_1"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field4",
+                  "value": "{{field1}}/{{field2}}/{{field3}}"
+                }
+              },
+              {
+                "append" : {
+                  "field" : "metadata",
+                  "value": ["{{field1}}", "{{field2}}", "{{field3}}"]
+                }
+              }
+
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_2"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "{{field1}}",
+                  "value": "value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_3"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "remove" : {
+                  "field" : "{{field_to_remove}}"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {
+          metadata: "0",
+          field1: "1",
+          field2: "2",
+          field3: "3"
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 5 }
+  - match: { _source.field1: "1" }
+  - match: { _source.field2: "2" }
+  - match: { _source.field3: "3" }
+  - match: { _source.field4: "1/2/3" }
+  - match: { _source.metadata: ["0","1","2","3"] }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_2"
+        body: {
+          field1: "field2"
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.field1: "field2" }
+  - match: { _source.field2: "value" }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_3"
+        body: {
+          field_to_remove: "field2",
+          field2: "2",
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 1 }
+  - match: { _source.field_to_remove: "field2" }
+
+---
+"Test on_failure metadata context templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_handled_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "remove" : {
+                  "tag" : "first_processor",
+                  "field" : "field_to_remove",
+                  "on_failure" : [
+                    {
+                      "set" : {
+                        "field" : "error",
+                        "value" : "processor {{ _ingest.on_failure_processor_tag }} [{{ _ingest.on_failure_processor_type }}]: {{ _ingest.on_failure_message }}"
+                      }
+                    }
+                  ]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_handled_pipeline"
+        body: {
+          do_nothing: "foo",
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.do_nothing: "foo" }
+  - match: { _source.error: "processor first_processor [remove]: field [field_to_remove] not present as part of path [field_to_remove]" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
index 37a04cb..20fc352 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/update.json
@@ -82,10 +82,6 @@
           "type": "enum",
           "options": ["internal", "force"],
           "description": "Specific version type"
-        },
-        "detect_noop": {
-          "type": "boolean",
-          "description": "Specifying as true will cause Elasticsearch to check if there are changes and, if there arent, turn the update request into a noop."
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
index 7480833..c9d7fd4 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
@@ -53,6 +53,7 @@
 ---
 "Test invalid processor config":
   - do:
+      catch: request
       ingest.put_pipeline:
         id: "my_pipeline"
         body:  >
@@ -66,12 +67,11 @@
               }
             ]
           }
-  - match: { "acknowledged": false }
-  - length: { "error": 4 }
-  - match: { "error.reason": "[field] required property is missing" }
-  - match: { "error.property_name": "field" }
-  - match: { "error.type": "set" }
-  - match: { "error.tag": "fritag" }
+  - match: { error.root_cause.0.type: "parse_exception" }
+  - match: { error.root_cause.0.reason: "[field] required property is missing" }
+  - match: { error.root_cause.0.header.processor_tag: "fritag" }
+  - match: { error.root_cause.0.header.processor_type: "set" }
+  - match: { error.root_cause.0.header.property_name: "field" }
 
 ---
 "Test basic pipeline with on_failure in processor":
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
index 92fad24..288806a 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
@@ -98,11 +98,11 @@
               }
             ]
           }
-  - length: { error: 4 }
-  - match: { error.tag: "fails" }
-  - match: { error.type: "set" }
-  - match: { error.reason: "[field] required property is missing" }
-  - match: { error.property_name: "field" }
+  - match: { error.root_cause.0.type: "parse_exception" }
+  - match: { error.root_cause.0.reason: "[field] required property is missing" }
+  - match: { error.root_cause.0.header.processor_tag: "fails" }
+  - match: { error.root_cause.0.header.processor_type: "set" }
+  - match: { error.root_cause.0.header.property_name: "field" }
 
 ---
 "Test simulate without index type and id":
@@ -191,10 +191,9 @@
               }
             ]
           }
-  - length: { error: 4 }
-  - is_false: error.processor_type
-  - is_false: error.processor_tag
-  - match: { error.property_name: "pipeline" }
+  - is_false: error.root_cause.0.header.processor_type
+  - is_false: error.root_cause.0.header.processor_tag
+  - match: { error.root_cause.0.header.property_name: "pipeline" }
   - match: { error.reason: "[pipeline] required property is missing" }
 
 ---
@@ -225,11 +224,11 @@
               }
             ]
           }
-  - length: { error: 4 }
-  - match: { error.type: "set" }
-  - is_false: error.tag
-  - match: { error.reason: "[value] required property is missing" }
-  - match: { error.property_name: "value" }
+  - match: { error.root_cause.0.type: "parse_exception" }
+  - match: { error.root_cause.0.reason: "[value] required property is missing" }
+  - match: { error.root_cause.0.header.processor_type: "set" }
+  - match: { error.root_cause.0.header.property_name: "value" }
+  - is_false: error.root_cause.0.header.processor_tag
 
 ---
 "Test simulate with verbose flag":
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
deleted file mode 100644
index bdc6457..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
+++ /dev/null
@@ -1,64 +0,0 @@
----
-"Test De-Dot Processor With Provided Separator":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "dedot" : {
-                  "separator" : "3"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {"a.b.c": "hello world"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.a3b3c: "hello world" }
-
----
-"Test De-Dot Processor With Default Separator":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "dedot" : {
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {"a.b.c": "hello world"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.a_b_c: "hello world" }
diff --git a/settings.gradle b/settings.gradle
index f7a7fc7..df2ce16 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -40,8 +40,8 @@ List projects = [
   'qa:smoke-test-client',
   'qa:smoke-test-multinode',
   'qa:smoke-test-plugins',
-  'qa:ingest-with-mustache',
-  'qa:ingest-disabled',
+  'qa:smoke-test-ingest-with-all-dependencies',
+  'qa:smoke-test-ingest-disabled',
   'qa:vagrant',
 ]
 
diff --git a/test/framework/src/main/java/org/elasticsearch/common/cli/CliToolTestCase.java b/test/framework/src/main/java/org/elasticsearch/common/cli/CliToolTestCase.java
index 5dfaef1..35c0897 100644
--- a/test/framework/src/main/java/org/elasticsearch/common/cli/CliToolTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/common/cli/CliToolTestCase.java
@@ -28,11 +28,8 @@ import org.junit.After;
 import org.junit.Before;
 
 import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.Writer;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Locale;
 
 import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.greaterThan;
@@ -73,7 +70,7 @@ public abstract class CliToolTestCase extends ESTestCase {
         }
 
         @Override
-        protected void doPrint(String msg, Object... args) {
+        protected void doPrint(String msg) {
         }
 
         @Override
@@ -87,7 +84,7 @@ public abstract class CliToolTestCase extends ESTestCase {
         }
 
         @Override
-        public void print(String msg, Object... args) {
+        public void print(String msg) {
         }
 
         @Override
@@ -99,7 +96,7 @@ public abstract class CliToolTestCase extends ESTestCase {
      */
     public static class CaptureOutputTerminal extends MockTerminal {
 
-        List<String> terminalOutput = new ArrayList();
+        List<String> terminalOutput = new ArrayList<>();
 
         public CaptureOutputTerminal() {
             super(Verbosity.NORMAL);
@@ -110,13 +107,13 @@ public abstract class CliToolTestCase extends ESTestCase {
         }
 
         @Override
-        protected void doPrint(String msg, Object... args) {
-            terminalOutput.add(String.format(Locale.ROOT, msg, args));
+        protected void doPrint(String msg) {
+            terminalOutput.add(msg);
         }
 
         @Override
-        public void print(String msg, Object... args) {
-            doPrint(msg, args);
+        public void print(String msg) {
+            doPrint(msg);
         }
 
         @Override
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
index cddf463..3950864 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -1435,11 +1435,8 @@ public abstract class ESIntegTestCase extends ESTestCase {
         if (!bogusIds.isEmpty()) {
             // delete the bogus types again - it might trigger merges or at least holes in the segments and enforces deleted docs!
             for (Tuple<String, String> doc : bogusIds) {
-                // see https://github.com/elasticsearch/elasticsearch/issues/8706
-                final DeleteResponse deleteResponse = client().prepareDelete(doc.v1(), RANDOM_BOGUS_TYPE, doc.v2()).get();
-                if (deleteResponse.isFound() == false) {
-                    logger.warn("failed to delete a dummy doc [{}][{}]", doc.v1(), doc.v2());
-                }
+                assertTrue("failed to delete a dummy doc [" + doc.v1() + "][" + doc.v2() + "]",
+                    client().prepareDelete(doc.v1(), RANDOM_BOGUS_TYPE, doc.v2()).get().isFound());
             }
         }
         if (forceRefresh) {
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
index 67cae85..e80bb93 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
@@ -382,9 +382,18 @@ public abstract class ESTestCase extends LuceneTestCase {
         return generateRandomStringArray(maxArraySize, maxStringSize, allowNull, true);
     }
 
+    private static String[] TIME_SUFFIXES = new String[]{"d", "H", "ms", "s", "S", "w"};
+
+    private static String randomTimeValue(int lower, int upper) {
+        return randomIntBetween(lower, upper) + randomFrom(TIME_SUFFIXES);
+    }
+
     public static String randomTimeValue() {
-        final String[] values = new String[]{"d", "H", "ms", "s", "S", "w"};
-        return randomIntBetween(0, 1000) + randomFrom(values);
+        return randomTimeValue(0, 1000);
+    }
+
+    public static String randomPositiveTimeValue() {
+        return randomTimeValue(1, 1000);
     }
 
     /**
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java b/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
index ea106bf..1a7164f 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
@@ -194,7 +194,7 @@ final class ExternalNode implements Closeable {
 
             Settings clientSettings = settingsBuilder().put(externalNodeSettings)
                     .put("client.transport.nodes_sampler_interval", "1s")
-                    .put("node.name", "transport_client_" + nodeInfo.getNode().name())
+                    .put("name", "transport_client_" + nodeInfo.getNode().name())
                     .put(ClusterName.CLUSTER_NAME_SETTING.getKey(), clusterName).put("client.transport.sniff", false).build();
             TransportClient client = TransportClient.builder().settings(clientSettings).build();
             client.addTransportAddress(addr);
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java b/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
index c997a41..21cf79f 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
@@ -73,7 +73,7 @@ public final class ExternalTestCluster extends TestCluster {
         super(0);
         Settings clientSettings = Settings.settingsBuilder()
                 .put(additionalSettings)
-                .put("node.name", InternalTestCluster.TRANSPORT_CLIENT_PREFIX + EXTERNAL_CLUSTER_PREFIX + counter.getAndIncrement())
+                .put("name", InternalTestCluster.TRANSPORT_CLIENT_PREFIX + EXTERNAL_CLUSTER_PREFIX + counter.getAndIncrement())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING.getKey(), true) // prevents any settings to be replaced by system properties.
                 .put("client.transport.ignore_cluster_name", true)
                 .put(Environment.PATH_HOME_SETTING.getKey(), tempDir)
diff --git a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
index 61a58c5..d9f634d 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
@@ -588,7 +588,7 @@ public final class InternalTestCluster extends TestCluster {
         Settings finalSettings = settingsBuilder()
                 .put(Environment.PATH_HOME_SETTING.getKey(), baseDir) // allow overriding path.home
                 .put(settings)
-                .put("node.name", name)
+                .put("name", name)
                 .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), seed)
                 .build();
         MockNode node = new MockNode(finalSettings, version, plugins);
@@ -768,7 +768,7 @@ public final class InternalTestCluster extends TestCluster {
             double nextDouble = random.nextDouble();
             if (nextDouble < transportClientRatio) {
                 if (logger.isTraceEnabled()) {
-                    logger.trace("Using transport client for node [{}] sniff: [{}]", node.settings().get("node.name"), false);
+                    logger.trace("Using transport client for node [{}] sniff: [{}]", node.settings().get("name"), false);
                 }
                 return getOrBuildTransportClient();
             } else {
@@ -883,7 +883,7 @@ public final class InternalTestCluster extends TestCluster {
             Builder builder = settingsBuilder()
                     .put("client.transport.nodes_sampler_interval", "1s")
                     .put(Environment.PATH_HOME_SETTING.getKey(), baseDir)
-                    .put("node.name", TRANSPORT_CLIENT_PREFIX + node.settings().get("node.name"))
+                    .put("name", TRANSPORT_CLIENT_PREFIX + node.settings().get("name"))
                     .put(ClusterName.CLUSTER_NAME_SETTING.getKey(), clusterName).put("client.transport.sniff", sniff)
                     .put(Node.NODE_MODE_SETTING.getKey(), Node.NODE_MODE_SETTING.exists(nodeSettings) ? Node.NODE_MODE_SETTING.get(nodeSettings) : nodeMode)
                     .put("logger.prefix", nodeSettings.get("logger.prefix", ""))
@@ -1763,7 +1763,7 @@ public final class InternalTestCluster extends TestCluster {
 
         @Override
         public boolean test(Settings settings) {
-            return nodeNames.contains(settings.get("node.name"));
+            return nodeNames.contains(settings.get("name"));
 
         }
     }
diff --git a/test/framework/src/main/java/org/elasticsearch/test/tasks/MockTaskManager.java b/test/framework/src/main/java/org/elasticsearch/test/tasks/MockTaskManager.java
index 4c48f99..9b6bc72 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/tasks/MockTaskManager.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/tasks/MockTaskManager.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.test.tasks;
 
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.tasks.Task;
 import org.elasticsearch.tasks.TaskManager;
@@ -33,7 +32,7 @@ import java.util.concurrent.CopyOnWriteArrayList;
  */
 public class MockTaskManager extends TaskManager {
 
-    public static final Setting<Boolean> USE_MOCK_TASK_MANAGER_SETTING = Setting.boolSetting("tests.mock.taskmanager.enabled", false, false, Setting.Scope.CLUSTER);
+    public static final String USE_MOCK_TASK_MANAGER = "tests.mock.taskmanager.enabled";
 
     private final Collection<MockTaskManagerListener> listeners = new CopyOnWriteArrayList<>();
 
diff --git a/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index fb31023..9e8d7a4 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -26,7 +26,6 @@ import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.VersionUtils;
@@ -60,11 +59,6 @@ public class AssertingLocalTransport extends LocalTransport {
         public Settings additionalSettings() {
             return Settings.builder().put(NetworkModule.TRANSPORT_TYPE_KEY, "mock").build();
         }
-
-        public void onModule(SettingsModule module) {
-            module.registerSetting(ASSERTING_TRANSPORT_MIN_VERSION_KEY);
-            module.registerSetting(ASSERTING_TRANSPORT_MAX_VERSION_KEY);
-        }
     }
 
     public static final Setting<Version> ASSERTING_TRANSPORT_MIN_VERSION_KEY = new Setting<>("transport.asserting.version.min",
diff --git a/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java b/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
index 41a83a0..84c981d 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
@@ -28,7 +28,6 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
 import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
@@ -81,10 +80,6 @@ public class MockTransportService extends TransportService {
         public void onModule(NetworkModule module) {
             module.registerTransportService("mock", MockTransportService.class);
         }
-
-        public void onModule(SettingsModule module) {
-            module.registerSetting(MockTaskManager.USE_MOCK_TASK_MANAGER_SETTING);
-        }
         @Override
         public Settings additionalSettings() {
             return Settings.builder().put(NetworkModule.TRANSPORT_SERVICE_TYPE_KEY, "mock").build();
@@ -109,7 +104,7 @@ public class MockTransportService extends TransportService {
 
     @Override
     protected TaskManager createTaskManager() {
-        if (MockTaskManager.USE_MOCK_TASK_MANAGER_SETTING.get(settings)) {
+        if (settings.getAsBoolean(MockTaskManager.USE_MOCK_TASK_MANAGER, false)) {
             return new MockTaskManager(settings);
          } else {
             return super.createTaskManager();
