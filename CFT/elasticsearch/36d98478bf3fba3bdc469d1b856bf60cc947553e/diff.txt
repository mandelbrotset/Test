diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
index 042e8d2..67d0e16 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
@@ -112,9 +112,6 @@ public class PluginBuildPlugin extends BuildPlugin {
                 include 'config/**'
                 include 'bin/**'
             }
-            from('src/site') {
-                include '_site/**'
-            }
         }
         project.assemble.dependsOn(bundle)
 
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesExtension.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesExtension.groovy
index dd5bcae..7b949b3 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesExtension.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesExtension.groovy
@@ -37,15 +37,9 @@ class PluginPropertiesExtension {
     String description
 
     @Input
-    boolean jvm = true
-
-    @Input
     String classname
 
     @Input
-    boolean site = false
-
-    @Input
     boolean isolated = true
 
     PluginPropertiesExtension(Project project) {
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesTask.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesTask.groovy
index 51853f8..de3d060 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesTask.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginPropertiesTask.groovy
@@ -51,11 +51,11 @@ class PluginPropertiesTask extends Copy {
             if (extension.description == null) {
                 throw new InvalidUserDataException('description is a required setting for esplugin')
             }
-            if (extension.jvm && extension.classname == null) {
-                throw new InvalidUserDataException('classname is a required setting for esplugin with jvm=true')
+            if (extension.classname == null) {
+                throw new InvalidUserDataException('classname is a required setting for esplugin')
             }
             doFirst {
-                if (extension.jvm && extension.isolated == false) {
+                if (extension.isolated == false) {
                     String warning = "WARNING: Disabling plugin isolation in ${project.path} is deprecated and will be removed in the future"
                     logger.warn("${'=' * warning.length()}\n${warning}\n${'=' * warning.length()}")
                 }
@@ -74,10 +74,8 @@ class PluginPropertiesTask extends Copy {
             'version': extension.version,
             'elasticsearchVersion': VersionProperties.elasticsearch,
             'javaVersion': project.targetCompatibility as String,
-            'jvm': extension.jvm as String,
-            'site': extension.site as String,
             'isolated': extension.isolated as String,
-            'classname': extension.jvm ? extension.classname : 'NA'
+            'classname': extension.classname
         ]
     }
 }
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
index b369d35..b41b182 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
@@ -129,7 +129,7 @@ class NodeInfo {
             'JAVA_HOME' : project.javaHome,
             'ES_GC_OPTS': config.jvmArgs // we pass these with the undocumented gc opts so the argline can set gc, etc
         ]
-        args.add("-Des.tests.portsfile=true")
+        args.add("-Des.node.portsfile=true")
         args.addAll(config.systemProperties.collect { key, value -> "-D${key}=${value}" })
         for (Map.Entry<String, String> property : System.properties.entrySet()) {
             if (property.getKey().startsWith('es.')) {
diff --git a/buildSrc/src/main/resources/plugin-descriptor.properties b/buildSrc/src/main/resources/plugin-descriptor.properties
index 4c676c2..e6a5f81 100644
--- a/buildSrc/src/main/resources/plugin-descriptor.properties
+++ b/buildSrc/src/main/resources/plugin-descriptor.properties
@@ -2,26 +2,13 @@
 # This file must exist as 'plugin-descriptor.properties' at
 # the root directory of all plugins.
 #
-# A plugin can be 'site', 'jvm', or both.
-#
-### example site plugin for "foo":
-#
-# foo.zip <-- zip file for the plugin, with this structure:
-#   _site/ <-- the contents that will be served
-#   plugin-descriptor.properties <-- example contents below:
-#
-# site=true
-# description=My cool plugin
-# version=1.0
-#
-### example jvm plugin for "foo"
+### example plugin for "foo"
 #
 # foo.zip <-- zip file for the plugin, with this structure:
 #   <arbitrary name1>.jar <-- classes, resources, dependencies
 #   <arbitrary nameN>.jar <-- any number of jars
 #   plugin-descriptor.properties <-- example contents below:
 #
-# jvm=true
 # classname=foo.bar.BazPlugin
 # description=My cool plugin
 # version=2.0
@@ -38,21 +25,6 @@ version=${version}
 #
 # 'name': the plugin name
 name=${name}
-
-### mandatory elements for site plugins:
-#
-# 'site': set to true to indicate contents of the _site/
-#  directory in the root of the plugin should be served.
-site=${site}
-#
-### mandatory elements for jvm plugins :
-#
-# 'jvm': true if the 'classname' class should be loaded
-#  from jar files in the root directory of the plugin.
-#  Note that only jar files in the root directory are
-#  added to the classpath for the plugin! If you need
-#  other resources, package them into a resources jar.
-jvm=${jvm}
 #
 # 'classname': the name of the class to load, fully-qualified.
 classname=${classname}
diff --git a/core/src/main/java/org/elasticsearch/action/ActionModule.java b/core/src/main/java/org/elasticsearch/action/ActionModule.java
index 39aa4b7..67f256c 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionModule.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionModule.java
@@ -149,16 +149,6 @@ import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.get.TransportGetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction;
-import org.elasticsearch.action.ingest.IngestActionFilter;
-import org.elasticsearch.action.ingest.IngestProxyActionFilter;
-import org.elasticsearch.action.ingest.DeletePipelineAction;
-import org.elasticsearch.action.ingest.DeletePipelineTransportAction;
-import org.elasticsearch.action.ingest.GetPipelineAction;
-import org.elasticsearch.action.ingest.GetPipelineTransportAction;
-import org.elasticsearch.action.ingest.PutPipelineAction;
-import org.elasticsearch.action.ingest.PutPipelineTransportAction;
-import org.elasticsearch.action.ingest.SimulatePipelineAction;
-import org.elasticsearch.action.ingest.SimulatePipelineTransportAction;
 import org.elasticsearch.action.percolate.MultiPercolateAction;
 import org.elasticsearch.action.percolate.PercolateAction;
 import org.elasticsearch.action.percolate.TransportMultiPercolateAction;
@@ -196,8 +186,6 @@ import org.elasticsearch.action.update.UpdateAction;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.NodeModule;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -222,13 +210,13 @@ public class ActionModule extends AbstractModule {
             this.transportAction = transportAction;
             this.supportTransportActions = supportTransportActions;
         }
+
+
     }
 
-    private final boolean ingestEnabled;
     private final boolean proxy;
 
-    public ActionModule(boolean ingestEnabled, boolean proxy) {
-        this.ingestEnabled = ingestEnabled;
+    public ActionModule(boolean proxy) {
         this.proxy = proxy;
     }
 
@@ -252,13 +240,6 @@ public class ActionModule extends AbstractModule {
 
     @Override
     protected void configure() {
-        if (proxy == false) {
-            if (ingestEnabled) {
-                registerFilter(IngestActionFilter.class);
-            } else {
-                registerFilter(IngestProxyActionFilter.class);
-            }
-        }
 
         Multibinder<ActionFilter> actionFilterMultibinder = Multibinder.newSetBinder(binder(), ActionFilter.class);
         for (Class<? extends ActionFilter> actionFilter : actionFilters) {
@@ -359,11 +340,6 @@ public class ActionModule extends AbstractModule {
 
         registerAction(FieldStatsAction.INSTANCE, TransportFieldStatsTransportAction.class);
 
-        registerAction(PutPipelineAction.INSTANCE, PutPipelineTransportAction.class);
-        registerAction(GetPipelineAction.INSTANCE, GetPipelineTransportAction.class);
-        registerAction(DeletePipelineAction.INSTANCE, DeletePipelineTransportAction.class);
-        registerAction(SimulatePipelineAction.INSTANCE, SimulatePipelineTransportAction.class);
-
         // register Name -> GenericAction Map that can be injected to instances.
         MapBinder<String, GenericAction> actionsBinder
                 = MapBinder.newMapBinder(binder(), String.class, GenericAction.class);
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
index c54b358..9a7299a 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
@@ -289,11 +289,11 @@ public class BulkProcessor implements Closeable {
     }
 
     public BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null);
+        return add(data, defaultIndex, defaultType, null);
     }
 
-    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultPipeline, @Nullable Object payload) throws Exception {
-        bulkRequest.add(data, defaultIndex, defaultType, null, null, defaultPipeline, payload, true);
+    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable Object payload) throws Exception {
+        bulkRequest.add(data, defaultIndex, defaultType, null, null, payload, true);
         executeIfNeeded();
         return this;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
index 3bc08d3..0026064 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
@@ -28,7 +28,6 @@ import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.update.UpdateRequest;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -254,17 +253,17 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, null, true);
+        return add(data, defaultIndex, defaultType, null, null, null, true);
     }
 
     /**
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, boolean allowExplicitIndex) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, null, allowExplicitIndex);
+        return add(data, defaultIndex, defaultType, null, null, null, allowExplicitIndex);
     }
 
-    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable String defaultPipeline, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
+    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
         XContent xContent = XContentFactory.xContent(data);
         int line = 0;
         int from = 0;
@@ -305,7 +304,6 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                 long version = Versions.MATCH_ANY;
                 VersionType versionType = VersionType.INTERNAL;
                 int retryOnConflict = 0;
-                String pipeline = defaultPipeline;
 
                 // at this stage, next token can either be END_OBJECT (and use default index and type, with auto generated id)
                 // or START_OBJECT which will have another set of parameters
@@ -346,8 +344,6 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                                 versionType = VersionType.fromString(parser.text());
                             } else if ("_retry_on_conflict".equals(currentFieldName) || "_retryOnConflict".equals(currentFieldName)) {
                                 retryOnConflict = parser.intValue();
-                            } else if ("pipeline".equals(currentFieldName)) {
-                                pipeline = parser.text();
                             } else if ("fields".equals(currentFieldName)) {
                                 throw new IllegalArgumentException("Action/metadata line [" + line + "] contains a simple value for parameter [fields] while a list is expected");
                             } else {
@@ -384,15 +380,15 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                     if ("index".equals(action)) {
                         if (opType == null) {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .setPipeline(pipeline).source(data.slice(from, nextMarker - from)), payload);
+                                    .source(data.slice(from, nextMarker - from)), payload);
                         } else {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .create("create".equals(opType)).setPipeline(pipeline)
+                                    .create("create".equals(opType))
                                     .source(data.slice(from, nextMarker - from)), payload);
                         }
                     } else if ("create".equals(action)) {
                         internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                .create(true).setPipeline(pipeline)
+                                .create(true)
                                 .source(data.slice(from, nextMarker - from)), payload);
                     } else if ("update".equals(action)) {
                         UpdateRequest updateRequest = new UpdateRequest(index, type, id).routing(routing).parent(parent).retryOnConflict(retryOnConflict)
@@ -483,22 +479,6 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
         return -1;
     }
 
-    /**
-     * @return Whether this bulk request contains index request with an ingest pipeline enabled.
-     */
-    public boolean hasIndexRequestsWithPipelines() {
-        for (ActionRequest actionRequest : requests) {
-            if (actionRequest instanceof IndexRequest) {
-                IndexRequest indexRequest = (IndexRequest) actionRequest;
-                if (Strings.hasText(indexRequest.getPipeline())) {
-                    return true;
-                }
-            }
-        }
-
-        return false;
-    }
-
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index 387f756..9899a54 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -155,8 +155,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     private XContentType contentType = Requests.INDEX_CONTENT_TYPE;
 
-    private String pipeline;
-
     public IndexRequest() {
     }
 
@@ -366,21 +364,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
     }
 
     /**
-     * Sets the ingest pipeline to be executed before indexing the document
-     */
-    public IndexRequest setPipeline(String pipeline) {
-        this.pipeline = pipeline;
-        return this;
-    }
-
-    /**
-     * Returns the ingest pipeline to be executed before indexing the document
-     */
-    public String getPipeline() {
-        return this.pipeline;
-    }
-
-    /**
      * The source of the document to index, recopied to a new array if it is unsage.
      */
     public BytesReference source() {
@@ -675,7 +658,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         refresh = in.readBoolean();
         version = in.readLong();
         versionType = VersionType.fromValue(in.readByte());
-        pipeline = in.readOptionalString();
     }
 
     @Override
@@ -697,7 +679,6 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         out.writeBoolean(refresh);
         out.writeLong(version);
         out.writeByte(versionType.getValue());
-        out.writeOptionalString(pipeline);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
index 4116755..f7134d8 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
@@ -278,12 +278,4 @@ public class IndexRequestBuilder extends ReplicationRequestBuilder<IndexRequest,
         request.ttl(ttl);
         return this;
     }
-
-    /**
-     * Sets the ingest pipeline to be executed before indexing the document
-     */
-    public IndexRequestBuilder setPipeline(String pipeline) {
-        request.setPipeline(pipeline);
-        return this;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
deleted file mode 100644
index ba1dd5d..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineAction extends Action<DeletePipelineRequest, WritePipelineResponse, DeletePipelineRequestBuilder> {
-
-    public static final DeletePipelineAction INSTANCE = new DeletePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/delete";
-
-    public DeletePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public DeletePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new DeletePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public WritePipelineResponse newResponse() {
-        return new WritePipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
deleted file mode 100644
index 6e5b9d8..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.action.support.master.AcknowledgedRequest;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class DeletePipelineRequest extends AcknowledgedRequest<DeletePipelineRequest> {
-
-    private String id;
-
-    public DeletePipelineRequest(String id) {
-        if (id == null) {
-            throw new IllegalArgumentException("id is missing");
-        }
-        this.id = id;
-    }
-
-    DeletePipelineRequest() {
-    }
-
-    public void setId(String id) {
-        this.id = Objects.requireNonNull(id);
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        return null;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
deleted file mode 100644
index fc14e0d..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class DeletePipelineRequestBuilder extends ActionRequestBuilder<DeletePipelineRequest, WritePipelineResponse, DeletePipelineRequestBuilder> {
-
-    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action) {
-        super(client, action, new DeletePipelineRequest());
-    }
-
-    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action, String id) {
-        super(client, action, new DeletePipelineRequest(id));
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
deleted file mode 100644
index 6378eb5..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.master.TransportMasterNodeAction;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.block.ClusterBlockException;
-import org.elasticsearch.cluster.block.ClusterBlockLevel;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class DeletePipelineTransportAction extends TransportMasterNodeAction<DeletePipelineRequest, WritePipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-    private final ClusterService clusterService;
-
-    @Inject
-    public DeletePipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-                                         TransportService transportService, ActionFilters actionFilters,
-                                         IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
-        super(settings, DeletePipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, DeletePipelineRequest::new);
-        this.clusterService = clusterService;
-        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
-    }
-
-    @Override
-    protected String executor() {
-        return ThreadPool.Names.SAME;
-    }
-
-    @Override
-    protected WritePipelineResponse newResponse() {
-        return new WritePipelineResponse();
-    }
-
-    @Override
-    protected void masterOperation(DeletePipelineRequest request, ClusterState state, ActionListener<WritePipelineResponse> listener) throws Exception {
-        pipelineStore.delete(clusterService, request, listener);
-    }
-
-    @Override
-    protected ClusterBlockException checkBlock(DeletePipelineRequest request, ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
deleted file mode 100644
index f6bc3d9..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineAction extends Action<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public static final GetPipelineAction INSTANCE = new GetPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/get";
-
-    public GetPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public GetPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new GetPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public GetPipelineResponse newResponse() {
-        return new GetPipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
deleted file mode 100644
index 6525c26..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.action.support.master.MasterNodeReadRequest;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class GetPipelineRequest extends MasterNodeReadRequest<GetPipelineRequest> {
-
-    private String[] ids;
-
-    public GetPipelineRequest(String... ids) {
-        if (ids == null || ids.length == 0) {
-            throw new IllegalArgumentException("No ids specified");
-        }
-        this.ids = ids;
-    }
-
-    GetPipelineRequest() {
-    }
-
-    public String[] getIds() {
-        return ids;
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        return null;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        ids = in.readStringArray();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeStringArray(ids);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
deleted file mode 100644
index f96a5ff..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.support.master.MasterNodeReadOperationRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class GetPipelineRequestBuilder extends MasterNodeReadOperationRequestBuilder<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
-
-    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action) {
-        super(client, action, new GetPipelineRequest());
-    }
-
-    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action, String[] ids) {
-        super(client, action, new GetPipelineRequest(ids));
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
deleted file mode 100644
index 9f0b229..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.StatusToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.PipelineConfiguration;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-public class GetPipelineResponse extends ActionResponse implements StatusToXContent {
-
-    private List<PipelineConfiguration> pipelines;
-
-    public GetPipelineResponse() {
-    }
-
-    public GetPipelineResponse(List<PipelineConfiguration> pipelines) {
-        this.pipelines = pipelines;
-    }
-
-    public List<PipelineConfiguration> pipelines() {
-        return pipelines;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        int size = in.readVInt();
-        pipelines = new ArrayList<>(size);
-        for (int i = 0; i < size; i++) {
-            pipelines.add(PipelineConfiguration.readPipelineConfiguration(in));
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeVInt(pipelines.size());
-        for (PipelineConfiguration pipeline : pipelines) {
-            pipeline.writeTo(out);
-        }
-    }
-
-    public boolean isFound() {
-        return !pipelines.isEmpty();
-    }
-
-    @Override
-    public RestStatus status() {
-        return isFound() ? RestStatus.OK : RestStatus.NOT_FOUND;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray("pipelines");
-        for (PipelineConfiguration pipeline : pipelines) {
-            pipeline.toXContent(builder, params);
-        }
-        builder.endArray();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
deleted file mode 100644
index e762d0b..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.block.ClusterBlockException;
-import org.elasticsearch.cluster.block.ClusterBlockLevel;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class GetPipelineTransportAction extends TransportMasterNodeReadAction<GetPipelineRequest, GetPipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-
-    @Inject
-    public GetPipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-                                      TransportService transportService, ActionFilters actionFilters,
-                                      IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
-        super(settings, GetPipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, GetPipelineRequest::new);
-        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
-    }
-
-    @Override
-    protected String executor() {
-        return ThreadPool.Names.SAME;
-    }
-
-    @Override
-    protected GetPipelineResponse newResponse() {
-        return new GetPipelineResponse();
-    }
-
-    @Override
-    protected void masterOperation(GetPipelineRequest request, ClusterState state, ActionListener<GetPipelineResponse> listener) throws Exception {
-        listener.onResponse(new GetPipelineResponse(pipelineStore.getPipelines(state, request.getIds())));
-    }
-
-    @Override
-    protected ClusterBlockException checkBlock(GetPipelineRequest request, ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
deleted file mode 100644
index b35e24c..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilter;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.PipelineExecutionService;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.tasks.Task;
-
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Set;
-
-public final class IngestActionFilter extends AbstractComponent implements ActionFilter {
-
-    private final PipelineExecutionService executionService;
-
-    @Inject
-    public IngestActionFilter(Settings settings, NodeService nodeService) {
-        super(settings);
-        this.executionService = nodeService.getIngestService().getPipelineExecutionService();
-    }
-
-    @Override
-    public <Request extends ActionRequest<Request>, Response extends ActionResponse> void apply(Task task, String action, Request request, ActionListener<Response> listener, ActionFilterChain<Request, Response> chain) {
-        switch (action) {
-            case IndexAction.NAME:
-                IndexRequest indexRequest = (IndexRequest) request;
-                if (Strings.hasText(indexRequest.getPipeline())) {
-                    processIndexRequest(task, action, listener, chain, (IndexRequest) request);
-                } else {
-                    chain.proceed(task, action, request, listener);
-                }
-                break;
-            case BulkAction.NAME:
-                BulkRequest bulkRequest = (BulkRequest) request;
-                if (bulkRequest.hasIndexRequestsWithPipelines()) {
-                    @SuppressWarnings("unchecked")
-                    ActionListener<BulkResponse> actionListener = (ActionListener<BulkResponse>) listener;
-                    processBulkIndexRequest(task, bulkRequest, action, chain, actionListener);
-                } else {
-                    chain.proceed(task, action, request, listener);
-                }
-                break;
-            default:
-                chain.proceed(task, action, request, listener);
-                break;
-        }
-    }
-
-    @Override
-    public <Response extends ActionResponse> void apply(String action, Response response, ActionListener<Response> listener, ActionFilterChain<?, Response> chain) {
-        chain.proceed(action, response, listener);
-    }
-
-    void processIndexRequest(Task task, String action, ActionListener listener, ActionFilterChain chain, IndexRequest indexRequest) {
-
-        executionService.execute(indexRequest, t -> {
-            logger.error("failed to execute pipeline [{}]", t, indexRequest.getPipeline());
-            listener.onFailure(t);
-        }, success -> {
-            // TransportIndexAction uses IndexRequest and same action name on the node that receives the request and the node that
-            // processes the primary action. This could lead to a pipeline being executed twice for the same
-            // index request, hence we set the pipeline to null once its execution completed.
-            indexRequest.setPipeline(null);
-            chain.proceed(task, action, indexRequest, listener);
-        });
-    }
-
-    void processBulkIndexRequest(Task task, BulkRequest original, String action, ActionFilterChain chain, ActionListener<BulkResponse> listener) {
-        BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original);
-        executionService.execute(() -> bulkRequestModifier, (indexRequest, throwable) -> {
-            logger.debug("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id(), throwable);
-            bulkRequestModifier.markCurrentItemAsFailed(throwable);
-        }, (throwable) -> {
-            if (throwable != null) {
-                logger.error("failed to execute pipeline for a bulk request", throwable);
-                listener.onFailure(throwable);
-            } else {
-                BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest();
-                ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(listener);
-                if (bulkRequest.requests().isEmpty()) {
-                    // at this stage, the transport bulk action can't deal with a bulk request with no requests,
-                    // so we stop and send an empty response back to the client.
-                    // (this will happen if pre-processing all items in the bulk failed)
-                    actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0));
-                } else {
-                    chain.proceed(task, action, bulkRequest, actionListener);
-                }
-            }
-        });
-    }
-
-    @Override
-    public int order() {
-        return Integer.MAX_VALUE;
-    }
-
-    final static class BulkRequestModifier implements Iterator<ActionRequest<?>> {
-
-        final BulkRequest bulkRequest;
-        final Set<Integer> failedSlots;
-        final List<BulkItemResponse> itemResponses;
-
-        int currentSlot = -1;
-        int[] originalSlots;
-
-        BulkRequestModifier(BulkRequest bulkRequest) {
-            this.bulkRequest = bulkRequest;
-            this.failedSlots = new HashSet<>();
-            this.itemResponses = new ArrayList<>(bulkRequest.requests().size());
-        }
-
-        @Override
-        public ActionRequest next() {
-            return bulkRequest.requests().get(++currentSlot);
-        }
-
-        @Override
-        public boolean hasNext() {
-            return (currentSlot + 1) < bulkRequest.requests().size();
-        }
-
-        BulkRequest getBulkRequest() {
-            if (itemResponses.isEmpty()) {
-                return bulkRequest;
-            } else {
-                BulkRequest modifiedBulkRequest = new BulkRequest(bulkRequest);
-                modifiedBulkRequest.refresh(bulkRequest.refresh());
-                modifiedBulkRequest.consistencyLevel(bulkRequest.consistencyLevel());
-                modifiedBulkRequest.timeout(bulkRequest.timeout());
-
-                int slot = 0;
-                originalSlots = new int[bulkRequest.requests().size() - failedSlots.size()];
-                for (int i = 0; i < bulkRequest.requests().size(); i++) {
-                    ActionRequest request = bulkRequest.requests().get(i);
-                    if (failedSlots.contains(i) == false) {
-                        modifiedBulkRequest.add(request);
-                        originalSlots[slot++] = i;
-                    }
-                }
-                return modifiedBulkRequest;
-            }
-        }
-
-        ActionListener<BulkResponse> wrapActionListenerIfNeeded(ActionListener<BulkResponse> actionListener) {
-            if (itemResponses.isEmpty()) {
-                return actionListener;
-            } else {
-                return new IngestBulkResponseListener(originalSlots, itemResponses, actionListener);
-            }
-        }
-
-        void markCurrentItemAsFailed(Throwable e) {
-            IndexRequest indexRequest = (IndexRequest) bulkRequest.requests().get(currentSlot);
-            // We hit a error during preprocessing a request, so we:
-            // 1) Remember the request item slot from the bulk, so that we're done processing all requests we know what failed
-            // 2) Add a bulk item failure for this request
-            // 3) Continue with the next request in the bulk.
-            failedSlots.add(currentSlot);
-            BulkItemResponse.Failure failure = new BulkItemResponse.Failure(indexRequest.index(), indexRequest.type(), indexRequest.id(), e);
-            itemResponses.add(new BulkItemResponse(currentSlot, indexRequest.opType().lowercase(), failure));
-        }
-
-    }
-
-    private final static class IngestBulkResponseListener implements ActionListener<BulkResponse> {
-
-        private final int[] originalSlots;
-        private final List<BulkItemResponse> itemResponses;
-        private final ActionListener<BulkResponse> actionListener;
-
-        IngestBulkResponseListener(int[] originalSlots, List<BulkItemResponse> itemResponses, ActionListener<BulkResponse> actionListener) {
-            this.itemResponses = itemResponses;
-            this.actionListener = actionListener;
-            this.originalSlots = originalSlots;
-        }
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            for (int i = 0; i < bulkItemResponses.getItems().length; i++) {
-                itemResponses.add(originalSlots[i], bulkItemResponses.getItems()[i]);
-            }
-            actionListener.onResponse(new BulkResponse(itemResponses.toArray(new BulkItemResponse[itemResponses.size()]), bulkItemResponses.getTookInMillis()));
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-            actionListener.onFailure(e);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java
deleted file mode 100644
index 39a4b1f..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionListenerResponseHandler;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilter;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.Randomness;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.tasks.Task;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.concurrent.atomic.AtomicInteger;
-
-public final class IngestProxyActionFilter implements ActionFilter {
-
-    private final ClusterService clusterService;
-    private final TransportService transportService;
-    private final AtomicInteger randomNodeGenerator = new AtomicInteger(Randomness.get().nextInt());
-
-    @Inject
-    public IngestProxyActionFilter(ClusterService clusterService, TransportService transportService) {
-        this.clusterService = clusterService;
-        this.transportService = transportService;
-    }
-
-    @Override
-    public <Request extends ActionRequest<Request>, Response extends ActionResponse> void apply(Task task, String action, Request request, ActionListener<Response> listener, ActionFilterChain<Request, Response> chain) {
-        Action ingestAction;
-        switch (action) {
-            case IndexAction.NAME:
-                ingestAction = IndexAction.INSTANCE;
-                IndexRequest indexRequest = (IndexRequest) request;
-                if (Strings.hasText(indexRequest.getPipeline())) {
-                    forwardIngestRequest(ingestAction, request, listener);
-                } else {
-                    chain.proceed(task, action, request, listener);
-                }
-                break;
-            case BulkAction.NAME:
-                ingestAction = BulkAction.INSTANCE;
-                BulkRequest bulkRequest = (BulkRequest) request;
-                if (bulkRequest.hasIndexRequestsWithPipelines()) {
-                    forwardIngestRequest(ingestAction, request, listener);
-                } else {
-                    chain.proceed(task, action, request, listener);
-                }
-                break;
-            default:
-                chain.proceed(task, action, request, listener);
-                break;
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private void forwardIngestRequest(Action<?, ?, ?> action, ActionRequest request, ActionListener<?> listener) {
-        transportService.sendRequest(randomIngestNode(), action.name(), request, new ActionListenerResponseHandler(listener) {
-            @Override
-            public TransportResponse newInstance() {
-                return action.newResponse();
-            }
-
-        });
-    }
-
-    @Override
-    public <Response extends ActionResponse> void apply(String action, Response response, ActionListener<Response> listener, ActionFilterChain<?, Response> chain) {
-        chain.proceed(action, response, listener);
-    }
-
-    @Override
-    public int order() {
-        return Integer.MAX_VALUE;
-    }
-
-    private DiscoveryNode randomIngestNode() {
-        assert clusterService.localNode().isIngestNode() == false;
-        DiscoveryNodes nodes = clusterService.state().getNodes();
-        DiscoveryNode[] ingestNodes = nodes.getIngestNodes().values().toArray(DiscoveryNode.class);
-        if (ingestNodes.length == 0) {
-            throw new IllegalStateException("There are no ingest nodes in this cluster, unable to forward request to an ingest node.");
-        }
-
-        int index = getNodeNumber();
-        return ingestNodes[(index) % ingestNodes.length];
-    }
-
-    private int getNodeNumber() {
-        int index = randomNodeGenerator.incrementAndGet();
-        if (index < 0) {
-            index = 0;
-            randomNodeGenerator.set(0);
-        }
-        return index;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
deleted file mode 100644
index 8f4b417..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class PutPipelineAction extends Action<PutPipelineRequest, WritePipelineResponse, PutPipelineRequestBuilder> {
-
-    public static final PutPipelineAction INSTANCE = new PutPipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/put";
-
-    public PutPipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public PutPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new PutPipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public WritePipelineResponse newResponse() {
-        return new WritePipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
deleted file mode 100644
index 1041614..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.action.support.master.AcknowledgedRequest;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-import java.util.Objects;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-
-public class PutPipelineRequest extends AcknowledgedRequest<PutPipelineRequest> {
-
-    private String id;
-    private BytesReference source;
-
-    public PutPipelineRequest(String id, BytesReference source) {
-        if (id == null) {
-            throw new IllegalArgumentException("id is missing");
-        }
-        if (source == null) {
-            throw new IllegalArgumentException("source is missing");
-        }
-
-        this.id = id;
-        this.source = source;
-    }
-
-    PutPipelineRequest() {
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        return null;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBytesReference(source);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
deleted file mode 100644
index bd92711..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class PutPipelineRequestBuilder extends ActionRequestBuilder<PutPipelineRequest, WritePipelineResponse, PutPipelineRequestBuilder> {
-
-    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action) {
-        super(client, action, new PutPipelineRequest());
-    }
-
-    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action, String id, BytesReference source) {
-        super(client, action, new PutPipelineRequest(id, source));
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
deleted file mode 100644
index 31a9112..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.master.TransportMasterNodeAction;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.block.ClusterBlockException;
-import org.elasticsearch.cluster.block.ClusterBlockLevel;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class PutPipelineTransportAction extends TransportMasterNodeAction<PutPipelineRequest, WritePipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-    private final ClusterService clusterService;
-
-    @Inject
-    public PutPipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-                                      TransportService transportService, ActionFilters actionFilters,
-                                      IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
-        super(settings, PutPipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, PutPipelineRequest::new);
-        this.clusterService = clusterService;
-        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
-    }
-
-    @Override
-    protected String executor() {
-        return ThreadPool.Names.SAME;
-    }
-
-    @Override
-    protected WritePipelineResponse newResponse() {
-        return new WritePipelineResponse();
-    }
-
-    @Override
-    protected void masterOperation(PutPipelineRequest request, ClusterState state, ActionListener<WritePipelineResponse> listener) throws Exception {
-        pipelineStore.put(clusterService, request, listener);
-    }
-
-    @Override
-    protected ClusterBlockException checkBlock(PutPipelineRequest request, ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java
deleted file mode 100644
index 036703e..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-
-/**
- * Holds the end result of what a pipeline did to sample document provided via the simulate api.
- */
-public final class SimulateDocumentBaseResult implements SimulateDocumentResult<SimulateDocumentBaseResult> {
-
-    private static final SimulateDocumentBaseResult PROTOTYPE = new SimulateDocumentBaseResult(new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
-
-    private WriteableIngestDocument ingestDocument;
-    private Exception failure;
-
-    public SimulateDocumentBaseResult(IngestDocument ingestDocument) {
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-    }
-
-    private SimulateDocumentBaseResult(WriteableIngestDocument ingestDocument) {
-        this.ingestDocument = ingestDocument;
-    }
-
-    public SimulateDocumentBaseResult(Exception failure) {
-        this.failure = failure;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    public static SimulateDocumentBaseResult readSimulateDocumentSimpleResult(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentBaseResult readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            Exception exception = in.readThrowable();
-            return new SimulateDocumentBaseResult(exception);
-        }
-        return new SimulateDocumentBaseResult(new WriteableIngestDocument(in));
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
deleted file mode 100644
index 7e7682b..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-
-public interface SimulateDocumentResult<T extends SimulateDocumentResult> extends Writeable<T>, ToXContent {
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
deleted file mode 100644
index d9d705f..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-/**
- * Holds the result of what a pipeline did to a sample document via the simulate api, but instead of {@link SimulateDocumentBaseResult}
- * this result class holds the intermediate result each processor did to the sample document.
- */
-public final class SimulateDocumentVerboseResult implements SimulateDocumentResult<SimulateDocumentVerboseResult> {
-
-    private static final SimulateDocumentVerboseResult PROTOTYPE = new SimulateDocumentVerboseResult(Collections.emptyList());
-
-    private final List<SimulateProcessorResult> processorResults;
-
-    public SimulateDocumentVerboseResult(List<SimulateProcessorResult> processorResults) {
-        this.processorResults = processorResults;
-    }
-
-    public List<SimulateProcessorResult> getProcessorResults() {
-        return processorResults;
-    }
-
-    public static SimulateDocumentVerboseResult readSimulateDocumentVerboseResultFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public SimulateDocumentVerboseResult readFrom(StreamInput in) throws IOException {
-        int size = in.readVInt();
-        List<SimulateProcessorResult> processorResults = new ArrayList<>();
-        for (int i = 0; i < size; i++) {
-            processorResults.add(new SimulateProcessorResult(in));
-        }
-        return new SimulateDocumentVerboseResult(processorResults);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(processorResults.size());
-        for (SimulateProcessorResult result : processorResults) {
-            result.writeTo(out);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.startArray("processor_results");
-        for (SimulateProcessorResult processorResult : processorResults) {
-            processorResult.toXContent(builder, params);
-        }
-        builder.endArray();
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
deleted file mode 100644
index 30efbe1..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRunnable;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.ArrayList;
-import java.util.List;
-
-class SimulateExecutionService {
-
-    private static final String THREAD_POOL_NAME = ThreadPool.Names.MANAGEMENT;
-
-    private final ThreadPool threadPool;
-
-    SimulateExecutionService(ThreadPool threadPool) {
-        this.threadPool = threadPool;
-    }
-
-    void executeVerboseDocument(Processor processor, IngestDocument ingestDocument, List<SimulateProcessorResult> processorResultList) throws Exception {
-        if (processor instanceof CompoundProcessor) {
-            CompoundProcessor cp = (CompoundProcessor) processor;
-            try {
-                for (Processor p : cp.getProcessors()) {
-                    executeVerboseDocument(p, ingestDocument, processorResultList);
-                }
-            } catch (Exception e) {
-                for (Processor p : cp.getOnFailureProcessors()) {
-                    executeVerboseDocument(p, ingestDocument, processorResultList);
-                }
-            }
-        } else {
-            try {
-                processor.execute(ingestDocument);
-                processorResultList.add(new SimulateProcessorResult(processor.getTag(), new IngestDocument(ingestDocument)));
-            } catch (Exception e) {
-                processorResultList.add(new SimulateProcessorResult(processor.getTag(), e));
-                throw e;
-            }
-        }
-    }
-
-    SimulateDocumentResult executeDocument(Pipeline pipeline, IngestDocument ingestDocument, boolean verbose) {
-        if (verbose) {
-            List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-            IngestDocument currentIngestDocument = new IngestDocument(ingestDocument);
-            CompoundProcessor pipelineProcessor = new CompoundProcessor(pipeline.getProcessors(), pipeline.getOnFailureProcessors());
-            try {
-                executeVerboseDocument(pipelineProcessor, currentIngestDocument, processorResultList);
-            } catch (Exception e) {
-                return new SimulateDocumentBaseResult(e);
-            }
-            return new SimulateDocumentVerboseResult(processorResultList);
-        } else {
-            try {
-                pipeline.execute(ingestDocument);
-                return new SimulateDocumentBaseResult(ingestDocument);
-            } catch (Exception e) {
-                return new SimulateDocumentBaseResult(e);
-            }
-        }
-    }
-
-    public void execute(SimulatePipelineRequest.Parsed request, ActionListener<SimulatePipelineResponse> listener) {
-        threadPool.executor(THREAD_POOL_NAME).execute(new ActionRunnable<SimulatePipelineResponse>(listener) {
-            @Override
-            protected void doRun() throws Exception {
-                List<SimulateDocumentResult> responses = new ArrayList<>();
-                for (IngestDocument ingestDocument : request.getDocuments()) {
-                    responses.add(executeDocument(request.getPipeline(), ingestDocument, request.isVerbose()));
-                }
-                listener.onResponse(new SimulatePipelineResponse(request.getPipeline().getId(), request.isVerbose(), responses));
-            }
-        });
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
deleted file mode 100644
index c1d219a..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class SimulatePipelineAction extends Action<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public static final SimulatePipelineAction INSTANCE = new SimulatePipelineAction();
-    public static final String NAME = "cluster:admin/ingest/pipeline/simulate";
-
-    public SimulatePipelineAction() {
-        super(NAME);
-    }
-
-    @Override
-    public SimulatePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new SimulatePipelineRequestBuilder(client, this);
-    }
-
-    @Override
-    public SimulatePipelineResponse newResponse() {
-        return new SimulatePipelineResponse();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
deleted file mode 100644
index af18ac5..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
+++ /dev/null
@@ -1,165 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.PipelineStore;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-import static org.elasticsearch.action.ValidateActions.addValidationError;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData;
-
-public class SimulatePipelineRequest extends ActionRequest<SimulatePipelineRequest> {
-
-    private String id;
-    private boolean verbose;
-    private BytesReference source;
-
-    public SimulatePipelineRequest(BytesReference source) {
-        if (source == null) {
-            throw new IllegalArgumentException("source is missing");
-        }
-        this.source = source;
-    }
-
-    SimulatePipelineRequest() {
-    }
-
-    @Override
-    public ActionRequestValidationException validate() {
-        return null;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public void setId(String id) {
-        this.id = id;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    public void setVerbose(boolean verbose) {
-        this.verbose = verbose;
-    }
-
-    public BytesReference getSource() {
-        return source;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readString();
-        verbose = in.readBoolean();
-        source = in.readBytesReference();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(id);
-        out.writeBoolean(verbose);
-        out.writeBytesReference(source);
-    }
-
-    public static final class Fields {
-        static final String PIPELINE = "pipeline";
-        static final String DOCS = "docs";
-        static final String SOURCE = "_source";
-    }
-
-    static class Parsed {
-        private final List<IngestDocument> documents;
-        private final Pipeline pipeline;
-        private final boolean verbose;
-
-        Parsed(Pipeline pipeline, List<IngestDocument> documents, boolean verbose) {
-            this.pipeline = pipeline;
-            this.documents = Collections.unmodifiableList(documents);
-            this.verbose = verbose;
-        }
-
-        public Pipeline getPipeline() {
-            return pipeline;
-        }
-
-        public List<IngestDocument> getDocuments() {
-            return documents;
-        }
-
-        public boolean isVerbose() {
-            return verbose;
-        }
-    }
-
-    private static final Pipeline.Factory PIPELINE_FACTORY = new Pipeline.Factory();
-    static final String SIMULATED_PIPELINE_ID = "_simulate_pipeline";
-
-    static Parsed parseWithPipelineId(String pipelineId, Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) {
-        if (pipelineId == null) {
-            throw new IllegalArgumentException("param [pipeline] is null");
-        }
-        Pipeline pipeline = pipelineStore.get(pipelineId);
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    static Parsed parse(Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) throws Exception {
-        Map<String, Object> pipelineConfig = ConfigurationUtils.readMap(config, Fields.PIPELINE);
-        Pipeline pipeline = PIPELINE_FACTORY.create(SIMULATED_PIPELINE_ID, pipelineConfig, pipelineStore.getProcessorFactoryRegistry());
-        List<IngestDocument> ingestDocumentList = parseDocs(config);
-        return new Parsed(pipeline, ingestDocumentList, verbose);
-    }
-
-    private static List<IngestDocument> parseDocs(Map<String, Object> config) {
-        List<Map<String, Object>> docs = ConfigurationUtils.readList(config, Fields.DOCS);
-        List<IngestDocument> ingestDocumentList = new ArrayList<>();
-        for (Map<String, Object> dataMap : docs) {
-            Map<String, Object> document = ConfigurationUtils.readMap(dataMap, Fields.SOURCE);
-            IngestDocument ingestDocument = new IngestDocument(ConfigurationUtils.readStringProperty(dataMap, MetaData.INDEX.getFieldName(), "_index"),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.TYPE.getFieldName(), "_type"),
-                    ConfigurationUtils.readStringProperty(dataMap, MetaData.ID.getFieldName(), "_id"),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.ROUTING.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.PARENT.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TIMESTAMP.getFieldName()),
-                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TTL.getFieldName()),
-                    document);
-            ingestDocumentList.add(ingestDocument);
-        }
-        return ingestDocumentList;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
deleted file mode 100644
index 4a13fa1..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.common.bytes.BytesReference;
-
-public class SimulatePipelineRequestBuilder extends ActionRequestBuilder<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
-
-    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action) {
-        super(client, action, new SimulatePipelineRequest());
-    }
-
-    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action, BytesReference source) {
-        super(client, action, new SimulatePipelineRequest(source));
-    }
-
-    public SimulatePipelineRequestBuilder setId(String id) {
-        request.setId(id);
-        return this;
-    }
-
-    public SimulatePipelineRequestBuilder setVerbose(boolean verbose) {
-        request.setVerbose(verbose);
-        return this;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
deleted file mode 100644
index c7c0822..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-public class SimulatePipelineResponse extends ActionResponse implements ToXContent {
-    private String pipelineId;
-    private boolean verbose;
-    private List<SimulateDocumentResult> results;
-
-    public SimulatePipelineResponse() {
-
-    }
-
-    public SimulatePipelineResponse(String pipelineId, boolean verbose, List<SimulateDocumentResult> responses) {
-        this.pipelineId = pipelineId;
-        this.verbose = verbose;
-        this.results = Collections.unmodifiableList(responses);
-    }
-
-    public String getPipelineId() {
-        return pipelineId;
-    }
-
-    public List<SimulateDocumentResult> getResults() {
-        return results;
-    }
-
-    public boolean isVerbose() {
-        return verbose;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeString(pipelineId);
-        out.writeBoolean(verbose);
-        out.writeVInt(results.size());
-        for (SimulateDocumentResult response : results) {
-            response.writeTo(out);
-        }
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        this.pipelineId = in.readString();
-        boolean verbose = in.readBoolean();
-        int responsesLength = in.readVInt();
-        results = new ArrayList<>();
-        for (int i = 0; i < responsesLength; i++) {
-            SimulateDocumentResult<?> simulateDocumentResult;
-            if (verbose) {
-                simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
-            } else {
-                simulateDocumentResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(in);
-            }
-            results.add(simulateDocumentResult);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(Fields.DOCUMENTS);
-        for (SimulateDocumentResult response : results) {
-            response.toXContent(builder, params);
-        }
-        builder.endArray();
-        return builder;
-    }
-
-    static final class Fields {
-        static final XContentBuilderString DOCUMENTS = new XContentBuilderString("docs");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
deleted file mode 100644
index 5640d7c..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.Map;
-
-public class SimulatePipelineTransportAction extends HandledTransportAction<SimulatePipelineRequest, SimulatePipelineResponse> {
-
-    private final PipelineStore pipelineStore;
-    private final SimulateExecutionService executionService;
-
-    @Inject
-    public SimulatePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
-        super(settings, SimulatePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, SimulatePipelineRequest::new);
-        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
-        this.executionService = new SimulateExecutionService(threadPool);
-    }
-
-    @Override
-    protected void doExecute(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
-        final Map<String, Object> source = XContentHelper.convertToMap(request.getSource(), false).v2();
-
-        final SimulatePipelineRequest.Parsed simulateRequest;
-        try {
-            if (request.getId() != null) {
-                simulateRequest = SimulatePipelineRequest.parseWithPipelineId(request.getId(), source, request.isVerbose(), pipelineStore);
-            } else {
-                simulateRequest = SimulatePipelineRequest.parse(source, request.isVerbose(), pipelineStore);
-            }
-        } catch (Exception e) {
-            listener.onFailure(e);
-            return;
-        }
-
-        executionService.execute(simulateRequest, listener);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
deleted file mode 100644
index 6a38434..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-
-public class SimulateProcessorResult implements Writeable<SimulateProcessorResult>, ToXContent {
-    private final String processorTag;
-    private final WriteableIngestDocument ingestDocument;
-    private final Exception failure;
-
-    public SimulateProcessorResult(StreamInput in) throws IOException {
-        this.processorTag = in.readString();
-        if (in.readBoolean()) {
-            this.failure = in.readThrowable();
-            this.ingestDocument = null;
-        } else {
-            this.ingestDocument =  new WriteableIngestDocument(in);
-            this.failure = null;
-        }
-    }
-
-    public SimulateProcessorResult(String processorTag, IngestDocument ingestDocument) {
-        this.processorTag = processorTag;
-        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
-        this.failure = null;
-    }
-
-    public SimulateProcessorResult(String processorTag, Exception failure) {
-        this.processorTag = processorTag;
-        this.failure = failure;
-        this.ingestDocument = null;
-    }
-
-    public IngestDocument getIngestDocument() {
-        if (ingestDocument == null) {
-            return null;
-        }
-        return ingestDocument.getIngestDocument();
-    }
-
-    public String getProcessorTag() {
-        return processorTag;
-    }
-
-    public Exception getFailure() {
-        return failure;
-    }
-
-    @Override
-    public SimulateProcessorResult readFrom(StreamInput in) throws IOException {
-        return new SimulateProcessorResult(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(processorTag);
-        if (failure == null) {
-            out.writeBoolean(false);
-            ingestDocument.writeTo(out);
-        } else {
-            out.writeBoolean(true);
-            out.writeThrowable(failure);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (processorTag != null) {
-            builder.field(AbstractProcessorFactory.TAG_KEY, processorTag);
-        }
-        if (failure == null) {
-            ingestDocument.toXContent(builder, params);
-        } else {
-            ElasticsearchException.renderThrowable(builder, params, failure);
-        }
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
deleted file mode 100644
index 885fd9f..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.support.master.AcknowledgedResponse;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-
-import java.io.IOException;
-
-public class WritePipelineResponse extends AcknowledgedResponse {
-
-    WritePipelineResponse() {
-    }
-
-    public WritePipelineResponse(boolean acknowledge) {
-        super(acknowledge);
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        readAcknowledged(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        writeAcknowledged(out);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
deleted file mode 100644
index 342e4bd..0000000
--- a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-import java.util.Objects;
-
-final class WriteableIngestDocument implements Writeable<WriteableIngestDocument>, ToXContent {
-
-    private final IngestDocument ingestDocument;
-
-    WriteableIngestDocument(IngestDocument ingestDocument) {
-        assert ingestDocument != null;
-        this.ingestDocument = ingestDocument;
-    }
-
-    WriteableIngestDocument(StreamInput in) throws IOException {
-        Map<String, Object> sourceAndMetadata = in.readMap();
-        @SuppressWarnings("unchecked")
-        Map<String, String> ingestMetadata = (Map<String, String>) in.readGenericValue();
-        this.ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
-    }
-
-    IngestDocument getIngestDocument() {
-        return ingestDocument;
-    }
-
-
-    @Override
-    public WriteableIngestDocument readFrom(StreamInput in) throws IOException {
-       return new WriteableIngestDocument(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeMap(ingestDocument.getSourceAndMetadata());
-        out.writeGenericValue(ingestDocument.getIngestMetadata());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject("doc");
-        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
-            builder.field(metadata.getKey().getFieldName(), metadata.getValue());
-        }
-        builder.field("_source", ingestDocument.getSourceAndMetadata());
-        builder.startObject("_ingest");
-        for (Map.Entry<String, String> ingestMetadata : ingestDocument.getIngestMetadata().entrySet()) {
-            builder.field(ingestMetadata.getKey(), ingestMetadata.getValue());
-        }
-        builder.endObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public boolean equals(Object o) {
-        if (this == o) {
-            return true;
-        }
-        if (o == null || getClass() != o.getClass()) {
-            return false;
-        }
-        WriteableIngestDocument that = (WriteableIngestDocument) o;
-        return Objects.equals(ingestDocument, that.ingestDocument);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(ingestDocument);
-    }
-
-    @Override
-    public String toString() {
-        return ingestDocument.toString();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Security.java b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
index 43ad73b..dc89ce3 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Security.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Security.java
@@ -241,26 +241,26 @@ final class Security {
      */
     static void addFilePermissions(Permissions policy, Environment environment) {
         // read-only dirs
-        addPath(policy, "path.home", environment.binFile(), "read,readlink");
-        addPath(policy, "path.home", environment.libFile(), "read,readlink");
-        addPath(policy, "path.home", environment.modulesFile(), "read,readlink");
-        addPath(policy, "path.plugins", environment.pluginsFile(), "read,readlink");
-        addPath(policy, "path.conf", environment.configFile(), "read,readlink");
-        addPath(policy, "path.scripts", environment.scriptsFile(), "read,readlink");
+        addPath(policy, Environment.PATH_HOME_SETTING.getKey(), environment.binFile(), "read,readlink");
+        addPath(policy, Environment.PATH_HOME_SETTING.getKey(), environment.libFile(), "read,readlink");
+        addPath(policy, Environment.PATH_HOME_SETTING.getKey(), environment.modulesFile(), "read,readlink");
+        addPath(policy, Environment.PATH_PLUGINS_SETTING.getKey(), environment.pluginsFile(), "read,readlink");
+        addPath(policy, Environment.PATH_CONF_SETTING.getKey(), environment.configFile(), "read,readlink");
+        addPath(policy, Environment.PATH_SCRIPTS_SETTING.getKey(), environment.scriptsFile(), "read,readlink");
         // read-write dirs
         addPath(policy, "java.io.tmpdir", environment.tmpFile(), "read,readlink,write,delete");
-        addPath(policy, "path.logs", environment.logsFile(), "read,readlink,write,delete");
+        addPath(policy, Environment.PATH_LOGS_SETTING.getKey(), environment.logsFile(), "read,readlink,write,delete");
         if (environment.sharedDataFile() != null) {
-            addPath(policy, "path.shared_data", environment.sharedDataFile(), "read,readlink,write,delete");
+            addPath(policy, Environment.PATH_SHARED_DATA_SETTING.getKey(), environment.sharedDataFile(), "read,readlink,write,delete");
         }
         for (Path path : environment.dataFiles()) {
-            addPath(policy, "path.data", path, "read,readlink,write,delete");
+            addPath(policy, Environment.PATH_DATA_SETTING.getKey(), path, "read,readlink,write,delete");
         }
         for (Path path : environment.dataWithClusterFiles()) {
-            addPath(policy, "path.data", path, "read,readlink,write,delete");
+            addPath(policy, Environment.PATH_DATA_SETTING.getKey(), path, "read,readlink,write,delete");
         }
         for (Path path : environment.repoFiles()) {
-            addPath(policy, "path.repo", path, "read,readlink,write,delete");
+            addPath(policy, Environment.PATH_REPO_SETTING.getKey(), path, "read,readlink,write,delete");
         }
         if (environment.pidFile() != null) {
             // we just need permission to remove the file if its elsewhere.
diff --git a/core/src/main/java/org/elasticsearch/client/Client.java b/core/src/main/java/org/elasticsearch/client/Client.java
index dbcd912..e7461da 100644
--- a/core/src/main/java/org/elasticsearch/client/Client.java
+++ b/core/src/main/java/org/elasticsearch/client/Client.java
@@ -51,17 +51,6 @@ import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.DeletePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineRequest;
-import org.elasticsearch.action.ingest.GetPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineResponse;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.PutPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.action.ingest.SimulatePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulatePipelineResponse;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
 import org.elasticsearch.action.percolate.MultiPercolateRequest;
 import org.elasticsearch.action.percolate.MultiPercolateRequestBuilder;
 import org.elasticsearch.action.percolate.MultiPercolateResponse;
@@ -93,7 +82,6 @@ import org.elasticsearch.action.update.UpdateRequestBuilder;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.settings.Settings;
 
@@ -605,66 +593,6 @@ public interface Client extends ElasticsearchClient, Releasable {
     void fieldStats(FieldStatsRequest request, ActionListener<FieldStatsResponse> listener);
 
     /**
-     * Stores an ingest pipeline
-     */
-    void putPipeline(PutPipelineRequest request, ActionListener<WritePipelineResponse> listener);
-
-    /**
-     * Stores an ingest pipeline
-     */
-    ActionFuture<WritePipelineResponse> putPipeline(PutPipelineRequest request);
-
-    /**
-     * Stores an ingest pipeline
-     */
-    PutPipelineRequestBuilder preparePutPipeline(String id, BytesReference source);
-
-    /**
-     * Deletes a stored ingest pipeline
-     */
-    void deletePipeline(DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener);
-
-    /**
-     * Deletes a stored ingest pipeline
-     */
-    ActionFuture<WritePipelineResponse> deletePipeline(DeletePipelineRequest request);
-
-    /**
-     * Deletes a stored ingest pipeline
-     */
-    DeletePipelineRequestBuilder prepareDeletePipeline();
-
-    /**
-     * Returns a stored ingest pipeline
-     */
-    void getPipeline(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener);
-
-    /**
-     * Returns a stored ingest pipeline
-     */
-    ActionFuture<GetPipelineResponse> getPipeline(GetPipelineRequest request);
-
-    /**
-     * Returns a stored ingest pipeline
-     */
-    GetPipelineRequestBuilder prepareGetPipeline(String... ids);
-
-    /**
-     * Simulates an ingest pipeline
-     */
-    void simulatePipeline(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener);
-
-    /**
-     * Simulates an ingest pipeline
-     */
-    ActionFuture<SimulatePipelineResponse> simulatePipeline(SimulatePipelineRequest request);
-
-    /**
-     * Simulates an ingest pipeline
-     */
-    SimulatePipelineRequestBuilder prepareSimulatePipeline(BytesReference source);
-
-    /**
      * Returns this clients settings
      */
     Settings settings();
diff --git a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
index 182f31a..e5e1bea 100644
--- a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
+++ b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
@@ -272,21 +272,6 @@ import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
-import org.elasticsearch.action.ingest.DeletePipelineAction;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.DeletePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineAction;
-import org.elasticsearch.action.ingest.GetPipelineRequest;
-import org.elasticsearch.action.ingest.GetPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.GetPipelineResponse;
-import org.elasticsearch.action.ingest.PutPipelineAction;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.PutPipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulatePipelineAction;
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.action.ingest.SimulatePipelineRequestBuilder;
-import org.elasticsearch.action.ingest.SimulatePipelineResponse;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
 import org.elasticsearch.action.percolate.MultiPercolateAction;
 import org.elasticsearch.action.percolate.MultiPercolateRequest;
 import org.elasticsearch.action.percolate.MultiPercolateRequestBuilder;
@@ -334,7 +319,6 @@ import org.elasticsearch.client.ClusterAdminClient;
 import org.elasticsearch.client.ElasticsearchClient;
 import org.elasticsearch.client.IndicesAdminClient;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -810,66 +794,6 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         return new FieldStatsRequestBuilder(this, FieldStatsAction.INSTANCE);
     }
 
-    @Override
-    public void putPipeline(PutPipelineRequest request, ActionListener<WritePipelineResponse> listener) {
-        execute(PutPipelineAction.INSTANCE, request, listener);
-    }
-
-    @Override
-    public ActionFuture<WritePipelineResponse> putPipeline(PutPipelineRequest request) {
-        return execute(PutPipelineAction.INSTANCE, request);
-    }
-
-    @Override
-    public PutPipelineRequestBuilder preparePutPipeline(String id, BytesReference source) {
-        return new PutPipelineRequestBuilder(this, PutPipelineAction.INSTANCE, id, source);
-    }
-
-    @Override
-    public void deletePipeline(DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener) {
-        execute(DeletePipelineAction.INSTANCE, request, listener);
-    }
-
-    @Override
-    public ActionFuture<WritePipelineResponse> deletePipeline(DeletePipelineRequest request) {
-        return execute(DeletePipelineAction.INSTANCE, request);
-    }
-
-    @Override
-    public DeletePipelineRequestBuilder prepareDeletePipeline() {
-        return new DeletePipelineRequestBuilder(this, DeletePipelineAction.INSTANCE);
-    }
-
-    @Override
-    public void getPipeline(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {
-        execute(GetPipelineAction.INSTANCE, request, listener);
-    }
-
-    @Override
-    public ActionFuture<GetPipelineResponse> getPipeline(GetPipelineRequest request) {
-        return execute(GetPipelineAction.INSTANCE, request);
-    }
-
-    @Override
-    public GetPipelineRequestBuilder prepareGetPipeline(String... ids) {
-        return new GetPipelineRequestBuilder(this, GetPipelineAction.INSTANCE, ids);
-    }
-
-    @Override
-    public void simulatePipeline(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
-        execute(SimulatePipelineAction.INSTANCE, request, listener);
-    }
-
-    @Override
-    public ActionFuture<SimulatePipelineResponse> simulatePipeline(SimulatePipelineRequest request) {
-        return execute(SimulatePipelineAction.INSTANCE, request);
-    }
-
-    @Override
-    public SimulatePipelineRequestBuilder prepareSimulatePipeline(BytesReference source) {
-        return new SimulatePipelineRequestBuilder(this, SimulatePipelineAction.INSTANCE, source);
-    }
-
     static class Admin implements AdminClient {
 
         private final ClusterAdmin clusterAdmin;
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index 9930a9d..3d68e64 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -19,6 +19,10 @@
 
 package org.elasticsearch.client.transport;
 
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.action.Action;
 import org.elasticsearch.action.ActionListener;
@@ -55,10 +59,6 @@ import org.elasticsearch.threadpool.ThreadPoolModule;
 import org.elasticsearch.transport.TransportService;
 import org.elasticsearch.transport.netty.NettyTransport;
 
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 
 /**
@@ -116,7 +116,7 @@ public class TransportClient extends AbstractClient {
                 .put("node.client", true)
                 .put(CLIENT_TYPE_SETTING, CLIENT_TYPE);
             return new PluginsService(settingsBuilder.build(), null, null, pluginClasses);
-        }
+        };
 
         /**
          * Builds a new instance of the transport client.
@@ -150,7 +150,7 @@ public class TransportClient extends AbstractClient {
                         // noop
                     }
                 });
-                modules.add(new ActionModule(false, true));
+                modules.add(new ActionModule(true));
                 modules.add(new CircuitBreakerModule(settings));
 
                 pluginsService.processModules(modules);
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
index 56befbb..99c7025 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java
@@ -34,6 +34,7 @@ import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
@@ -101,6 +102,11 @@ public class TransportClientNodesService extends AbstractComponent {
 
     private volatile boolean closed;
 
+
+    public static final Setting<TimeValue> CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL = Setting.positiveTimeSetting("client.transport.nodes_sampler_interval", timeValueSeconds(5), false, Setting.Scope.CLUSTER);
+    public static final Setting<TimeValue> CLIENT_TRANSPORT_PING_TIMEOUT = Setting.positiveTimeSetting("client.transport.ping_timeout", timeValueSeconds(5), false, Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME = Setting.boolSetting("client.transport.ignore_cluster_name", false, false, Setting.Scope.CLUSTER);
+
     @Inject
     public TransportClientNodesService(Settings settings, ClusterName clusterName, TransportService transportService,
                                        ThreadPool threadPool, Headers headers, Version version) {
@@ -111,9 +117,9 @@ public class TransportClientNodesService extends AbstractComponent {
         this.minCompatibilityVersion = version.minimumCompatibilityVersion();
         this.headers = headers;
 
-        this.nodesSamplerInterval = this.settings.getAsTime("client.transport.nodes_sampler_interval", timeValueSeconds(5));
-        this.pingTimeout = this.settings.getAsTime("client.transport.ping_timeout", timeValueSeconds(5)).millis();
-        this.ignoreClusterName = this.settings.getAsBoolean("client.transport.ignore_cluster_name", false);
+        this.nodesSamplerInterval = CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL.get(this.settings);
+        this.pingTimeout = CLIENT_TRANSPORT_PING_TIMEOUT.get(this.settings).millis();
+        this.ignoreClusterName = CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME.get(this.settings);
 
         if (logger.isDebugEnabled()) {
             logger.debug("node_sampler_interval[" + nodesSamplerInterval + "]");
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
index 3a837d8..170d6fa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java
@@ -89,7 +89,7 @@ public class ShardStateAction extends AbstractComponent {
             logger.warn("{} no master known for action [{}] for shard [{}]", shardRoutingEntry.getShardRouting().shardId(), actionName, shardRoutingEntry.getShardRouting());
             waitForNewMasterAndRetry(actionName, observer, shardRoutingEntry, listener);
         } else {
-            logger.debug("{} sending [{}] to [{}] for shard [{}]", shardRoutingEntry.getShardRouting().getId(), actionName, masterNode.getId(), shardRoutingEntry);
+            logger.debug("{} sending [{}] to [{}] for shard [{}]", shardRoutingEntry.getShardRouting().shardId(), actionName, masterNode.getId(), shardRoutingEntry);
             transportService.sendRequest(masterNode,
                 actionName, shardRoutingEntry, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
                     @Override
@@ -144,7 +144,7 @@ public class ShardStateAction extends AbstractComponent {
 
             @Override
             public void onClusterServiceClose() {
-                logger.warn("{} node closed while execution action [{}] for shard [{}]", shardRoutingEntry.failure, shardRoutingEntry.getShardRouting().getId(), actionName, shardRoutingEntry.getShardRouting());
+                logger.warn("{} node closed while execution action [{}] for shard [{}]", shardRoutingEntry.failure, shardRoutingEntry.getShardRouting().shardId(), actionName, shardRoutingEntry.getShardRouting());
                 listener.onFailure(new NodeClosedException(clusterService.localNode()));
             }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index 0e41dda..002d1a5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -54,7 +54,6 @@ import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
-import org.elasticsearch.ingest.IngestMetadata;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -112,7 +111,6 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
     static {
         // register non plugin custom metadata
         registerPrototype(RepositoriesMetaData.TYPE, RepositoriesMetaData.PROTO);
-        registerPrototype(IngestMetadata.TYPE, IngestMetadata.PROTO);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index c6ec2a4..8bbd6f0 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -20,11 +20,15 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.misc.IndexMergeTool;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.IndexScopedSettings;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.MergePolicyConfig;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.MapperService;
@@ -32,6 +36,7 @@ import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.indices.mapper.MapperRegistry;
 
 import java.util.Collections;
+import java.util.Map;
 import java.util.Set;
 
 import static java.util.Collections.unmodifiableSet;
@@ -48,11 +53,13 @@ import static org.elasticsearch.common.util.set.Sets.newHashSet;
 public class MetaDataIndexUpgradeService extends AbstractComponent {
 
     private final MapperRegistry mapperRegistry;
+    private final IndexScopedSettings indexScopedSettigns;
 
     @Inject
-    public MetaDataIndexUpgradeService(Settings settings, MapperRegistry mapperRegistry) {
+    public MetaDataIndexUpgradeService(Settings settings, MapperRegistry mapperRegistry, IndexScopedSettings indexScopedSettings) {
         super(settings);
         this.mapperRegistry = mapperRegistry;
+        this.indexScopedSettigns = indexScopedSettings;
     }
 
     /**
@@ -65,21 +72,25 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
     public IndexMetaData upgradeIndexMetaData(IndexMetaData indexMetaData) {
         // Throws an exception if there are too-old segments:
         if (isUpgraded(indexMetaData)) {
+            assert indexMetaData == archiveBrokenIndexSettings(indexMetaData) : "all settings must have been upgraded before";
             return indexMetaData;
         }
         checkSupportedVersion(indexMetaData);
         IndexMetaData newMetaData = indexMetaData;
+        // we have to run this first otherwise in we try to create IndexSettings
+        // with broken settings and fail in checkMappingsCompatibility
+        newMetaData = archiveBrokenIndexSettings(newMetaData);
+        // only run the check with the upgraded settings!!
         checkMappingsCompatibility(newMetaData);
-        newMetaData = markAsUpgraded(newMetaData);
-        return newMetaData;
+        return markAsUpgraded(newMetaData);
     }
 
 
     /**
      * Checks if the index was already opened by this version of Elasticsearch and doesn't require any additional checks.
      */
-    private boolean isUpgraded(IndexMetaData indexMetaData) {
-        return indexMetaData.getUpgradedVersion().onOrAfter(Version.V_3_0_0);
+    boolean isUpgraded(IndexMetaData indexMetaData) {
+        return indexMetaData.getUpgradedVersion().onOrAfter(Version.CURRENT);
     }
 
     /**
@@ -171,4 +182,39 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
         }
     }
 
+    private static final String ARCHIVED_SETTINGS_PREFIX = "archived.";
+
+    IndexMetaData archiveBrokenIndexSettings(IndexMetaData indexMetaData) {
+        Settings settings = indexMetaData.getSettings();
+        Settings.Builder builder = Settings.builder();
+        boolean changed = false;
+        for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) {
+            try {
+                Setting<?> setting = indexScopedSettigns.get(entry.getKey());
+                if (setting != null) {
+                    setting.get(settings);
+                    builder.put(entry.getKey(), entry.getValue());
+                } else {
+                    if (indexScopedSettigns.isPrivateSetting(entry.getKey()) || entry.getKey().startsWith(ARCHIVED_SETTINGS_PREFIX)) {
+                        builder.put(entry.getKey(), entry.getValue());
+                    } else {
+                        changed = true;
+                        logger.warn("[{}] found unknown index setting: {} value: {} - archiving", indexMetaData.getIndex(), entry.getKey(), entry.getValue());
+                        // we put them back in here such that tools can check from the outside if there are any indices with broken settings. The setting can remain there
+                        // but we want users to be aware that some of their setting are broken and they can research why and what they need to do to replace them.
+                        builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue());
+                    }
+                }
+            } catch (IllegalArgumentException ex) {
+                changed = true;
+                logger.warn("[{}] found invalid index setting: {} value: {} - archiving",ex, indexMetaData.getIndex(), entry.getKey(), entry.getValue());
+                // we put them back in here such that tools can check from the outside if there are any indices with broken settings. The setting can remain there
+                // but we want users to be aware that some of their setting sare broken and they can research why and what they need to do to replace them.
+                builder.put(ARCHIVED_SETTINGS_PREFIX + entry.getKey(), entry.getValue());
+            }
+        }
+
+        return changed ? IndexMetaData.builder(indexMetaData).settings(builder.build()).build() : indexMetaData;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index e05bab6..7dce217 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.transport.TransportAddressSerializers;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -88,10 +87,6 @@ public class DiscoveryNode implements Streamable, ToXContent {
         return Booleans.isExplicitTrue(data);
     }
 
-    public static boolean ingestNode(Settings settings) {
-        return Node.NODE_INGEST_SETTING.get(settings);
-    }
-
     public static final List<DiscoveryNode> EMPTY_LIST = Collections.emptyList();
 
     private String nodeName = "";
@@ -321,14 +316,6 @@ public class DiscoveryNode implements Streamable, ToXContent {
         return masterNode();
     }
 
-    /**
-     * Returns a boolean that tells whether this an ingest node or not
-     */
-    public boolean isIngestNode() {
-        String ingest = attributes.get("ingest");
-        return ingest == null ? true : Booleans.parseBooleanExact(ingest);
-    }
-
     public Version version() {
         return this.version;
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
index e24c25d..d07d3c3 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
@@ -52,20 +52,16 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
     private final ImmutableOpenMap<String, DiscoveryNode> nodes;
     private final ImmutableOpenMap<String, DiscoveryNode> dataNodes;
     private final ImmutableOpenMap<String, DiscoveryNode> masterNodes;
-    private final ImmutableOpenMap<String, DiscoveryNode> ingestNodes;
 
     private final String masterNodeId;
     private final String localNodeId;
     private final Version minNodeVersion;
     private final Version minNonClientNodeVersion;
 
-    private DiscoveryNodes(ImmutableOpenMap<String, DiscoveryNode> nodes, ImmutableOpenMap<String, DiscoveryNode> dataNodes,
-                           ImmutableOpenMap<String, DiscoveryNode> masterNodes, ImmutableOpenMap<String, DiscoveryNode> ingestNodes,
-                           String masterNodeId, String localNodeId, Version minNodeVersion, Version minNonClientNodeVersion) {
+    private DiscoveryNodes(ImmutableOpenMap<String, DiscoveryNode> nodes, ImmutableOpenMap<String, DiscoveryNode> dataNodes, ImmutableOpenMap<String, DiscoveryNode> masterNodes, String masterNodeId, String localNodeId, Version minNodeVersion, Version minNonClientNodeVersion) {
         this.nodes = nodes;
         this.dataNodes = dataNodes;
         this.masterNodes = masterNodes;
-        this.ingestNodes = ingestNodes;
         this.masterNodeId = masterNodeId;
         this.localNodeId = localNodeId;
         this.minNodeVersion = minNodeVersion;
@@ -169,13 +165,6 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
     }
 
     /**
-     * @return All the ingest nodes arranged by their ids
-     */
-    public ImmutableOpenMap<String, DiscoveryNode> getIngestNodes() {
-        return ingestNodes;
-    }
-
-    /**
      * Get a {@link Map} of the discovered master and data nodes arranged by their ids
      *
      * @return {@link Map} of the discovered master and data nodes arranged by their ids
@@ -665,7 +654,6 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
         public DiscoveryNodes build() {
             ImmutableOpenMap.Builder<String, DiscoveryNode> dataNodesBuilder = ImmutableOpenMap.builder();
             ImmutableOpenMap.Builder<String, DiscoveryNode> masterNodesBuilder = ImmutableOpenMap.builder();
-            ImmutableOpenMap.Builder<String, DiscoveryNode> ingestNodesBuilder = ImmutableOpenMap.builder();
             Version minNodeVersion = Version.CURRENT;
             Version minNonClientNodeVersion = Version.CURRENT;
             for (ObjectObjectCursor<String, DiscoveryNode> nodeEntry : nodes) {
@@ -677,16 +665,10 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
                     masterNodesBuilder.put(nodeEntry.key, nodeEntry.value);
                     minNonClientNodeVersion = Version.smallest(minNonClientNodeVersion, nodeEntry.value.version());
                 }
-                if (nodeEntry.value.isIngestNode()) {
-                    ingestNodesBuilder.put(nodeEntry.key, nodeEntry.value);
-                }
                 minNodeVersion = Version.smallest(minNodeVersion, nodeEntry.value.version());
             }
 
-            return new DiscoveryNodes(
-                nodes.build(), dataNodesBuilder.build(), masterNodesBuilder.build(), ingestNodesBuilder.build(),
-                masterNodeId, localNodeId, minNodeVersion, minNonClientNodeVersion
-            );
+            return new DiscoveryNodes(nodes.build(), dataNodesBuilder.build(), masterNodesBuilder.build(), masterNodeId, localNodeId, minNodeVersion, minNonClientNodeVersion);
         }
 
         public static DiscoveryNodes readFrom(StreamInput in, @Nullable DiscoveryNode localNode) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/Randomness.java b/core/src/main/java/org/elasticsearch/common/Randomness.java
index 7f71afc..154ebf3 100644
--- a/core/src/main/java/org/elasticsearch/common/Randomness.java
+++ b/core/src/main/java/org/elasticsearch/common/Randomness.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common;
 
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 
 import java.lang.reflect.Method;
@@ -40,7 +41,7 @@ import java.util.concurrent.ThreadLocalRandom;
  * setting a reproducible seed. When running the Elasticsearch server
  * process, non-reproducible sources of randomness are provided (unless
  * a setting is provided for a module that exposes a seed setting (e.g.,
- * DiscoveryService#SETTING_DISCOVERY_SEED)).
+ * DiscoveryService#DISCOVERY_SEED_SETTING)).
  */
 public final class Randomness {
     private static final Method currentMethod;
@@ -68,13 +69,12 @@ public final class Randomness {
      * seed in the settings with the key setting.
      *
      * @param settings the settings containing the seed
-     * @param setting  the key to access the seed
+     * @param setting  the setting to access the seed
      * @return a reproducible source of randomness
      */
-    public static Random get(Settings settings, String setting) {
-        Long maybeSeed = settings.getAsLong(setting, null);
-        if (maybeSeed != null) {
-            return new Random(maybeSeed);
+    public static Random get(Settings settings, Setting<Long> setting) {
+        if (setting.exists(settings)) {
+            return new Random(setting.get(settings));
         } else {
             return get();
         }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/Queries.java b/core/src/main/java/org/elasticsearch/common/lucene/search/Queries.java
index 01184d1..73c3fc9 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/Queries.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/search/Queries.java
@@ -117,12 +117,6 @@ public class Queries {
         if (minimumShouldMatch == null) {
             return query;
         }
-        // Queries with a single word expanded with synonyms 
-        // have their coordination factor disabled (@see org.apache.lucene.util.QueryBuilder#analyzeBoolean()).
-        // minimumShouldMatch should not be applicable in such case.
-        if (query.isCoordDisabled()) {
-            return query;
-        }
         int optionalClauses = 0;
         for (BooleanClause c : query.clauses()) {
             if (c.getOccur() == BooleanClause.Occur.SHOULD) {
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
index e5977c9..8be907e 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
@@ -28,7 +28,9 @@ import org.elasticsearch.client.transport.support.TransportProxyClient;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.Setting.Scope;
 import org.elasticsearch.common.util.ExtensionPoint;
 import org.elasticsearch.http.HttpServer;
 import org.elasticsearch.http.HttpServerTransport;
@@ -116,10 +118,6 @@ import org.elasticsearch.rest.action.get.RestGetSourceAction;
 import org.elasticsearch.rest.action.get.RestHeadAction;
 import org.elasticsearch.rest.action.get.RestMultiGetAction;
 import org.elasticsearch.rest.action.index.RestIndexAction;
-import org.elasticsearch.rest.action.ingest.RestDeletePipelineAction;
-import org.elasticsearch.rest.action.ingest.RestGetPipelineAction;
-import org.elasticsearch.rest.action.ingest.RestPutPipelineAction;
-import org.elasticsearch.rest.action.ingest.RestSimulatePipelineAction;
 import org.elasticsearch.rest.action.main.RestMainAction;
 import org.elasticsearch.rest.action.percolate.RestMultiPercolateAction;
 import org.elasticsearch.rest.action.percolate.RestPercolateAction;
@@ -154,7 +152,7 @@ public class NetworkModule extends AbstractModule {
     public static final String NETTY_TRANSPORT = "netty";
 
     public static final String HTTP_TYPE_KEY = "http.type";
-    public static final String HTTP_ENABLED = "http.enabled";
+    public static final Setting<Boolean> HTTP_ENABLED = Setting.boolSetting("http.enabled", true, false, Scope.CLUSTER);
 
     private static final List<Class<? extends RestHandler>> builtinRestHandlers = Arrays.asList(
         RestMainAction.class,
@@ -260,13 +258,7 @@ public class NetworkModule extends AbstractModule {
         RestCatAction.class,
 
         // Tasks API
-        RestListTasksAction.class,
-
-        // Ingest API
-        RestPutPipelineAction.class,
-        RestGetPipelineAction.class,
-        RestDeletePipelineAction.class,
-        RestSimulatePipelineAction.class
+        RestListTasksAction.class
     );
 
     private static final List<Class<? extends AbstractCatAction>> builtinCatHandlers = Arrays.asList(
@@ -378,7 +370,7 @@ public class NetworkModule extends AbstractModule {
             bind(TransportProxyClient.class).asEagerSingleton();
             bind(TransportClientNodesService.class).asEagerSingleton();
         } else {
-            if (settings.getAsBoolean(HTTP_ENABLED, true)) {
+            if (HTTP_ENABLED.get(settings)) {
                 bind(HttpServer.class).asEagerSingleton();
                 httpTransportTypes.bindType(binder(), settings, HTTP_TYPE_KEY, NETTY_TRANSPORT);
             }
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 835a35d..a1286aa 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.common.network;
 
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
@@ -41,31 +43,30 @@ public class NetworkService extends AbstractComponent {
     /** By default, we bind to loopback interfaces */
     public static final String DEFAULT_NETWORK_HOST = "_local_";
 
-    private static final String GLOBAL_NETWORK_HOST_SETTING = "network.host";
-    private static final String GLOBAL_NETWORK_BINDHOST_SETTING = "network.bind_host";
-    private static final String GLOBAL_NETWORK_PUBLISHHOST_SETTING = "network.publish_host";
+    public static final Setting<List<String>> GLOBAL_NETWORK_HOST_SETTING = Setting.listSetting("network.host", Arrays.asList(DEFAULT_NETWORK_HOST),
+            s -> s, false, Setting.Scope.CLUSTER);
+    public static final Setting<List<String>> GLOBAL_NETWORK_BINDHOST_SETTING = Setting.listSetting("network.bind_host", GLOBAL_NETWORK_HOST_SETTING,
+            s -> s, false, Setting.Scope.CLUSTER);
+    public static final Setting<List<String>> GLOBAL_NETWORK_PUBLISHHOST_SETTING = Setting.listSetting("network.publish_host", GLOBAL_NETWORK_HOST_SETTING,
+            s -> s, false, Setting.Scope.CLUSTER);
 
     public static final class TcpSettings {
-        public static final String TCP_NO_DELAY = "network.tcp.no_delay";
-        public static final String TCP_KEEP_ALIVE = "network.tcp.keep_alive";
-        public static final String TCP_REUSE_ADDRESS = "network.tcp.reuse_address";
-        public static final String TCP_SEND_BUFFER_SIZE = "network.tcp.send_buffer_size";
-        public static final String TCP_RECEIVE_BUFFER_SIZE = "network.tcp.receive_buffer_size";
-        public static final String TCP_BLOCKING = "network.tcp.blocking";
-        public static final String TCP_BLOCKING_SERVER = "network.tcp.blocking_server";
-        public static final String TCP_BLOCKING_CLIENT = "network.tcp.blocking_client";
-        public static final String TCP_CONNECT_TIMEOUT = "network.tcp.connect_timeout";
-
-        public static final ByteSizeValue TCP_DEFAULT_SEND_BUFFER_SIZE = null;
-        public static final ByteSizeValue TCP_DEFAULT_RECEIVE_BUFFER_SIZE = null;
-        public static final TimeValue TCP_DEFAULT_CONNECT_TIMEOUT = new TimeValue(30, TimeUnit.SECONDS);
+        public static final Setting<Boolean> TCP_NO_DELAY = Setting.boolSetting("network.tcp.no_delay", true, false, Setting.Scope.CLUSTER);
+        public static final Setting<Boolean> TCP_KEEP_ALIVE = Setting.boolSetting("network.tcp.keep_alive", true, false, Setting.Scope.CLUSTER);
+        public static final Setting<Boolean> TCP_REUSE_ADDRESS = Setting.boolSetting("network.tcp.reuse_address", NetworkUtils.defaultReuseAddress(), false, Setting.Scope.CLUSTER);
+        public static final Setting<ByteSizeValue> TCP_SEND_BUFFER_SIZE = Setting.byteSizeSetting("network.tcp.send_buffer_size", new ByteSizeValue(-1), false, Setting.Scope.CLUSTER);
+        public static final Setting<ByteSizeValue> TCP_RECEIVE_BUFFER_SIZE = Setting.byteSizeSetting("network.tcp.receive_buffer_size", new ByteSizeValue(-1), false, Setting.Scope.CLUSTER);
+        public static final Setting<Boolean> TCP_BLOCKING = Setting.boolSetting("network.tcp.blocking", false, false, Setting.Scope.CLUSTER);
+        public static final Setting<Boolean> TCP_BLOCKING_SERVER = Setting.boolSetting("network.tcp.blocking_server", TCP_BLOCKING, false, Setting.Scope.CLUSTER);
+        public static final Setting<Boolean> TCP_BLOCKING_CLIENT = Setting.boolSetting("network.tcp.blocking_client", TCP_BLOCKING, false, Setting.Scope.CLUSTER);
+        public static final Setting<TimeValue> TCP_CONNECT_TIMEOUT = Setting.timeSetting("network.tcp.connect_timeout", new TimeValue(30, TimeUnit.SECONDS), false, Setting.Scope.CLUSTER);
     }
 
     /**
      * A custom name resolver can support custom lookup keys (my_net_key:ipv4) and also change
      * the default inet address used in case no settings is provided.
      */
-    public static interface CustomNameResolver {
+    public interface CustomNameResolver {
         /**
          * Resolves the default value if possible. If not, return <tt>null</tt>.
          */
@@ -94,6 +95,7 @@ public class NetworkService extends AbstractComponent {
     /**
      * Resolves {@code bindHosts} to a list of internet addresses. The list will
      * not contain duplicate addresses.
+     *
      * @param bindHosts list of hosts to bind to. this may contain special pseudo-hostnames
      *                  such as _local_ (see the documentation). if it is null, it will be populated
      *                  based on global default settings.
@@ -102,21 +104,22 @@ public class NetworkService extends AbstractComponent {
     public InetAddress[] resolveBindHostAddresses(String bindHosts[]) throws IOException {
         // first check settings
         if (bindHosts == null) {
-            bindHosts = settings.getAsArray(GLOBAL_NETWORK_BINDHOST_SETTING, settings.getAsArray(GLOBAL_NETWORK_HOST_SETTING, null));
-        }
-        // next check any registered custom resolvers
-        if (bindHosts == null) {
-            for (CustomNameResolver customNameResolver : customNameResolvers) {
-                InetAddress addresses[] = customNameResolver.resolveDefault();
-                if (addresses != null) {
-                    return addresses;
+            if (GLOBAL_NETWORK_BINDHOST_SETTING.exists(settings) || GLOBAL_NETWORK_HOST_SETTING.exists(settings)) {
+                // if we have settings use them (we have a fallback to GLOBAL_NETWORK_HOST_SETTING inline
+                bindHosts = GLOBAL_NETWORK_BINDHOST_SETTING.get(settings).toArray(Strings.EMPTY_ARRAY);
+            } else {
+                // next check any registered custom resolvers
+                for (CustomNameResolver customNameResolver : customNameResolvers) {
+                    InetAddress addresses[] = customNameResolver.resolveDefault();
+                    if (addresses != null) {
+                        return addresses;
+                    }
                 }
+                // we know it's not here. get the defaults
+                bindHosts = GLOBAL_NETWORK_BINDHOST_SETTING.get(settings).toArray(Strings.EMPTY_ARRAY);
             }
         }
-        // finally, fill with our default
-        if (bindHosts == null) {
-            bindHosts = new String[] { DEFAULT_NETWORK_HOST };
-        }
+
         InetAddress addresses[] = resolveInetAddresses(bindHosts);
 
         // try to deal with some (mis)configuration
@@ -138,6 +141,7 @@ public class NetworkService extends AbstractComponent {
      * only one address is just a current limitation.
      * <p>
      * If {@code publishHosts} resolves to more than one address, <b>then one is selected with magic</b>
+     *
      * @param publishHosts list of hosts to publish as. this may contain special pseudo-hostnames
      *                     such as _local_ (see the documentation). if it is null, it will be populated
      *                     based on global default settings.
@@ -145,23 +149,23 @@ public class NetworkService extends AbstractComponent {
      */
     // TODO: needs to be InetAddress[]
     public InetAddress resolvePublishHostAddresses(String publishHosts[]) throws IOException {
-        // first check settings
         if (publishHosts == null) {
-            publishHosts = settings.getAsArray(GLOBAL_NETWORK_PUBLISHHOST_SETTING, settings.getAsArray(GLOBAL_NETWORK_HOST_SETTING, null));
-        }
-        // next check any registered custom resolvers
-        if (publishHosts == null) {
-            for (CustomNameResolver customNameResolver : customNameResolvers) {
-                InetAddress addresses[] = customNameResolver.resolveDefault();
-                if (addresses != null) {
-                    return addresses[0];
+            if (GLOBAL_NETWORK_PUBLISHHOST_SETTING.exists(settings) || GLOBAL_NETWORK_HOST_SETTING.exists(settings)) {
+                // if we have settings use them (we have a fallback to GLOBAL_NETWORK_HOST_SETTING inline
+                publishHosts = GLOBAL_NETWORK_PUBLISHHOST_SETTING.get(settings).toArray(Strings.EMPTY_ARRAY);
+            } else {
+                // next check any registered custom resolvers
+                for (CustomNameResolver customNameResolver : customNameResolvers) {
+                    InetAddress addresses[] = customNameResolver.resolveDefault();
+                    if (addresses != null) {
+                        return addresses[0];
+                    }
                 }
+                // we know it's not here. get the defaults
+                publishHosts = GLOBAL_NETWORK_PUBLISHHOST_SETTING.get(settings).toArray(Strings.EMPTY_ARRAY);
             }
         }
-        // finally, fill with our default
-        if (publishHosts == null) {
-            publishHosts = new String[] { DEFAULT_NETWORK_HOST };
-        }
+
         InetAddress addresses[] = resolveInetAddresses(publishHosts);
         // TODO: allow publishing multiple addresses
         // for now... the hack begins
@@ -184,17 +188,17 @@ public class NetworkService extends AbstractComponent {
                 throw new IllegalArgumentException("publish address: {" + NetworkAddress.format(address) + "} is wildcard, but multiple addresses specified: this makes no sense");
             }
         }
-        
+
         // 3. if we end out with multiple publish addresses, select by preference.
         // don't warn the user, or they will get confused by bind_host vs publish_host etc.
         if (addresses.length > 1) {
             List<InetAddress> sorted = new ArrayList<>(Arrays.asList(addresses));
             NetworkUtils.sortAddresses(sorted);
-            addresses = new InetAddress[] { sorted.get(0) };
+            addresses = new InetAddress[]{sorted.get(0)};
         }
         return addresses[0];
     }
-    
+
     /** resolves (and deduplicates) host specification */
     private InetAddress[] resolveInetAddresses(String hosts[]) throws IOException {
         if (hosts.length == 0) {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
index b260cd6..4680146 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
@@ -20,6 +20,7 @@ package org.elasticsearch.common.settings;
 
 import org.elasticsearch.action.admin.indices.close.TransportCloseIndexAction;
 import org.elasticsearch.action.support.DestructiveOperations;
+import org.elasticsearch.client.transport.TransportClientNodesService;
 import org.elasticsearch.cluster.InternalClusterInfoService;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -35,16 +36,32 @@ import org.elasticsearch.cluster.routing.allocation.decider.SnapshotInProgressAl
 import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.cluster.service.InternalClusterService;
 import org.elasticsearch.common.logging.ESLoggerFactory;
+import org.elasticsearch.discovery.DiscoveryModule;
+import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.common.network.NetworkModule;
+import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.gateway.GatewayService;
+import org.elasticsearch.discovery.zen.fd.FaultDetection;
+import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.gateway.PrimaryShardAllocator;
+import org.elasticsearch.http.netty.NettyHttpServerTransport;
 import org.elasticsearch.index.IndexSettings;
 import org.elasticsearch.index.store.IndexStoreConfig;
+import org.elasticsearch.indices.analysis.HunspellService;
 import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
+import org.elasticsearch.indices.cache.request.IndicesRequestCache;
+import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
 import org.elasticsearch.indices.recovery.RecoverySettings;
+import org.elasticsearch.indices.store.IndicesStore;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
 import org.elasticsearch.node.Node;
+import org.elasticsearch.repositories.fs.FsRepository;
+import org.elasticsearch.repositories.uri.URLRepository;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.Transport;
@@ -99,6 +116,9 @@ public final class ClusterSettings extends AbstractScopedSettings {
 
 
     public static Set<Setting<?>> BUILT_IN_CLUSTER_SETTINGS = Collections.unmodifiableSet(new HashSet<>(Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
+        TransportClientNodesService.CLIENT_TRANSPORT_NODES_SAMPLER_INTERVAL, // TODO these transport client settings are kind of odd here and should only be valid if we are a transport client
+        TransportClientNodesService.CLIENT_TRANSPORT_PING_TIMEOUT,
+        TransportClientNodesService.CLIENT_TRANSPORT_IGNORE_CLUSTER_NAME,
         AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
         BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
         BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
@@ -111,6 +131,9 @@ public final class ClusterSettings extends AbstractScopedSettings {
         FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
         FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
         FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
+        FsRepository.REPOSITORIES_CHUNK_SIZE_SETTING,
+        FsRepository.REPOSITORIES_COMPRESS_SETTING,
+        FsRepository.REPOSITORIES_LOCATION_SETTING,
         IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
         IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
         IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
@@ -140,6 +163,19 @@ public final class ClusterSettings extends AbstractScopedSettings {
         DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
         DiscoverySettings.COMMIT_TIMEOUT_SETTING,
         DiscoverySettings.NO_MASTER_BLOCK_SETTING,
+        GatewayService.EXPECTED_DATA_NODES_SETTING,
+        GatewayService.EXPECTED_MASTER_NODES_SETTING,
+        GatewayService.EXPECTED_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_DATA_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_MASTER_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_NODES_SETTING,
+        GatewayService.RECOVER_AFTER_TIME_SETTING,
+        NetworkModule.HTTP_ENABLED,
+        NettyHttpServerTransport.SETTING_CORS_ALLOW_CREDENTIALS,
+        NettyHttpServerTransport.SETTING_CORS_ENABLED,
+        NettyHttpServerTransport.SETTING_CORS_MAX_AGE,
+        NettyHttpServerTransport.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
+        NettyHttpServerTransport.SETTING_PIPELINING,       
         HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
         HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
         HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
@@ -157,8 +193,64 @@ public final class ClusterSettings extends AbstractScopedSettings {
         HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING,
         Transport.TRANSPORT_PROFILES_SETTING,
         Transport.TRANSPORT_TCP_COMPRESS,
+        NetworkService.GLOBAL_NETWORK_HOST_SETTING,
+        NetworkService.GLOBAL_NETWORK_BINDHOST_SETTING,
+        NetworkService.GLOBAL_NETWORK_PUBLISHHOST_SETTING,
+        NetworkService.TcpSettings.TCP_NO_DELAY,
+        NetworkService.TcpSettings.TCP_KEEP_ALIVE,
+        NetworkService.TcpSettings.TCP_REUSE_ADDRESS,
+        NetworkService.TcpSettings.TCP_SEND_BUFFER_SIZE,
+        NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE,
+        NetworkService.TcpSettings.TCP_BLOCKING,
+        NetworkService.TcpSettings.TCP_BLOCKING_SERVER,
+        NetworkService.TcpSettings.TCP_BLOCKING_CLIENT,
+        NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT,
         IndexSettings.QUERY_STRING_ANALYZE_WILDCARD,
         IndexSettings.QUERY_STRING_ALLOW_LEADING_WILDCARD,
         PrimaryShardAllocator.NODE_INITIAL_SHARDS_SETTING,
-        Node.NODE_INGEST_SETTING)));
+        ScriptService.SCRIPT_CACHE_SIZE_SETTING,
+        IndicesFieldDataCache.INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING,
+        IndicesFieldDataCache.INDICES_FIELDDATA_CACHE_SIZE_KEY,
+        IndicesRequestCache.INDICES_CACHE_QUERY_SIZE,
+        IndicesRequestCache.INDICES_CACHE_QUERY_EXPIRE,
+        HunspellService.HUNSPELL_LAZY_LOAD,
+        HunspellService.HUNSPELL_IGNORE_CASE,
+        HunspellService.HUNSPELL_DICTIONARY_OPTIONS,
+        IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT,
+        Environment.PATH_CONF_SETTING,
+        Environment.PATH_DATA_SETTING,
+        Environment.PATH_HOME_SETTING,
+        Environment.PATH_LOGS_SETTING,
+        Environment.PATH_PLUGINS_SETTING,
+        Environment.PATH_REPO_SETTING,
+        Environment.PATH_SCRIPTS_SETTING,
+        Environment.PATH_SHARED_DATA_SETTING,
+        Environment.PIDFILE_SETTING,
+        DiscoveryService.DISCOVERY_SEED_SETTING,
+        DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING,
+        DiscoveryModule.DISCOVERY_TYPE_SETTING,
+        DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING,
+        FaultDetection.PING_RETRIES_SETTING,
+        FaultDetection.PING_TIMEOUT_SETTING,
+        FaultDetection.REGISTER_CONNECTION_LISTENER_SETTING,
+        FaultDetection.PING_INTERVAL_SETTING,
+        FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING,
+        ZenDiscovery.PING_TIMEOUT_SETTING,
+        ZenDiscovery.JOIN_TIMEOUT_SETTING,
+        ZenDiscovery.JOIN_RETRY_ATTEMPTS_SETTING,
+        ZenDiscovery.JOIN_RETRY_DELAY_SETTING,
+        ZenDiscovery.MAX_PINGS_FROM_ANOTHER_MASTER_SETTING,
+        ZenDiscovery.SEND_LEAVE_REQUEST_SETTING,
+        ZenDiscovery.MASTER_ELECTION_FILTER_CLIENT_SETTING,
+        ZenDiscovery.MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING,
+        ZenDiscovery.MASTER_ELECTION_FILTER_DATA_SETTING,
+        UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING,
+        UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING,
+        SearchService.DEFAULT_KEEPALIVE_SETTING,
+        SearchService.KEEPALIVE_INTERVAL_SETTING,
+        Node.WRITE_PORTS_FIELD_SETTING,
+        URLRepository.ALLOWED_URLS_SETTING,
+        URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
+        URLRepository.REPOSITORIES_URL_SETTING,
+        URLRepository.SUPPORTED_PROTOCOLS_SETTING)));
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java b/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
index 997ea79..86ead8c 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java
@@ -152,4 +152,17 @@ public final class IndexScopedSettings extends AbstractScopedSettings {
     public IndexScopedSettings copy(Settings settings, IndexMetaData metaData) {
         return new IndexScopedSettings(settings, this, metaData);
     }
+
+    public boolean isPrivateSetting(String key) {
+        switch (key) {
+            case IndexMetaData.SETTING_CREATION_DATE:
+            case IndexMetaData.SETTING_INDEX_UUID:
+            case IndexMetaData.SETTING_VERSION_CREATED:
+            case IndexMetaData.SETTING_VERSION_UPGRADED:
+            case MergePolicyConfig.INDEX_MERGE_ENABLED:
+                return true;
+            default:
+                return false;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Setting.java b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
index 0fc5b06..22cab99 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Setting.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
@@ -41,6 +41,7 @@ import java.util.function.BiConsumer;
 import java.util.function.Consumer;
 import java.util.function.Function;
 import java.util.regex.Pattern;
+import java.util.stream.Collectors;
 
 /**
  * A setting. Encapsulates typical stuff like default value, parsing, and scope.
@@ -71,7 +72,20 @@ public class Setting<T> extends ToXContentToBytes {
     }
 
     /**
+     * Creates a new Setting instance
+     * @param key the settings key for this setting.
+     * @param fallBackSetting a setting to fall back to if the current setting is not set.
+     * @param parser a parser that parses the string rep into a complex datatype.
+     * @param dynamic true iff this setting can be dynamically updateable
+     * @param scope the scope of this setting
+     */
+    public Setting(String key, Setting<T> fallBackSetting, Function<String, T> parser, boolean dynamic, Scope scope) {
+        this(key, fallBackSetting::getRaw, parser, dynamic, scope);
+    }
+
+    /**
      * Returns the settings key or a prefix if this setting is a group setting
+     *
      * @see #isGroupSetting()
      */
     public final String getKey() {
@@ -106,14 +120,22 @@ public class Setting<T> extends ToXContentToBytes {
     }
 
     /**
-     * Returns the default values string representation for this setting.
+     * Returns the default value string representation for this setting.
      * @param settings a settings object for settings that has a default value depending on another setting if available
      */
-    public final String getDefault(Settings settings) {
+    public final String getDefaultRaw(Settings settings) {
         return defaultValue.apply(settings);
     }
 
     /**
+     * Returns the default value for this setting.
+     * @param settings a settings object for settings that has a default value depending on another setting if available
+     */
+    public final T getDefault(Settings settings) {
+        return parser.apply(getDefaultRaw(settings));
+    }
+
+    /**
      * Returns <code>true</code> iff this setting is present in the given settings object. Otherwise <code>false</code>
      */
     public final boolean exists(Settings settings) {
@@ -309,6 +331,10 @@ public class Setting<T> extends ToXContentToBytes {
         return new Setting<>(key, (s) -> Long.toString(defaultValue), (s) -> parseLong(s, minValue, key), dynamic, scope);
     }
 
+    public static Setting<String> simpleString(String key, boolean dynamic, Scope scope) {
+        return new Setting<>(key, "", Function.identity(), dynamic, scope);
+    }
+
     public static int parseInt(String s, int minValue, String key) {
         int value = Integer.parseInt(s);
         if (value < minValue) {
@@ -333,6 +359,10 @@ public class Setting<T> extends ToXContentToBytes {
         return new Setting<>(key, (s) -> Boolean.toString(defaultValue), Booleans::parseBooleanExact, dynamic, scope);
     }
 
+    public static Setting<Boolean> boolSetting(String key, Setting<Boolean> fallbackSetting, boolean dynamic, Scope scope) {
+        return new Setting<>(key, fallbackSetting, Booleans::parseBooleanExact, dynamic, scope);
+    }
+
     public static Setting<ByteSizeValue> byteSizeSetting(String key, String percentage, boolean dynamic, Scope scope) {
         return new Setting<>(key, (s) -> percentage, (s) -> MemorySizeValue.parseBytesSizeValueOrHeapRatio(s, key), dynamic, scope);
     }
@@ -348,25 +378,15 @@ public class Setting<T> extends ToXContentToBytes {
     public static <T> Setting<List<T>> listSetting(String key, List<String> defaultStringValue, Function<String, T> singleValueParser, boolean dynamic, Scope scope) {
         return listSetting(key, (s) -> defaultStringValue, singleValueParser, dynamic, scope);
     }
+
+    public static <T> Setting<List<T>> listSetting(String key, Setting<List<T>> fallbackSetting, Function<String, T> singleValueParser, boolean dynamic, Scope scope) {
+        return listSetting(key, (s) -> parseableStringToList(fallbackSetting.getRaw(s)), singleValueParser, dynamic, scope);
+    }
+
     public static <T> Setting<List<T>> listSetting(String key, Function<Settings, List<String>> defaultStringValue, Function<String, T> singleValueParser, boolean dynamic, Scope scope) {
-        Function<String, List<T>> parser = (s) -> {
-            try (XContentParser xContentParser = XContentType.JSON.xContent().createParser(s)){
-                XContentParser.Token token = xContentParser.nextToken();
-                if (token != XContentParser.Token.START_ARRAY) {
-                    throw new IllegalArgumentException("expected START_ARRAY but got " + token);
-                }
-                ArrayList<T> list = new ArrayList<>();
-                while ((token = xContentParser.nextToken()) !=XContentParser.Token.END_ARRAY) {
-                    if (token != XContentParser.Token.VALUE_STRING) {
-                        throw new IllegalArgumentException("expected VALUE_STRING but got " + token);
-                    }
-                    list.add(singleValueParser.apply(xContentParser.text()));
-                }
-                return list;
-            } catch (IOException e) {
-                throw new IllegalArgumentException("failed to parse array", e);
-            }
-        };
+        Function<String, List<T>> parser = (s) ->
+                parseableStringToList(s).stream().map(singleValueParser).collect(Collectors.toList());
+
         return new Setting<List<T>>(key, (s) -> arrayToParsableString(defaultStringValue.apply(s).toArray(Strings.EMPTY_ARRAY)), parser, dynamic, scope) {
             private final Pattern pattern = Pattern.compile(Pattern.quote(key)+"(\\.\\d+)?");
             @Override
@@ -387,6 +407,26 @@ public class Setting<T> extends ToXContentToBytes {
         };
     }
 
+    private static List<String> parseableStringToList(String parsableString) {
+        try (XContentParser xContentParser = XContentType.JSON.xContent().createParser(parsableString)) {
+            XContentParser.Token token = xContentParser.nextToken();
+            if (token != XContentParser.Token.START_ARRAY) {
+                throw new IllegalArgumentException("expected START_ARRAY but got " + token);
+            }
+            ArrayList<String> list = new ArrayList<>();
+            while ((token = xContentParser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                if (token != XContentParser.Token.VALUE_STRING) {
+                    throw new IllegalArgumentException("expected VALUE_STRING but got " + token);
+                }
+                list.add(xContentParser.text());
+            }
+            return list;
+        } catch (IOException e) {
+            throw new IllegalArgumentException("failed to parse array", e);
+        }
+    }
+
+
     private static String arrayToParsableString(String[] array) {
         try {
             XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent());
diff --git a/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java b/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
index feebd93..d0e9164 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/DistanceUnit.java
@@ -22,6 +22,7 @@ package org.elasticsearch.common.unit;
 import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
 
 import java.io.IOException;
 
@@ -32,7 +33,7 @@ import java.io.IOException;
  * the earth ellipsoid defined in {@link GeoUtils}. The default unit used within
  * this project is <code>METERS</code> which is defined by <code>DEFAULT</code>
  */
-public enum DistanceUnit {
+public enum DistanceUnit implements Writeable<DistanceUnit> {
     INCH(0.0254, "in", "inch"),
     YARD(0.9144, "yd", "yards"),
     FEET(0.3048, "ft", "feet"),
@@ -322,4 +323,24 @@ public enum DistanceUnit {
             return new Distance(Double.parseDouble(distance), defaultUnit);
         }
     }
+
+    private static final DistanceUnit PROTOTYPE = DEFAULT;
+
+    @Override
+    public DistanceUnit readFrom(StreamInput in) throws IOException {
+        int ordinal = in.readVInt();
+        if (ordinal < 0 || ordinal >= values().length) {
+            throw new IOException("Unknown DistanceUnit ordinal [" + ordinal + "]");
+        }
+        return values()[ordinal];
+    }
+
+    public static DistanceUnit readUnitFrom(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeVInt(this.ordinal());
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/MultiDataPathUpgrader.java b/core/src/main/java/org/elasticsearch/common/util/MultiDataPathUpgrader.java
deleted file mode 100644
index 8d04900..0000000
--- a/core/src/main/java/org/elasticsearch/common/util/MultiDataPathUpgrader.java
+++ /dev/null
@@ -1,383 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.util;
-
-import org.apache.lucene.index.CheckIndex;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.Lock;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.io.FileSystemUtils;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.env.NodeEnvironment;
-import org.elasticsearch.env.ShardLock;
-import org.elasticsearch.gateway.MetaDataStateFormat;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.index.shard.ShardStateMetaData;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.DirectoryStream;
-import java.nio.file.FileStore;
-import java.nio.file.FileVisitResult;
-import java.nio.file.FileVisitor;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.StandardCopyOption;
-import java.nio.file.attribute.BasicFileAttributes;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-/**
- */
-public class MultiDataPathUpgrader {
-
-    private final NodeEnvironment nodeEnvironment;
-    private final ESLogger logger = Loggers.getLogger(getClass());
-
-
-    /**
-     * Creates a new upgrader instance
-     * @param nodeEnvironment the node env to operate on.
-     *
-     */
-    public MultiDataPathUpgrader(NodeEnvironment nodeEnvironment) {
-        this.nodeEnvironment = nodeEnvironment;
-    }
-
-
-    /**
-     * Upgrades the given shard Id from multiple shard paths into the given target path.
-     *
-     * @see #pickShardPath(org.elasticsearch.index.shard.ShardId)
-     */
-    public void upgrade(ShardId shard, ShardPath targetPath) throws IOException {
-        final Path[] paths = nodeEnvironment.availableShardPaths(shard); // custom data path doesn't need upgrading
-        if (isTargetPathConfigured(paths, targetPath) == false) {
-            throw new IllegalArgumentException("shard path must be one of the shards data paths");
-        }
-        assert needsUpgrading(shard) : "Should not upgrade a path that needs no upgrading";
-        logger.info("{} upgrading multi data dir to {}", shard, targetPath.getDataPath());
-        final ShardStateMetaData loaded = ShardStateMetaData.FORMAT.loadLatestState(logger, paths);
-        if (loaded == null) {
-            throw new IllegalStateException(shard + " no shard state found in any of: " + Arrays.toString(paths) + " please check and remove them if possible");
-        }
-        logger.info("{} loaded shard state {}", shard, loaded);
-
-        ShardStateMetaData.FORMAT.write(loaded, loaded.version, targetPath.getShardStatePath());
-        Files.createDirectories(targetPath.resolveIndex());
-        try (SimpleFSDirectory directory = new SimpleFSDirectory(targetPath.resolveIndex())) {
-            try (final Lock lock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {
-                upgradeFiles(shard, targetPath, targetPath.resolveIndex(), ShardPath.INDEX_FOLDER_NAME, paths);
-            } catch (LockObtainFailedException ex) {
-                throw new IllegalStateException("Can't obtain lock on " + targetPath.resolveIndex(), ex);
-            }
-
-        }
-
-
-        upgradeFiles(shard, targetPath, targetPath.resolveTranslog(), ShardPath.TRANSLOG_FOLDER_NAME, paths);
-
-        logger.info("{} wipe upgraded directories", shard);
-        for (Path path : paths) {
-            if (path.equals(targetPath.getShardStatePath()) == false) {
-                logger.info("{} wipe shard directories: [{}]", shard, path);
-                IOUtils.rm(path);
-            }
-        }
-
-        if (FileSystemUtils.files(targetPath.resolveIndex()).length == 0) {
-            throw new IllegalStateException("index folder [" + targetPath.resolveIndex() + "] is empty");
-        }
-
-        if (FileSystemUtils.files(targetPath.resolveTranslog()).length == 0) {
-            throw new IllegalStateException("translog folder [" + targetPath.resolveTranslog() + "] is empty");
-        }
-    }
-
-    /**
-     * Runs check-index on the target shard and throws an exception if it failed
-     */
-    public void checkIndex(ShardPath targetPath) throws IOException {
-        BytesStreamOutput os = new BytesStreamOutput();
-        PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name());
-        try (Directory directory = new SimpleFSDirectory(targetPath.resolveIndex());
-            final CheckIndex checkIndex = new CheckIndex(directory)) {
-            checkIndex.setInfoStream(out);
-            CheckIndex.Status status = checkIndex.checkIndex();
-            out.flush();
-            if (!status.clean) {
-                logger.warn("check index [failure]\n{}", new String(os.bytes().toBytes(), StandardCharsets.UTF_8));
-                throw new IllegalStateException("index check failure");
-            }
-        }
-    }
-
-    /**
-     * Returns true iff the given shard needs upgrading.
-     */
-    public boolean needsUpgrading(ShardId shard) {
-        final Path[] paths = nodeEnvironment.availableShardPaths(shard);
-         // custom data path doesn't need upgrading neither single path envs
-        if (paths.length > 1) {
-            int numPathsExist = 0;
-            for (Path path : paths) {
-                if (Files.exists(path.resolve(MetaDataStateFormat.STATE_DIR_NAME))) {
-                    numPathsExist++;
-                    if (numPathsExist > 1) {
-                        return true;
-                    }
-                }
-            }
-        }
-        return false;
-    }
-
-    /**
-     * Picks a target ShardPath to allocate and upgrade the given shard to. It picks the target based on a simple
-     * heuristic:
-     * <ul>
-     *  <li>if the smallest datapath has 2x more space available that the shards total size the datapath with the most bytes for that shard is picked to minimize the amount of bytes to copy</li>
-     *  <li>otherwise the largest available datapath is used as the target no matter how big of a slice of the shard it already holds.</li>
-     * </ul>
-     */
-    public ShardPath pickShardPath(ShardId shard) throws IOException {
-        if (needsUpgrading(shard) == false) {
-            throw new IllegalStateException("Shard doesn't need upgrading");
-        }
-        final NodeEnvironment.NodePath[] paths = nodeEnvironment.nodePaths();
-
-        // if we need upgrading make sure we have all paths.
-        for (NodeEnvironment.NodePath path : paths) {
-            Files.createDirectories(path.resolve(shard));
-        }
-        final ShardFileInfo[] shardFileInfo = getShardFileInfo(shard, paths);
-        long totalBytesUsedByShard = 0;
-        long leastUsableSpace = Long.MAX_VALUE;
-        long mostUsableSpace = Long.MIN_VALUE;
-        assert shardFileInfo.length ==  nodeEnvironment.availableShardPaths(shard).length;
-        for (ShardFileInfo info : shardFileInfo) {
-            totalBytesUsedByShard += info.spaceUsedByShard;
-            leastUsableSpace = Math.min(leastUsableSpace, info.usableSpace + info.spaceUsedByShard);
-            mostUsableSpace = Math.max(mostUsableSpace, info.usableSpace + info.spaceUsedByShard);
-        }
-
-        if (mostUsableSpace < totalBytesUsedByShard) {
-            throw new IllegalStateException("Can't upgrade path available space: " + new ByteSizeValue(mostUsableSpace) + " required space: " + new ByteSizeValue(totalBytesUsedByShard));
-        }
-        ShardFileInfo target = shardFileInfo[0];
-        if (leastUsableSpace >= (2 * totalBytesUsedByShard)) {
-            for (ShardFileInfo info : shardFileInfo) {
-                if (info.spaceUsedByShard > target.spaceUsedByShard) {
-                    target = info;
-                }
-            }
-        } else {
-            for (ShardFileInfo info : shardFileInfo) {
-                if (info.usableSpace > target.usableSpace) {
-                    target = info;
-                }
-            }
-        }
-        return new ShardPath(false, target.path, target.path, IndexMetaData.INDEX_UUID_NA_VALUE /* we don't know */, shard);
-    }
-
-    private ShardFileInfo[] getShardFileInfo(ShardId shard, NodeEnvironment.NodePath[] paths) throws IOException {
-        final ShardFileInfo[] info = new ShardFileInfo[paths.length];
-        for (int i = 0; i < info.length; i++) {
-            Path path = paths[i].resolve(shard);
-            final long usabelSpace = getUsabelSpace(paths[i]);
-            info[i] = new ShardFileInfo(path, usabelSpace, getSpaceUsedByShard(path));
-        }
-        return info;
-    }
-
-    protected long getSpaceUsedByShard(Path path) throws IOException {
-        final long[] spaceUsedByShard = new long[] {0};
-        if (Files.exists(path)) {
-            Files.walkFileTree(path, new FileVisitor<Path>() {
-                @Override
-                public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException {
-                    return FileVisitResult.CONTINUE;
-                }
-
-                @Override
-                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
-                    if (attrs.isRegularFile()) {
-                        spaceUsedByShard[0] += attrs.size();
-                    }
-                    return FileVisitResult.CONTINUE;
-                }
-
-                @Override
-                public FileVisitResult visitFileFailed(Path file, IOException exc) throws IOException {
-                    return FileVisitResult.CONTINUE;
-                }
-
-                @Override
-                public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {
-                    return FileVisitResult.CONTINUE;
-                }
-            });
-        }
-        return spaceUsedByShard[0];
-    }
-
-    protected long getUsabelSpace(NodeEnvironment.NodePath path) throws IOException {
-        FileStore fileStore = path.fileStore;
-        return fileStore.getUsableSpace();
-    }
-
-    static class ShardFileInfo {
-        final Path path;
-        final long usableSpace;
-        final long spaceUsedByShard;
-
-        ShardFileInfo(Path path, long usableSpace, long spaceUsedByShard) {
-            this.path = path;
-            this.usableSpace = usableSpace;
-            this.spaceUsedByShard = spaceUsedByShard;
-        }
-    }
-
-
-
-    private void upgradeFiles(ShardId shard, ShardPath targetPath, final Path targetDir, String folderName, Path[] paths) throws IOException {
-        List<Path> movedFiles = new ArrayList<>();
-        for (Path path : paths) {
-            if (path.equals(targetPath.getDataPath()) == false) {
-                final Path sourceDir = path.resolve(folderName);
-                if (Files.exists(sourceDir)) {
-                    logger.info("{} upgrading [{}] from [{}] to [{}]", shard, folderName, sourceDir, targetDir);
-                    try (DirectoryStream<Path> stream = Files.newDirectoryStream(sourceDir)) {
-                        Files.createDirectories(targetDir);
-                        for (Path file : stream) {
-                            if (IndexWriter.WRITE_LOCK_NAME.equals(file.getFileName().toString()) || Files.isDirectory(file)) {
-                                continue; // skip write.lock
-                            }
-                            logger.info("{} move file [{}] size: [{}]", shard, file.getFileName(), Files.size(file));
-                            final Path targetFile = targetDir.resolve(file.getFileName());
-                            /* We are pessimistic and do a copy first to the other path and then and atomic move to rename it such that
-                               in the worst case the file exists twice but is never lost or half written.*/
-                            final Path targetTempFile = Files.createTempFile(targetDir, "upgrade_", "_" + file.getFileName().toString());
-                            Files.copy(file, targetTempFile, StandardCopyOption.COPY_ATTRIBUTES, StandardCopyOption.REPLACE_EXISTING);
-                            Files.move(targetTempFile, targetFile, StandardCopyOption.ATOMIC_MOVE); // we are on the same FS - this must work otherwise all bets are off
-                            Files.delete(file);
-                            movedFiles.add(targetFile);
-                        }
-                    }
-                }
-            }
-        }
-        if (movedFiles.isEmpty() == false) {
-            // fsync later it might be on disk already
-            logger.info("{} fsync files", shard);
-            for (Path moved : movedFiles) {
-                logger.info("{} syncing [{}]", shard, moved.getFileName());
-                IOUtils.fsync(moved, false);
-            }
-            logger.info("{} syncing directory [{}]", shard, targetDir);
-            IOUtils.fsync(targetDir, true);
-        }
-    }
-
-
-    /**
-     * Returns <code>true</code> iff the target path is one of the given paths.
-     */
-    private boolean isTargetPathConfigured(final Path[] paths, ShardPath targetPath) {
-        for (Path path : paths) {
-            if (path.equals(targetPath.getDataPath())) {
-                return true;
-            }
-        }
-        return false;
-    }
-
-    /**
-     * Runs an upgrade on all shards located under the given node environment if there is more than 1 data.path configured
-     * otherwise this method will return immediately.
-     */
-    public static void upgradeMultiDataPath(NodeEnvironment nodeEnv, ESLogger logger) throws IOException {
-        if (nodeEnv.nodeDataPaths().length > 1) {
-            final MultiDataPathUpgrader upgrader = new MultiDataPathUpgrader(nodeEnv);
-            final Set<String> allIndices = nodeEnv.findAllIndices();
-
-            for (String index : allIndices) {
-                for (ShardId shardId : findAllShardIds(nodeEnv.indexPaths(new Index(index)))) {
-                    try (ShardLock lock = nodeEnv.shardLock(shardId, 0)) {
-                        if (upgrader.needsUpgrading(shardId)) {
-                            final ShardPath shardPath = upgrader.pickShardPath(shardId);
-                            upgrader.upgrade(shardId, shardPath);
-                            // we have to check if the index path exists since we might
-                            // have only upgraded the shard state that is written under /indexname/shardid/_state
-                            // in the case we upgraded a dedicated index directory index
-                            if (Files.exists(shardPath.resolveIndex())) {
-                                upgrader.checkIndex(shardPath);
-                            }
-                        } else {
-                            logger.debug("{} no upgrade needed - already upgraded");
-                        }
-                    }
-                }
-            }
-        }
-    }
-
-    private static Set<ShardId> findAllShardIds(Path... locations) throws IOException {
-        final Set<ShardId> shardIds = new HashSet<>();
-        for (final Path location : locations) {
-            if (Files.isDirectory(location)) {
-                shardIds.addAll(findAllShardsForIndex(location));
-            }
-        }
-        return shardIds;
-    }
-
-    private static Set<ShardId> findAllShardsForIndex(Path indexPath) throws IOException {
-        Set<ShardId> shardIds = new HashSet<>();
-        if (Files.isDirectory(indexPath)) {
-            try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexPath)) {
-                String currentIndex = indexPath.getFileName().toString();
-                for (Path shardPath : stream) {
-                    String fileName = shardPath.getFileName().toString();
-                    if (Files.isDirectory(shardPath) && fileName.chars().allMatch(Character::isDigit)) {
-                        int shardId = Integer.parseInt(fileName);
-                        ShardId id = new ShardId(currentIndex, shardId);
-                        shardIds.add(id);
-                    }
-                }
-            }
-        }
-        return shardIds;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
index 395dcad..979a1f2 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
@@ -223,7 +223,7 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
             list.add(supplier.get()); // single value
         } else {
             while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
-                if (parser.currentToken().isValue() || parser.currentToken() == XContentParser.Token.START_OBJECT) {
+                if (parser.currentToken().isValue()) {
                     list.add(supplier.get());
                 } else {
                     throw new IllegalStateException("expected value but got [" + parser.currentToken() + "]");
@@ -237,11 +237,6 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
         declareField((p, v, c) -> consumer.accept(v, objectParser.apply(p, c)), field, ValueType.OBJECT);
     }
 
-    public <T> void declareObjectArray(BiConsumer<Value, List<T>> consumer, BiFunction<XContentParser, Context, T> objectParser, ParseField field) {
-        declareField((p, v, c) -> consumer.accept(v, parseArray(p, () -> objectParser.apply(p, c))), field, ValueType.OBJECT_ARRAY);
-    }
-
-
     public <T> void declareObjectOrDefault(BiConsumer<Value, T> consumer, BiFunction<XContentParser, Context, T> objectParser, Supplier<T> defaultValue, ParseField field) {
         declareField((p, v, c) -> {
             if (p.currentToken() == XContentParser.Token.VALUE_BOOLEAN) {
@@ -338,7 +333,6 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
         INT_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY, XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
         BOOLEAN_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY, XContentParser.Token.VALUE_BOOLEAN)),
         OBJECT(EnumSet.of(XContentParser.Token.START_OBJECT)),
-        OBJECT_ARRAY(EnumSet.of(XContentParser.Token.START_OBJECT, XContentParser.Token.START_ARRAY)),
         OBJECT_OR_BOOLEAN(EnumSet.of(XContentParser.Token.START_OBJECT, XContentParser.Token.VALUE_BOOLEAN)),
         VALUE(EnumSet.of(XContentParser.Token.VALUE_BOOLEAN, XContentParser.Token.VALUE_NULL ,XContentParser.Token.VALUE_EMBEDDED_OBJECT,XContentParser.Token.VALUE_NUMBER,XContentParser.Token.VALUE_STRING));
 
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java b/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
index 1ab6087..b51339a 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java
@@ -22,6 +22,7 @@ package org.elasticsearch.discovery;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.ExtensionPoint;
 import org.elasticsearch.discovery.local.LocalDiscovery;
@@ -36,14 +37,17 @@ import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.function.Function;
 
 /**
  * A module for loading classes for node discovery.
  */
 public class DiscoveryModule extends AbstractModule {
 
-    public static final String DISCOVERY_TYPE_KEY = "discovery.type";
-    public static final String ZEN_MASTER_SERVICE_TYPE_KEY = "discovery.zen.masterservice.type";
+    public static final Setting<String> DISCOVERY_TYPE_SETTING = new Setting<>("discovery.type",
+        settings -> DiscoveryNode.localNode(settings) ? "local" : "zen", Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<String> ZEN_MASTER_SERVICE_TYPE_SETTING = new Setting<>("discovery.zen.masterservice.type",
+            "zen", Function.identity(), false, Setting.Scope.CLUSTER);
 
     private final Settings settings;
     private final List<Class<? extends UnicastHostsProvider>> unicastHostProviders = new ArrayList<>();
@@ -93,15 +97,14 @@ public class DiscoveryModule extends AbstractModule {
 
     @Override
     protected void configure() {
-        String defaultType = DiscoveryNode.localNode(settings) ? "local" : "zen";
-        String discoveryType = settings.get(DISCOVERY_TYPE_KEY, defaultType);
+        String discoveryType = DISCOVERY_TYPE_SETTING.get(settings);
         Class<? extends Discovery> discoveryClass = discoveryTypes.get(discoveryType);
         if (discoveryClass == null) {
             throw new IllegalArgumentException("Unknown Discovery type [" + discoveryType + "]");
         }
 
         if (discoveryType.equals("local") == false) {
-            String masterServiceTypeKey = settings.get(ZEN_MASTER_SERVICE_TYPE_KEY, "zen");
+            String masterServiceTypeKey = ZEN_MASTER_SERVICE_TYPE_SETTING.get(settings);
             final Class<? extends ElectMasterService> masterService = masterServiceType.get(masterServiceTypeKey);
             if (masterService == null) {
                 throw new IllegalArgumentException("Unknown master service type [" + masterServiceTypeKey + "]");
@@ -121,4 +124,4 @@ public class DiscoveryModule extends AbstractModule {
         bind(Discovery.class).to(discoveryClass).asEagerSingleton();
         bind(DiscoveryService.class).asEagerSingleton();
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
index a820996..22f68a7 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
@@ -27,6 +27,7 @@ import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 
@@ -39,8 +40,8 @@ import java.util.concurrent.TimeUnit;
  */
 public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryService> {
 
-    public static final String SETTING_INITIAL_STATE_TIMEOUT = "discovery.initial_state_timeout";
-    public static final String SETTING_DISCOVERY_SEED = "discovery.id.seed";
+    public static final Setting<TimeValue> INITIAL_STATE_TIMEOUT_SETTING = Setting.positiveTimeSetting("discovery.initial_state_timeout", TimeValue.timeValueSeconds(30), false, Setting.Scope.CLUSTER);
+    public static final Setting<Long> DISCOVERY_SEED_SETTING = Setting.longSetting("discovery.id.seed", 0l, Long.MIN_VALUE, false, Setting.Scope.CLUSTER);
 
     private static class InitialStateListener implements InitialStateDiscoveryListener {
 
@@ -71,7 +72,7 @@ public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryServic
         super(settings);
         this.discoverySettings = discoverySettings;
         this.discovery = discovery;
-        this.initialStateTimeout = settings.getAsTime(SETTING_INITIAL_STATE_TIMEOUT, TimeValue.timeValueSeconds(30));
+        this.initialStateTimeout = INITIAL_STATE_TIMEOUT_SETTING.get(settings);
     }
 
     public ClusterBlock getNoMasterBlock() {
@@ -132,7 +133,7 @@ public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryServic
     }
 
     public static String generateNodeId(Settings settings) {
-        Random random = Randomness.get(settings, DiscoveryService.SETTING_DISCOVERY_SEED);
+        Random random = Randomness.get(settings, DiscoveryService.DISCOVERY_SEED_SETTING);
         return Strings.randomBase64UUID(random);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index 6398f31..55eaf78 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -90,15 +90,17 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
 public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implements Discovery, PingContextProvider {
 
     public final static Setting<Boolean> REJOIN_ON_MASTER_GONE_SETTING = Setting.boolSetting("discovery.zen.rejoin_on_master_gone", true, true, Setting.Scope.CLUSTER);
-    public final static String SETTING_PING_TIMEOUT = "discovery.zen.ping_timeout";
-    public final static String SETTING_JOIN_TIMEOUT = "discovery.zen.join_timeout";
-    public final static String SETTING_JOIN_RETRY_ATTEMPTS = "discovery.zen.join_retry_attempts";
-    public final static String SETTING_JOIN_RETRY_DELAY = "discovery.zen.join_retry_delay";
-    public final static String SETTING_MAX_PINGS_FROM_ANOTHER_MASTER = "discovery.zen.max_pings_from_another_master";
-    public final static String SETTING_SEND_LEAVE_REQUEST = "discovery.zen.send_leave_request";
-    public final static String SETTING_MASTER_ELECTION_FILTER_CLIENT = "discovery.zen.master_election.filter_client";
-    public final static String SETTING_MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT = "discovery.zen.master_election.wait_for_joins_timeout";
-    public final static String SETTING_MASTER_ELECTION_FILTER_DATA = "discovery.zen.master_election.filter_data";
+    public final static Setting<TimeValue> PING_TIMEOUT_SETTING = Setting.positiveTimeSetting("discovery.zen.ping_timeout", timeValueSeconds(3), false, Setting.Scope.CLUSTER);
+    public final static Setting<TimeValue> JOIN_TIMEOUT_SETTING = Setting.timeSetting("discovery.zen.join_timeout",
+        settings -> TimeValue.timeValueMillis(PING_TIMEOUT_SETTING.get(settings).millis() * 20).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
+    public final static Setting<Integer> JOIN_RETRY_ATTEMPTS_SETTING = Setting.intSetting("discovery.zen.join_retry_attempts", 3, 1, false, Setting.Scope.CLUSTER);
+    public final static Setting<TimeValue> JOIN_RETRY_DELAY_SETTING = Setting.positiveTimeSetting("discovery.zen.join_retry_delay", TimeValue.timeValueMillis(100), false, Setting.Scope.CLUSTER);
+    public final static Setting<Integer> MAX_PINGS_FROM_ANOTHER_MASTER_SETTING = Setting.intSetting("discovery.zen.max_pings_from_another_master", 3, 1, false, Setting.Scope.CLUSTER);
+    public final static Setting<Boolean> SEND_LEAVE_REQUEST_SETTING = Setting.boolSetting("discovery.zen.send_leave_request", true, false, Setting.Scope.CLUSTER);
+    public final static Setting<Boolean> MASTER_ELECTION_FILTER_CLIENT_SETTING = Setting.boolSetting("discovery.zen.master_election.filter_client", true, false, Setting.Scope.CLUSTER);
+    public final static Setting<TimeValue> MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING = Setting.timeSetting("discovery.zen.master_election.wait_for_joins_timeout",
+        settings -> TimeValue.timeValueMillis(JOIN_TIMEOUT_SETTING.get(settings).millis() / 2).toString(), TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
+    public final static Setting<Boolean> MASTER_ELECTION_FILTER_DATA_SETTING = Setting.boolSetting("discovery.zen.master_election.filter_data", false, false, Setting.Scope.CLUSTER);
 
     public static final String DISCOVERY_REJOIN_ACTION_NAME = "internal:discovery/zen/rejoin";
 
@@ -164,26 +166,19 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.discoverySettings = discoverySettings;
         this.pingService = pingService;
         this.electMaster = electMasterService;
-        this.pingTimeout = settings.getAsTime(SETTING_PING_TIMEOUT, timeValueSeconds(3));
+        this.pingTimeout = PING_TIMEOUT_SETTING.get(settings);
 
-        this.joinTimeout = settings.getAsTime(SETTING_JOIN_TIMEOUT, TimeValue.timeValueMillis(this.pingTimeout.millis() * 20));
-        this.joinRetryAttempts = settings.getAsInt(SETTING_JOIN_RETRY_ATTEMPTS, 3);
-        this.joinRetryDelay = settings.getAsTime(SETTING_JOIN_RETRY_DELAY, TimeValue.timeValueMillis(100));
-        this.maxPingsFromAnotherMaster = settings.getAsInt(SETTING_MAX_PINGS_FROM_ANOTHER_MASTER, 3);
-        this.sendLeaveRequest = settings.getAsBoolean(SETTING_SEND_LEAVE_REQUEST, true);
+        this.joinTimeout = JOIN_TIMEOUT_SETTING.get(settings);
+        this.joinRetryAttempts = JOIN_RETRY_ATTEMPTS_SETTING.get(settings);
+        this.joinRetryDelay = JOIN_RETRY_DELAY_SETTING.get(settings);
+        this.maxPingsFromAnotherMaster = MAX_PINGS_FROM_ANOTHER_MASTER_SETTING.get(settings);
+        this.sendLeaveRequest = SEND_LEAVE_REQUEST_SETTING.get(settings);
 
-        this.masterElectionFilterClientNodes = settings.getAsBoolean(SETTING_MASTER_ELECTION_FILTER_CLIENT, true);
-        this.masterElectionFilterDataNodes = settings.getAsBoolean(SETTING_MASTER_ELECTION_FILTER_DATA, false);
-        this.masterElectionWaitForJoinsTimeout = settings.getAsTime(SETTING_MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT, TimeValue.timeValueMillis(joinTimeout.millis() / 2));
+        this.masterElectionFilterClientNodes = MASTER_ELECTION_FILTER_CLIENT_SETTING.get(settings);
+        this.masterElectionFilterDataNodes = MASTER_ELECTION_FILTER_DATA_SETTING.get(settings);
+        this.masterElectionWaitForJoinsTimeout = MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT_SETTING.get(settings);
         this.rejoinOnMasterGone = REJOIN_ON_MASTER_GONE_SETTING.get(settings);
 
-        if (this.joinRetryAttempts < 1) {
-            throw new IllegalArgumentException("'" + SETTING_JOIN_RETRY_ATTEMPTS + "' must be a positive number. got [" + SETTING_JOIN_RETRY_ATTEMPTS + "]");
-        }
-        if (this.maxPingsFromAnotherMaster < 1) {
-            throw new IllegalArgumentException("'" + SETTING_MAX_PINGS_FROM_ANOTHER_MASTER + "' must be a positive number. got [" + this.maxPingsFromAnotherMaster + "]");
-        }
-
         logger.debug("using ping_timeout [{}], join.timeout [{}], master_election.filter_client [{}], master_election.filter_data [{}]", this.pingTimeout, joinTimeout, masterElectionFilterClientNodes, masterElectionFilterDataNodes);
 
         clusterSettings.addSettingsUpdateConsumer(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING, this::handleMinimumMasterNodesChanged, (value) -> {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java b/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
index 436ef6b..62b0250 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/fd/FaultDetection.java
@@ -21,6 +21,8 @@ package org.elasticsearch.discovery.zen.fd;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.settings.Setting;
+import org.elasticsearch.common.settings.Setting.Scope;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -35,11 +37,11 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
  */
 public abstract class FaultDetection extends AbstractComponent {
 
-    public static final String SETTING_CONNECT_ON_NETWORK_DISCONNECT = "discovery.zen.fd.connect_on_network_disconnect";
-    public static final String SETTING_PING_INTERVAL = "discovery.zen.fd.ping_interval";
-    public static final String SETTING_PING_TIMEOUT = "discovery.zen.fd.ping_timeout";
-    public static final String SETTING_PING_RETRIES = "discovery.zen.fd.ping_retries";
-    public static final String SETTING_REGISTER_CONNECTION_LISTENER = "discovery.zen.fd.register_connection_listener";
+    public static final Setting<Boolean> CONNECT_ON_NETWORK_DISCONNECT_SETTING = Setting.boolSetting("discovery.zen.fd.connect_on_network_disconnect", false, false, Scope.CLUSTER);
+    public static final Setting<TimeValue> PING_INTERVAL_SETTING = Setting.positiveTimeSetting("discovery.zen.fd.ping_interval", timeValueSeconds(1), false, Scope.CLUSTER);
+    public static final Setting<TimeValue> PING_TIMEOUT_SETTING = Setting.timeSetting("discovery.zen.fd.ping_timeout", timeValueSeconds(30), false, Scope.CLUSTER);
+    public static final Setting<Integer> PING_RETRIES_SETTING = Setting.intSetting("discovery.zen.fd.ping_retries", 3, false, Scope.CLUSTER);
+    public static final Setting<Boolean> REGISTER_CONNECTION_LISTENER_SETTING = Setting.boolSetting("discovery.zen.fd.register_connection_listener", true, false, Scope.CLUSTER);
 
     protected final ThreadPool threadPool;
     protected final ClusterName clusterName;
@@ -60,11 +62,11 @@ public abstract class FaultDetection extends AbstractComponent {
         this.transportService = transportService;
         this.clusterName = clusterName;
 
-        this.connectOnNetworkDisconnect = settings.getAsBoolean(SETTING_CONNECT_ON_NETWORK_DISCONNECT, false);
-        this.pingInterval = settings.getAsTime(SETTING_PING_INTERVAL, timeValueSeconds(1));
-        this.pingRetryTimeout = settings.getAsTime(SETTING_PING_TIMEOUT, timeValueSeconds(30));
-        this.pingRetryCount = settings.getAsInt(SETTING_PING_RETRIES, 3);
-        this.registerConnectionListener = settings.getAsBoolean(SETTING_REGISTER_CONNECTION_LISTENER, true);
+        this.connectOnNetworkDisconnect = CONNECT_ON_NETWORK_DISCONNECT_SETTING.get(settings);
+        this.pingInterval = PING_INTERVAL_SETTING.get(settings);
+        this.pingRetryTimeout = PING_TIMEOUT_SETTING.get(settings);
+        this.pingRetryCount = PING_RETRIES_SETTING.get(settings);
+        this.registerConnectionListener = REGISTER_CONNECTION_LISTENER_SETTING.get(settings);
 
         this.connectionListener = new FDConnectionListener();
         if (registerConnectionListener) {
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
index 99feb4b..a996027 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
@@ -58,6 +59,7 @@ import java.io.Closeable;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -72,6 +74,7 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Function;
 
 import static org.elasticsearch.common.unit.TimeValue.readTimeValue;
 import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;
@@ -83,7 +86,8 @@ import static org.elasticsearch.discovery.zen.ping.ZenPing.PingResponse.readPing
 public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implements ZenPing {
 
     public static final String ACTION_NAME = "internal:discovery/zen/unicast";
-    public static final String DISCOVERY_ZEN_PING_UNICAST_HOSTS = "discovery.zen.ping.unicast.hosts";
+    public static final Setting<List<String>> DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING = Setting.listSetting("discovery.zen.ping.unicast.hosts", Collections.emptyList(), Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING = Setting.intSetting("discovery.zen.ping.unicast.concurrent_connects", 10, 0, false, Setting.Scope.CLUSTER);
 
     // these limits are per-address
     public static final int LIMIT_FOREIGN_PORTS_COUNT = 1;
@@ -135,13 +139,8 @@ public class UnicastZenPing extends AbstractLifecycleComponent<ZenPing> implemen
             }
         }
 
-        this.concurrentConnects = this.settings.getAsInt("discovery.zen.ping.unicast.concurrent_connects", 10);
-        String[] hostArr = this.settings.getAsArray(DISCOVERY_ZEN_PING_UNICAST_HOSTS);
-        // trim the hosts
-        for (int i = 0; i < hostArr.length; i++) {
-            hostArr[i] = hostArr[i].trim();
-        }
-        List<String> hosts = CollectionUtils.arrayAsArrayList(hostArr);
+        this.concurrentConnects = DISCOVERY_ZEN_PING_UNICAST_CONCURRENT_CONNECTS_SETTING.get(settings);
+        List<String> hosts = DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING.get(settings);
         final int limitPortCounts;
         if (hosts.isEmpty()) {
             // if unicast hosts are not specified, fill with simple defaults on the local machine
diff --git a/core/src/main/java/org/elasticsearch/env/Environment.java b/core/src/main/java/org/elasticsearch/env/Environment.java
index b6453a4..65d62bd 100644
--- a/core/src/main/java/org/elasticsearch/env/Environment.java
+++ b/core/src/main/java/org/elasticsearch/env/Environment.java
@@ -23,6 +23,7 @@ import org.apache.lucene.util.Constants;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 
 import java.io.IOException;
@@ -33,6 +34,9 @@ import java.nio.file.FileStore;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.function.Function;
 
 import static org.elasticsearch.common.Strings.cleanPath;
 
@@ -43,6 +47,15 @@ import static org.elasticsearch.common.Strings.cleanPath;
 // TODO: move PathUtils to be package-private here instead of
 // public+forbidden api!
 public class Environment {
+    public static final Setting<String> PATH_HOME_SETTING = Setting.simpleString("path.home", false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PATH_CONF_SETTING = Setting.simpleString("path.conf", false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PATH_SCRIPTS_SETTING = Setting.simpleString("path.scripts", false, Setting.Scope.CLUSTER);
+    public static final Setting<List<String>> PATH_DATA_SETTING = Setting.listSetting("path.data", Collections.emptyList(), Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PATH_LOGS_SETTING = Setting.simpleString("path.logs", false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PATH_PLUGINS_SETTING = Setting.simpleString("path.plugins", false, Setting.Scope.CLUSTER);
+    public static final Setting<List<String>> PATH_REPO_SETTING = Setting.listSetting("path.repo", Collections.emptyList(), Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PATH_SHARED_DATA_SETTING = Setting.simpleString("path.shared_data", false, Setting.Scope.CLUSTER);
+    public static final Setting<String> PIDFILE_SETTING = Setting.simpleString("pidfile", false, Setting.Scope.CLUSTER);
 
     private final Settings settings;
 
@@ -95,64 +108,64 @@ public class Environment {
     public Environment(Settings settings) {
         this.settings = settings;
         final Path homeFile;
-        if (settings.get("path.home") != null) {
-            homeFile = PathUtils.get(cleanPath(settings.get("path.home")));
+        if (PATH_HOME_SETTING.exists(settings)) {
+            homeFile = PathUtils.get(cleanPath(PATH_HOME_SETTING.get(settings)));
         } else {
-            throw new IllegalStateException("path.home is not configured");
+            throw new IllegalStateException(PATH_HOME_SETTING.getKey() + " is not configured");
         }
 
-        if (settings.get("path.conf") != null) {
-            configFile = PathUtils.get(cleanPath(settings.get("path.conf")));
+        if (PATH_CONF_SETTING.exists(settings)) {
+            configFile = PathUtils.get(cleanPath(PATH_CONF_SETTING.get(settings)));
         } else {
             configFile = homeFile.resolve("config");
         }
 
-        if (settings.get("path.scripts") != null) {
-            scriptsFile = PathUtils.get(cleanPath(settings.get("path.scripts")));
+        if (PATH_SCRIPTS_SETTING.exists(settings)) {
+            scriptsFile = PathUtils.get(cleanPath(PATH_SCRIPTS_SETTING.get(settings)));
         } else {
             scriptsFile = configFile.resolve("scripts");
         }
 
-        if (settings.get("path.plugins") != null) {
-            pluginsFile = PathUtils.get(cleanPath(settings.get("path.plugins")));
+        if (PATH_PLUGINS_SETTING.exists(settings)) {
+            pluginsFile = PathUtils.get(cleanPath(PATH_PLUGINS_SETTING.get(settings)));
         } else {
             pluginsFile = homeFile.resolve("plugins");
         }
 
-        String[] dataPaths = settings.getAsArray("path.data");
-        if (dataPaths.length > 0) {
-            dataFiles = new Path[dataPaths.length];
-            dataWithClusterFiles = new Path[dataPaths.length];
-            for (int i = 0; i < dataPaths.length; i++) {
-                dataFiles[i] = PathUtils.get(dataPaths[i]);
+        List<String> dataPaths = PATH_DATA_SETTING.get(settings);
+        if (dataPaths.isEmpty() == false) {
+            dataFiles = new Path[dataPaths.size()];
+            dataWithClusterFiles = new Path[dataPaths.size()];
+            for (int i = 0; i < dataPaths.size(); i++) {
+                dataFiles[i] = PathUtils.get(dataPaths.get(i));
                 dataWithClusterFiles[i] = dataFiles[i].resolve(ClusterName.clusterNameFromSettings(settings).value());
             }
         } else {
             dataFiles = new Path[]{homeFile.resolve("data")};
             dataWithClusterFiles = new Path[]{homeFile.resolve("data").resolve(ClusterName.clusterNameFromSettings(settings).value())};
         }
-        if (settings.get("path.shared_data") != null) {
-            sharedDataFile = PathUtils.get(cleanPath(settings.get("path.shared_data")));
+        if (PATH_SHARED_DATA_SETTING.exists(settings)) {
+            sharedDataFile = PathUtils.get(cleanPath(PATH_SHARED_DATA_SETTING.get(settings)));
         } else {
             sharedDataFile = null;
         }
-        String[] repoPaths = settings.getAsArray("path.repo");
-        if (repoPaths.length > 0) {
-            repoFiles = new Path[repoPaths.length];
-            for (int i = 0; i < repoPaths.length; i++) {
-                repoFiles[i] = PathUtils.get(repoPaths[i]);
+        List<String> repoPaths = PATH_REPO_SETTING.get(settings);
+        if (repoPaths.isEmpty() == false) {
+            repoFiles = new Path[repoPaths.size()];
+            for (int i = 0; i < repoPaths.size(); i++) {
+                repoFiles[i] = PathUtils.get(repoPaths.get(i));
             }
         } else {
             repoFiles = new Path[0];
         }
-        if (settings.get("path.logs") != null) {
-            logsFile = PathUtils.get(cleanPath(settings.get("path.logs")));
+        if (PATH_LOGS_SETTING.exists(settings)) {
+            logsFile = PathUtils.get(cleanPath(PATH_LOGS_SETTING.get(settings)));
         } else {
             logsFile = homeFile.resolve("logs");
         }
 
-        if (settings.get("pidfile") != null) {
-            pidFile = PathUtils.get(cleanPath(settings.get("pidfile")));
+        if (PIDFILE_SETTING.exists(settings)) {
+            pidFile = PathUtils.get(cleanPath(PIDFILE_SETTING.get(settings)));
         } else {
             pidFile = null;
         }
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java b/core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java
index 117a0c6..c6a65ff 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayMetaState.java
@@ -34,7 +34,6 @@ import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.MultiDataPathUpgrader;
 import org.elasticsearch.env.NodeEnvironment;
 
 import java.nio.file.DirectoryStream;
@@ -77,7 +76,6 @@ public class GatewayMetaState extends AbstractComponent implements ClusterStateL
 
         if (DiscoveryNode.dataNode(settings)) {
             ensureNoPre019ShardState(nodeEnv);
-            MultiDataPathUpgrader.upgradeMultiDataPath(nodeEnv, logger);
         }
 
         if (DiscoveryNode.masterNode(settings) || DiscoveryNode.dataNode(settings)) {
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
index 80e3be7..af565a6 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
@@ -36,6 +36,7 @@ import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.discovery.DiscoveryService;
@@ -49,6 +50,21 @@ import java.util.concurrent.atomic.AtomicBoolean;
  */
 public class GatewayService extends AbstractLifecycleComponent<GatewayService> implements ClusterStateListener {
 
+    public static final Setting<Integer> EXPECTED_NODES_SETTING = Setting.intSetting(
+            "gateway.expected_nodes", -1, -1, false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> EXPECTED_DATA_NODES_SETTING = Setting.intSetting(
+            "gateway.expected_data_nodes", -1, -1, false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> EXPECTED_MASTER_NODES_SETTING = Setting.intSetting(
+            "gateway.expected_master_nodes", -1, -1, false, Setting.Scope.CLUSTER);
+    public static final Setting<TimeValue> RECOVER_AFTER_TIME_SETTING = Setting.positiveTimeSetting(
+            "gateway.recover_after_time", TimeValue.timeValueMillis(0), false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> RECOVER_AFTER_NODES_SETTING = Setting.intSetting(
+            "gateway.recover_after_nodes", -1, -1, false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> RECOVER_AFTER_DATA_NODES_SETTING = Setting.intSetting(
+            "gateway.recover_after_data_nodes", -1, -1, false, Setting.Scope.CLUSTER);
+    public static final Setting<Integer> RECOVER_AFTER_MASTER_NODES_SETTING = Setting.intSetting(
+            "gateway.recover_after_master_nodes", 0, 0, false, Setting.Scope.CLUSTER);
+
     public static final ClusterBlock STATE_NOT_RECOVERED_BLOCK = new ClusterBlock(1, "state not recovered / initialized", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL);
 
     public static final TimeValue DEFAULT_RECOVER_AFTER_TIME_IF_EXPECTED_NODES_IS_SET = TimeValue.timeValueMinutes(5);
@@ -84,20 +100,26 @@ public class GatewayService extends AbstractLifecycleComponent<GatewayService> i
         this.discoveryService = discoveryService;
         this.threadPool = threadPool;
         // allow to control a delay of when indices will get created
-        this.expectedNodes = this.settings.getAsInt("gateway.expected_nodes", -1);
-        this.expectedDataNodes = this.settings.getAsInt("gateway.expected_data_nodes", -1);
-        this.expectedMasterNodes = this.settings.getAsInt("gateway.expected_master_nodes", -1);
-
-        TimeValue defaultRecoverAfterTime = null;
-        if (expectedNodes >= 0 || expectedDataNodes >= 0 || expectedMasterNodes >= 0) {
-            defaultRecoverAfterTime = DEFAULT_RECOVER_AFTER_TIME_IF_EXPECTED_NODES_IS_SET;
+        this.expectedNodes = EXPECTED_NODES_SETTING.get(this.settings);
+        this.expectedDataNodes = EXPECTED_DATA_NODES_SETTING.get(this.settings);
+        this.expectedMasterNodes = EXPECTED_MASTER_NODES_SETTING.get(this.settings);
+
+        if (RECOVER_AFTER_TIME_SETTING.exists(this.settings)) {
+            recoverAfterTime = RECOVER_AFTER_TIME_SETTING.get(this.settings);
+        } else if (expectedNodes >= 0 || expectedDataNodes >= 0 || expectedMasterNodes >= 0) {
+            recoverAfterTime = DEFAULT_RECOVER_AFTER_TIME_IF_EXPECTED_NODES_IS_SET;
+        } else {
+            recoverAfterTime = null;
         }
-
-        this.recoverAfterTime = this.settings.getAsTime("gateway.recover_after_time", defaultRecoverAfterTime);
-        this.recoverAfterNodes = this.settings.getAsInt("gateway.recover_after_nodes", -1);
-        this.recoverAfterDataNodes = this.settings.getAsInt("gateway.recover_after_data_nodes", -1);
+        this.recoverAfterNodes = RECOVER_AFTER_NODES_SETTING.get(this.settings);
+        this.recoverAfterDataNodes = RECOVER_AFTER_DATA_NODES_SETTING.get(this.settings);
         // default the recover after master nodes to the minimum master nodes in the discovery
-        this.recoverAfterMasterNodes = this.settings.getAsInt("gateway.recover_after_master_nodes", settings.getAsInt("discovery.zen.minimum_master_nodes", -1));
+        if (RECOVER_AFTER_MASTER_NODES_SETTING.exists(this.settings)) {
+            recoverAfterMasterNodes = RECOVER_AFTER_MASTER_NODES_SETTING.get(this.settings);
+        } else {
+            // TODO: change me once the minimum_master_nodes is changed too
+            recoverAfterMasterNodes = settings.getAsInt("discovery.zen.minimum_master_nodes", -1);
+        }
 
         // Add the not recovered as initial state block, we don't allow anything until
         this.clusterService.addInitialStateBlock(STATE_NOT_RECOVERED_BLOCK);
diff --git a/core/src/main/java/org/elasticsearch/http/HttpServer.java b/core/src/main/java/org/elasticsearch/http/HttpServer.java
index 9971ce7..35c46f4 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpServer.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpServer.java
@@ -51,7 +51,7 @@ import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
 import static org.elasticsearch.rest.RestStatus.OK;
 
 /**
- *
+ * A component to serve http requests, backed by rest handlers.
  */
 public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
 
@@ -63,10 +63,6 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
 
     private final NodeService nodeService;
 
-    private final boolean disableSites;
-
-    private final PluginSiteFilter pluginSiteFilter = new PluginSiteFilter();
-
     @Inject
     public HttpServer(Settings settings, Environment environment, HttpServerTransport transport,
                       RestController restController,
@@ -77,9 +73,6 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         this.restController = restController;
         this.nodeService = nodeService;
         nodeService.setHttpServer(this);
-
-        this.disableSites = this.settings.getAsBoolean("http.disable_sites", false);
-
         transport.httpServerAdapter(new Dispatcher(this));
     }
 
@@ -126,27 +119,13 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
     }
 
     public void internalDispatchRequest(final HttpRequest request, final HttpChannel channel) {
-        String rawPath = request.rawPath();
-        if (rawPath.startsWith("/_plugin/")) {
-            RestFilterChain filterChain = restController.filterChain(pluginSiteFilter);
-            filterChain.continueProcessing(request, channel);
-            return;
-        } else if (rawPath.equals("/favicon.ico")) {
+        if (request.rawPath().equals("/favicon.ico")) {
             handleFavicon(request, channel);
             return;
         }
         restController.dispatchRequest(request, channel);
     }
 
-
-    class PluginSiteFilter extends RestFilter {
-
-        @Override
-        public void process(RestRequest request, RestChannel channel, RestFilterChain filterChain) throws IOException {
-            handlePluginSite((HttpRequest) request, (HttpChannel) channel);
-        }
-    }
-
     void handleFavicon(HttpRequest request, HttpChannel channel) {
         if (request.method() == RestRequest.Method.GET) {
             try {
@@ -163,129 +142,4 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
             channel.sendResponse(new BytesRestResponse(FORBIDDEN));
         }
     }
-
-    void handlePluginSite(HttpRequest request, HttpChannel channel) throws IOException {
-        if (disableSites) {
-            channel.sendResponse(new BytesRestResponse(FORBIDDEN));
-            return;
-        }
-        if (request.method() == RestRequest.Method.OPTIONS) {
-            // when we have OPTIONS request, simply send OK by default (with the Access Control Origin header which gets automatically added)
-            channel.sendResponse(new BytesRestResponse(OK));
-            return;
-        }
-        if (request.method() != RestRequest.Method.GET) {
-            channel.sendResponse(new BytesRestResponse(FORBIDDEN));
-            return;
-        }
-        // TODO for a "/_plugin" endpoint, we should have a page that lists all the plugins?
-
-        String path = request.rawPath().substring("/_plugin/".length());
-        int i1 = path.indexOf('/');
-        String pluginName;
-        String sitePath;
-        if (i1 == -1) {
-            pluginName = path;
-            sitePath = null;
-            // If a trailing / is missing, we redirect to the right page #2654
-            String redirectUrl = request.rawPath() + "/";
-            BytesRestResponse restResponse = new BytesRestResponse(RestStatus.MOVED_PERMANENTLY, "text/html", "<head><meta http-equiv=\"refresh\" content=\"0; URL=" + redirectUrl + "\"></head>");
-            restResponse.addHeader("Location", redirectUrl);
-            channel.sendResponse(restResponse);
-            return;
-        } else {
-            pluginName = path.substring(0, i1);
-            sitePath = path.substring(i1 + 1);
-        }
-
-        // we default to index.html, or what the plugin provides (as a unix-style path)
-        // this is a relative path under _site configured by the plugin.
-        if (sitePath.length() == 0) {
-            sitePath = "index.html";
-        } else {
-            // remove extraneous leading slashes, its not an absolute path.
-            while (sitePath.length() > 0 && sitePath.charAt(0) == '/') {
-                sitePath = sitePath.substring(1);
-            }
-        }
-        final Path siteFile = environment.pluginsFile().resolve(pluginName).resolve("_site");
-
-        final String separator = siteFile.getFileSystem().getSeparator();
-        // Convert file separators.
-        sitePath = sitePath.replace("/", separator);
-
-        Path file = siteFile.resolve(sitePath);
-
-        // return not found instead of forbidden to prevent malicious requests to find out if files exist or dont exist
-        if (!Files.exists(file) || FileSystemUtils.isHidden(file) || !file.toAbsolutePath().normalize().startsWith(siteFile.toAbsolutePath().normalize())) {
-            channel.sendResponse(new BytesRestResponse(NOT_FOUND));
-            return;
-        }
-
-        BasicFileAttributes attributes = Files.readAttributes(file, BasicFileAttributes.class);
-        if (!attributes.isRegularFile()) {
-            // If it's not a dir, we send a 403
-            if (!attributes.isDirectory()) {
-                channel.sendResponse(new BytesRestResponse(FORBIDDEN));
-                return;
-            }
-            // We don't serve dir but if index.html exists in dir we should serve it
-            file = file.resolve("index.html");
-            if (!Files.exists(file) || FileSystemUtils.isHidden(file) || !Files.isRegularFile(file)) {
-                channel.sendResponse(new BytesRestResponse(FORBIDDEN));
-                return;
-            }
-        }
-
-        try {
-            byte[] data = Files.readAllBytes(file);
-            channel.sendResponse(new BytesRestResponse(OK, guessMimeType(sitePath), data));
-        } catch (IOException e) {
-            channel.sendResponse(new BytesRestResponse(INTERNAL_SERVER_ERROR));
-        }
-    }
-
-
-    // TODO: Don't respond with a mime type that violates the request's Accept header
-    private String guessMimeType(String path) {
-        int lastDot = path.lastIndexOf('.');
-        if (lastDot == -1) {
-            return "";
-        }
-        String extension = path.substring(lastDot + 1).toLowerCase(Locale.ROOT);
-        String mimeType = DEFAULT_MIME_TYPES.get(extension);
-        if (mimeType == null) {
-            return "";
-        }
-        return mimeType;
-    }
-
-    static {
-        // This is not an exhaustive list, just the most common types. Call registerMimeType() to add more.
-        Map<String, String> mimeTypes = new HashMap<>();
-        mimeTypes.put("txt", "text/plain");
-        mimeTypes.put("css", "text/css");
-        mimeTypes.put("csv", "text/csv");
-        mimeTypes.put("htm", "text/html");
-        mimeTypes.put("html", "text/html");
-        mimeTypes.put("xml", "text/xml");
-        mimeTypes.put("js", "text/javascript"); // Technically it should be application/javascript (RFC 4329), but IE8 struggles with that
-        mimeTypes.put("xhtml", "application/xhtml+xml");
-        mimeTypes.put("json", "application/json");
-        mimeTypes.put("pdf", "application/pdf");
-        mimeTypes.put("zip", "application/zip");
-        mimeTypes.put("tar", "application/x-tar");
-        mimeTypes.put("gif", "image/gif");
-        mimeTypes.put("jpeg", "image/jpeg");
-        mimeTypes.put("jpg", "image/jpeg");
-        mimeTypes.put("tiff", "image/tiff");
-        mimeTypes.put("tif", "image/tiff");
-        mimeTypes.put("png", "image/png");
-        mimeTypes.put("svg", "image/svg+xml");
-        mimeTypes.put("ico", "image/vnd.microsoft.icon");
-        mimeTypes.put("mp3", "audio/mpeg");
-        DEFAULT_MIME_TYPES = unmodifiableMap(mimeTypes);
-    }
-
-    public static final Map<String, String> DEFAULT_MIME_TYPES;
 }
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java
index 7fcc7b6..316799d 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java
@@ -113,7 +113,7 @@ public class NettyHttpChannel extends HttpChannel {
             resp = new DefaultHttpResponse(HttpVersion.HTTP_1_1, status);
         }
         if (RestUtils.isBrowser(nettyRequest.headers().get(USER_AGENT))) {
-            if (transport.settings().getAsBoolean(SETTING_CORS_ENABLED, false)) {
+            if (SETTING_CORS_ENABLED.get(transport.settings())) {
                 String originHeader = request.header(ORIGIN);
                 if (!Strings.isNullOrEmpty(originHeader)) {
                     if (corsPattern == null) {
@@ -127,12 +127,12 @@ public class NettyHttpChannel extends HttpChannel {
                 }
                 if (nettyRequest.getMethod() == HttpMethod.OPTIONS) {
                     // Allow Ajax requests based on the CORS "preflight" request
-                    resp.headers().add(ACCESS_CONTROL_MAX_AGE, transport.settings().getAsInt(SETTING_CORS_MAX_AGE, 1728000));
+                    resp.headers().add(ACCESS_CONTROL_MAX_AGE, SETTING_CORS_MAX_AGE.get(transport.settings()));
                     resp.headers().add(ACCESS_CONTROL_ALLOW_METHODS, transport.settings().get(SETTING_CORS_ALLOW_METHODS, "OPTIONS, HEAD, GET, POST, PUT, DELETE"));
                     resp.headers().add(ACCESS_CONTROL_ALLOW_HEADERS, transport.settings().get(SETTING_CORS_ALLOW_HEADERS, "X-Requested-With, Content-Type, Content-Length"));
                 }
 
-                if (transport.settings().getAsBoolean(SETTING_CORS_ALLOW_CREDENTIALS, false)) {
+                if (SETTING_CORS_ALLOW_CREDENTIALS.get(transport.settings())) {
                     resp.headers().add(ACCESS_CONTROL_ALLOW_CREDENTIALS, "true");
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
index 899bbdc..0cd0cef 100644
--- a/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
+++ b/core/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.http.netty;
 
-import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -27,7 +26,8 @@ import org.elasticsearch.common.netty.NettyUtils;
 import org.elasticsearch.common.netty.OpenChannelsHandler;
 import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.network.NetworkUtils;
+import org.elasticsearch.common.settings.Setting;
+import org.elasticsearch.common.settings.Setting.Scope;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
@@ -75,9 +75,6 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.atomic.AtomicReference;
 
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_SERVER;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_DEFAULT_RECEIVE_BUFFER_SIZE;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_DEFAULT_SEND_BUFFER_SIZE;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_KEEP_ALIVE;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_NO_DELAY;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE;
@@ -94,19 +91,19 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         NettyUtils.setup();
     }
 
-    public static final String SETTING_CORS_ENABLED = "http.cors.enabled";
+    public static final Setting<Boolean> SETTING_CORS_ENABLED = Setting.boolSetting("http.cors.enabled", false, false, Scope.CLUSTER);
     public static final String SETTING_CORS_ALLOW_ORIGIN = "http.cors.allow-origin";
-    public static final String SETTING_CORS_MAX_AGE = "http.cors.max-age";
+    public static final Setting<Integer> SETTING_CORS_MAX_AGE = Setting.intSetting("http.cors.max-age", 1728000, false, Scope.CLUSTER);
     public static final String SETTING_CORS_ALLOW_METHODS = "http.cors.allow-methods";
     public static final String SETTING_CORS_ALLOW_HEADERS = "http.cors.allow-headers";
-    public static final String SETTING_CORS_ALLOW_CREDENTIALS = "http.cors.allow-credentials";
-    public static final String SETTING_PIPELINING = "http.pipelining";
+    public static final Setting<Boolean> SETTING_CORS_ALLOW_CREDENTIALS = Setting.boolSetting("http.cors.allow-credentials", false, false, Scope.CLUSTER);
+
+    public static final Setting<Boolean> SETTING_PIPELINING = Setting.boolSetting("http.pipelining", true, false, Scope.CLUSTER);
     public static final String SETTING_PIPELINING_MAX_EVENTS = "http.pipelining.max_events";
     public static final String SETTING_HTTP_COMPRESSION = "http.compression";
     public static final String SETTING_HTTP_COMPRESSION_LEVEL = "http.compression_level";
-    public static final String SETTING_HTTP_DETAILED_ERRORS_ENABLED = "http.detailed_errors.enabled";
+    public static final Setting<Boolean> SETTING_HTTP_DETAILED_ERRORS_ENABLED = Setting.boolSetting("http.detailed_errors.enabled", true, false, Scope.CLUSTER);
 
-    public static final boolean DEFAULT_SETTING_PIPELINING = true;
     public static final int DEFAULT_SETTING_PIPELINING_MAX_EVENTS = 10000;
     public static final String DEFAULT_PORT_RANGE = "9200-9300";
 
@@ -142,8 +139,8 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
 
     protected int publishPort;
 
-    protected final String tcpNoDelay;
-    protected final String tcpKeepAlive;
+    protected final boolean tcpNoDelay;
+    protected final boolean tcpKeepAlive;
     protected final boolean reuseAddress;
 
     protected final ByteSizeValue tcpSendBufferSize;
@@ -186,17 +183,17 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
         this.maxCumulationBufferCapacity = settings.getAsBytesSize("http.netty.max_cumulation_buffer_capacity", null);
         this.maxCompositeBufferComponents = settings.getAsInt("http.netty.max_composite_buffer_components", -1);
         this.workerCount = settings.getAsInt("http.netty.worker_count", EsExecutors.boundedNumberOfProcessors(settings) * 2);
-        this.blockingServer = settings.getAsBoolean("http.netty.http.blocking_server", settings.getAsBoolean(TCP_BLOCKING_SERVER, settings.getAsBoolean(TCP_BLOCKING, false)));
+        this.blockingServer = settings.getAsBoolean("http.netty.http.blocking_server", TCP_BLOCKING.get(settings));
         this.port = settings.get("http.netty.port", settings.get("http.port", DEFAULT_PORT_RANGE));
         this.bindHosts = settings.getAsArray("http.netty.bind_host", settings.getAsArray("http.bind_host", settings.getAsArray("http.host", null)));
         this.publishHosts = settings.getAsArray("http.netty.publish_host", settings.getAsArray("http.publish_host", settings.getAsArray("http.host", null)));
         this.publishPort = settings.getAsInt("http.netty.publish_port", settings.getAsInt("http.publish_port", 0));
-        this.tcpNoDelay = settings.get("http.netty.tcp_no_delay", settings.get(TCP_NO_DELAY, "true"));
-        this.tcpKeepAlive = settings.get("http.netty.tcp_keep_alive", settings.get(TCP_KEEP_ALIVE, "true"));
-        this.reuseAddress = settings.getAsBoolean("http.netty.reuse_address", settings.getAsBoolean(TCP_REUSE_ADDRESS, NetworkUtils.defaultReuseAddress()));
-        this.tcpSendBufferSize = settings.getAsBytesSize("http.netty.tcp_send_buffer_size", settings.getAsBytesSize(TCP_SEND_BUFFER_SIZE, TCP_DEFAULT_SEND_BUFFER_SIZE));
-        this.tcpReceiveBufferSize = settings.getAsBytesSize("http.netty.tcp_receive_buffer_size", settings.getAsBytesSize(TCP_RECEIVE_BUFFER_SIZE, TCP_DEFAULT_RECEIVE_BUFFER_SIZE));
-        this.detailedErrorsEnabled = settings.getAsBoolean(SETTING_HTTP_DETAILED_ERRORS_ENABLED, true);
+        this.tcpNoDelay = settings.getAsBoolean("http.netty.tcp_no_delay", TCP_NO_DELAY.get(settings));
+        this.tcpKeepAlive = settings.getAsBoolean("http.netty.tcp_keep_alive", TCP_KEEP_ALIVE.get(settings));
+        this.reuseAddress = settings.getAsBoolean("http.netty.reuse_address", TCP_REUSE_ADDRESS.get(settings));
+        this.tcpSendBufferSize = settings.getAsBytesSize("http.netty.tcp_send_buffer_size", TCP_SEND_BUFFER_SIZE.get(settings));
+        this.tcpReceiveBufferSize = settings.getAsBytesSize("http.netty.tcp_receive_buffer_size", TCP_RECEIVE_BUFFER_SIZE.get(settings));
+        this.detailedErrorsEnabled = SETTING_HTTP_DETAILED_ERRORS_ENABLED.get(settings);
 
         long defaultReceiverPredictor = 512 * 1024;
         if (JvmInfo.jvmInfo().getMem().getDirectMemoryMax().bytes() > 0) {
@@ -216,7 +213,7 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
 
         this.compression = settings.getAsBoolean(SETTING_HTTP_COMPRESSION, false);
         this.compressionLevel = settings.getAsInt(SETTING_HTTP_COMPRESSION_LEVEL, 6);
-        this.pipelining = settings.getAsBoolean(SETTING_PIPELINING, DEFAULT_SETTING_PIPELINING);
+        this.pipelining = SETTING_PIPELINING.get(settings);
         this.pipeliningMaxEvents = settings.getAsInt(SETTING_PIPELINING_MAX_EVENTS, DEFAULT_SETTING_PIPELINING_MAX_EVENTS);
 
         // validate max content length
@@ -257,16 +254,13 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
 
         serverBootstrap.setPipelineFactory(configureServerChannelPipelineFactory());
 
-        if (!"default".equals(tcpNoDelay)) {
-            serverBootstrap.setOption("child.tcpNoDelay", Booleans.parseBoolean(tcpNoDelay, null));
-        }
-        if (!"default".equals(tcpKeepAlive)) {
-            serverBootstrap.setOption("child.keepAlive", Booleans.parseBoolean(tcpKeepAlive, null));
-        }
-        if (tcpSendBufferSize != null && tcpSendBufferSize.bytes() > 0) {
+        serverBootstrap.setOption("child.tcpNoDelay", tcpNoDelay);
+        serverBootstrap.setOption("child.keepAlive", tcpKeepAlive);
+        if (tcpSendBufferSize.bytes() > 0) {
+
             serverBootstrap.setOption("child.sendBufferSize", tcpSendBufferSize.bytes());
         }
-        if (tcpReceiveBufferSize != null && tcpReceiveBufferSize.bytes() > 0) {
+        if (tcpReceiveBufferSize.bytes() > 0) {
             serverBootstrap.setOption("child.receiveBufferSize", tcpReceiveBufferSize.bytes());
         }
         serverBootstrap.setOption("receiveBufferSizePredictorFactory", receiveBufferSizePredictorFactory);
@@ -308,7 +302,8 @@ public class NettyHttpServerTransport extends AbstractLifecycleComponent<HttpSer
             throw new BindHttpException("Publish address [" + publishInetAddress + "] does not match any of the bound addresses [" + boundAddresses + "]");
         }
 
-        final InetSocketAddress publishAddress = new InetSocketAddress(publishInetAddress, publishPort);;
+        final InetSocketAddress publishAddress = new InetSocketAddress(publishInetAddress, publishPort);
+        ;
         this.boundAddress = new BoundTransportAddress(boundAddresses.toArray(new TransportAddress[boundAddresses.size()]), new InetSocketTransportAddress(publishAddress));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index 7961090..0e11211 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -1065,7 +1065,7 @@ public abstract class Engine implements Closeable {
         }
     }
 
-    public static class CommitId implements Writeable<CommitId> {
+    public static class CommitId implements Writeable {
 
         private final byte[] id;
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index d9a99cc..9184281 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -372,7 +372,10 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
             return null;
         }
 
-        if (query instanceof BooleanQuery) {
+        // If the coordination factor is disabled on a boolean query we don't apply the minimum should match.
+        // This is done to make sure that the minimum_should_match doesn't get applied when there is only one word
+        // and multiple variations of the same word in the query (synonyms for instance).
+        if (query instanceof BooleanQuery && !((BooleanQuery) query).isCoordDisabled()) {
             query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
         } else if (query instanceof ExtendedCommonTermsQuery) {
             ((ExtendedCommonTermsQuery)query).setLowFreqMinimumNumberShouldMatch(minimumShouldMatch);
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index 59e04e4..fcab39b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -735,7 +735,10 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         }
 
         query = Queries.fixNegativeQueryIfNeeded(query);
-        if (query instanceof BooleanQuery) {
+        // If the coordination factor is disabled on a boolean query we don't apply the minimum should match.
+        // This is done to make sure that the minimum_should_match doesn't get applied when there is only one word
+        // and multiple variations of the same word in the query (synonyms for instance).
+        if (query instanceof BooleanQuery && !((BooleanQuery) query).isCoordDisabled()) {
             query = Queries.applyMinimumShouldMatch((BooleanQuery) query, this.minimumShouldMatch());
         }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
index 17240a2..5bb10b7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
@@ -285,7 +285,10 @@ public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQuerySt
         sqp.setDefaultOperator(defaultOperator.toBooleanClauseOccur());
 
         Query query = sqp.parse(queryText);
-        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
+        // If the coordination factor is disabled on a boolean query we don't apply the minimum should match.
+        // This is done to make sure that the minimum_should_match doesn't get applied when there is only one word
+        // and multiple variations of the same word in the query (synonyms for instance).
+        if (minimumShouldMatch != null && query instanceof BooleanQuery && !((BooleanQuery) query).isCoordDisabled()) {
             query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
         }
         return query;
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index c9e59b5..dbe4de5 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -54,7 +54,10 @@ public class MultiMatchQuery extends MatchQuery {
 
     private Query parseAndApply(Type type, String fieldName, Object value, String minimumShouldMatch, Float boostValue) throws IOException {
         Query query = parse(type, fieldName, value);
-        if (query instanceof BooleanQuery) {
+        // If the coordination factor is disabled on a boolean query we don't apply the minimum should match.
+        // This is done to make sure that the minimum_should_match doesn't get applied when there is only one word
+        // and multiple variations of the same word in the query (synonyms for instance).
+        if (query instanceof BooleanQuery && !((BooleanQuery) query).isCoordDisabled()) {
             query = Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
         }
         if (query != null && boostValue != null && boostValue != AbstractQueryBuilder.DEFAULT_BOOST) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
index f7d0cd5..a1fc708 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
@@ -192,7 +192,17 @@ public class TranslogWriter extends BaseTranslogReader implements Closeable {
             throw e;
         }
         if (closed.compareAndSet(false, true)) {
-            return new TranslogReader(generation, channel, path, firstOperationOffset, getWrittenOffset(), operationCounter);
+            boolean success = false;
+            try {
+                final TranslogReader reader = new TranslogReader(generation, channel, path, firstOperationOffset, getWrittenOffset(), operationCounter);
+                success = true;
+                return reader;
+            } finally {
+                if (success == false) {
+                    // close the channel, as we are closed and failed to create a new reader
+                    IOUtils.closeWhileHandlingException(channel);
+                }
+            }
         } else {
             throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed (path [" + path + "]", tragedy);
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java b/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
index 3e63b6f..f99b39e 100644
--- a/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
+++ b/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java
@@ -23,6 +23,7 @@ import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 
@@ -70,9 +71,9 @@ import java.util.function.Function;
  */
 public class HunspellService extends AbstractComponent {
 
-    public final static String HUNSPELL_LAZY_LOAD = "indices.analysis.hunspell.dictionary.lazy";
-    public final static String HUNSPELL_IGNORE_CASE = "indices.analysis.hunspell.dictionary.ignore_case";
-    private final static String OLD_HUNSPELL_LOCATION = "indices.analysis.hunspell.dictionary.location";
+    public final static Setting<Boolean> HUNSPELL_LAZY_LOAD = Setting.boolSetting("indices.analysis.hunspell.dictionary.lazy", Boolean.FALSE, false, Setting.Scope.CLUSTER);
+    public final static Setting<Boolean> HUNSPELL_IGNORE_CASE = Setting.boolSetting("indices.analysis.hunspell.dictionary.ignore_case", Boolean.FALSE, false, Setting.Scope.CLUSTER);
+    public final static Setting<Settings> HUNSPELL_DICTIONARY_OPTIONS = Setting.groupSetting("indices.analysis.hunspell.dictionary.", false, Setting.Scope.CLUSTER);
     private final ConcurrentHashMap<String, Dictionary> dictionaries = new ConcurrentHashMap<>();
     private final Map<String, Dictionary> knownDictionaries;
     private final boolean defaultIgnoreCase;
@@ -82,8 +83,8 @@ public class HunspellService extends AbstractComponent {
     public HunspellService(final Settings settings, final Environment env, final Map<String, Dictionary> knownDictionaries) throws IOException {
         super(settings);
         this.knownDictionaries = Collections.unmodifiableMap(knownDictionaries);
-        this.hunspellDir = resolveHunspellDirectory(settings, env);
-        this.defaultIgnoreCase = settings.getAsBoolean(HUNSPELL_IGNORE_CASE, false);
+        this.hunspellDir = resolveHunspellDirectory(env);
+        this.defaultIgnoreCase = HUNSPELL_IGNORE_CASE.get(settings);
         this.loadingFunction = (locale) -> {
             try {
                 return loadDictionary(locale, settings, env);
@@ -91,7 +92,7 @@ public class HunspellService extends AbstractComponent {
                 throw new IllegalStateException("failed to load hunspell dictionary for locale: " + locale, e);
             }
         };
-        if (!settings.getAsBoolean(HUNSPELL_LAZY_LOAD, false)) {
+        if (!HUNSPELL_LAZY_LOAD.get(settings)) {
             scanAndLoadDictionaries();
         }
 
@@ -110,11 +111,7 @@ public class HunspellService extends AbstractComponent {
         return dictionary;
     }
 
-    private Path resolveHunspellDirectory(Settings settings, Environment env) {
-        String location = settings.get(OLD_HUNSPELL_LOCATION, null);
-        if (location != null) {
-            throw new IllegalArgumentException("please, put your hunspell dictionaries under config/hunspell !");
-        }
+    private Path resolveHunspellDirectory(Environment env) {
         return env.configFile().resolve("hunspell");
     }
 
@@ -162,7 +159,8 @@ public class HunspellService extends AbstractComponent {
         }
 
         // merging node settings with hunspell dictionary specific settings
-        nodeSettings = loadDictionarySettings(dicDir, nodeSettings.getByPrefix("indices.analysis.hunspell.dictionary." + locale + "."));
+        Settings dictSettings = HUNSPELL_DICTIONARY_OPTIONS.get(nodeSettings);
+        nodeSettings = loadDictionarySettings(dicDir, dictSettings.getByPrefix(locale));
 
         boolean ignoreCase = nodeSettings.getAsBoolean("ignore_case", defaultIgnoreCase);
 
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index 6a25217..3cdb637 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -40,6 +40,7 @@ import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
 import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.MemorySizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
@@ -80,10 +81,10 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
      * since we are checking on the cluster state IndexMetaData always.
      */
     public static final Setting<Boolean> INDEX_CACHE_REQUEST_ENABLED_SETTING = Setting.boolSetting("index.requests.cache.enable", true, true, Setting.Scope.INDEX);
-    public static final String INDICES_CACHE_REQUEST_CLEAN_INTERVAL = "indices.requests.cache.clean_interval";
+    public static final Setting<TimeValue> INDICES_CACHE_REQUEST_CLEAN_INTERVAL = Setting.positiveTimeSetting("indices.requests.cache.clean_interval", TimeValue.timeValueSeconds(60), false, Setting.Scope.CLUSTER);
 
-    public static final String INDICES_CACHE_QUERY_SIZE = "indices.requests.cache.size";
-    public static final String INDICES_CACHE_QUERY_EXPIRE = "indices.requests.cache.expire";
+    public static final Setting<ByteSizeValue> INDICES_CACHE_QUERY_SIZE = Setting.byteSizeSetting("indices.requests.cache.size", "1%", false, Setting.Scope.CLUSTER);
+    public static final Setting<TimeValue> INDICES_CACHE_QUERY_EXPIRE = Setting.positiveTimeSetting("indices.requests.cache.expire", new TimeValue(0), false, Setting.Scope.CLUSTER);
 
     private static final Set<SearchType> CACHEABLE_SEARCH_TYPES = EnumSet.of(SearchType.QUERY_THEN_FETCH, SearchType.QUERY_AND_FETCH);
 
@@ -98,7 +99,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
 
     //TODO make these changes configurable on the cluster level
-    private final String size;
+    private final ByteSizeValue size;
     private final TimeValue expire;
 
     private volatile Cache<Key, Value> cache;
@@ -108,11 +109,11 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         super(settings);
         this.clusterService = clusterService;
         this.threadPool = threadPool;
-        this.cleanInterval = settings.getAsTime(INDICES_CACHE_REQUEST_CLEAN_INTERVAL, TimeValue.timeValueSeconds(60));
+        this.cleanInterval = INDICES_CACHE_REQUEST_CLEAN_INTERVAL.get(settings);
 
-        this.size = settings.get(INDICES_CACHE_QUERY_SIZE, "1%");
+        this.size = INDICES_CACHE_QUERY_SIZE.get(settings);
 
-        this.expire = settings.getAsTime(INDICES_CACHE_QUERY_EXPIRE, null);
+        this.expire = INDICES_CACHE_QUERY_EXPIRE.exists(settings) ? INDICES_CACHE_QUERY_EXPIRE.get(settings) : null;
         buildCache();
 
         this.reaper = new Reaper();
@@ -121,7 +122,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
 
     private void buildCache() {
-        long sizeInBytes = MemorySizeValue.parseBytesSizeValueOrHeapRatio(size, INDICES_CACHE_QUERY_SIZE).bytes();
+        long sizeInBytes = size.bytes();
 
         CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.<Key, Value>builder()
                 .setMaximumWeight(sizeInBytes).weigher((k, v) -> k.ramBytesUsed() + v.ramBytesUsed()).removalListener(this);
diff --git a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
index 9181c62..06d4c21 100644
--- a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
@@ -33,6 +33,7 @@ import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.index.ElasticsearchDirectoryReader;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
@@ -53,8 +54,8 @@ import java.util.function.ToLongBiFunction;
  */
 public class IndicesFieldDataCache extends AbstractComponent implements RemovalListener<IndicesFieldDataCache.Key, Accountable> {
 
-    public static final String FIELDDATA_CLEAN_INTERVAL_SETTING = "indices.fielddata.cache.cleanup_interval";
-    public static final String INDICES_FIELDDATA_CACHE_SIZE_KEY = "indices.fielddata.cache.size";
+    public static final Setting<TimeValue> INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING = Setting.positiveTimeSetting("indices.fielddata.cache.cleanup_interval", TimeValue.timeValueMinutes(1), false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> INDICES_FIELDDATA_CACHE_SIZE_KEY = Setting.byteSizeSetting("indices.fielddata.cache.size", new ByteSizeValue(-1), false, Setting.Scope.CLUSTER);
 
 
     private final IndicesFieldDataCacheListener indicesFieldDataCacheListener;
@@ -68,18 +69,16 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         super(settings);
         this.threadPool = threadPool;
         this.indicesFieldDataCacheListener = indicesFieldDataCacheListener;
-        final String size = settings.get(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1");
-        final long sizeInBytes = settings.getAsMemory(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1").bytes();
+        final long sizeInBytes = INDICES_FIELDDATA_CACHE_SIZE_KEY.get(settings).bytes();
         CacheBuilder<Key, Accountable> cacheBuilder = CacheBuilder.<Key, Accountable>builder()
                 .removalListener(this);
         if (sizeInBytes > 0) {
             cacheBuilder.setMaximumWeight(sizeInBytes).weigher(new FieldDataWeigher());
         }
 
-        logger.debug("using size [{}] [{}]", size, new ByteSizeValue(sizeInBytes));
         cache = cacheBuilder.build();
 
-        this.cleanInterval = settings.getAsTime(FIELDDATA_CLEAN_INTERVAL_SETTING, TimeValue.timeValueMinutes(1));
+        this.cleanInterval = INDICES_FIELDDATA_CLEAN_INTERVAL_SETTING.get(settings);
         // Start thread that will manage cleaning the field data cache periodically
         threadPool.schedule(this.cleanInterval, ThreadPool.Names.SAME,
                 new FieldDataCacheCleaner(this.cache, this.logger, this.threadPool, this.cleanInterval));
diff --git a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
index d107503..4985118 100644
--- a/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
+++ b/core/src/main/java/org/elasticsearch/indices/store/IndicesStore.java
@@ -35,6 +35,7 @@ import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
@@ -68,7 +69,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 public class IndicesStore extends AbstractComponent implements ClusterStateListener, Closeable {
 
     // TODO this class can be foled into either IndicesService and partially into IndicesClusterStateService there is no need for a seperate public service
-    public static final String INDICES_STORE_DELETE_SHARD_TIMEOUT = "indices.store.delete.shard.timeout";
+    public static final Setting<TimeValue> INDICES_STORE_DELETE_SHARD_TIMEOUT = Setting.positiveTimeSetting("indices.store.delete.shard.timeout", new TimeValue(30, TimeUnit.SECONDS), false, Setting.Scope.CLUSTER);
     public static final String ACTION_SHARD_EXISTS = "internal:index/shard/exists";
     private static final EnumSet<IndexShardState> ACTIVE_STATES = EnumSet.of(IndexShardState.STARTED, IndexShardState.RELOCATED);
     private final IndicesService indicesService;
@@ -85,7 +86,7 @@ public class IndicesStore extends AbstractComponent implements ClusterStateListe
         this.clusterService = clusterService;
         this.transportService = transportService;
         transportService.registerRequestHandler(ACTION_SHARD_EXISTS, ShardActiveRequest::new, ThreadPool.Names.SAME, new ShardActiveRequestHandler());
-        this.deleteShardTimeout = settings.getAsTime(INDICES_STORE_DELETE_SHARD_TIMEOUT, new TimeValue(30, TimeUnit.SECONDS));
+        this.deleteShardTimeout = INDICES_STORE_DELETE_SHARD_TIMEOUT.get(settings);
         clusterService.addLast(this);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java b/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
deleted file mode 100644
index 0e50751..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.cluster.AbstractDiffable;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.collect.HppcMaps;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ObjectParser;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Holds the ingest pipelines that are available in the cluster
- */
-public final class IngestMetadata extends AbstractDiffable<MetaData.Custom> implements MetaData.Custom {
-
-    public final static String TYPE = "ingest";
-    public final static IngestMetadata PROTO = new IngestMetadata();
-    private static final ParseField PIPELINES_FIELD = new ParseField("pipeline");
-    private static final ObjectParser<List<PipelineConfiguration>, Void> INGEST_METADATA_PARSER = new ObjectParser<>("ingest_metadata", ArrayList::new);
-
-    static {
-        INGEST_METADATA_PARSER.declareObjectArray(List::addAll , PipelineConfiguration.getParser(), PIPELINES_FIELD);
-    }
-
-
-    // We can't use Pipeline class directly in cluster state, because we don't have the processor factories around when
-    // IngestMetadata is registered as custom metadata.
-    private final Map<String, PipelineConfiguration> pipelines;
-
-    private IngestMetadata() {
-        this.pipelines = Collections.emptyMap();
-    }
-
-    public IngestMetadata(Map<String, PipelineConfiguration> pipelines) {
-        this.pipelines = Collections.unmodifiableMap(pipelines);
-    }
-
-    @Override
-    public String type() {
-        return TYPE;
-    }
-
-    public Map<String, PipelineConfiguration> getPipelines() {
-        return pipelines;
-    }
-
-    @Override
-    public MetaData.Custom readFrom(StreamInput in) throws IOException {
-        int size = in.readVInt();
-        Map<String, PipelineConfiguration> pipelines = new HashMap<>(size);
-        for (int i = 0; i < size; i++) {
-            PipelineConfiguration pipeline = PipelineConfiguration.readPipelineConfiguration(in);
-            pipelines.put(pipeline.getId(), pipeline);
-        }
-        return new IngestMetadata(pipelines);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(pipelines.size());
-        for (PipelineConfiguration pipeline : pipelines.values()) {
-            pipeline.writeTo(out);
-        }
-    }
-
-    @Override
-    public MetaData.Custom fromXContent(XContentParser parser) throws IOException {
-        Map<String, PipelineConfiguration> pipelines = new HashMap<>();
-        List<PipelineConfiguration> configs = INGEST_METADATA_PARSER.parse(parser);
-        for (PipelineConfiguration pipeline : configs) {
-            pipelines.put(pipeline.getId(), pipeline);
-        }
-        return new IngestMetadata(pipelines);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(PIPELINES_FIELD.getPreferredName());
-        for (PipelineConfiguration pipeline : pipelines.values()) {
-            pipeline.toXContent(builder, params);
-        }
-        builder.endArray();
-        return builder;
-    }
-
-    @Override
-    public EnumSet<MetaData.XContentContext> context() {
-        return MetaData.API_AND_GATEWAY;
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestService.java b/core/src/main/java/org/elasticsearch/ingest/IngestService.java
deleted file mode 100644
index bc7cd75..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/IngestService.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/**
- * Instantiates and wires all the services that the ingest plugin will be needing.
- * Also the bootstrapper is in charge of starting and stopping the ingest plugin based on the cluster state.
- */
-public class IngestService implements Closeable {
-
-    private final PipelineStore pipelineStore;
-    private final PipelineExecutionService pipelineExecutionService;
-    private final ProcessorsRegistry processorsRegistry;
-
-    public IngestService(Settings settings, ThreadPool threadPool, ProcessorsRegistry processorsRegistry) {
-        this.processorsRegistry = processorsRegistry;
-        this.pipelineStore = new PipelineStore(settings);
-        this.pipelineExecutionService = new PipelineExecutionService(pipelineStore, threadPool);
-    }
-
-    public PipelineStore getPipelineStore() {
-        return pipelineStore;
-    }
-
-    public PipelineExecutionService getPipelineExecutionService() {
-        return pipelineExecutionService;
-    }
-
-    public void setScriptService(ScriptService scriptService) {
-        pipelineStore.buildProcessorFactoryRegistry(processorsRegistry, scriptService);
-    }
-
-    @Override
-    public void close() throws IOException {
-        pipelineStore.close();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java b/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
deleted file mode 100644
index b4b5ce8..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptService;
-
-import java.util.Collections;
-import java.util.Map;
-
-public class InternalTemplateService implements TemplateService {
-
-    private final ScriptService scriptService;
-
-    InternalTemplateService(ScriptService scriptService) {
-        this.scriptService = scriptService;
-    }
-
-    @Override
-    public Template compile(String template) {
-        int mustacheStart = template.indexOf("{{");
-        int mustacheEnd = template.indexOf("}}");
-        if (mustacheStart != -1 && mustacheEnd != -1 && mustacheStart < mustacheEnd) {
-            Script script = new Script(template, ScriptService.ScriptType.INLINE, "mustache", Collections.emptyMap());
-            CompiledScript compiledScript = scriptService.compile(
-                script,
-                ScriptContext.Standard.INGEST,
-                null /* we can supply null here, because ingest doesn't use indexed scripts */,
-                Collections.emptyMap()
-            );
-            return new Template() {
-                @Override
-                public String execute(Map<String, Object> model) {
-                    ExecutableScript executableScript = scriptService.executable(compiledScript, model);
-                    Object result = executableScript.run();
-                    if (result instanceof BytesReference) {
-                        return ((BytesReference) result).toUtf8();
-                    }
-                    return String.valueOf(result);
-                }
-
-                @Override
-                public String getKey() {
-                    return template;
-                }
-            };
-        } else {
-            return new StringTemplate(template);
-        }
-    }
-
-    class StringTemplate implements Template {
-
-        private final String value;
-
-        public StringTemplate(String value) {
-            this.value = value;
-        }
-
-        @Override
-        public String execute(Map<String, Object> model) {
-            return value;
-        }
-
-        @Override
-        public String getKey() {
-            return value;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java b/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
deleted file mode 100644
index 90ab2a7..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Build;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ObjectParser;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Objects;
-import java.util.function.BiFunction;
-
-/**
- * Encapsulates a pipeline's id and configuration as a blob
- */
-public final class PipelineConfiguration implements Writeable<PipelineConfiguration>, ToXContent {
-
-    private final static PipelineConfiguration PROTOTYPE = new PipelineConfiguration(null, null);
-
-    public static PipelineConfiguration readPipelineConfiguration(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-    private final static ObjectParser<Builder, Void> PARSER = new ObjectParser<>("pipeline_config", Builder::new);
-    static {
-        PARSER.declareString(Builder::setId, new ParseField("id"));
-        PARSER.declareField((parser, builder, aVoid) -> {
-            XContentBuilder contentBuilder = XContentBuilder.builder(parser.contentType().xContent());
-            XContentHelper.copyCurrentStructure(contentBuilder.generator(), parser);
-            builder.setConfig(contentBuilder.bytes());
-        }, new ParseField("config"), ObjectParser.ValueType.OBJECT);
-    }
-
-    public static BiFunction<XContentParser, Void,PipelineConfiguration> getParser() {
-        return (p, c) -> PARSER.apply(p ,c).build();
-    }
-    private static class Builder {
-
-        private String id;
-        private BytesReference config;
-
-        void setId(String id) {
-            this.id = id;
-        }
-
-        void setConfig(BytesReference config) {
-            this.config = config;
-        }
-
-        PipelineConfiguration build() {
-            return new PipelineConfiguration(id, config);
-        }
-    }
-
-    private final String id;
-    // Store config as bytes reference, because the config is only used when the pipeline store reads the cluster state
-    // and the way the map of maps config is read requires a deep copy (it removes instead of gets entries to check for unused options)
-    // also the get pipeline api just directly returns this to the caller
-    private final BytesReference config;
-
-    public PipelineConfiguration(String id, BytesReference config) {
-        this.id = id;
-        this.config = config;
-    }
-
-    public String getId() {
-        return id;
-    }
-
-    public Map<String, Object> getConfigAsMap() {
-        return XContentHelper.convertToMap(config, true).v2();
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.field("id", id);
-        builder.field("config", getConfigAsMap());
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public PipelineConfiguration readFrom(StreamInput in) throws IOException {
-        return new PipelineConfiguration(in.readString(), in.readBytesReference());
-    }
-
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(id);
-        out.writeBytesReference(config);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java b/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
deleted file mode 100644
index c6a3b4b..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.util.concurrent.AbstractRunnable;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.threadpool.ThreadPool;
-
-import java.util.Map;
-import java.util.function.BiConsumer;
-import java.util.function.Consumer;
-
-public class PipelineExecutionService {
-
-    private final PipelineStore store;
-    private final ThreadPool threadPool;
-
-    public PipelineExecutionService(PipelineStore store, ThreadPool threadPool) {
-        this.store = store;
-        this.threadPool = threadPool;
-    }
-
-    public void execute(IndexRequest request, Consumer<Throwable> failureHandler, Consumer<Boolean> completionHandler) {
-        Pipeline pipeline = getPipeline(request.getPipeline());
-        threadPool.executor(ThreadPool.Names.INDEX).execute(new AbstractRunnable() {
-
-            @Override
-            public void onFailure(Throwable t) {
-                failureHandler.accept(t);
-            }
-
-            @Override
-            protected void doRun() throws Exception {
-                innerExecute(request, pipeline);
-                completionHandler.accept(true);
-            }
-        });
-    }
-
-    public void execute(Iterable<ActionRequest<?>> actionRequests,
-                        BiConsumer<IndexRequest, Throwable> itemFailureHandler,
-                        Consumer<Throwable> completionHandler) {
-        threadPool.executor(ThreadPool.Names.INDEX).execute(new AbstractRunnable() {
-
-            @Override
-            public void onFailure(Throwable t) {
-                completionHandler.accept(t);
-            }
-
-            @Override
-            protected void doRun() throws Exception {
-                for (ActionRequest actionRequest : actionRequests) {
-                    if ((actionRequest instanceof IndexRequest)) {
-                        IndexRequest indexRequest = (IndexRequest) actionRequest;
-                        if (Strings.hasText(indexRequest.getPipeline())) {
-                            try {
-                                innerExecute(indexRequest, getPipeline(indexRequest.getPipeline()));
-                                //this shouldn't be needed here but we do it for consistency with index api which requires it to prevent double execution
-                                indexRequest.setPipeline(null);
-                            } catch (Throwable e) {
-                                itemFailureHandler.accept(indexRequest, e);
-                            }
-                        }
-                    }
-                }
-                completionHandler.accept(null);
-            }
-        });
-    }
-
-    private void innerExecute(IndexRequest indexRequest, Pipeline pipeline) throws Exception {
-        String index = indexRequest.index();
-        String type = indexRequest.type();
-        String id = indexRequest.id();
-        String routing = indexRequest.routing();
-        String parent = indexRequest.parent();
-        String timestamp = indexRequest.timestamp();
-        String ttl = indexRequest.ttl() == null ? null : indexRequest.ttl().toString();
-        Map<String, Object> sourceAsMap = indexRequest.sourceAsMap();
-        IngestDocument ingestDocument = new IngestDocument(index, type, id, routing, parent, timestamp, ttl, sourceAsMap);
-        pipeline.execute(ingestDocument);
-
-        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-        //it's fine to set all metadata fields all the time, as ingest document holds their starting values
-        //before ingestion, which might also get modified during ingestion.
-        indexRequest.index(metadataMap.get(IngestDocument.MetaData.INDEX));
-        indexRequest.type(metadataMap.get(IngestDocument.MetaData.TYPE));
-        indexRequest.id(metadataMap.get(IngestDocument.MetaData.ID));
-        indexRequest.routing(metadataMap.get(IngestDocument.MetaData.ROUTING));
-        indexRequest.parent(metadataMap.get(IngestDocument.MetaData.PARENT));
-        indexRequest.timestamp(metadataMap.get(IngestDocument.MetaData.TIMESTAMP));
-        indexRequest.ttl(metadataMap.get(IngestDocument.MetaData.TTL));
-        indexRequest.source(ingestDocument.getSourceAndMetadata());
-    }
-
-    private Pipeline getPipeline(String pipelineId) {
-        Pipeline pipeline = store.get(pipelineId);
-        if (pipeline == null) {
-            throw new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist");
-        }
-        return pipeline;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
deleted file mode 100644
index 805f1e4..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
+++ /dev/null
@@ -1,242 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.ResourceNotFoundException;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
-import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.ScriptService;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.function.Function;
-
-public class PipelineStore extends AbstractComponent implements Closeable, ClusterStateListener {
-
-    private final Pipeline.Factory factory = new Pipeline.Factory();
-    private Map<String, Processor.Factory> processorFactoryRegistry;
-
-    // Ideally this should be in IngestMetadata class, but we don't have the processor factories around there.
-    // We know of all the processor factories when a node with all its plugin have been initialized. Also some
-    // processor factories rely on other node services. Custom metadata is statically registered when classes
-    // are loaded, so in the cluster state we just save the pipeline config and here we keep the actual pipelines around.
-    volatile Map<String, Pipeline> pipelines = new HashMap<>();
-
-    public PipelineStore(Settings settings) {
-        super(settings);
-    }
-
-    public void buildProcessorFactoryRegistry(ProcessorsRegistry processorsRegistry, ScriptService scriptService) {
-        Map<String, Processor.Factory> processorFactories = new HashMap<>();
-        TemplateService templateService = new InternalTemplateService(scriptService);
-        for (Map.Entry<String, Function<TemplateService, Processor.Factory<?>>> entry : processorsRegistry.entrySet()) {
-            Processor.Factory processorFactory = entry.getValue().apply(templateService);
-            processorFactories.put(entry.getKey(), processorFactory);
-        }
-        this.processorFactoryRegistry = Collections.unmodifiableMap(processorFactories);
-    }
-
-    @Override
-    public void close() throws IOException {
-        // TODO: When org.elasticsearch.node.Node can close Closable instances we should try to remove this code,
-        // since any wired closable should be able to close itself
-        List<Closeable> closeables = new ArrayList<>();
-        for (Processor.Factory factory : processorFactoryRegistry.values()) {
-            if (factory instanceof Closeable) {
-                closeables.add((Closeable) factory);
-            }
-        }
-        IOUtils.close(closeables);
-    }
-
-    @Override
-    public void clusterChanged(ClusterChangedEvent event) {
-        innerUpdatePipelines(event.state());
-    }
-
-    void innerUpdatePipelines(ClusterState state) {
-        IngestMetadata ingestMetadata = state.getMetaData().custom(IngestMetadata.TYPE);
-        if (ingestMetadata == null) {
-            return;
-        }
-
-        Map<String, Pipeline> pipelines = new HashMap<>();
-        for (PipelineConfiguration pipeline : ingestMetadata.getPipelines().values()) {
-            try {
-                pipelines.put(pipeline.getId(), constructPipeline(pipeline.getId(), pipeline.getConfigAsMap()));
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        }
-        this.pipelines = Collections.unmodifiableMap(pipelines);
-    }
-
-    /**
-     * Deletes the pipeline specified by id in the request.
-     */
-    public void delete(ClusterService clusterService, DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener) {
-        clusterService.submitStateUpdateTask("delete-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<WritePipelineResponse>(request, listener) {
-
-            @Override
-            protected WritePipelineResponse newResponse(boolean acknowledged) {
-                return new WritePipelineResponse(acknowledged);
-            }
-
-            @Override
-            public ClusterState execute(ClusterState currentState) throws Exception {
-                return innerDelete(request, currentState);
-            }
-        });
-    }
-
-    ClusterState innerDelete(DeletePipelineRequest request, ClusterState currentState) {
-        IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE);
-        if (currentIngestMetadata == null) {
-            return currentState;
-        }
-        Map<String, PipelineConfiguration> pipelines = currentIngestMetadata.getPipelines();
-        if (pipelines.containsKey(request.getId()) == false) {
-            throw new ResourceNotFoundException("pipeline [{}] is missing", request.getId());
-        } else {
-            pipelines = new HashMap<>(pipelines);
-            pipelines.remove(request.getId());
-            ClusterState.Builder newState = ClusterState.builder(currentState);
-            newState.metaData(MetaData.builder(currentState.getMetaData())
-                .putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelines))
-                .build());
-            return newState.build();
-        }
-    }
-
-    /**
-     * Stores the specified pipeline definition in the request.
-     *
-     * @throws IllegalArgumentException If the pipeline holds incorrect configuration
-     */
-    public void put(ClusterService clusterService, PutPipelineRequest request, ActionListener<WritePipelineResponse> listener) throws IllegalArgumentException {
-        try {
-            // validates the pipeline and processor configuration before submitting a cluster update task:
-            Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.getSource(), false).v2();
-            constructPipeline(request.getId(), pipelineConfig);
-        } catch (Exception e) {
-            throw new IllegalArgumentException("Invalid pipeline configuration", e);
-        }
-        clusterService.submitStateUpdateTask("put-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<WritePipelineResponse>(request, listener) {
-
-            @Override
-            protected WritePipelineResponse newResponse(boolean acknowledged) {
-                return new WritePipelineResponse(acknowledged);
-            }
-
-            @Override
-            public ClusterState execute(ClusterState currentState) throws Exception {
-                return innerPut(request, currentState);
-            }
-        });
-    }
-
-    ClusterState innerPut(PutPipelineRequest request, ClusterState currentState) {
-        IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE);
-        Map<String, PipelineConfiguration> pipelines;
-        if (currentIngestMetadata != null) {
-            pipelines = new HashMap<>(currentIngestMetadata.getPipelines());
-        } else {
-            pipelines = new HashMap<>();
-        }
-
-        pipelines.put(request.getId(), new PipelineConfiguration(request.getId(), request.getSource()));
-        ClusterState.Builder newState = ClusterState.builder(currentState);
-        newState.metaData(MetaData.builder(currentState.getMetaData())
-            .putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelines))
-            .build());
-        return newState.build();
-    }
-
-    /**
-     * Returns the pipeline by the specified id
-     */
-    public Pipeline get(String id) {
-        return pipelines.get(id);
-    }
-
-    public Map<String, Processor.Factory> getProcessorFactoryRegistry() {
-        return processorFactoryRegistry;
-    }
-
-    /**
-     * @return pipeline configuration specified by id. If multiple ids or wildcards are specified multiple pipelines
-     * may be returned
-     */
-    // Returning PipelineConfiguration instead of Pipeline, because Pipeline and Processor interface don't
-    // know how to serialize themselves.
-    public List<PipelineConfiguration> getPipelines(ClusterState clusterState, String... ids) {
-        IngestMetadata ingestMetadata = clusterState.getMetaData().custom(IngestMetadata.TYPE);
-        return innerGetPipelines(ingestMetadata, ids);
-    }
-
-    List<PipelineConfiguration> innerGetPipelines(IngestMetadata ingestMetadata, String... ids) {
-        if (ingestMetadata == null) {
-            return Collections.emptyList();
-        }
-
-        List<PipelineConfiguration> result = new ArrayList<>(ids.length);
-        for (String id : ids) {
-            if (Regex.isSimpleMatchPattern(id)) {
-                for (Map.Entry<String, PipelineConfiguration> entry : ingestMetadata.getPipelines().entrySet()) {
-                    if (Regex.simpleMatch(id, entry.getKey())) {
-                        result.add(entry.getValue());
-                    }
-                }
-            } else {
-                PipelineConfiguration pipeline = ingestMetadata.getPipelines().get(id);
-                if (pipeline != null) {
-                    result.add(pipeline);
-                }
-            }
-        }
-        return result;
-    }
-
-    private Pipeline constructPipeline(String id, Map<String, Object> config) throws Exception {
-        return factory.create(id, config, processorFactoryRegistry);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java b/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
deleted file mode 100644
index 766ba77..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.function.Function;
-
-public class ProcessorsRegistry {
-
-    private final Map<String, Function<TemplateService, Processor.Factory<?>>> processorFactoryProviders = new HashMap<>();
-
-    /**
-     * Adds a processor factory under a specific name.
-     */
-    public void registerProcessor(String name, Function<TemplateService, Processor.Factory<?>> processorFactoryProvider) {
-        Function<TemplateService, Processor.Factory<?>> provider = processorFactoryProviders.putIfAbsent(name, processorFactoryProvider);
-        if (provider != null) {
-            throw new IllegalArgumentException("Processor factory already registered for name [" + name + "]");
-        }
-    }
-
-    public Set<Map.Entry<String, Function<TemplateService, Processor.Factory<?>>>> entrySet() {
-        return processorFactoryProviders.entrySet();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java
deleted file mode 100644
index e709ae3..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-/**
- * An Abstract Processor that holds a processorTag field to be used
- * by other processors.
- */
-public abstract class AbstractProcessor implements Processor {
-    protected final String tag;
-
-    protected AbstractProcessor(String tag) {
-        this.tag = tag;
-    }
-
-    @Override
-    public String getTag() {
-        return tag;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java
deleted file mode 100644
index 1082461..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-import java.util.Map;
-
-/**
- * A processor implementation may modify the data belonging to a document.
- * Whether changes are made and what exactly is modified is up to the implementation.
- */
-public abstract class AbstractProcessorFactory<P extends Processor> implements Processor.Factory<P> {
-    public static final String TAG_KEY = "tag";
-
-    @Override
-    public P create(Map<String, Object> config) throws Exception {
-        String tag = ConfigurationUtils.readOptionalStringProperty(config, TAG_KEY);
-        return doCreate(tag, config);
-    }
-
-    protected abstract P doCreate(String tag, Map<String, Object> config) throws Exception;
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
deleted file mode 100644
index bc5fd19..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-//TODO(simonw): can all these classes go into org.elasticsearch.ingest?
-
-package org.elasticsearch.ingest.core;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-import java.util.stream.Collectors;
-
-/**
- * A Processor that executes a list of other "processors". It executes a separate list of
- * "onFailureProcessors" when any of the processors throw an {@link Exception}.
- */
-public class CompoundProcessor implements Processor {
-    static final String ON_FAILURE_MESSAGE_FIELD = "on_failure_message";
-    static final String ON_FAILURE_PROCESSOR_FIELD = "on_failure_processor";
-
-    private final List<Processor> processors;
-    private final List<Processor> onFailureProcessors;
-
-    public CompoundProcessor(Processor... processor) {
-        this(Arrays.asList(processor), Collections.emptyList());
-    }
-
-    public CompoundProcessor(List<Processor> processors, List<Processor> onFailureProcessors) {
-        super();
-        this.processors = processors;
-        this.onFailureProcessors = onFailureProcessors;
-    }
-
-    public List<Processor> getOnFailureProcessors() {
-        return onFailureProcessors;
-    }
-
-    public List<Processor> getProcessors() {
-        return processors;
-    }
-
-    @Override
-    public String getType() {
-        return "compound";
-    }
-
-    @Override
-    public String getTag() {
-        return "compound-processor-" + Objects.hash(processors, onFailureProcessors);
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        for (Processor processor : processors) {
-            try {
-                processor.execute(ingestDocument);
-            } catch (Exception e) {
-                if (onFailureProcessors.isEmpty()) {
-                    throw e;
-                } else {
-                    executeOnFailure(ingestDocument, e, processor.getType());
-                }
-                break;
-            }
-        }
-    }
-
-    void executeOnFailure(IngestDocument ingestDocument, Exception cause, String failedProcessorType) throws Exception {
-        Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-        try {
-            ingestMetadata.put(ON_FAILURE_MESSAGE_FIELD, cause.getMessage());
-            ingestMetadata.put(ON_FAILURE_PROCESSOR_FIELD, failedProcessorType);
-            for (Processor processor : onFailureProcessors) {
-                processor.execute(ingestDocument);
-            }
-        } finally {
-            ingestMetadata.remove(ON_FAILURE_MESSAGE_FIELD);
-            ingestMetadata.remove(ON_FAILURE_PROCESSOR_FIELD);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
deleted file mode 100644
index c620416..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
+++ /dev/null
@@ -1,163 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import java.util.List;
-import java.util.Map;
-
-public final class ConfigurationUtils {
-
-    private ConfigurationUtils() {
-    }
-
-    /**
-     * Returns and removes the specified optional property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     */
-    public static String readOptionalStringProperty(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        return readString(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName) {
-        return readStringProperty(configuration, propertyName, null);
-    }
-
-    /**
-     * Returns and removes the specified property from the specified configuration map.
-     *
-     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
-     * If the property is missing and no default value has been specified a {@link IllegalArgumentException} is thrown
-     */
-    public static String readStringProperty(Map<String, Object> configuration, String propertyName, String defaultValue) {
-        Object value = configuration.remove(propertyName);
-        if (value == null && defaultValue != null) {
-            return defaultValue;
-        } else if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return readString(propertyName, value);
-    }
-
-    private static String readString(String propertyName, Object value) {
-        if (value == null) {
-            return null;
-        }
-        if (value instanceof String) {
-            return (String) value;
-        }
-        throw new IllegalArgumentException("property [" + propertyName + "] isn't a string, but of type [" + value.getClass().getName() + "]");
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> List<T> readOptionalList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-        return readList(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type list from the specified configuration map.
-     *
-     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> List<T> readList(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readList(propertyName, value);
-    }
-
-    private static <T> List<T> readList(String propertyName, Object value) {
-        if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<T> stringList = (List<T>) value;
-            return stringList;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a list, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     * If the property is missing an {@link IllegalArgumentException} is thrown
-     */
-    public static <T> Map<String, T> readMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    /**
-     * Returns and removes the specified property of type map from the specified configuration map.
-     *
-     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
-     */
-    public static <T> Map<String, T> readOptionalMap(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            return null;
-        }
-
-        return readMap(propertyName, value);
-    }
-
-    private static <T> Map<String, T> readMap(String propertyName, Object value) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, T> map = (Map<String, T>) value;
-            return map;
-        } else {
-            throw new IllegalArgumentException("property [" + propertyName + "] isn't a map, but of type [" + value.getClass().getName() + "]");
-        }
-    }
-
-    /**
-     * Returns and removes the specified property as an {@link Object} from the specified configuration map.
-     */
-    public static Object readObject(Map<String, Object> configuration, String propertyName) {
-        Object value = configuration.remove(propertyName);
-        if (value == null) {
-            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
-        }
-        return value;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
deleted file mode 100644
index c8f87fa..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
+++ /dev/null
@@ -1,544 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.index.mapper.internal.IdFieldMapper;
-import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
-import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
-import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
-import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Objects;
-import java.util.TimeZone;
-
-/**
- * Represents a single document being captured before indexing and holds the source and metadata (like id, type and index).
- */
-public final class IngestDocument {
-
-    public final static String INGEST_KEY = "_ingest";
-
-    static final String TIMESTAMP = "timestamp";
-
-    private final Map<String, Object> sourceAndMetadata;
-    private final Map<String, String> ingestMetadata;
-
-    public IngestDocument(String index, String type, String id, String routing, String parent, String timestamp, String ttl, Map<String, Object> source) {
-        this.sourceAndMetadata = new HashMap<>();
-        this.sourceAndMetadata.putAll(source);
-        this.sourceAndMetadata.put(MetaData.INDEX.getFieldName(), index);
-        this.sourceAndMetadata.put(MetaData.TYPE.getFieldName(), type);
-        this.sourceAndMetadata.put(MetaData.ID.getFieldName(), id);
-        if (routing != null) {
-            this.sourceAndMetadata.put(MetaData.ROUTING.getFieldName(), routing);
-        }
-        if (parent != null) {
-            this.sourceAndMetadata.put(MetaData.PARENT.getFieldName(), parent);
-        }
-        if (timestamp != null) {
-            this.sourceAndMetadata.put(MetaData.TIMESTAMP.getFieldName(), timestamp);
-        }
-        if (ttl != null) {
-            this.sourceAndMetadata.put(MetaData.TTL.getFieldName(), ttl);
-        }
-
-        this.ingestMetadata = new HashMap<>();
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        df.setTimeZone(TimeZone.getTimeZone("UTC"));
-        this.ingestMetadata.put(TIMESTAMP, df.format(new Date()));
-    }
-
-    /**
-     * Copy constructor that creates a new {@link IngestDocument} which has exactly the same properties as the one provided as argument
-     */
-    public IngestDocument(IngestDocument other) {
-        this(new HashMap<>(other.sourceAndMetadata), new HashMap<>(other.ingestMetadata));
-    }
-
-    /**
-     * Constructor needed for testing that allows to create a new {@link IngestDocument} given the provided elasticsearch metadata,
-     * source and ingest metadata. This is needed because the ingest metadata will be initialized with the current timestamp at
-     * init time, which makes equality comparisons impossible in tests.
-     */
-    public IngestDocument(Map<String, Object> sourceAndMetadata, Map<String, String> ingestMetadata) {
-        this.sourceAndMetadata = sourceAndMetadata;
-        this.ingestMetadata = ingestMetadata;
-    }
-
-    /**
-     * Returns the value contained in the document for the provided path
-     * @param path The path within the document in dot-notation
-     * @param clazz The expected class of the field value
-     * @return the value for the provided path if existing, null otherwise
-     * @throws IllegalArgumentException if the path is null, empty, invalid, if the field doesn't exist
-     * or if the field that is found at the provided path is not of the expected type.
-     */
-    public <T> T getFieldValue(String path, Class<T> clazz) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (String pathElement : fieldPath.pathElements) {
-            context = resolve(pathElement, path, context);
-        }
-        return cast(path, context, clazz);
-    }
-
-    /**
-     * Checks whether the document contains a value for the provided path
-     * @param path The path within the document in dot-notation
-     * @return true if the document contains a value for the field, false otherwise
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public boolean hasField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                return false;
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                context = map.get(pathElement);
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                try {
-                    int index = Integer.parseInt(pathElement);
-                    if (index < 0 || index >= list.size()) {
-                        return false;
-                    }
-                    context = list.get(index);
-                } catch (NumberFormatException e) {
-                    return false;
-                }
-
-            } else {
-                return false;
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            return map.containsKey(leafKey);
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            try {
-                int index = Integer.parseInt(leafKey);
-                return index >= 0 && index < list.size();
-            } catch (NumberFormatException e) {
-                return false;
-            }
-        }
-        return false;
-    }
-
-    /**
-     * Removes the field identified by the provided path.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void removeField(TemplateService.Template fieldPathTemplate) {
-        removeField(renderTemplate(fieldPathTemplate));
-    }
-
-    /**
-     * Removes the field identified by the provided path.
-     * @param path the path of the field to be removed
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
-     */
-    public void removeField(String path) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            context = resolve(fieldPath.pathElements[i], path, context);
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(leafKey)) {
-                map.remove(leafKey);
-                return;
-            }
-            throw new IllegalArgumentException("field [" + leafKey + "] not present as part of path [" + path + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            list.remove(index);
-            return;
-        }
-
-        if (context == null) {
-            throw new IllegalArgumentException("cannot remove [" + leafKey + "] from null as part of path [" + path + "]");
-        }
-        throw new IllegalArgumentException("cannot remove [" + leafKey + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-    }
-
-    private static Object resolve(String pathElement, String fullPath, Object context) {
-        if (context == null) {
-            throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + fullPath + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (map.containsKey(pathElement)) {
-                return map.get(pathElement);
-            }
-            throw new IllegalArgumentException("field [" + pathElement + "] not present as part of path [" + fullPath + "]");
-        }
-        if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(pathElement);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + fullPath + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + fullPath + "]");
-            }
-            return list.get(index);
-        }
-        throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + fullPath + "]");
-    }
-
-    /**
-     * Appends the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the path identifies a list, the value will be appended to the existing list.
-     * If the path identifies a scalar, the scalar will be converted to a list and
-     * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
-     * existing (or newly created) list.
-     * @param path The path within the document in dot-notation
-     * @param value The value or values to append to the existing ones
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public void appendFieldValue(String path, Object value) {
-        setFieldValue(path, value, true);
-    }
-
-    /**
-     * Appends the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the path identifies a list, the value will be appended to the existing list.
-     * If the path identifies a scalar, the scalar will be converted to a list and
-     * the provided value will be added to the newly created list.
-     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
-     * existing (or newly created) list.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @param valueSource The value source that will produce the value or values to append to the existing ones
-     * @throws IllegalArgumentException if the path is null, empty or invalid.
-     */
-    public void appendFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
-        Map<String, Object> model = createTemplateModel();
-        appendFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model));
-    }
-
-    /**
-     * Sets the provided value to the provided path in the document.
-     * Any non existing path element will be created.
-     * If the last item in the path is a list, the value will replace the existing list as a whole.
-     * Use {@link #appendFieldValue(String, Object)} to append values to lists instead.
-     * @param path The path within the document in dot-notation
-     * @param value The value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
-     * item identified by the provided path.
-     */
-    public void setFieldValue(String path, Object value) {
-        setFieldValue(path, value, false);
-    }
-
-    /**
-     * Sets the provided value to the provided path in the document.
-     * Any non existing path element will be created. If the last element is a list,
-     * the value will replace the existing list.
-     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
-     * @param valueSource The value source that will produce the value to put in for the path key
-     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
-     * item identified by the provided path.
-     */
-    public void setFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
-        Map<String, Object> model = createTemplateModel();
-        setFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model), false);
-    }
-
-    private void setFieldValue(String path, Object value, boolean append) {
-        FieldPath fieldPath = new FieldPath(path);
-        Object context = fieldPath.initialContext;
-        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
-            String pathElement = fieldPath.pathElements[i];
-            if (context == null) {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + path + "]");
-            }
-            if (context instanceof Map) {
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) context;
-                if (map.containsKey(pathElement)) {
-                    context = map.get(pathElement);
-                } else {
-                    HashMap<Object, Object> newMap = new HashMap<>();
-                    map.put(pathElement, newMap);
-                    context = newMap;
-                }
-            } else if (context instanceof List) {
-                @SuppressWarnings("unchecked")
-                List<Object> list = (List<Object>) context;
-                int index;
-                try {
-                    index = Integer.parseInt(pathElement);
-                } catch (NumberFormatException e) {
-                    throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-                }
-                if (index < 0 || index >= list.size()) {
-                    throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-                }
-                context = list.get(index);
-            } else {
-                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-            }
-        }
-
-        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
-        if (context == null) {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with null parent as part of path [" + path + "]");
-        }
-        if (context instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) context;
-            if (append) {
-                if (map.containsKey(leafKey)) {
-                    Object object = map.get(leafKey);
-                    List<Object> list = appendValues(object, value);
-                    if (list != object) {
-                        map.put(leafKey, list);
-                    }
-                } else {
-                    List<Object> list = new ArrayList<>();
-                    appendValues(list, value);
-                    map.put(leafKey, list);
-                }
-                return;
-            }
-            map.put(leafKey, value);
-        } else if (context instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List<Object>) context;
-            int index;
-            try {
-                index = Integer.parseInt(leafKey);
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
-            }
-            if (index < 0 || index >= list.size()) {
-                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
-            }
-            if (append) {
-                Object object = list.get(index);
-                List<Object> newList = appendValues(object, value);
-                if (newList != object) {
-                    list.set(index, newList);
-                }
-                return;
-            }
-            list.set(index, value);
-        } else {
-            throw new IllegalArgumentException("cannot set [" + leafKey + "] with parent object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
-        }
-    }
-
-    @SuppressWarnings("unchecked")
-    private static List<Object> appendValues(Object maybeList, Object value) {
-        List<Object> list;
-        if (maybeList instanceof List) {
-            //maybeList is already a list, we append the provided values to it
-            list = (List<Object>) maybeList;
-        } else {
-            //maybeList is a scalar, we convert it to a list and append the provided values to it
-            list = new ArrayList<>();
-            list.add(maybeList);
-        }
-        appendValues(list, value);
-        return list;
-    }
-
-    private static void appendValues(List<Object> list, Object value) {
-        if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<?> valueList = (List<?>) value;
-            valueList.stream().forEach(list::add);
-        } else {
-            list.add(value);
-        }
-    }
-
-    private static <T> T cast(String path, Object object, Class<T> clazz) {
-        if (object == null) {
-            return null;
-        }
-        if (clazz.isInstance(object)) {
-            return clazz.cast(object);
-        }
-        throw new IllegalArgumentException("field [" + path + "] of type [" + object.getClass().getName() + "] cannot be cast to [" + clazz.getName() + "]");
-    }
-
-    public String renderTemplate(TemplateService.Template template) {
-        return template.execute(createTemplateModel());
-    }
-
-    private Map<String, Object> createTemplateModel() {
-        Map<String, Object> model = new HashMap<>(sourceAndMetadata);
-        model.put(SourceFieldMapper.NAME, sourceAndMetadata);
-        // If there is a field in the source with the name '_ingest' it gets overwritten here,
-        // if access to that field is required then it get accessed via '_source._ingest'
-        model.put(INGEST_KEY, ingestMetadata);
-        return model;
-    }
-
-    /**
-     * one time operation that extracts the metadata fields from the ingest document and returns them.
-     * Metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.
-     */
-    public Map<MetaData, String> extractMetadata() {
-        Map<MetaData, String> metadataMap = new HashMap<>();
-        for (MetaData metaData : MetaData.values()) {
-            metadataMap.put(metaData, cast(metaData.getFieldName(), sourceAndMetadata.remove(metaData.getFieldName()), String.class));
-        }
-        return metadataMap;
-    }
-
-    /**
-     * Returns the available ingest metadata fields, by default only timestamp, but it is possible to set additional ones.
-     * Use only for reading values, modify them instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, String> getIngestMetadata() {
-        return this.ingestMetadata;
-    }
-
-    /**
-     * Returns the document including its metadata fields, unless {@link #extractMetadata()} has been called, in which case the
-     * metadata fields will not be present anymore.
-     * Modify the document instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
-     */
-    public Map<String, Object> getSourceAndMetadata() {
-        return this.sourceAndMetadata;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == this) { return true; }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-
-        IngestDocument other = (IngestDocument) obj;
-        return Objects.equals(sourceAndMetadata, other.sourceAndMetadata) &&
-                Objects.equals(ingestMetadata, other.ingestMetadata);
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(sourceAndMetadata, ingestMetadata);
-    }
-
-    @Override
-    public String toString() {
-        return "IngestDocument{" +
-                " sourceAndMetadata=" + sourceAndMetadata +
-                ", ingestMetadata=" + ingestMetadata +
-                '}';
-    }
-
-    public enum MetaData {
-        INDEX(IndexFieldMapper.NAME),
-        TYPE(TypeFieldMapper.NAME),
-        ID(IdFieldMapper.NAME),
-        ROUTING(RoutingFieldMapper.NAME),
-        PARENT(ParentFieldMapper.NAME),
-        TIMESTAMP(TimestampFieldMapper.NAME),
-        TTL(TTLFieldMapper.NAME);
-
-        private final String fieldName;
-
-        MetaData(String fieldName) {
-            this.fieldName = fieldName;
-        }
-
-        public String getFieldName() {
-            return fieldName;
-        }
-    }
-
-    private class FieldPath {
-        private final String[] pathElements;
-        private final Object initialContext;
-
-        private FieldPath(String path) {
-            if (Strings.isEmpty(path)) {
-                throw new IllegalArgumentException("path cannot be null nor empty");
-            }
-            String newPath;
-            if (path.startsWith(INGEST_KEY + ".")) {
-                initialContext = ingestMetadata;
-                newPath = path.substring(8, path.length());
-            } else {
-                initialContext = sourceAndMetadata;
-                if (path.startsWith(SourceFieldMapper.NAME + ".")) {
-                    newPath = path.substring(8, path.length());
-                } else {
-                    newPath = path;
-                }
-            }
-            this.pathElements = Strings.splitStringToArray(newPath, '.');
-            if (pathElements.length == 0) {
-                throw new IllegalArgumentException("path [" + path + "] is not valid");
-            }
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
deleted file mode 100644
index 68ba8da..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-/**
- * A pipeline is a list of {@link Processor} instances grouped under a unique id.
- */
-public final class Pipeline {
-
-    final static String DESCRIPTION_KEY = "description";
-    final static String PROCESSORS_KEY = "processors";
-    final static String ON_FAILURE_KEY = "on_failure";
-
-    private final String id;
-    private final String description;
-    private final CompoundProcessor compoundProcessor;
-
-    public Pipeline(String id, String description, CompoundProcessor compoundProcessor) {
-        this.id = id;
-        this.description = description;
-        this.compoundProcessor = compoundProcessor;
-    }
-
-    /**
-     * Modifies the data of a document to be indexed based on the processor this pipeline holds
-     */
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        compoundProcessor.execute(ingestDocument);
-    }
-
-    /**
-     * The unique id of this pipeline
-     */
-    public String getId() {
-        return id;
-    }
-
-    /**
-     * An optional description of what this pipeline is doing to the data gets processed by this pipeline.
-     */
-    public String getDescription() {
-        return description;
-    }
-
-    /**
-     * Unmodifiable list containing each processor that operates on the data.
-     */
-    public List<Processor> getProcessors() {
-        return compoundProcessor.getProcessors();
-    }
-
-    /**
-     * Unmodifiable list containing each on_failure processor that operates on the data in case of
-     * exception thrown in pipeline processors
-     */
-    public List<Processor> getOnFailureProcessors() {
-        return compoundProcessor.getOnFailureProcessors();
-    }
-
-    public final static class Factory {
-
-        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws Exception {
-            String description = ConfigurationUtils.readOptionalStringProperty(config, DESCRIPTION_KEY);
-            List<Processor> processors = readProcessors(PROCESSORS_KEY, processorRegistry, config);
-            List<Processor> onFailureProcessors = readProcessors(ON_FAILURE_KEY, processorRegistry, config);
-            if (config.isEmpty() == false) {
-                throw new IllegalArgumentException("pipeline [" + id + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
-            }
-            CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.unmodifiableList(processors), Collections.unmodifiableList(onFailureProcessors));
-            return new Pipeline(id, description, compoundProcessor);
-        }
-
-        private List<Processor> readProcessors(String fieldName, Map<String, Processor.Factory> processorRegistry, Map<String, Object> config) throws Exception {
-            List<Map<String, Map<String, Object>>> processorConfigs = ConfigurationUtils.readOptionalList(config, fieldName);
-            List<Processor> processors = new ArrayList<>();
-            if (processorConfigs != null) {
-                for (Map<String, Map<String, Object>> processorConfigWithKey : processorConfigs) {
-                    for (Map.Entry<String, Map<String, Object>> entry : processorConfigWithKey.entrySet()) {
-                        processors.add(readProcessor(processorRegistry, entry.getKey(), entry.getValue()));
-                    }
-                }
-            }
-
-            return processors;
-        }
-
-        private Processor readProcessor(Map<String, Processor.Factory> processorRegistry, String type, Map<String, Object> config) throws Exception {
-            Processor.Factory factory = processorRegistry.get(type);
-            if (factory != null) {
-                List<Processor> onFailureProcessors = readProcessors(ON_FAILURE_KEY, processorRegistry, config);
-                Processor processor = factory.create(config);
-                if (config.isEmpty() == false) {
-                    throw new IllegalArgumentException("processor [" + type + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
-                }
-                if (onFailureProcessors.isEmpty()) {
-                    return processor;
-                }
-                return new CompoundProcessor(Collections.singletonList(processor), onFailureProcessors);
-            }
-            throw new IllegalArgumentException("No processor type exists with name [" + type + "]");
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
deleted file mode 100644
index f178051..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-
-package org.elasticsearch.ingest.core;
-
-import java.util.Map;
-
-/**
- * A processor implementation may modify the data belonging to a document.
- * Whether changes are made and what exactly is modified is up to the implementation.
- */
-public interface Processor {
-
-    /**
-     * Introspect and potentially modify the incoming data.
-     */
-    void execute(IngestDocument ingestDocument) throws Exception;
-
-    /**
-     * Gets the type of a processor
-     */
-    String getType();
-
-    /**
-     * Gets the tag of a processor.
-     */
-    String getTag();
-
-    /**
-     * A factory that knows how to construct a processor based on a map of maps.
-     */
-    interface Factory<P extends Processor> {
-
-        /**
-         * Creates a processor based on the specified map of maps config.
-         *
-         * Implementations are responsible for removing the used keys, so that after creating a pipeline ingest can
-         * verify if all configurations settings have been used.
-         */
-        P create(Map<String, Object> config) throws Exception;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java b/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
deleted file mode 100644
index 8988c92..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.ingest.core;
-
-import java.util.Map;
-
-/**
- * Abstraction for the ingest template engine used to decouple {@link IngestDocument} from {@link org.elasticsearch.script.ScriptService}.
- * Allows to compile a template into an ingest {@link Template} object.
- * A compiled template can be executed by calling its {@link Template#execute(Map)} method.
- */
-public interface TemplateService {
-
-    Template compile(String template);
-
-    interface Template {
-
-        String execute(Map<String, Object> model);
-
-        String getKey();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java b/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
deleted file mode 100644
index e9f09a1..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
+++ /dev/null
@@ -1,191 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-/**
- * Holds a value. If the value is requested a copy is made and optionally template snippets are resolved too.
- */
-public interface ValueSource {
-
-    /**
-     * Returns a copy of the value this ValueSource holds and resolves templates if there're any.
-     *
-     * For immutable values only a copy of the reference to the value is made.
-     *
-     * @param model The model to be used when resolving any templates
-     * @return copy of the wrapped value
-     */
-    Object copyAndResolve(Map<String, Object> model);
-
-    static ValueSource wrap(Object value, TemplateService templateService) {
-        if (value instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<Object, Object> mapValue = (Map) value;
-            Map<ValueSource, ValueSource> valueTypeMap = new HashMap<>(mapValue.size());
-            for (Map.Entry<Object, Object> entry : mapValue.entrySet()) {
-                valueTypeMap.put(wrap(entry.getKey(), templateService), wrap(entry.getValue(), templateService));
-            }
-            return new MapValue(valueTypeMap);
-        } else if (value instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> listValue = (List) value;
-            List<ValueSource> valueSourceList = new ArrayList<>(listValue.size());
-            for (Object item : listValue) {
-                valueSourceList.add(wrap(item, templateService));
-            }
-            return new ListValue(valueSourceList);
-        } else if (value == null || value instanceof Number || value instanceof Boolean) {
-            return new ObjectValue(value);
-        } else if (value instanceof String) {
-            return new TemplatedValue(templateService.compile((String) value));
-        } else {
-            throw new IllegalArgumentException("unexpected value type [" + value.getClass() + "]");
-        }
-    }
-
-    final class MapValue implements ValueSource {
-
-        private final Map<ValueSource, ValueSource> map;
-
-        MapValue(Map<ValueSource, ValueSource> map) {
-            this.map = map;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            Map<Object, Object> copy = new HashMap<>();
-            for (Map.Entry<ValueSource, ValueSource> entry : this.map.entrySet()) {
-                copy.put(entry.getKey().copyAndResolve(model), entry.getValue().copyAndResolve(model));
-            }
-            return copy;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            MapValue mapValue = (MapValue) o;
-            return map.equals(mapValue.map);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return map.hashCode();
-        }
-    }
-
-    final class ListValue implements ValueSource {
-
-        private final List<ValueSource> values;
-
-        ListValue(List<ValueSource> values) {
-            this.values = values;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            List<Object> copy = new ArrayList<>(values.size());
-            for (ValueSource value : values) {
-                copy.add(value.copyAndResolve(model));
-            }
-            return copy;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            ListValue listValue = (ListValue) o;
-            return values.equals(listValue.values);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return values.hashCode();
-        }
-    }
-
-    final class ObjectValue implements ValueSource {
-
-        private final Object value;
-
-        ObjectValue(Object value) {
-            this.value = value;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            return value;
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            ObjectValue objectValue = (ObjectValue) o;
-            return Objects.equals(value, objectValue.value);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hashCode(value);
-        }
-    }
-
-    final class TemplatedValue implements ValueSource {
-
-        private final TemplateService.Template template;
-
-        TemplatedValue(TemplateService.Template template) {
-            this.template = template;
-        }
-
-        @Override
-        public Object copyAndResolve(Map<String, Object> model) {
-            return template.execute(model);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            TemplatedValue templatedValue = (TemplatedValue) o;
-            return Objects.equals(template.getKey(), templatedValue.template.getKey());
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hashCode(template.getKey());
-        }
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
deleted file mode 100644
index 32e5476..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Base class for processors that manipulate strings and require a single "fields" array config value, which
- * holds a list of field names in string format.
- */
-public abstract class AbstractStringProcessor extends AbstractProcessor {
-    private final String field;
-
-    protected AbstractStringProcessor(String tag, String field) {
-        super(tag);
-        this.field = field;
-    }
-
-    public String getField() {
-        return field;
-    }
-
-    @Override
-    public final void execute(IngestDocument document) {
-        String val = document.getFieldValue(field, String.class);
-        if (val == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot process it.");
-        }
-        document.setFieldValue(field, process(val));
-    }
-
-    protected abstract String process(String value);
-
-    public static abstract class Factory<T extends AbstractStringProcessor> extends AbstractProcessorFactory<T> {
-
-        @Override
-        public T doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return newProcessor(processorTag, field);
-        }
-
-        protected abstract T newProcessor(String processorTag, String field);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
deleted file mode 100644
index deff384..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-
-import java.util.Map;
-
-/**
- * Processor that appends value or values to existing lists. If the field is not present a new list holding the
- * provided values will be added. If the field is a scalar it will be converted to a single item list and the provided
- * values will be added to the newly created list.
- */
-public class AppendProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "append";
-
-    private final TemplateService.Template field;
-    private final ValueSource value;
-
-    AppendProcessor(String tag, TemplateService.Template field, ValueSource value) {
-        super(tag);
-        this.field = field;
-        this.value = value;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    public ValueSource getValue() {
-        return value;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        ingestDocument.appendFieldValue(field, value);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static final class Factory extends AbstractProcessorFactory<AppendProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public AppendProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Object value = ConfigurationUtils.readObject(config, "value");
-            return new AppendProcessor(processorTag, templateService.compile(field), ValueSource.wrap(value, templateService));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
deleted file mode 100644
index 5b6bacf..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-/**
- * Processor that converts fields content to a different type. Supported types are: integer, float, boolean and string.
- * Throws exception if the field is not there or the conversion fails.
- */
-public class ConvertProcessor extends AbstractProcessor {
-
-    enum Type {
-        INTEGER {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Integer.parseInt(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to integer", e);
-                }
-
-            }
-        }, FLOAT {
-            @Override
-            public Object convert(Object value) {
-                try {
-                    return Float.parseFloat(value.toString());
-                } catch(NumberFormatException e) {
-                    throw new IllegalArgumentException("unable to convert [" + value + "] to float", e);
-                }
-            }
-        }, BOOLEAN {
-            @Override
-            public Object convert(Object value) {
-                if (value.toString().equalsIgnoreCase("true")) {
-                    return true;
-                } else if (value.toString().equalsIgnoreCase("false")) {
-                    return false;
-                } else {
-                    throw new IllegalArgumentException("[" + value + "] is not a boolean value, cannot convert to boolean");
-                }
-            }
-        }, STRING {
-            @Override
-            public Object convert(Object value) {
-                return value.toString();
-            }
-        };
-
-        @Override
-        public final String toString() {
-            return name().toLowerCase(Locale.ROOT);
-        }
-
-        public abstract Object convert(Object value);
-
-        public static Type fromString(String type) {
-            try {
-                return Type.valueOf(type.toUpperCase(Locale.ROOT));
-            } catch(IllegalArgumentException e) {
-                throw new IllegalArgumentException("type [" + type + "] not supported, cannot convert field.", e);
-            }
-        }
-    }
-
-    public static final String TYPE = "convert";
-
-    private final String field;
-    private final Type convertType;
-
-    ConvertProcessor(String tag, String field, Type convertType) {
-        super(tag);
-        this.field = field;
-        this.convertType = convertType;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Type getConvertType() {
-        return convertType;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        Object oldValue = document.getFieldValue(field, Object.class);
-        Object newValue;
-        if (oldValue == null) {
-            throw new IllegalArgumentException("Field [" + field + "] is null, cannot be converted to type [" + convertType + "]");
-        }
-
-        if (oldValue instanceof List) {
-            List<?> list = (List<?>) oldValue;
-            List<Object> newList = new ArrayList<>();
-            for (Object value : list) {
-                newList.add(convertType.convert(value));
-            }
-            newValue = newList;
-        } else {
-            newValue = convertType.convert(oldValue);
-        }
-        document.setFieldValue(field, newValue);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<ConvertProcessor> {
-        @Override
-        public ConvertProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Type convertType = Type.fromString(ConfigurationUtils.readStringProperty(config, "type"));
-            return new ConvertProcessor(processorTag, field, convertType);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
deleted file mode 100644
index 282b291..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.DateTimeFormat;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.Locale;
-import java.util.function.Function;
-
-enum DateFormat {
-    Iso8601 {
-        @Override
-        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
-            return ISODateTimeFormat.dateTimeParser().withZone(timezone)::parseDateTime;
-        }
-    },
-    Unix {
-        @Override
-        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
-            return (date) -> new DateTime((long)(Float.parseFloat(date) * 1000), timezone);
-        }
-    },
-    UnixMs {
-        @Override
-        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
-            return (date) -> new DateTime(Long.parseLong(date), timezone);
-        }
-    },
-    Tai64n {
-        @Override
-        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
-            return (date) -> new DateTime(parseMillis(date), timezone);
-        }
-
-        private long parseMillis(String date) {
-            if (date.startsWith("@")) {
-                date = date.substring(1);
-            }
-            long base = Long.parseLong(date.substring(1, 16), 16);
-            // 1356138046000
-            long rest = Long.parseLong(date.substring(16, 24), 16);
-            return ((base * 1000) - 10000) + (rest/1000000);
-        }
-    },
-    Joda {
-        @Override
-        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
-            return DateTimeFormat.forPattern(format)
-                .withDefaultYear((new DateTime(DateTimeZone.UTC)).getYear())
-                .withZone(timezone).withLocale(locale)::parseDateTime;
-        }
-    };
-
-    abstract Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale);
-
-    static DateFormat fromString(String format) {
-        switch (format) {
-            case "ISO8601":
-                return Iso8601;
-            case "UNIX":
-                return Unix;
-            case "UNIX_MS":
-                return UnixMs;
-            case "TAI64N":
-                return Tai64n;
-            default:
-                return Joda;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
deleted file mode 100644
index 9fc0378..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.joda.time.format.ISODateTimeFormat;
-
-import java.util.ArrayList;
-import java.util.IllformedLocaleException;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.function.Function;
-
-public final class DateProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "date";
-    static final String DEFAULT_TARGET_FIELD = "@timestamp";
-
-    private final DateTimeZone timezone;
-    private final Locale locale;
-    private final String matchField;
-    private final String targetField;
-    private final List<String> matchFormats;
-    private final List<Function<String, DateTime>> dateParsers;
-
-    DateProcessor(String tag, DateTimeZone timezone, Locale locale, String matchField, List<String> matchFormats, String targetField) {
-        super(tag);
-        this.timezone = timezone;
-        this.locale = locale;
-        this.matchField = matchField;
-        this.targetField = targetField;
-        this.matchFormats = matchFormats;
-        this.dateParsers = new ArrayList<>();
-        for (String matchFormat : matchFormats) {
-            DateFormat dateFormat = DateFormat.fromString(matchFormat);
-            dateParsers.add(dateFormat.getFunction(matchFormat, timezone, locale));
-        }
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String value = ingestDocument.getFieldValue(matchField, String.class);
-
-        DateTime dateTime = null;
-        Exception lastException = null;
-        for (Function<String, DateTime> dateParser : dateParsers) {
-            try {
-                dateTime = dateParser.apply(value);
-            } catch (Exception e) {
-                //try the next parser and keep track of the exceptions
-                lastException = ExceptionsHelper.useOrSuppress(lastException, e);
-            }
-        }
-
-        if (dateTime == null) {
-            throw new IllegalArgumentException("unable to parse date [" + value + "]", lastException);
-        }
-
-        ingestDocument.setFieldValue(targetField, ISODateTimeFormat.dateTime().print(dateTime));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    DateTimeZone getTimezone() {
-        return timezone;
-    }
-
-    Locale getLocale() {
-        return locale;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    List<String> getMatchFormats() {
-        return matchFormats;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<DateProcessor> {
-
-        @SuppressWarnings("unchecked")
-        public DateProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "match_field");
-            String targetField = ConfigurationUtils.readStringProperty(config, "target_field", DEFAULT_TARGET_FIELD);
-            String timezoneString = ConfigurationUtils.readOptionalStringProperty(config, "timezone");
-            DateTimeZone timezone = timezoneString == null ? DateTimeZone.UTC : DateTimeZone.forID(timezoneString);
-            String localeString = ConfigurationUtils.readOptionalStringProperty(config, "locale");
-            Locale locale = Locale.ENGLISH;
-            if (localeString != null) {
-                try {
-                    locale = (new Locale.Builder()).setLanguageTag(localeString).build();
-                } catch (IllformedLocaleException e) {
-                    throw new IllegalArgumentException("Invalid language tag specified: " + localeString);
-                }
-            }
-            List<String> matchFormats = ConfigurationUtils.readList(config, "match_formats");
-            return new DateProcessor(processorTag, timezone, locale, matchField, matchFormats, targetField);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
deleted file mode 100644
index 295a988..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Processor that replaces dots in document field names with a
- * specified separator.
- */
-public class DeDotProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "dedot";
-    static final String DEFAULT_SEPARATOR = "_";
-
-    private final String separator;
-
-    DeDotProcessor(String tag, String separator) {
-        super(tag);
-        this.separator = separator;
-    }
-
-    public String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        deDot(document.getSourceAndMetadata());
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    /**
-     * Recursively iterates through Maps and Lists in search of map entries with
-     * keys containing dots. The dots in these fields are replaced with {@link #separator}.
-     *
-     * @param obj The current object in context to be checked for dots in its fields.
-     */
-    private void deDot(Object obj) {
-        if (obj instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> doc = (Map) obj;
-            Iterator<Map.Entry<String, Object>> it = doc.entrySet().iterator();
-            Map<String, Object> deDottedFields = new HashMap<>();
-            while (it.hasNext()) {
-                Map.Entry<String, Object> entry = it.next();
-                deDot(entry.getValue());
-                String fieldName = entry.getKey();
-                if (fieldName.contains(".")) {
-                    String deDottedFieldName = fieldName.replaceAll("\\.", separator);
-                    deDottedFields.put(deDottedFieldName, entry.getValue());
-                    it.remove();
-                }
-            }
-            doc.putAll(deDottedFields);
-        } else if (obj instanceof List) {
-            @SuppressWarnings("unchecked")
-            List<Object> list = (List) obj;
-            for (Object value : list) {
-                deDot(value);
-            }
-        }
-    }
-
-    public static class Factory extends AbstractProcessorFactory<DeDotProcessor> {
-
-        @Override
-        public DeDotProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String separator = ConfigurationUtils.readOptionalStringProperty(config, "separator");
-            if (separator == null) {
-                separator = DEFAULT_SEPARATOR;
-            }
-            return new DeDotProcessor(processorTag, separator);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
deleted file mode 100644
index 65b4b60..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Map;
-
-/**
- * Processor that raises a runtime exception with a provided
- * error message.
- */
-public class FailProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "fail";
-
-    private final TemplateService.Template message;
-
-    FailProcessor(String tag, TemplateService.Template message) {
-        super(tag);
-        this.message = message;
-    }
-
-    public TemplateService.Template getMessage() {
-        return message;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        throw new FailProcessorException(document.renderTemplate(message));
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<FailProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public FailProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String message = ConfigurationUtils.readStringProperty(config, "message");
-            return new FailProcessor(processorTag, templateService.compile(message));
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
deleted file mode 100644
index bfdfe11..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-/**
- * Exception class thrown by {@link FailProcessor}.
- *
- * This exception is caught in the {@link org.elasticsearch.ingest.core.CompoundProcessor} and
- * then changes the state of {@link org.elasticsearch.ingest.core.IngestDocument}. This
- * exception should get serialized.
- */
-public class FailProcessorException extends RuntimeException {
-
-    public FailProcessorException(String message) {
-        super(message);
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
deleted file mode 100644
index 3dc4b3f..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-/**
- * Processor that allows to search for patterns in field content and replace them with corresponding string replacement.
- * Support fields of string type only, throws exception if a field is of a different type.
- */
-public class GsubProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "gsub";
-
-    private final String field;
-    private final Pattern pattern;
-    private final String replacement;
-
-    GsubProcessor(String tag, String field, Pattern pattern, String replacement) {
-        super(tag);
-        this.field = field;
-        this.pattern = pattern;
-        this.replacement = replacement;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    Pattern getPattern() {
-        return pattern;
-    }
-
-    String getReplacement() {
-        return replacement;
-    }
-
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot match pattern.");
-        }
-        Matcher matcher = pattern.matcher(oldVal);
-        String newVal = matcher.replaceAll(replacement);
-        document.setFieldValue(field, newVal);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<GsubProcessor> {
-        @Override
-        public GsubProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String pattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            String replacement = ConfigurationUtils.readStringProperty(config, "replacement");
-            Pattern searchPattern = Pattern.compile(pattern);
-            return new GsubProcessor(processorTag, field, searchPattern, replacement);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
deleted file mode 100644
index 3516929..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.List;
-import java.util.Map;
-import java.util.stream.Collectors;
-
-/**
- * Processor that joins the different items of an array into a single string value using a separator between each item.
- * Throws exception is the specified field is not an array.
- */
-public class JoinProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "join";
-
-    private final String field;
-    private final String separator;
-
-    JoinProcessor(String tag, String field, String separator) {
-        super(tag);
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        List<?> list = document.getFieldValue(field, List.class);
-        if (list == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot join.");
-        }
-        String joined = list.stream()
-                .map(Object::toString)
-                .collect(Collectors.joining(separator));
-        document.setFieldValue(field, joined);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<JoinProcessor> {
-        @Override
-        public JoinProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String separator = ConfigurationUtils.readStringProperty(config, "separator");
-            return new JoinProcessor(processorTag, field, separator);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
deleted file mode 100644
index 617efd9..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to lowercase.
- * Throws exception is the field is not of type string.
- */
-
-public class LowercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "lowercase";
-
-    LowercaseProcessor(String processorTag, String field) {
-        super(processorTag, field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toLowerCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<LowercaseProcessor> {
-        @Override
-        protected LowercaseProcessor newProcessor(String tag, String field) {
-            return new LowercaseProcessor(tag, field);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
deleted file mode 100644
index e994954..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that removes existing fields. Nothing happens if the field is not present.
- */
-public class RemoveProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "remove";
-
-    private final TemplateService.Template field;
-
-    RemoveProcessor(String tag, TemplateService.Template field) {
-        super(tag);
-        this.field = field;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.removeField(field);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<RemoveProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public RemoveProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new RemoveProcessor(processorTag, templateService.compile(field));
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
deleted file mode 100644
index 7726a72..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-
-/**
- * Processor that allows to rename existing fields. Will throw exception if the field is not present.
- */
-public class RenameProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "rename";
-
-    private final String oldFieldName;
-    private final String newFieldName;
-
-    RenameProcessor(String tag, String oldFieldName, String newFieldName) {
-        super(tag);
-        this.oldFieldName = oldFieldName;
-        this.newFieldName = newFieldName;
-    }
-
-    String getOldFieldName() {
-        return oldFieldName;
-    }
-
-    String getNewFieldName() {
-        return newFieldName;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        if (document.hasField(oldFieldName) == false) {
-            throw new IllegalArgumentException("field [" + oldFieldName + "] doesn't exist");
-        }
-        if (document.hasField(newFieldName)) {
-            throw new IllegalArgumentException("field [" + newFieldName + "] already exists");
-        }
-
-        Object oldValue = document.getFieldValue(oldFieldName, Object.class);
-        document.setFieldValue(newFieldName, oldValue);
-        try {
-            document.removeField(oldFieldName);
-        } catch (Exception e) {
-            //remove the new field if the removal of the old one failed
-            document.removeField(newFieldName);
-            throw e;
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<RenameProcessor> {
-        @Override
-        public RenameProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            String newField = ConfigurationUtils.readStringProperty(config, "to");
-            return new RenameProcessor(processorTag, field, newField);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
deleted file mode 100644
index e046a5f..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-
-import java.util.Map;
-
-/**
- * Processor that adds new fields with their corresponding values. If the field is already present, its value
- * will be replaced with the provided one.
- */
-public class SetProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "set";
-
-    private final TemplateService.Template field;
-    private final ValueSource value;
-
-    SetProcessor(String tag, TemplateService.Template field, ValueSource value) {
-        super(tag);
-        this.field = field;
-        this.value = value;
-    }
-
-    public TemplateService.Template getField() {
-        return field;
-    }
-
-    public ValueSource getValue() {
-        return value;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        document.setFieldValue(field, value);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static final class Factory extends AbstractProcessorFactory<SetProcessor> {
-
-        private final TemplateService templateService;
-
-        public Factory(TemplateService templateService) {
-            this.templateService = templateService;
-        }
-
-        @Override
-        public SetProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            Object value = ConfigurationUtils.readObject(config, "value");
-            return new SetProcessor(processorTag, templateService.compile(field), ValueSource.wrap(value, templateService));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
deleted file mode 100644
index ad0bffb..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Processor that splits fields content into different items based on the occurrence of a specified separator.
- * New field value will be an array containing all of the different extracted items.
- * Throws exception if the field is null or a type other than string.
- */
-public class SplitProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "split";
-
-    private final String field;
-    private final String separator;
-
-    SplitProcessor(String tag, String field, String separator) {
-        super(tag);
-        this.field = field;
-        this.separator = separator;
-    }
-
-    String getField() {
-        return field;
-    }
-
-    String getSeparator() {
-        return separator;
-    }
-
-    @Override
-    public void execute(IngestDocument document) {
-        String oldVal = document.getFieldValue(field, String.class);
-        if (oldVal == null) {
-            throw new IllegalArgumentException("field [" + field + "] is null, cannot split.");
-        }
-        String[] strings = oldVal.split(separator);
-        List<String> splitList = new ArrayList<>(strings.length);
-        Collections.addAll(splitList, strings);
-        document.setFieldValue(field, splitList);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractProcessorFactory<SplitProcessor> {
-        @Override
-        public SplitProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String field = ConfigurationUtils.readStringProperty(config, "field");
-            return new SplitProcessor(processorTag, field, ConfigurationUtils.readStringProperty(config, "separator"));
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
deleted file mode 100644
index c66cc84..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-/**
- * Processor that trims the content of string fields.
- * Throws exception is the field is not of type string.
- */
-public class TrimProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "trim";
-
-    TrimProcessor(String processorTag, String field) {
-        super(processorTag, field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.trim();
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<TrimProcessor> {
-        @Override
-        protected TrimProcessor newProcessor(String tag, String field) {
-            return new TrimProcessor(tag, field);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
deleted file mode 100644
index e6a1f77..0000000
--- a/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-/**
- * Processor that converts the content of string fields to uppercase.
- * Throws exception is the field is not of type string.
- */
-public class UppercaseProcessor extends AbstractStringProcessor {
-
-    public static final String TYPE = "uppercase";
-
-    UppercaseProcessor(String processorTag, String field) {
-        super(processorTag, field);
-    }
-
-    @Override
-    protected String process(String value) {
-        return value.toUpperCase(Locale.ROOT);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    public static class Factory extends AbstractStringProcessor.Factory<UppercaseProcessor> {
-        @Override
-        protected UppercaseProcessor newProcessor(String tag, String field) {
-            return new UppercaseProcessor(tag, field);
-        }
-    }
-}
-
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 1db7626..724efe9 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -30,7 +30,6 @@ import org.elasticsearch.cluster.ClusterModule;
 import org.elasticsearch.cluster.ClusterNameModule;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
-import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.common.StopWatch;
 import org.elasticsearch.common.component.Lifecycle;
@@ -77,7 +76,6 @@ import org.elasticsearch.indices.ttl.IndicesTTLService;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
-import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.percolator.PercolatorModule;
 import org.elasticsearch.percolator.PercolatorService;
 import org.elasticsearch.plugins.Plugin;
@@ -121,9 +119,8 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
  */
 public class Node implements Releasable {
 
-    public static final Setting<Boolean> NODE_INGEST_SETTING = Setting.boolSetting("node.ingest", true, false, Setting.Scope.CLUSTER);
     private static final String CLIENT_TYPE = "node";
-    public static final String HTTP_ENABLED = "http.enabled";
+    public static final Setting<Boolean> WRITE_PORTS_FIELD_SETTING = Setting.boolSetting("node.portsfile", false, false, Setting.Scope.CLUSTER);
     private final Lifecycle lifecycle = new Lifecycle();
     private final Injector injector;
     private final Settings settings;
@@ -193,7 +190,7 @@ public class Node implements Releasable {
             modules.add(new ClusterModule(this.settings));
             modules.add(new IndicesModule());
             modules.add(new SearchModule(settings, namedWriteableRegistry));
-            modules.add(new ActionModule(DiscoveryNode.ingestNode(settings), false));
+            modules.add(new ActionModule(false));
             modules.add(new GatewayModule(settings));
             modules.add(new NodeClientModule());
             modules.add(new PercolatorModule());
@@ -236,13 +233,6 @@ public class Node implements Releasable {
     }
 
     /**
-     * Returns the environment of the node
-     */
-    public Environment getEnvironment() {
-        return environment;
-    }
-
-    /**
      * Start the node. If the node is already started, this method is no-op.
      */
     public Node start() {
@@ -286,7 +276,7 @@ public class Node implements Releasable {
         injector.getInstance(ResourceWatcherService.class).start();
         injector.getInstance(TribeService.class).start();
 
-        if (System.getProperty("es.tests.portsfile", "false").equals("true")) {
+        if (WRITE_PORTS_FIELD_SETTING.get(settings)) {
             if (settings.getAsBoolean("http.enabled", true)) {
                 HttpServerTransport http = injector.getInstance(HttpServerTransport.class);
                 writePortsFile("http", http.boundAddress());
@@ -357,12 +347,6 @@ public class Node implements Releasable {
         StopWatch stopWatch = new StopWatch("node_close");
         stopWatch.start("tribe");
         injector.getInstance(TribeService.class).close();
-        stopWatch.stop().start("node_service");
-        try {
-            injector.getInstance(NodeService.class).close();
-        } catch (IOException e) {
-            logger.warn("NodeService close failed", e);
-        }
         stopWatch.stop().start("http");
         if (settings.getAsBoolean("http.enabled", true)) {
             injector.getInstance(HttpServer.class).close();
diff --git a/core/src/main/java/org/elasticsearch/node/NodeModule.java b/core/src/main/java/org/elasticsearch/node/NodeModule.java
index 442dc72..aa52d38 100644
--- a/core/src/main/java/org/elasticsearch/node/NodeModule.java
+++ b/core/src/main/java/org/elasticsearch/node/NodeModule.java
@@ -22,28 +22,9 @@ package org.elasticsearch.node;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.ingest.ProcessorsRegistry;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.processor.AppendProcessor;
-import org.elasticsearch.ingest.processor.ConvertProcessor;
-import org.elasticsearch.ingest.processor.DateProcessor;
-import org.elasticsearch.ingest.processor.DeDotProcessor;
-import org.elasticsearch.ingest.processor.FailProcessor;
-import org.elasticsearch.ingest.processor.GsubProcessor;
-import org.elasticsearch.ingest.processor.JoinProcessor;
-import org.elasticsearch.ingest.processor.LowercaseProcessor;
-import org.elasticsearch.ingest.processor.RemoveProcessor;
-import org.elasticsearch.ingest.processor.RenameProcessor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.elasticsearch.ingest.processor.SplitProcessor;
-import org.elasticsearch.ingest.processor.TrimProcessor;
-import org.elasticsearch.ingest.processor.UppercaseProcessor;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.node.service.NodeService;
 
-import java.util.function.Function;
-
 /**
  *
  */
@@ -51,7 +32,6 @@ public class NodeModule extends AbstractModule {
 
     private final Node node;
     private final MonitorService monitorService;
-    private final ProcessorsRegistry processorsRegistry;
 
     // pkg private so tests can mock
     Class<? extends PageCacheRecycler> pageCacheRecyclerImpl = PageCacheRecycler.class;
@@ -60,22 +40,6 @@ public class NodeModule extends AbstractModule {
     public NodeModule(Node node, MonitorService monitorService) {
         this.node = node;
         this.monitorService = monitorService;
-        this.processorsRegistry = new ProcessorsRegistry();
-
-        registerProcessor(DateProcessor.TYPE, (templateService) -> new DateProcessor.Factory());
-        registerProcessor(SetProcessor.TYPE, SetProcessor.Factory::new);
-        registerProcessor(AppendProcessor.TYPE, AppendProcessor.Factory::new);
-        registerProcessor(RenameProcessor.TYPE, (templateService) -> new RenameProcessor.Factory());
-        registerProcessor(RemoveProcessor.TYPE, RemoveProcessor.Factory::new);
-        registerProcessor(SplitProcessor.TYPE, (templateService) -> new SplitProcessor.Factory());
-        registerProcessor(JoinProcessor.TYPE, (templateService) -> new JoinProcessor.Factory());
-        registerProcessor(UppercaseProcessor.TYPE, (templateService) -> new UppercaseProcessor.Factory());
-        registerProcessor(LowercaseProcessor.TYPE, (templateService) -> new LowercaseProcessor.Factory());
-        registerProcessor(TrimProcessor.TYPE, (templateService) -> new TrimProcessor.Factory());
-        registerProcessor(ConvertProcessor.TYPE, (templateService) -> new ConvertProcessor.Factory());
-        registerProcessor(GsubProcessor.TYPE, (templateService) -> new GsubProcessor.Factory());
-        registerProcessor(FailProcessor.TYPE, FailProcessor.Factory::new);
-        registerProcessor(DeDotProcessor.TYPE, (templateService) -> new DeDotProcessor.Factory());
     }
 
     @Override
@@ -94,20 +58,5 @@ public class NodeModule extends AbstractModule {
         bind(Node.class).toInstance(node);
         bind(MonitorService.class).toInstance(monitorService);
         bind(NodeService.class).asEagerSingleton();
-        bind(ProcessorsRegistry.class).toInstance(processorsRegistry);
-    }
-
-    /**
-     * Returns the node
-     */
-    public Node getNode() {
-        return node;
-    }
-
-    /**
-     * Adds a processor factory under a specific type name.
-     */
-    public void registerProcessor(String type, Function<TemplateService, Processor.Factory<?>> processorFactoryProvider) {
-        processorsRegistry.registerProcessor(type, processorFactoryProvider);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 1c2ab33..df4e09d 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -21,7 +21,6 @@ package org.elasticsearch.node.internal;
 
 import org.elasticsearch.bootstrap.BootstrapInfo;
 import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
@@ -108,8 +107,7 @@ public class InternalSettingsPreparer {
         environment = new Environment(output.build());
 
         // we put back the path.logs so we can use it in the logging configuration file
-        output.put("path.logs", cleanPath(environment.logsFile().toAbsolutePath().toString()));
-
+        output.put(Environment.PATH_LOGS_SETTING.getKey(), cleanPath(environment.logsFile().toAbsolutePath().toString()));
         return new Environment(output.build());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/node/service/NodeService.java b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
index 7c385b5..b4fe59e 100644
--- a/core/src/main/java/org/elasticsearch/node/service/NodeService.java
+++ b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
@@ -24,25 +24,20 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
-import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.env.Environment;
 import org.elasticsearch.http.HttpServer;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.ingest.IngestService;
-import org.elasticsearch.ingest.ProcessorsRegistry;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.plugins.PluginsService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.io.Closeable;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
@@ -52,7 +47,7 @@ import static java.util.Collections.unmodifiableMap;
 
 /**
  */
-public class NodeService extends AbstractComponent implements Closeable {
+public class NodeService extends AbstractComponent {
 
     private final ThreadPool threadPool;
     private final MonitorService monitorService;
@@ -60,7 +55,6 @@ public class NodeService extends AbstractComponent implements Closeable {
     private final IndicesService indicesService;
     private final PluginsService pluginService;
     private final CircuitBreakerService circuitBreakerService;
-    private final IngestService ingestService;
     private ScriptService scriptService;
 
     @Nullable
@@ -73,10 +67,10 @@ public class NodeService extends AbstractComponent implements Closeable {
     private final Discovery discovery;
 
     @Inject
-    public NodeService(Settings settings, Environment environment, ThreadPool threadPool, MonitorService monitorService,
-                       Discovery discovery, TransportService transportService, IndicesService indicesService,
-                       PluginsService pluginService, CircuitBreakerService circuitBreakerService, Version version,
-                       ProcessorsRegistry processorsRegistry, ClusterService clusterService) {
+    public NodeService(Settings settings, ThreadPool threadPool, MonitorService monitorService, Discovery discovery,
+                       TransportService transportService, IndicesService indicesService,
+                       PluginsService pluginService, CircuitBreakerService circuitBreakerService,
+                       Version version) {
         super(settings);
         this.threadPool = threadPool;
         this.monitorService = monitorService;
@@ -87,15 +81,12 @@ public class NodeService extends AbstractComponent implements Closeable {
         this.version = version;
         this.pluginService = pluginService;
         this.circuitBreakerService = circuitBreakerService;
-        this.ingestService = new IngestService(settings, threadPool, processorsRegistry);
-        clusterService.add(ingestService.getPipelineStore());
     }
 
     // can not use constructor injection or there will be a circular dependency
     @Inject(optional = true)
     public void setScriptService(ScriptService scriptService) {
         this.scriptService = scriptService;
-        this.ingestService.setScriptService(scriptService);
     }
 
     public void setHttpServer(@Nullable HttpServer httpServer) {
@@ -185,13 +176,4 @@ public class NodeService extends AbstractComponent implements Closeable {
                 discoveryStats ? discovery.stats() : null
         );
     }
-
-    public IngestService getIngestService() {
-        return ingestService;
-    }
-
-    @Override
-    public void close() throws IOException {
-        indicesService.close();
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/DummyPluginInfo.java b/core/src/main/java/org/elasticsearch/plugins/DummyPluginInfo.java
index a57a96c..a7d088c 100644
--- a/core/src/main/java/org/elasticsearch/plugins/DummyPluginInfo.java
+++ b/core/src/main/java/org/elasticsearch/plugins/DummyPluginInfo.java
@@ -20,9 +20,9 @@ package org.elasticsearch.plugins;
 
 public class DummyPluginInfo extends PluginInfo {
 
-    private DummyPluginInfo(String name, String description, boolean site, String version, boolean jvm, String classname, boolean isolated) {
-        super(name, description, site, version, jvm, classname, isolated);
+    private DummyPluginInfo(String name, String description, String version, String classname, boolean isolated) {
+        super(name, description, version, classname, isolated);
     }
 
-    public static final DummyPluginInfo INSTANCE = new DummyPluginInfo("dummy_plugin_name", "dummy plugin description", true, "dummy_plugin_version", true, "DummyPluginName", true);
+    public static final DummyPluginInfo INSTANCE = new DummyPluginInfo("dummy_plugin_name", "dummy plugin description", "dummy_plugin_version", "DummyPluginName", true);
 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java b/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
index 3062f01..76af783 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginInfo.java
@@ -42,19 +42,14 @@ public class PluginInfo implements Streamable, ToXContent {
         static final XContentBuilderString NAME = new XContentBuilderString("name");
         static final XContentBuilderString DESCRIPTION = new XContentBuilderString("description");
         static final XContentBuilderString URL = new XContentBuilderString("url");
-        static final XContentBuilderString SITE = new XContentBuilderString("site");
         static final XContentBuilderString VERSION = new XContentBuilderString("version");
-        static final XContentBuilderString JVM = new XContentBuilderString("jvm");
         static final XContentBuilderString CLASSNAME = new XContentBuilderString("classname");
         static final XContentBuilderString ISOLATED = new XContentBuilderString("isolated");
     }
 
     private String name;
     private String description;
-    private boolean site;
     private String version;
-
-    private boolean jvm;
     private String classname;
     private boolean isolated;
 
@@ -66,15 +61,11 @@ public class PluginInfo implements Streamable, ToXContent {
      *
      * @param name        Its name
      * @param description Its description
-     * @param site        true if it's a site plugin
-     * @param jvm         true if it's a jvm plugin
      * @param version     Version number
      */
-    PluginInfo(String name, String description, boolean site, String version, boolean jvm, String classname, boolean isolated) {
+    PluginInfo(String name, String description, String version, String classname, boolean isolated) {
         this.name = name;
         this.description = description;
-        this.site = site;
-        this.jvm = jvm;
         this.version = version;
         this.classname = classname;
         this.isolated = isolated;
@@ -101,43 +92,28 @@ public class PluginInfo implements Streamable, ToXContent {
             throw new IllegalArgumentException("Property [version] is missing for plugin [" + name + "]");
         }
 
-        boolean jvm = Boolean.parseBoolean(props.getProperty("jvm"));
-        boolean site = Boolean.parseBoolean(props.getProperty("site"));
-        if (jvm == false && site == false) {
-            throw new IllegalArgumentException("Plugin [" + name + "] must be at least a jvm or site plugin");
+        String esVersionString = props.getProperty("elasticsearch.version");
+        if (esVersionString == null) {
+            throw new IllegalArgumentException("Property [elasticsearch.version] is missing for plugin [" + name + "]");
         }
-        boolean isolated = true;
-        String classname = "NA";
-        if (jvm) {
-            String esVersionString = props.getProperty("elasticsearch.version");
-            if (esVersionString == null) {
-                throw new IllegalArgumentException("Property [elasticsearch.version] is missing for jvm plugin [" + name + "]");
-            }
-            Version esVersion = Version.fromString(esVersionString);
-            if (esVersion.equals(Version.CURRENT) == false) {
-                throw new IllegalArgumentException("Plugin [" + name + "] is incompatible with Elasticsearch [" + Version.CURRENT.toString() +
-                        "]. Was designed for version [" + esVersionString + "]");
-            }
-            String javaVersionString = props.getProperty("java.version");
-            if (javaVersionString == null) {
-                throw new IllegalArgumentException("Property [java.version] is missing for jvm plugin [" + name + "]");
-            }
-            JarHell.checkVersionFormat(javaVersionString);
-            JarHell.checkJavaVersion(name, javaVersionString);
-            isolated = Boolean.parseBoolean(props.getProperty("isolated", "true"));
-            classname = props.getProperty("classname");
-            if (classname == null) {
-                throw new IllegalArgumentException("Property [classname] is missing for jvm plugin [" + name + "]");
-            }
+        Version esVersion = Version.fromString(esVersionString);
+        if (esVersion.equals(Version.CURRENT) == false) {
+            throw new IllegalArgumentException("Plugin [" + name + "] is incompatible with Elasticsearch [" + Version.CURRENT.toString() +
+                    "]. Was designed for version [" + esVersionString + "]");
         }
-
-        if (site) {
-            if (!Files.exists(dir.resolve("_site"))) {
-                throw new IllegalArgumentException("Plugin [" + name + "] is a site plugin but has no '_site/' directory");
-            }
+        String javaVersionString = props.getProperty("java.version");
+        if (javaVersionString == null) {
+            throw new IllegalArgumentException("Property [java.version] is missing for plugin [" + name + "]");
+        }
+        JarHell.checkVersionFormat(javaVersionString);
+        JarHell.checkJavaVersion(name, javaVersionString);
+        boolean isolated = Boolean.parseBoolean(props.getProperty("isolated", "true"));
+        String classname = props.getProperty("classname");
+        if (classname == null) {
+            throw new IllegalArgumentException("Property [classname] is missing for plugin [" + name + "]");
         }
 
-        return new PluginInfo(name, description, site, version, jvm, classname, isolated);
+        return new PluginInfo(name, description, version, classname, isolated);
     }
 
     /**
@@ -155,47 +131,20 @@ public class PluginInfo implements Streamable, ToXContent {
     }
 
     /**
-     * @return true if it's a site plugin
-     */
-    public boolean isSite() {
-        return site;
-    }
-
-    /**
-     * @return true if it's a plugin running in the jvm
-     */
-    public boolean isJvm() {
-        return jvm;
-    }
-
-    /**
-     * @return true if jvm plugin has isolated classloader
+     * @return true if plugin has isolated classloader
      */
     public boolean isIsolated() {
         return isolated;
     }
 
     /**
-     * @return jvm plugin's classname
+     * @return plugin's classname
      */
     public String getClassname() {
         return classname;
     }
 
     /**
-     * We compute the URL for sites: "/_plugin/" + name + "/"
-     *
-     * @return relative URL for site plugin
-     */
-    public String getUrl() {
-        if (site) {
-            return ("/_plugin/" + name + "/");
-        } else {
-            return null;
-        }
-    }
-
-    /**
      * @return Version number for the plugin
      */
     public String getVersion() {
@@ -212,8 +161,6 @@ public class PluginInfo implements Streamable, ToXContent {
     public void readFrom(StreamInput in) throws IOException {
         this.name = in.readString();
         this.description = in.readString();
-        this.site = in.readBoolean();
-        this.jvm = in.readBoolean();
         this.version = in.readString();
         this.classname = in.readString();
         this.isolated = in.readBoolean();
@@ -223,8 +170,6 @@ public class PluginInfo implements Streamable, ToXContent {
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(name);
         out.writeString(description);
-        out.writeBoolean(site);
-        out.writeBoolean(jvm);
         out.writeString(version);
         out.writeString(classname);
         out.writeBoolean(isolated);
@@ -236,15 +181,8 @@ public class PluginInfo implements Streamable, ToXContent {
         builder.field(Fields.NAME, name);
         builder.field(Fields.VERSION, version);
         builder.field(Fields.DESCRIPTION, description);
-        if (site) {
-            builder.field(Fields.URL, getUrl());
-        }
-        builder.field(Fields.JVM, jvm);
-        if (jvm) {
-            builder.field(Fields.CLASSNAME, classname);
-            builder.field(Fields.ISOLATED, isolated);
-        }
-        builder.field(Fields.SITE, site);
+        builder.field(Fields.CLASSNAME, classname);
+        builder.field(Fields.ISOLATED, isolated);
         builder.endObject();
 
         return builder;
@@ -274,14 +212,9 @@ public class PluginInfo implements Streamable, ToXContent {
                 .append("- Plugin information:\n")
                 .append("Name: ").append(name).append("\n")
                 .append("Description: ").append(description).append("\n")
-                .append("Site: ").append(site).append("\n")
                 .append("Version: ").append(version).append("\n")
-                .append("JVM: ").append(jvm).append("\n");
-
-        if (jvm) {
-            information.append(" * Classname: ").append(classname).append("\n");
-            information.append(" * Isolated: ").append(isolated);
-        }
+                .append(" * Classname: ").append(classname).append("\n")
+                .append(" * Isolated: ").append(isolated);
 
         return information.toString();
     }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 56da717..8e6391e 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -101,7 +101,6 @@ public class PluginManager {
             "discovery-ec2",
             "discovery-gce",
             "discovery-multicast",
-            "ingest-geoip",
             "lang-javascript",
             "lang-plan-a",
             "lang-python",
@@ -259,9 +258,7 @@ public class PluginManager {
         }
 
         // check for jar hell before any copying
-        if (info.isJvm()) {
-            jarHellCheck(root, info.isIsolated());
-        }
+        jarHellCheck(root, info.isIsolated());
 
         // read optional security policy (extra permissions)
         // if it exists, confirm or warn the user
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index 50938a1..4e61185 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -98,7 +98,7 @@ public class PluginsService extends AbstractComponent {
         // first we load plugins that are on the classpath. this is for tests and transport clients
         for (Class<? extends Plugin> pluginClass : classpathPlugins) {
             Plugin plugin = loadPlugin(pluginClass, settings);
-            PluginInfo pluginInfo = new PluginInfo(plugin.name(), plugin.description(), false, "NA", true, pluginClass.getName(), false);
+            PluginInfo pluginInfo = new PluginInfo(plugin.name(), plugin.description(), "NA", pluginClass.getName(), false);
             if (logger.isTraceEnabled()) {
                 logger.trace("plugin loaded from classpath [{}]", pluginInfo);
             }
@@ -136,18 +136,10 @@ public class PluginsService extends AbstractComponent {
 
         plugins = Collections.unmodifiableList(pluginsLoaded);
 
-        // We need to build a List of jvm and site plugins for checking mandatory plugins
-        Map<String, Plugin> jvmPlugins = new HashMap<>();
-        List<String> sitePlugins = new ArrayList<>();
-
+        // We need to build a List of plugins for checking mandatory plugins
+        Set<String> pluginsNames = new HashSet<>();
         for (Tuple<PluginInfo, Plugin> tuple : plugins) {
-            PluginInfo info = tuple.v1();
-            if (info.isJvm()) {
-                jvmPlugins.put(info.getName(), tuple.v2());
-            }
-            if (info.isSite()) {
-                sitePlugins.add(info.getName());
-            }
+            pluginsNames.add(tuple.v1().getName());
         }
 
         // Checking expected plugins
@@ -155,7 +147,7 @@ public class PluginsService extends AbstractComponent {
         if (mandatoryPlugins != null) {
             Set<String> missingPlugins = new HashSet<>();
             for (String mandatoryPlugin : mandatoryPlugins) {
-                if (!jvmPlugins.containsKey(mandatoryPlugin) && !sitePlugins.contains(mandatoryPlugin) && !missingPlugins.contains(mandatoryPlugin)) {
+                if (!pluginsNames.contains(mandatoryPlugin) && !missingPlugins.contains(mandatoryPlugin)) {
                     missingPlugins.add(mandatoryPlugin);
                 }
             }
@@ -175,10 +167,11 @@ public class PluginsService extends AbstractComponent {
             jvmPluginNames.add(pluginInfo.getName());
         }
 
-        logger.info("modules {}, plugins {}, sites {}", moduleNames, jvmPluginNames, sitePlugins);
+        logger.info("modules {}, plugins {}", moduleNames, jvmPluginNames);
 
         Map<Plugin, List<OnModuleReference>> onModuleReferences = new HashMap<>();
-        for (Plugin plugin : jvmPlugins.values()) {
+        for (Tuple<PluginInfo, Plugin> pluginEntry : plugins) {
+            Plugin plugin = pluginEntry.v2();
             List<OnModuleReference> list = new ArrayList<>();
             for (Method method : plugin.getClass().getMethods()) {
                 if (!method.getName().equals("onModule")) {
@@ -304,9 +297,6 @@ public class PluginsService extends AbstractComponent {
                     continue; // skip over .DS_Store etc
                 }
                 PluginInfo info = PluginInfo.readFromProperties(module);
-                if (!info.isJvm()) {
-                    throw new IllegalStateException("modules must be jvm plugins: " + info);
-                }
                 if (!info.isIsolated()) {
                     throw new IllegalStateException("modules must be isolated: " + info);
                 }
@@ -353,17 +343,14 @@ public class PluginsService extends AbstractComponent {
                 }
 
                 List<URL> urls = new ArrayList<>();
-                if (info.isJvm()) {
-                    // a jvm plugin: gather urls for jar files
-                    try (DirectoryStream<Path> jarStream = Files.newDirectoryStream(plugin, "*.jar")) {
-                        for (Path jar : jarStream) {
-                            // normalize with toRealPath to get symlinks out of our hair
-                            urls.add(jar.toRealPath().toUri().toURL());
-                        }
+                try (DirectoryStream<Path> jarStream = Files.newDirectoryStream(plugin, "*.jar")) {
+                    for (Path jar : jarStream) {
+                        // normalize with toRealPath to get symlinks out of our hair
+                        urls.add(jar.toRealPath().toUri().toURL());
                     }
                 }
                 final Bundle bundle;
-                if (info.isJvm() && info.isIsolated() == false) {
+                if (info.isIsolated() == false) {
                     bundle = bundles.get(0); // purgatory
                 } else {
                     bundle = new Bundle();
@@ -395,15 +382,10 @@ public class PluginsService extends AbstractComponent {
             // create a child to load the plugins in this bundle
             ClassLoader loader = URLClassLoader.newInstance(bundle.urls.toArray(new URL[0]), getClass().getClassLoader());
             for (PluginInfo pluginInfo : bundle.plugins) {
-                final Plugin plugin;
-                if (pluginInfo.isJvm()) {
-                    // reload lucene SPI with any new services from the plugin
-                    reloadLuceneSPI(loader);
-                    Class<? extends Plugin> pluginClass = loadPluginClass(pluginInfo.getClassname(), loader);
-                    plugin = loadPlugin(pluginClass, settings);
-                } else {
-                    plugin = new SitePlugin(pluginInfo.getName(), pluginInfo.getDescription());
-                }
+                // reload lucene SPI with any new services from the plugin
+                reloadLuceneSPI(loader);
+                final Class<? extends Plugin> pluginClass = loadPluginClass(pluginInfo.getClassname(), loader);
+                final Plugin plugin = loadPlugin(pluginClass, settings);
                 plugins.add(new Tuple<>(pluginInfo, plugin));
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/plugins/SitePlugin.java b/core/src/main/java/org/elasticsearch/plugins/SitePlugin.java
deleted file mode 100644
index 4c12f20..0000000
--- a/core/src/main/java/org/elasticsearch/plugins/SitePlugin.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.plugins;
-
-/** A site-only plugin, just serves resources */
-final class SitePlugin extends Plugin {
-    final String name;
-    final String description;
-    
-    SitePlugin(String name, String description) {
-        this.name = name;
-        this.description = description;
-    }
-
-    @Override
-    public String name() {
-        return name;
-    }
-
-    @Override
-    public String description() {
-        return description;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java b/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
index 33f9d4e..0aa6222 100644
--- a/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/fs/FsRepository.java
@@ -23,6 +23,7 @@ import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStore;
 import org.elasticsearch.common.blobstore.fs.FsBlobStore;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
@@ -33,6 +34,7 @@ import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
 
 import java.io.IOException;
 import java.nio.file.Path;
+import java.util.function.Function;
 
 /**
  * Shared file system implementation of the BlobStoreRepository
@@ -49,6 +51,13 @@ public class FsRepository extends BlobStoreRepository {
 
     public final static String TYPE = "fs";
 
+    public static final Setting<String> LOCATION_SETTING = new Setting<>("location", "", Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<String> REPOSITORIES_LOCATION_SETTING = new Setting<>("repositories.fs.location", LOCATION_SETTING, Function.identity(), false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> CHUNK_SIZE_SETTING = Setting.byteSizeSetting("chunk_size", "-1", false, Setting.Scope.CLUSTER);
+    public static final Setting<ByteSizeValue> REPOSITORIES_CHUNK_SIZE_SETTING = Setting.byteSizeSetting("repositories.fs.chunk_size", "-1", false, Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> COMPRESS_SETTING = Setting.boolSetting("compress", false, false, Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> REPOSITORIES_COMPRESS_SETTING = Setting.boolSetting("repositories.fs.compress", false, false, Setting.Scope.CLUSTER);
+
     private final FsBlobStore blobStore;
 
     private ByteSizeValue chunkSize;
@@ -68,8 +77,8 @@ public class FsRepository extends BlobStoreRepository {
     public FsRepository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, Environment environment) throws IOException {
         super(name.getName(), repositorySettings, indexShardRepository);
         Path locationFile;
-        String location = repositorySettings.settings().get("location", settings.get("repositories.fs.location"));
-        if (location == null) {
+        String location = REPOSITORIES_LOCATION_SETTING.get(repositorySettings.settings());
+        if (location.isEmpty()) {
             logger.warn("the repository location is missing, it should point to a shared file system location that is available on all master and data nodes");
             throw new RepositoryException(name.name(), "missing location");
         }
@@ -85,8 +94,14 @@ public class FsRepository extends BlobStoreRepository {
         }
 
         blobStore = new FsBlobStore(settings, locationFile);
-        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize("repositories.fs.chunk_size", null));
-        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean("repositories.fs.compress", false));
+        if (CHUNK_SIZE_SETTING.exists(repositorySettings.settings())) {
+            this.chunkSize = CHUNK_SIZE_SETTING.get(repositorySettings.settings());
+        } else if (REPOSITORIES_CHUNK_SIZE_SETTING.exists(settings)) {
+            this.chunkSize = REPOSITORIES_CHUNK_SIZE_SETTING.get(settings);
+        } else {
+            this.chunkSize = null;
+        }
+        this.compress = COMPRESS_SETTING.exists(repositorySettings.settings()) ? COMPRESS_SETTING.get(repositorySettings.settings()) : REPOSITORIES_COMPRESS_SETTING.get(settings);
         this.basePath = BlobPath.cleanPath();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
index 4d36168..2d15db2 100644
--- a/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
+++ b/core/src/main/java/org/elasticsearch/repositories/uri/URLRepository.java
@@ -20,11 +20,11 @@
 package org.elasticsearch.repositories.uri;
 
 import org.elasticsearch.cluster.metadata.SnapshotId;
-import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.BlobStore;
 import org.elasticsearch.common.blobstore.url.URLBlobStore;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.util.URIPattern;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.snapshots.IndexShardRepository;
@@ -34,9 +34,13 @@ import org.elasticsearch.repositories.RepositorySettings;
 import org.elasticsearch.repositories.blobstore.BlobStoreRepository;
 
 import java.io.IOException;
+import java.net.MalformedURLException;
 import java.net.URISyntaxException;
 import java.net.URL;
+import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
+import java.util.function.Function;
 
 /**
  * Read-only URL-based implementation of the BlobStoreRepository
@@ -51,13 +55,21 @@ public class URLRepository extends BlobStoreRepository {
 
     public final static String TYPE = "url";
 
-    public final static String[] DEFAULT_SUPPORTED_PROTOCOLS = {"http", "https", "ftp", "file", "jar"};
+    public static final Setting<List<String>> SUPPORTED_PROTOCOLS_SETTING = Setting.listSetting("repositories.url.supported_protocols",
+            Arrays.asList("http", "https", "ftp", "file", "jar"), Function.identity(), false, Setting.Scope.CLUSTER);
 
-    public final static String SUPPORTED_PROTOCOLS_SETTING = "repositories.url.supported_protocols";
+    public static final Setting<List<URIPattern>> ALLOWED_URLS_SETTING = Setting.listSetting("repositories.url.allowed_urls",
+            Collections.emptyList(), URIPattern::new, false, Setting.Scope.CLUSTER);
 
-    public final static String ALLOWED_URLS_SETTING = "repositories.url.allowed_urls";
+    public static final Setting<URL> URL_SETTING = new Setting<>("url", "http:", URLRepository::parseURL, false, Setting.Scope.CLUSTER);
+    public static final Setting<URL> REPOSITORIES_URL_SETTING = new Setting<>("repositories.url.url", (s) -> s.get("repositories.uri.url", "http:"),
+            URLRepository::parseURL, false, Setting.Scope.CLUSTER);
 
-    private final String[] supportedProtocols;
+    public static final Setting<Boolean> LIST_DIRECTORIES_SETTING = Setting.boolSetting("list_directories", true, false, Setting.Scope.CLUSTER);
+    public static final Setting<Boolean> REPOSITORIES_LIST_DIRECTORIES_SETTING = Setting.boolSetting("repositories.uri.list_directories", true,
+            false, Setting.Scope.CLUSTER);
+
+    private final List<String> supportedProtocols;
 
     private final URIPattern[] urlWhiteList;
 
@@ -79,21 +91,16 @@ public class URLRepository extends BlobStoreRepository {
     @Inject
     public URLRepository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, Environment environment) throws IOException {
         super(name.getName(), repositorySettings, indexShardRepository);
-        URL url;
-        String path = repositorySettings.settings().get("url", settings.get("repositories.url.url", settings.get("repositories.uri.url")));
-        if (path == null) {
+
+        if (URL_SETTING.exists(repositorySettings.settings()) == false && REPOSITORIES_URL_SETTING.exists(settings) ==  false) {
             throw new RepositoryException(name.name(), "missing url");
-        } else {
-            url = new URL(path);
-        }
-        supportedProtocols = settings.getAsArray(SUPPORTED_PROTOCOLS_SETTING, DEFAULT_SUPPORTED_PROTOCOLS);
-        String[] urlWhiteList = settings.getAsArray(ALLOWED_URLS_SETTING, Strings.EMPTY_ARRAY);
-        this.urlWhiteList = new URIPattern[urlWhiteList.length];
-        for (int i = 0; i < urlWhiteList.length; i++) {
-            this.urlWhiteList[i] = new URIPattern(urlWhiteList[i]);
         }
+        supportedProtocols = SUPPORTED_PROTOCOLS_SETTING.get(settings);
+        urlWhiteList = ALLOWED_URLS_SETTING.get(settings).toArray(new URIPattern[]{});
         this.environment = environment;
-        listDirectories = repositorySettings.settings().getAsBoolean("list_directories", settings.getAsBoolean("repositories.uri.list_directories", true));
+        listDirectories = LIST_DIRECTORIES_SETTING.exists(repositorySettings.settings()) ? LIST_DIRECTORIES_SETTING.get(repositorySettings.settings()) : REPOSITORIES_LIST_DIRECTORIES_SETTING.get(settings);
+
+        URL url = URL_SETTING.exists(repositorySettings.settings()) ? URL_SETTING.get(repositorySettings.settings()) : REPOSITORIES_URL_SETTING.get(settings);
         URL normalizedURL = checkURL(url);
         blobStore = new URLBlobStore(settings, normalizedURL);
         basePath = BlobPath.cleanPath();
@@ -147,8 +154,8 @@ public class URLRepository extends BlobStoreRepository {
                 // We didn't match white list - try to resolve against path.repo
                 URL normalizedUrl = environment.resolveRepoURL(url);
                 if (normalizedUrl == null) {
-                    logger.warn("The specified url [{}] doesn't start with any repository paths specified by the path.repo setting: [{}] or by repositories.url.allowed_urls setting: [{}] ", url, environment.repoFiles());
-                    throw new RepositoryException(repositoryName, "file url [" + url + "] doesn't match any of the locations specified by path.repo or repositories.url.allowed_urls");
+                    logger.warn("The specified url [{}] doesn't start with any repository paths specified by the path.repo setting or by {} setting: [{}] ", url, ALLOWED_URLS_SETTING.getKey(), environment.repoFiles());
+                    throw new RepositoryException(repositoryName, "file url [" + url + "] doesn't match any of the locations specified by path.repo or " + ALLOWED_URLS_SETTING.getKey());
                 }
                 return normalizedUrl;
             }
@@ -161,4 +168,11 @@ public class URLRepository extends BlobStoreRepository {
         return true;
     }
 
+    private static URL parseURL(String s) {
+        try {
+            return new URL(s);
+        } catch (MalformedURLException e) {
+            throw new IllegalArgumentException("Unable to parse URL repository setting", e);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
index df20438..37ce03b 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
@@ -77,7 +77,6 @@ public class RestBulkAction extends BaseRestHandler {
         String defaultType = request.param("type");
         String defaultRouting = request.param("routing");
         String fieldsParam = request.param("fields");
-        String defaultPipeline = request.param("pipeline");
         String[] defaultFields = fieldsParam != null ? Strings.commaDelimitedListToStringArray(fieldsParam) : null;
 
         String consistencyLevel = request.param("consistency");
@@ -86,7 +85,7 @@ public class RestBulkAction extends BaseRestHandler {
         }
         bulkRequest.timeout(request.paramAsTime("timeout", BulkShardRequest.DEFAULT_TIMEOUT));
         bulkRequest.refresh(request.paramAsBoolean("refresh", bulkRequest.refresh()));
-        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, defaultPipeline, null, allowExplicitIndex);
+        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, null, allowExplicitIndex);
 
         client.bulk(bulkRequest, new RestBuilderListener<BulkResponse>(channel) {
             @Override
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java
index 34e0522..1a37ab6 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java
@@ -84,8 +84,6 @@ public class RestPluginsAction extends AbstractCatAction {
         table.addCell("name", "alias:n;desc:node name");
         table.addCell("component", "alias:c;desc:component");
         table.addCell("version", "alias:v;desc:component version");
-        table.addCell("type", "alias:t;desc:type (j for JVM, s for Site)");
-        table.addCell("url", "alias:u;desc:url for site plugins");
         table.addCell("description", "alias:d;default:false;desc:plugin details");
         table.endHeaders();
         return table;
@@ -104,22 +102,6 @@ public class RestPluginsAction extends AbstractCatAction {
                 table.addCell(node.name());
                 table.addCell(pluginInfo.getName());
                 table.addCell(pluginInfo.getVersion());
-                String type;
-                if (pluginInfo.isSite()) {
-                    if (pluginInfo.isJvm()) {
-                        type = "j/s";
-                    } else {
-                        type = "s";
-                    }
-                } else {
-                    if (pluginInfo.isJvm()) {
-                        type = "j";
-                    } else {
-                        type = "";
-                    }
-                }
-                table.addCell(type);
-                table.addCell(pluginInfo.getUrl());
                 table.addCell(pluginInfo.getDescription());
                 table.endRow();
             }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
index 0fc1545..13a9329 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
@@ -77,7 +77,6 @@ public class RestIndexAction extends BaseRestHandler {
         if (request.hasParam("ttl")) {
             indexRequest.ttl(request.param("ttl"));
         }
-        indexRequest.setPipeline(request.param("pipeline"));
         indexRequest.source(request.content());
         indexRequest.timeout(request.paramAsTime("timeout", IndexRequest.DEFAULT_TIMEOUT));
         indexRequest.refresh(request.paramAsBoolean("refresh", indexRequest.refresh()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
deleted file mode 100644
index c4526d4..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-
-public class RestDeletePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestDeletePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.DELETE, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        DeletePipelineRequest request = new DeletePipelineRequest(restRequest.param("id"));
-        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
-        request.timeout(restRequest.paramAsTime("timeout", request.timeout()));
-        client.deletePipeline(request, new AcknowledgedRestListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
deleted file mode 100644
index b483a84..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.action.ingest.GetPipelineRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-
-public class RestGetPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestGetPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        GetPipelineRequest request = new GetPipelineRequest(Strings.splitStringByCommaToArray(restRequest.param("id")));
-        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
-        client.getPipeline(request, new RestStatusToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
deleted file mode 100644
index 5cdd9a8..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-import org.elasticsearch.rest.action.support.RestActions;
-
-public class RestPutPipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestPutPipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.PUT, "/_ingest/pipeline/{id}", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        PutPipelineRequest request = new PutPipelineRequest(restRequest.param("id"), RestActions.getRestContent(restRequest));
-        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
-        request.timeout(restRequest.paramAsTime("timeout", request.timeout()));
-        client.putPipeline(request, new AcknowledgedRestListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
deleted file mode 100644
index 35cf437..0000000
--- a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.rest.action.ingest;
-
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.action.support.RestActions;
-import org.elasticsearch.rest.action.support.RestToXContentListener;
-
-public class RestSimulatePipelineAction extends BaseRestHandler {
-
-    @Inject
-    public RestSimulatePipelineAction(Settings settings, RestController controller, Client client) {
-        super(settings, controller, client);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}/_simulate", this);
-        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/_simulate", this);
-        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/_simulate", this);
-    }
-
-    @Override
-    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
-        SimulatePipelineRequest request = new SimulatePipelineRequest(RestActions.getRestContent(restRequest));
-        request.setId(restRequest.param("id"));
-        request.setVerbose(restRequest.paramAsBoolean("verbose", false));
-        client.simulatePipeline(request, new RestToXContentListener<>(channel));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContext.java b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
index 3ab2bb5..4b1b6de 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContext.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
@@ -37,7 +37,7 @@ public interface ScriptContext {
      */
     enum Standard implements ScriptContext {
 
-        AGGS("aggs"), SEARCH("search"), UPDATE("update"), INGEST("ingest");
+        AGGS("aggs"), SEARCH("search"), UPDATE("update");
 
         private final String key;
 
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptService.java b/core/src/main/java/org/elasticsearch/script/ScriptService.java
index c9e9f9a..9883d62 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptService.java
@@ -46,6 +46,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
@@ -84,8 +85,7 @@ public class ScriptService extends AbstractComponent implements Closeable {
     static final String DISABLE_DYNAMIC_SCRIPTING_SETTING = "script.disable_dynamic";
 
     public static final String DEFAULT_SCRIPTING_LANGUAGE_SETTING = "script.default_lang";
-    public static final String SCRIPT_CACHE_SIZE_SETTING = "script.cache.max_size";
-    public static final int SCRIPT_CACHE_SIZE_DEFAULT = 100;
+    public static final Setting<Integer> SCRIPT_CACHE_SIZE_SETTING = Setting.intSetting("script.cache.max_size", 100, 0, false, Setting.Scope.CLUSTER);
     public static final String SCRIPT_CACHE_EXPIRE_SETTING = "script.cache.expire";
     public static final String SCRIPT_INDEX = ".scripts";
     public static final String DEFAULT_LANG = "groovy";
@@ -148,7 +148,7 @@ public class ScriptService extends AbstractComponent implements Closeable {
 
         this.scriptEngines = scriptEngines;
         this.scriptContextRegistry = scriptContextRegistry;
-        int cacheMaxSize = settings.getAsInt(SCRIPT_CACHE_SIZE_SETTING, SCRIPT_CACHE_SIZE_DEFAULT);
+        int cacheMaxSize = SCRIPT_CACHE_SIZE_SETTING.get(settings);
         TimeValue cacheExpire = settings.getAsTime(SCRIPT_CACHE_EXPIRE_SETTING, null);
         logger.debug("using script cache with max_size [{}], expire [{}]", cacheMaxSize, cacheExpire);
 
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index 6a84bb4..9a569ce 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -122,8 +122,9 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
 public class SearchService extends AbstractLifecycleComponent<SearchService> implements IndexEventListener {
 
     public static final Setting<Loading> INDEX_NORMS_LOADING_SETTING = new Setting<>("index.norms.loading", Loading.LAZY.toString(), (s) -> Loading.parse(s, Loading.LAZY), false, Setting.Scope.INDEX);
-    public static final String DEFAULT_KEEPALIVE_KEY = "search.default_keep_alive";
-    public static final String KEEPALIVE_INTERVAL_KEY = "search.keep_alive_interval";
+    // we can have 5 minutes here, since we make sure to clean with search requests and when shard/index closes
+    public static final Setting<TimeValue> DEFAULT_KEEPALIVE_SETTING = Setting.positiveTimeSetting("search.default_keep_alive", timeValueMinutes(5), false, Setting.Scope.CLUSTER);
+    public static final Setting<TimeValue> KEEPALIVE_INTERVAL_SETTING = Setting.positiveTimeSetting("search.keep_alive_interval", timeValueMinutes(1), false, Setting.Scope.CLUSTER);
 
     public static final TimeValue NO_TIMEOUT = timeValueMillis(-1);
     public static final Setting<TimeValue> DEFAULT_SEARCH_TIMEOUT_SETTING = Setting.timeSetting("search.default_search_timeout", NO_TIMEOUT, true, Setting.Scope.CLUSTER);
@@ -183,9 +184,8 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         this.fetchPhase = fetchPhase;
         this.indicesQueryCache = indicesQueryCache;
 
-        TimeValue keepAliveInterval = settings.getAsTime(KEEPALIVE_INTERVAL_KEY, timeValueMinutes(1));
-        // we can have 5 minutes here, since we make sure to clean with search requests and when shard/index closes
-        this.defaultKeepAlive = settings.getAsTime(DEFAULT_KEEPALIVE_KEY, timeValueMinutes(5)).millis();
+        TimeValue keepAliveInterval = KEEPALIVE_INTERVAL_SETTING.get(settings);
+        this.defaultKeepAlive = DEFAULT_KEEPALIVE_SETTING.get(settings).millis();
 
         Map<String, SearchParseElement> elementParsers = new HashMap<>();
         elementParsers.putAll(dfsPhase.parseElements());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index 60302f2..6473b5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -62,8 +62,7 @@ public class GeoHashGridParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context)
-                .build();
+        ValuesSourceParser vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context).build();
 
         int precision = GeoHashGridParams.DEFAULT_PRECISION;
         int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
@@ -132,7 +131,6 @@ public class GeoHashGridParser implements Aggregator.Parser {
             final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
                     Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
             return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
-                @Override
                 public InternalAggregation buildEmptyAggregation() {
                     return aggregation;
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index 52d77e1..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -28,7 +28,6 @@ import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
@@ -79,7 +78,7 @@ public class DateHistogramParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ValuesSourceParser<Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
                 .targetValueType(ValueType.DATE)
                 .formattable(true)
                 .timezoneAware(true)
@@ -191,7 +190,7 @@ public class DateHistogramParser implements Aggregator.Parser {
                 .timeZone(vsParser.input().timezone())
                 .offset(offset).build();
 
-        ValuesSourceConfig<Numeric> config = vsParser.config();
+        ValuesSourceConfig config = vsParser.config();
         return new HistogramAggregator.Factory(aggregationName, config, rounding, order, keyed, minDocCount, extendedBounds,
                 new InternalDateHistogram.Factory());
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
index 31ee668..c738251 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
 import org.elasticsearch.search.internal.SearchContext;
@@ -47,7 +46,7 @@ public class HistogramParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ValuesSourceParser<Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
                 .targetValueType(ValueType.NUMERIC)
                 .formattable(true)
                 .build();
@@ -128,7 +127,7 @@ public class HistogramParser implements Aggregator.Parser {
 
         Rounding rounding = new Rounding.Interval(interval);
         if (offset != 0) {
-            rounding = new Rounding.OffsetRounding(rounding, offset);
+            rounding = new Rounding.OffsetRounding((Rounding.Interval) rounding, offset);
         }
 
         if (extendedBounds != null) {
@@ -137,7 +136,7 @@ public class HistogramParser implements Aggregator.Parser {
         }
 
         return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds,
-                new InternalHistogram.Factory<>());
+                new InternalHistogram.Factory());
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
index 38e15e2..1ae7341 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
@@ -81,9 +81,9 @@ public class MissingAggregator extends SingleBucketAggregator {
         return new InternalMissing(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource>  {
 
-        public Factory(String name, ValuesSourceConfig<ValuesSource> valueSourceConfig) {
+        public Factory(String name, ValuesSourceConfig valueSourceConfig) {
             super(name, InternalMissing.TYPE.name(), valueSourceConfig);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
index 4210e02..6ecdc12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
@@ -22,7 +22,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -40,7 +39,8 @@ public class MissingParser implements Aggregator.Parser {
 
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
-        ValuesSourceParser<ValuesSource> vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
                 .scriptable(false)
                 .build();
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
index 4541aa9..8cb9809 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
@@ -203,8 +203,7 @@ public class SamplerAggregator extends SingleBucketAggregator {
         private int maxDocsPerValue;
         private String executionHint;
 
-        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig<ValuesSource> vsConfig,
-                int maxDocsPerValue) {
+        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig vsConfig, int maxDocsPerValue) {
             super(name, InternalSampler.TYPE.name(), vsConfig);
             this.shardSize = shardSize;
             this.maxDocsPerValue = maxDocsPerValue;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
index d51f436..498a7cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
@@ -23,7 +23,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
@@ -56,10 +55,10 @@ public class SamplerParser implements Aggregator.Parser {
         String executionHint = null;
         int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
         int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
+        ValuesSourceParser vsParser = null;
         boolean diversityChoiceMade = false;
 
-        ValuesSourceParser<ValuesSource> vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true)
-                .formattable(false).build();
+        vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true).formattable(false).build();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -89,7 +88,7 @@ public class SamplerParser implements Aggregator.Parser {
             }
         }
 
-        ValuesSourceConfig<ValuesSource> vsConfig = vsParser.config();
+        ValuesSourceConfig vsConfig = vsParser.config();
         if (vsConfig.valid()) {
             return new SamplerAggregator.DiversifiedFactory(aggregationName, shardSize, executionHint, vsConfig, maxDocsPerValue);
         } else {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
index 9b66fe0..399e857 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
@@ -20,6 +20,7 @@ package org.elasticsearch.search.aggregations.bucket.significant;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchException;
@@ -79,6 +80,8 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                               TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                     AggregationContext aggregationContext, Aggregator parent, SignificantTermsAggregatorFactory termsAggregatorFactory,
                     List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+                ValuesSource.Bytes.WithOrdinals valueSourceWithOrdinals = (ValuesSource.Bytes.WithOrdinals) valuesSource;
+                IndexSearcher indexSearcher = aggregationContext.searchContext().searcher();
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsSignificantTermsAggregator(name, factories,
                         (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext,
@@ -95,8 +98,9 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                     List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        termsAggregatorFactory, pipelineAggregators, metaData);
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter,
+ aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
             }
         };
 
@@ -139,7 +143,7 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
         return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
     }
 
-    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig<ValuesSource> valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
                                              String executionHint, Query filter, SignificanceHeuristic significanceHeuristic) {
 
         super(name, SignificantStringTerms.TYPE.name(), valueSourceConfig);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
index b4b89c2..28e0fb5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
@@ -28,7 +28,6 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.Signi
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -54,7 +53,7 @@ public class SignificantTermsParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         SignificantTermsParametersParser aggParser = new SignificantTermsParametersParser(significanceHeuristicParserMapper);
-        ValuesSourceParser<ValuesSource> vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
                 .scriptable(false)
                 .formattable(true)
                 .build();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
index ecd9d3b..891526c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
@@ -36,13 +36,13 @@ public abstract class AbstractTermsParametersParser {
     public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
     public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
     public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
-
+    
 
     //These are the results of the parsing.
     private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds();
 
     private String executionHint = null;
-
+    
     private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
 
 
@@ -59,12 +59,12 @@ public abstract class AbstractTermsParametersParser {
     public IncludeExclude getIncludeExclude() {
         return includeExclude;
     }
-
+    
     public SubAggCollectionMode getCollectionMode() {
         return collectMode;
     }
 
-    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser<?> vsParser, IncludeExclude.Parser incExcParser) throws IOException {
+    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser vsParser, IncludeExclude.Parser incExcParser) throws IOException {
         bucketCountThresholds = getDefaultBucketCountThresholds();
         XContentParser.Token token;
         String currentFieldName = null;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
index 04f7adf..270dc00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
@@ -165,7 +165,7 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
     private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
     private final boolean showTermDocCountError;
 
-    public TermsAggregatorFactory(String name, ValuesSourceConfig<ValuesSource> config, Terms.Order order,
+    public TermsAggregatorFactory(String name, ValuesSourceConfig config, Terms.Order order,
             TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude, String executionHint,
             SubAggCollectionMode executionMode, boolean showTermDocCountError) {
         super(name, StringTerms.TYPE.name(), config);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
index a7b60e9..478309d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsParametersParser.OrderElement;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -46,8 +45,7 @@ public class TermsParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         TermsParametersParser aggParser = new TermsParametersParser();
-        ValuesSourceParser<ValuesSource> vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true)
-                .formattable(true).build();
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true).formattable(true).build();
         IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
         aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
index 8193314..e675548 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregationBuilder.java
@@ -62,7 +62,6 @@ public abstract class ValuesSourceMetricsAggregationBuilder<B extends ValuesSour
     /**
      * Configure the value to use when documents miss a value.
      */
-    @SuppressWarnings("unchecked")
     public B missing(Object missingValue) {
         this.missing = missingValue;
         return (B) this;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 4df8dc7..1b2d5fc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -35,7 +35,7 @@ final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.L
 
     private final long precisionThreshold;
 
-    CardinalityAggregatorFactory(String name, ValuesSourceConfig<ValuesSource> config, long precisionThreshold) {
+    CardinalityAggregatorFactory(String name, ValuesSourceConfig config, long precisionThreshold) {
         super(name, InternalCardinality.TYPE.name(), config);
         this.precisionThreshold = precisionThreshold;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
index 3155232..6833945 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -44,7 +43,7 @@ public class CardinalityParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String name, XContentParser parser, SearchContext context) throws IOException {
 
-        ValuesSourceParser<ValuesSource> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
+        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
 
         long precisionThreshold = -1;
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
index 0a9ea4a..764f6ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
@@ -40,7 +40,7 @@ public class ValueCountParser implements Aggregator.Parser {
     @Override
     public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
                 .build();
 
         XContentParser.Token token;
@@ -54,6 +54,6 @@ public class ValueCountParser implements Aggregator.Parser {
             }
         }
 
-        return new ValueCountAggregator.Factory<>(aggregationName, vsParser.config());
+        return new ValueCountAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
index d9fe3ad..b03bc8d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java
@@ -53,9 +53,6 @@ import org.elasticsearch.search.aggregations.support.values.ScriptLongValues;
 
 import java.io.IOException;
 
-/**
- * How to load values for an aggregation.
- */
 public abstract class ValuesSource {
 
     /**
@@ -531,7 +528,6 @@ public abstract class ValuesSource {
                 return indexFieldData.load(context).getBytesValues();
             }
 
-            @Override
             public org.elasticsearch.index.fielddata.MultiGeoPointValues geoPointValues(LeafReaderContext context) {
                 return indexFieldData.load(context).getGeoPointValues();
             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
index 3f56162..d0eaec2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
@@ -78,20 +78,19 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource> ext
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException;
 
-    @SuppressWarnings("unchecked") // Safe because we check the types with isAssignableFrom
     private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) {
-        ValuesSourceConfig<?> config;
+        ValuesSourceConfig config;
         while (parent != null) {
             if (parent instanceof ValuesSourceAggregatorFactory) {
-                config = ((ValuesSourceAggregatorFactory<?>) parent).config;
+                config = ((ValuesSourceAggregatorFactory) parent).config;
                 if (config != null && config.valid()) {
                     if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) {
                         ValueFormat format = config.format;
-                        this.config = (ValuesSourceConfig<VS>) config;
+                        this.config = config;
                         // if the user explicitly defined a format pattern, we'll do our best to keep it even when we inherit the
                         // value source form one of the ancestor aggregations
                         if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) {
-                            this.config.format = ((ValueFormat.Patternable<?>) format).create(this.config.formatPattern);
+                            this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern);
                         }
                         return;
                     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
index 7c26061..fced5fd 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
@@ -48,16 +48,13 @@ import java.util.HashMap;
 import java.util.Map;
 
 /**
- * Parses a description of where to load the value sent by a user into a
- * ValuesSourceConfig which can be used to work with the values in various ways,
- * one of which is to create an actual ValueSource (done with the help of
- * AggregationContext).
+ *
  */
 public class ValuesSourceParser<VS extends ValuesSource> {
 
     static final ParseField TIME_ZONE = new ParseField("time_zone");
 
-    public static Builder<ValuesSource> any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
+    public static Builder any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
         return new Builder<>(aggName, aggType, context, ValuesSource.class);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/profile/CollectorResult.java b/core/src/main/java/org/elasticsearch/search/profile/CollectorResult.java
index d0c006e..8da14d2 100644
--- a/core/src/main/java/org/elasticsearch/search/profile/CollectorResult.java
+++ b/core/src/main/java/org/elasticsearch/search/profile/CollectorResult.java
@@ -36,7 +36,7 @@ import java.util.Locale;
  * Collectors used in the search.  Children CollectorResult's may be
  * embedded inside of a parent CollectorResult
  */
-public class CollectorResult implements ToXContent, Writeable<CollectorResult> {
+public class CollectorResult implements ToXContent, Writeable {
 
     public static final String REASON_SEARCH_COUNT = "search_count";
     public static final String REASON_SEARCH_TOP_HITS = "search_top_hits";
@@ -125,7 +125,7 @@ public class CollectorResult implements ToXContent, Writeable<CollectorResult> {
         builder = builder.startObject()
                 .field(NAME.getPreferredName(), getName())
                 .field(REASON.getPreferredName(), getReason())
-                .field(TIME.getPreferredName(), String.format(Locale.US, "%.10gms", getTime() / 1000000.0));
+                .field(TIME.getPreferredName(), String.format(Locale.US, "%.10gms", (double) (getTime() / 1000000.0)));
 
         if (!children.isEmpty()) {
             builder = builder.startArray(CHILDREN.getPreferredName());
@@ -150,7 +150,7 @@ public class CollectorResult implements ToXContent, Writeable<CollectorResult> {
     }
 
     @Override
-    public CollectorResult readFrom(StreamInput in) throws IOException {
+    public Object readFrom(StreamInput in) throws IOException {
         return new CollectorResult(in);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortOrder.java b/core/src/main/java/org/elasticsearch/search/sort/SortOrder.java
index cb2bca2..001924d 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortOrder.java
@@ -19,12 +19,19 @@
 
 package org.elasticsearch.search.sort;
 
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
+
+import java.io.IOException;
+import java.util.Locale;
+
 /**
  * A sorting order.
  *
  *
  */
-public enum SortOrder {
+public enum SortOrder implements Writeable<SortOrder> {
     /**
      * Ascending order.
      */
@@ -42,5 +49,30 @@ public enum SortOrder {
         public String toString() {
             return "desc";
         }
+    };
+    
+    public static final SortOrder DEFAULT = DESC;
+    private static final SortOrder PROTOTYPE = DEFAULT;
+
+    @Override
+    public SortOrder readFrom(StreamInput in) throws IOException {
+        int ordinal = in.readVInt();
+        if (ordinal < 0 || ordinal >= values().length) {
+            throw new IOException("Unknown SortOrder ordinal [" + ordinal + "]");
+        }
+        return values()[ordinal];
+    }
+
+    public static SortOrder readOrderFrom(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeVInt(this.ordinal());
+    }
+
+    public static SortOrder fromString(String op) {
+        return valueOf(op.toUpperCase(Locale.ROOT));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 6a6a6c3..8ea1709 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -117,13 +117,9 @@ import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
 import static java.util.Collections.unmodifiableMap;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_CLIENT;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_SERVER;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_DEFAULT_CONNECT_TIMEOUT;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_DEFAULT_RECEIVE_BUFFER_SIZE;
-import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_DEFAULT_SEND_BUFFER_SIZE;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_KEEP_ALIVE;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_NO_DELAY;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_RECEIVE_BUFFER_SIZE;
@@ -221,8 +217,8 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         }
 
         this.workerCount = settings.getAsInt(WORKER_COUNT, EsExecutors.boundedNumberOfProcessors(settings) * 2);
-        this.blockingClient = settings.getAsBoolean("transport.netty.transport.tcp.blocking_client", settings.getAsBoolean(TCP_BLOCKING_CLIENT, settings.getAsBoolean(TCP_BLOCKING, false)));
-        this.connectTimeout = this.settings.getAsTime("transport.netty.connect_timeout", settings.getAsTime("transport.tcp.connect_timeout", settings.getAsTime(TCP_CONNECT_TIMEOUT, TCP_DEFAULT_CONNECT_TIMEOUT)));
+        this.blockingClient = settings.getAsBoolean("transport.netty.transport.tcp.blocking_client", TCP_BLOCKING_CLIENT.get(settings));
+        this.connectTimeout = this.settings.getAsTime("transport.netty.connect_timeout", settings.getAsTime("transport.tcp.connect_timeout", TCP_CONNECT_TIMEOUT.get(settings)));
         this.maxCumulationBufferCapacity = this.settings.getAsBytesSize("transport.netty.max_cumulation_buffer_capacity", null);
         this.maxCompositeBufferComponents = this.settings.getAsInt("transport.netty.max_composite_buffer_components", -1);
         this.compress = Transport.TRANSPORT_TCP_COMPRESS.get(settings);
@@ -362,29 +358,25 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         clientBootstrap.setPipelineFactory(configureClientChannelPipelineFactory());
         clientBootstrap.setOption("connectTimeoutMillis", connectTimeout.millis());
 
-        String tcpNoDelay = settings.get("transport.netty.tcp_no_delay", settings.get(TCP_NO_DELAY, "true"));
-        if (!"default".equals(tcpNoDelay)) {
-            clientBootstrap.setOption("tcpNoDelay", Booleans.parseBoolean(tcpNoDelay, null));
-        }
+        boolean tcpNoDelay = settings.getAsBoolean("transport.netty.tcp_no_delay", TCP_NO_DELAY.get(settings));
+        clientBootstrap.setOption("tcpNoDelay", tcpNoDelay);
 
-        String tcpKeepAlive = settings.get("transport.netty.tcp_keep_alive", settings.get(TCP_KEEP_ALIVE, "true"));
-        if (!"default".equals(tcpKeepAlive)) {
-            clientBootstrap.setOption("keepAlive", Booleans.parseBoolean(tcpKeepAlive, null));
-        }
+        boolean tcpKeepAlive = settings.getAsBoolean("transport.netty.tcp_keep_alive", TCP_KEEP_ALIVE.get(settings));
+        clientBootstrap.setOption("keepAlive", tcpKeepAlive);
 
-        ByteSizeValue tcpSendBufferSize = settings.getAsBytesSize("transport.netty.tcp_send_buffer_size", settings.getAsBytesSize(TCP_SEND_BUFFER_SIZE, TCP_DEFAULT_SEND_BUFFER_SIZE));
-        if (tcpSendBufferSize != null && tcpSendBufferSize.bytes() > 0) {
+        ByteSizeValue tcpSendBufferSize = settings.getAsBytesSize("transport.netty.tcp_send_buffer_size", TCP_SEND_BUFFER_SIZE.get(settings));
+        if (tcpSendBufferSize.bytes() > 0) {
             clientBootstrap.setOption("sendBufferSize", tcpSendBufferSize.bytes());
         }
 
-        ByteSizeValue tcpReceiveBufferSize = settings.getAsBytesSize("transport.netty.tcp_receive_buffer_size", settings.getAsBytesSize(TCP_RECEIVE_BUFFER_SIZE, TCP_DEFAULT_RECEIVE_BUFFER_SIZE));
-        if (tcpReceiveBufferSize != null && tcpReceiveBufferSize.bytes() > 0) {
+        ByteSizeValue tcpReceiveBufferSize = settings.getAsBytesSize("transport.netty.tcp_receive_buffer_size", TCP_RECEIVE_BUFFER_SIZE.get(settings));
+        if (tcpReceiveBufferSize.bytes() > 0) {
             clientBootstrap.setOption("receiveBufferSize", tcpReceiveBufferSize.bytes());
         }
 
         clientBootstrap.setOption("receiveBufferSizePredictorFactory", receiveBufferSizePredictorFactory);
 
-        boolean reuseAddress = settings.getAsBoolean("transport.netty.reuse_address", settings.getAsBoolean(TCP_REUSE_ADDRESS, NetworkUtils.defaultReuseAddress()));
+        boolean reuseAddress = settings.getAsBoolean("transport.netty.reuse_address", TCP_REUSE_ADDRESS.get(settings));
         clientBootstrap.setOption("reuseAddress", reuseAddress);
 
         return clientBootstrap;
@@ -403,26 +395,22 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             fallbackSettingsBuilder.put("publish_host", fallbackPublishHost);
         }
 
-        String fallbackTcpNoDelay = settings.get("transport.netty.tcp_no_delay", settings.get(TCP_NO_DELAY, "true"));
-        if (fallbackTcpNoDelay != null) {
-            fallbackSettingsBuilder.put("tcp_no_delay", fallbackTcpNoDelay);
-        }
+        boolean fallbackTcpNoDelay = settings.getAsBoolean("transport.netty.tcp_no_delay", TCP_NO_DELAY.get(settings));
+        fallbackSettingsBuilder.put("tcp_no_delay", fallbackTcpNoDelay);
 
-        String fallbackTcpKeepAlive = settings.get("transport.netty.tcp_keep_alive", settings.get(TCP_KEEP_ALIVE, "true"));
-        if (fallbackTcpKeepAlive != null) {
+        boolean fallbackTcpKeepAlive = settings.getAsBoolean("transport.netty.tcp_keep_alive", TCP_KEEP_ALIVE.get(settings));
             fallbackSettingsBuilder.put("tcp_keep_alive", fallbackTcpKeepAlive);
-        }
 
-        boolean fallbackReuseAddress = settings.getAsBoolean("transport.netty.reuse_address", settings.getAsBoolean(TCP_REUSE_ADDRESS, NetworkUtils.defaultReuseAddress()));
+        boolean fallbackReuseAddress = settings.getAsBoolean("transport.netty.reuse_address", TCP_REUSE_ADDRESS.get(settings));
         fallbackSettingsBuilder.put("reuse_address", fallbackReuseAddress);
 
-        ByteSizeValue fallbackTcpSendBufferSize = settings.getAsBytesSize("transport.netty.tcp_send_buffer_size", settings.getAsBytesSize(TCP_SEND_BUFFER_SIZE, TCP_DEFAULT_SEND_BUFFER_SIZE));
-        if (fallbackTcpSendBufferSize != null) {
+        ByteSizeValue fallbackTcpSendBufferSize = settings.getAsBytesSize("transport.netty.tcp_send_buffer_size", TCP_SEND_BUFFER_SIZE.get(settings));
+        if (fallbackTcpSendBufferSize.bytes() >= 0) {
             fallbackSettingsBuilder.put("tcp_send_buffer_size", fallbackTcpSendBufferSize);
         }
 
-        ByteSizeValue fallbackTcpBufferSize = settings.getAsBytesSize("transport.netty.tcp_receive_buffer_size", settings.getAsBytesSize(TCP_RECEIVE_BUFFER_SIZE, TCP_DEFAULT_RECEIVE_BUFFER_SIZE));
-        if (fallbackTcpBufferSize != null) {
+        ByteSizeValue fallbackTcpBufferSize = settings.getAsBytesSize("transport.netty.tcp_receive_buffer_size", TCP_RECEIVE_BUFFER_SIZE.get(settings));
+        if (fallbackTcpBufferSize.bytes() >= 0) {
             fallbackSettingsBuilder.put("tcp_receive_buffer_size", fallbackTcpBufferSize);
         }
 
@@ -552,15 +540,15 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     private void createServerBootstrap(String name, Settings settings) {
-        boolean blockingServer = settings.getAsBoolean("transport.tcp.blocking_server", this.settings.getAsBoolean(TCP_BLOCKING_SERVER, this.settings.getAsBoolean(TCP_BLOCKING, false)));
+        boolean blockingServer = settings.getAsBoolean("transport.tcp.blocking_server", TCP_BLOCKING_SERVER.get(settings));
         String port = settings.get("port");
         String bindHost = settings.get("bind_host");
         String publishHost = settings.get("publish_host");
         String tcpNoDelay = settings.get("tcp_no_delay");
         String tcpKeepAlive = settings.get("tcp_keep_alive");
         boolean reuseAddress = settings.getAsBoolean("reuse_address", NetworkUtils.defaultReuseAddress());
-        ByteSizeValue tcpSendBufferSize = settings.getAsBytesSize("tcp_send_buffer_size", TCP_DEFAULT_SEND_BUFFER_SIZE);
-        ByteSizeValue tcpReceiveBufferSize = settings.getAsBytesSize("tcp_receive_buffer_size", TCP_DEFAULT_RECEIVE_BUFFER_SIZE);
+        ByteSizeValue tcpSendBufferSize = settings.getAsBytesSize("tcp_send_buffer_size", TCP_SEND_BUFFER_SIZE.getDefault(settings));
+        ByteSizeValue tcpReceiveBufferSize = settings.getAsBytesSize("tcp_receive_buffer_size", TCP_RECEIVE_BUFFER_SIZE.getDefault(settings));
 
         logger.debug("using profile[{}], worker_count[{}], port[{}], bind_host[{}], publish_host[{}], compress[{}], connect_timeout[{}], connections_per_node[{}/{}/{}/{}/{}], receive_predictor[{}->{}]",
                 name, workerCount, port, bindHost, publishHost, compress, connectTimeout, connectionsPerNodeRecovery, connectionsPerNodeBulk, connectionsPerNodeReg, connectionsPerNodeState, connectionsPerNodePing, receivePredictorMin, receivePredictorMax);
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index 78453c9..48f04e3 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -44,7 +44,9 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.rest.RestStatus;
@@ -100,8 +102,8 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
         // its a tribe configured node..., force settings
         Settings.Builder sb = Settings.builder().put(settings);
         sb.put("node.client", true); // this node should just act as a node client
-        sb.put("discovery.type", "local"); // a tribe node should not use zen discovery
-        sb.put("discovery.initial_state_timeout", 0); // nothing is going to be discovered, since no master will be elected
+        sb.put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "local"); // a tribe node should not use zen discovery
+        sb.put(DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING.getKey(), 0); // nothing is going to be discovered, since no master will be elected
         if (sb.get("cluster.name") == null) {
             sb.put("cluster.name", "tribe_" + Strings.randomBase64UUID()); // make sure it won't join other tribe nodes in the same JVM
         }
@@ -132,7 +134,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
         for (Map.Entry<String, Settings> entry : nodesSettings.entrySet()) {
             Settings.Builder sb = Settings.builder().put(entry.getValue());
             sb.put("name", settings.get("name") + "/" + entry.getKey());
-            sb.put("path.home", settings.get("path.home")); // pass through ES home dir
+            sb.put(Environment.PATH_HOME_SETTING.getKey(), Environment.PATH_HOME_SETTING.get(settings)); // pass through ES home dir
             sb.put(TRIBE_NAME, entry.getKey());
             if (sb.get("http.enabled") == null) {
                 sb.put("http.enabled", false);
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index d46f7dc..8c73e38 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -43,7 +43,6 @@ OFFICIAL PLUGINS
     - discovery-ec2
     - discovery-gce
     - discovery-multicast
-    - ingest-geoip
     - lang-javascript
     - lang-plan-a
     - lang-python
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/TransportAnalyzeActionTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/TransportAnalyzeActionTests.java
index cb0e0fa..4d53d6c 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/TransportAnalyzeActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/TransportAnalyzeActionTests.java
@@ -47,7 +47,7 @@ public class TransportAnalyzeActionTests extends ESTestCase {
     @Override
     public void setUp() throws Exception {
         super.setUp();
-        Settings settings = Settings.builder().put("path.home", createTempDir().toString()).build();
+        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
 
         Settings indexSettings = settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
diff --git a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
index 04b58e6..70d78e7 100644
--- a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
+++ b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.Arrays;
@@ -156,7 +157,7 @@ public class BulkProcessorIT extends ESIntegTestCase {
     public void testBulkProcessorConcurrentRequestsNoNodeAvailableException() throws Exception {
         //we create a transport client with no nodes to make sure it throws NoNodeAvailableException
         Settings settings = Settings.builder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         Client transportClient = TransportClient.builder().settings(settings).build();
 
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java b/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
deleted file mode 100644
index aa30c89..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
+++ /dev/null
@@ -1,165 +0,0 @@
-package org.elasticsearch.action.ingest;
-
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-
-public class BulkRequestModifierTests extends ESTestCase {
-
-    public void testBulkRequestModifier() {
-        int numRequests = scaledRandomIntBetween(8, 64);
-        BulkRequest bulkRequest = new BulkRequest();
-        for (int i = 0; i < numRequests; i++) {
-            bulkRequest.add(new IndexRequest("_index", "_type", String.valueOf(i)).source("{}"));
-        }
-        CaptureActionListener actionListener = new CaptureActionListener();
-        IngestActionFilter.BulkRequestModifier bulkRequestModifier = new IngestActionFilter.BulkRequestModifier(bulkRequest);
-
-        int i = 0;
-        Set<Integer> failedSlots = new HashSet<>();
-        while (bulkRequestModifier.hasNext()) {
-            bulkRequestModifier.next();
-            if (randomBoolean()) {
-                bulkRequestModifier.markCurrentItemAsFailed(new RuntimeException());
-                failedSlots.add(i);
-            }
-            i++;
-        }
-
-        assertThat(bulkRequestModifier.getBulkRequest().requests().size(), equalTo(numRequests - failedSlots.size()));
-        // simulate that we actually executed the modified bulk request:
-        ActionListener<BulkResponse> result = bulkRequestModifier.wrapActionListenerIfNeeded(actionListener);
-        result.onResponse(new BulkResponse(new BulkItemResponse[numRequests - failedSlots.size()], 0));
-
-        BulkResponse bulkResponse = actionListener.getResponse();
-        for (int j = 0; j < bulkResponse.getItems().length; j++) {
-            if (failedSlots.contains(j)) {
-                BulkItemResponse item =  bulkResponse.getItems()[j];
-                assertThat(item.isFailed(), is(true));
-                assertThat(item.getFailure().getIndex(), equalTo("_index"));
-                assertThat(item.getFailure().getType(), equalTo("_type"));
-                assertThat(item.getFailure().getId(), equalTo(String.valueOf(j)));
-                assertThat(item.getFailure().getMessage(), equalTo("java.lang.RuntimeException"));
-            } else {
-                assertThat(bulkResponse.getItems()[j], nullValue());
-            }
-        }
-    }
-
-    public void testPipelineFailures() {
-        BulkRequest originalBulkRequest = new BulkRequest();
-        for (int i = 0; i < 32; i++) {
-            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
-        }
-
-        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
-        for (int i = 0; modifier.hasNext(); i++) {
-            modifier.next();
-            if (i % 2 == 0) {
-                modifier.markCurrentItemAsFailed(new RuntimeException());
-            }
-        }
-
-        // So half of the requests have "failed", so only the successful requests are left:
-        BulkRequest bulkRequest = modifier.getBulkRequest();
-        assertThat(bulkRequest.requests().size(), Matchers.equalTo(16));
-
-        List<BulkItemResponse> responses = new ArrayList<>();
-        ActionListener<BulkResponse> bulkResponseListener = modifier.wrapActionListenerIfNeeded(new ActionListener<BulkResponse>() {
-            @Override
-            public void onResponse(BulkResponse bulkItemResponses) {
-                responses.addAll(Arrays.asList(bulkItemResponses.getItems()));
-            }
-
-            @Override
-            public void onFailure(Throwable e) {
-            }
-        });
-
-        List<BulkItemResponse> originalResponses = new ArrayList<>();
-        for (ActionRequest actionRequest : bulkRequest.requests()) {
-            IndexRequest indexRequest = (IndexRequest) actionRequest;
-            IndexResponse indexResponse = new IndexResponse(new ShardId("index", 0), indexRequest.type(), indexRequest.id(), 1, true);
-            originalResponses.add(new BulkItemResponse(Integer.parseInt(indexRequest.id()), indexRequest.opType().lowercase(), indexResponse));
-        }
-        bulkResponseListener.onResponse(new BulkResponse(originalResponses.toArray(new BulkItemResponse[originalResponses.size()]), 0));
-
-        assertThat(responses.size(), Matchers.equalTo(32));
-        for (int i = 0; i < 32; i++) {
-            assertThat(responses.get(i).getId(), Matchers.equalTo(String.valueOf(i)));
-        }
-    }
-
-    public void testNoFailures() {
-        BulkRequest originalBulkRequest = new BulkRequest();
-        for (int i = 0; i < 32; i++) {
-            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
-        }
-
-        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
-        while (modifier.hasNext()) {
-            modifier.next();
-        }
-
-        BulkRequest bulkRequest = modifier.getBulkRequest();
-        assertThat(bulkRequest, Matchers.sameInstance(originalBulkRequest));
-        @SuppressWarnings("unchecked")
-        ActionListener<BulkResponse> actionListener = mock(ActionListener.class);
-        assertThat(modifier.wrapActionListenerIfNeeded(actionListener), Matchers.sameInstance(actionListener));
-    }
-
-    private static class CaptureActionListener implements ActionListener<BulkResponse> {
-
-        private BulkResponse response;
-
-        @Override
-        public void onResponse(BulkResponse bulkItemResponses) {
-            this.response = bulkItemResponses ;
-        }
-
-        @Override
-        public void onFailure(Throwable e) {
-        }
-
-        public BulkResponse getResponse() {
-            return response;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java b/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
deleted file mode 100644
index e1ffe94..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
+++ /dev/null
@@ -1,249 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.IngestService;
-import org.elasticsearch.ingest.PipelineExecutionService;
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.tasks.Task;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.Before;
-import org.mockito.stubbing.Answer;
-
-import java.util.function.Consumer;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Matchers.same;
-import static org.mockito.Mockito.any;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.verifyZeroInteractions;
-import static org.mockito.Mockito.when;
-
-public class IngestActionFilterTests extends ESTestCase {
-
-    private IngestActionFilter filter;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        executionService = mock(PipelineExecutionService.class);
-        IngestService ingestService = mock(IngestService.class);
-        when(ingestService.getPipelineExecutionService()).thenReturn(executionService);
-        NodeService nodeService = mock(NodeService.class);
-        when(nodeService.getIngestService()).thenReturn(ingestService);
-        filter = new IngestActionFilter(Settings.EMPTY, nodeService);
-    }
-
-    public void testApplyNoPipelineId() throws Exception {
-        IndexRequest indexRequest = new IndexRequest();
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-        verifyZeroInteractions(executionService, actionFilterChain);
-    }
-
-    public void testApplyBulkNoPipelineId() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.add(new IndexRequest());
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
-
-        verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
-        verifyZeroInteractions(executionService, actionFilterChain);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyIngestIdViaRequestParam() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyExecuted() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        Answer answer = invocationOnMock -> {
-            @SuppressWarnings("unchecked")
-            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
-            listener.accept(true);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-        verifyZeroInteractions(actionListener);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyFailed() throws Exception {
-        Task task = mock(Task.class);
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
-        indexRequest.source("field", "value");
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        RuntimeException exception = new RuntimeException();
-        Answer answer = invocationOnMock -> {
-            Consumer<Throwable> handler = (Consumer) invocationOnMock.getArguments()[1];
-            handler.accept(exception);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionListener).onFailure(exception);
-        verifyZeroInteractions(actionFilterChain);
-    }
-
-    public void testApplyWithBulkRequest() throws Exception {
-        Task task = mock(Task.class);
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(any())).thenReturn(Runnable::run);
-        PipelineStore store = mock(PipelineStore.class);
-
-        Processor processor = new Processor() {
-            @Override
-            public void execute(IngestDocument ingestDocument) {
-                ingestDocument.setFieldValue("field2", "value2");
-            }
-
-            @Override
-            public String getType() {
-                return null;
-            }
-
-            @Override
-            public String getTag() {
-                return null;
-            }
-        };
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-        executionService = new PipelineExecutionService(store, threadPool);
-        IngestService ingestService = mock(IngestService.class);
-        when(ingestService.getPipelineExecutionService()).thenReturn(executionService);
-        NodeService nodeService = mock(NodeService.class);
-        when(nodeService.getIngestService()).thenReturn(ingestService);
-        filter = new IngestActionFilter(Settings.EMPTY, nodeService);
-
-        BulkRequest bulkRequest = new BulkRequest();
-        int numRequest = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < numRequest; i++) {
-            if (rarely()) {
-                ActionRequest request;
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-                bulkRequest.add(request);
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
-                indexRequest.source("field1", "value1");
-                bulkRequest.add(indexRequest);
-            }
-        }
-
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
-
-        assertBusy(() -> {
-            verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
-            verifyZeroInteractions(actionListener);
-
-            int assertedRequests = 0;
-            for (ActionRequest actionRequest : bulkRequest.requests()) {
-                if (actionRequest instanceof IndexRequest) {
-                    IndexRequest indexRequest = (IndexRequest) actionRequest;
-                    assertThat(indexRequest.sourceAsMap().size(), equalTo(2));
-                    assertThat(indexRequest.sourceAsMap().get("field1"), equalTo("value1"));
-                    assertThat(indexRequest.sourceAsMap().get("field2"), equalTo("value2"));
-                }
-                assertedRequests++;
-            }
-            assertThat(assertedRequests, equalTo(numRequest));
-        });
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testIndexApiSinglePipelineExecution() {
-        Answer answer = invocationOnMock -> {
-            @SuppressWarnings("unchecked")
-            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
-            listener.accept(true);
-            return null;
-        };
-        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
-
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id").source("field", "value");
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-        assertThat(indexRequest.getPipeline(), nullValue());
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-        verify(executionService, times(1)).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
-        verify(actionFilterChain, times(2)).proceed(task, IndexAction.NAME, indexRequest, actionListener);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java b/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java
deleted file mode 100644
index fa9728c..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java
+++ /dev/null
@@ -1,251 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkAction;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexAction;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.support.ActionFilterChain;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.tasks.Task;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportService;
-import org.hamcrest.CustomTypeSafeMatcher;
-import org.mockito.stubbing.Answer;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.argThat;
-import static org.mockito.Matchers.eq;
-import static org.mockito.Matchers.same;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.verifyZeroInteractions;
-import static org.mockito.Mockito.when;
-
-public class IngestProxyActionFilterTests extends ESTestCase {
-
-    private TransportService transportService;
-
-    @SuppressWarnings("unchecked")
-    private IngestProxyActionFilter buildFilter(int ingestNodes, int totalNodes) {
-        ClusterState.Builder clusterState = new ClusterState.Builder(new ClusterName("_name"));
-        DiscoveryNodes.Builder builder = new DiscoveryNodes.Builder();
-        DiscoveryNode localNode = null;
-        for (int i = 0; i < totalNodes; i++) {
-            String nodeId = "node" + i;
-            Map<String, String> attributes = new HashMap<>();
-            if (i >= ingestNodes) {
-                attributes.put("ingest", "false");
-            } else if (randomBoolean()) {
-                attributes.put("ingest", "true");
-            }
-            DiscoveryNode node = new DiscoveryNode(nodeId, nodeId, DummyTransportAddress.INSTANCE, attributes, VersionUtils.randomVersion(random()));
-            builder.put(node);
-            if (i == totalNodes - 1) {
-                localNode = node;
-            }
-        }
-        clusterState.nodes(builder);
-        ClusterService clusterService = mock(ClusterService.class);
-        when(clusterService.localNode()).thenReturn(localNode);
-        when(clusterService.state()).thenReturn(clusterState.build());
-        transportService = mock(TransportService.class);
-        return new IngestProxyActionFilter(clusterService, transportService);
-    }
-
-    public void testApplyNoIngestNodes() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        int totalNodes = randomIntBetween(1, 5);
-        IngestProxyActionFilter filter = buildFilter(0, totalNodes);
-
-        String action;
-        ActionRequest request;
-        if (randomBoolean()) {
-            action = IndexAction.NAME;
-            request = new IndexRequest().setPipeline("_id");
-        } else {
-            action = BulkAction.NAME;
-            request = new BulkRequest().add(new IndexRequest().setPipeline("_id"));
-        }
-        try {
-            filter.apply(task, action, request, actionListener, actionFilterChain);
-            fail("should have failed because there are no ingest nodes");
-        } catch(IllegalStateException e) {
-            assertThat(e.getMessage(), equalTo("There are no ingest nodes in this cluster, unable to forward request to an ingest node."));
-        }
-        verifyZeroInteractions(transportService);
-        verifyZeroInteractions(actionFilterChain);
-        verifyZeroInteractions(actionListener);
-    }
-
-    public void testApplyNoPipelineId() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        int totalNodes = randomIntBetween(1, 5);
-        IngestProxyActionFilter filter = buildFilter(randomIntBetween(0, totalNodes - 1), totalNodes);
-
-        String action;
-        ActionRequest request;
-        if (randomBoolean()) {
-            action = IndexAction.NAME;
-            request = new IndexRequest();
-        } else {
-            action = BulkAction.NAME;
-            request = new BulkRequest().add(new IndexRequest());
-        }
-        filter.apply(task, action, request, actionListener, actionFilterChain);
-        verifyZeroInteractions(transportService);
-        verify(actionFilterChain).proceed(any(Task.class), eq(action), same(request), same(actionListener));
-        verifyZeroInteractions(actionListener);
-    }
-
-    public void testApplyAnyAction() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        ActionRequest request = mock(ActionRequest.class);
-        int totalNodes = randomIntBetween(1, 5);
-        IngestProxyActionFilter filter = buildFilter(randomIntBetween(0, totalNodes - 1), totalNodes);
-
-        String action = randomAsciiOfLengthBetween(1, 20);
-        filter.apply(task, action, request, actionListener, actionFilterChain);
-        verifyZeroInteractions(transportService);
-        verify(actionFilterChain).proceed(any(Task.class), eq(action), same(request), same(actionListener));
-        verifyZeroInteractions(actionListener);
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyIndexRedirect() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        int totalNodes = randomIntBetween(2, 5);
-        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
-        Answer<Void> answer = invocationOnMock -> {
-            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
-            transportResponseHandler.handleResponse(new IndexResponse());
-            return null;
-        };
-        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
-
-        IndexRequest indexRequest = new IndexRequest().setPipeline("_id");
-        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
-
-        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(IndexAction.NAME), same(indexRequest), any(TransportResponseHandler.class));
-        verifyZeroInteractions(actionFilterChain);
-        verify(actionListener).onResponse(any(IndexResponse.class));
-        verify(actionListener, never()).onFailure(any(TransportException.class));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyBulkRedirect() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        int totalNodes = randomIntBetween(2, 5);
-        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
-        Answer<Void> answer = invocationOnMock -> {
-            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
-            transportResponseHandler.handleResponse(new BulkResponse(null, -1));
-            return null;
-        };
-        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
-
-        BulkRequest bulkRequest = new BulkRequest();
-        bulkRequest.add(new IndexRequest().setPipeline("_id"));
-        int numNoPipelineRequests = randomIntBetween(0, 10);
-        for (int i = 0; i < numNoPipelineRequests; i++) {
-            bulkRequest.add(new IndexRequest());
-        }
-        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
-
-        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(BulkAction.NAME), same(bulkRequest), any(TransportResponseHandler.class));
-        verifyZeroInteractions(actionFilterChain);
-        verify(actionListener).onResponse(any(BulkResponse.class));
-        verify(actionListener, never()).onFailure(any(TransportException.class));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testApplyFailures() {
-        Task task = mock(Task.class);
-        ActionListener actionListener = mock(ActionListener.class);
-        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
-        int totalNodes = randomIntBetween(2, 5);
-        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
-        Answer<Void> answer = invocationOnMock -> {
-            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
-            transportResponseHandler.handleException(new TransportException(new IllegalArgumentException()));
-            return null;
-        };
-        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
-
-        String action;
-        ActionRequest request;
-        if (randomBoolean()) {
-            action = IndexAction.NAME;
-            request = new IndexRequest().setPipeline("_id");
-        } else {
-            action = BulkAction.NAME;
-            request = new BulkRequest().add(new IndexRequest().setPipeline("_id"));
-        }
-
-        filter.apply(task, action, request, actionListener, actionFilterChain);
-
-        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(action), same(request), any(TransportResponseHandler.class));
-        verifyZeroInteractions(actionFilterChain);
-        verify(actionListener).onFailure(any(TransportException.class));
-        verify(actionListener, never()).onResponse(any(TransportResponse.class));
-    }
-
-    private static class IngestNodeMatcher extends CustomTypeSafeMatcher<DiscoveryNode> {
-        private IngestNodeMatcher() {
-            super("discovery node should be an ingest node");
-        }
-
-        @Override
-        protected boolean matchesSafely(DiscoveryNode node) {
-            return node.isIngestNode();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
deleted file mode 100644
index 882fca7..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SimulateDocumentSimpleResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isFailure = randomBoolean();
-        SimulateDocumentBaseResult simulateDocumentBaseResult;
-        if (isFailure) {
-            simulateDocumentBaseResult = new SimulateDocumentBaseResult(new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateDocumentBaseResult = new SimulateDocumentBaseResult(ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateDocumentBaseResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateDocumentBaseResult otherSimulateDocumentBaseResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(streamInput);
-
-        assertThat(otherSimulateDocumentBaseResult.getIngestDocument(), equalTo(simulateDocumentBaseResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateDocumentBaseResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateDocumentBaseResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
deleted file mode 100644
index d58b9bf..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
+++ /dev/null
@@ -1,206 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.junit.After;
-import org.junit.Before;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.List;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class SimulateExecutionServiceTests extends ESTestCase {
-
-    private ThreadPool threadPool;
-    private SimulateExecutionService executionService;
-    private Pipeline pipeline;
-    private Processor processor;
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setup() {
-        threadPool = new ThreadPool(
-                Settings.builder()
-                        .put("name", getClass().getName())
-                        .build()
-        );
-        executionService = new SimulateExecutionService(threadPool);
-        processor = new TestProcessor("id", "mock", ingestDocument -> {});
-        pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-    }
-
-    @After
-    public void destroy() {
-        threadPool.shutdown();
-    }
-
-    public void testExecuteVerboseDocumentSimple() throws Exception {
-        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-        executionService.executeVerboseDocument(processor, ingestDocument, processorResultList);
-        SimulateProcessorResult result = new SimulateProcessorResult("id", ingestDocument);
-        assertThat(processorResultList.size(), equalTo(1));
-        assertThat(processorResultList.get(0).getProcessorTag(), equalTo(result.getProcessorTag()));
-        assertThat(processorResultList.get(0).getIngestDocument(), equalTo(result.getIngestDocument()));
-        assertThat(processorResultList.get(0).getFailure(), nullValue());
-    }
-
-    public void testExecuteVerboseDocumentSimpleException() throws Exception {
-        RuntimeException exception = new RuntimeException("mock_exception");
-        TestProcessor processor = new TestProcessor("id", "mock", ingestDocument -> { throw exception; });
-        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-        try {
-            executionService.executeVerboseDocument(processor, ingestDocument, processorResultList);
-            fail("should throw exception");
-        } catch (RuntimeException e) {
-            assertThat(e.getMessage(), equalTo("mock_exception"));
-        }
-        SimulateProcessorResult result = new SimulateProcessorResult("id", exception);
-        assertThat(processorResultList.size(), equalTo(1));
-        assertThat(processorResultList.get(0).getProcessorTag(), equalTo(result.getProcessorTag()));
-        assertThat(processorResultList.get(0).getFailure(), equalTo(result.getFailure()));
-    }
-
-    public void testExecuteVerboseDocumentCompoundSuccess() throws Exception {
-        TestProcessor processor1 = new TestProcessor("p1", "mock", ingestDocument -> { });
-        TestProcessor processor2 = new TestProcessor("p2", "mock", ingestDocument -> { });
-
-        Processor compoundProcessor = new CompoundProcessor(processor1, processor2);
-        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-        executionService.executeVerboseDocument(compoundProcessor, ingestDocument, processorResultList);
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-        assertThat(processorResultList.size(), equalTo(2));
-        assertThat(processorResultList.get(0).getProcessorTag(), equalTo("p1"));
-        assertThat(processorResultList.get(0).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(processorResultList.get(0).getFailure(), nullValue());
-        assertThat(processorResultList.get(1).getProcessorTag(), equalTo("p2"));
-        assertThat(processorResultList.get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(processorResultList.get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteVerboseDocumentCompoundOnFailure() throws Exception {
-        TestProcessor processor1 = new TestProcessor("p1", "mock", ingestDocument -> { });
-        TestProcessor processor2 = new TestProcessor("p2", "mock", ingestDocument -> { throw new RuntimeException("p2_exception"); });
-        TestProcessor onFailureProcessor1 = new TestProcessor("fail_p1", "mock", ingestDocument -> { });
-        TestProcessor onFailureProcessor2 = new TestProcessor("fail_p2", "mock", ingestDocument -> { throw new RuntimeException("fail_p2_exception"); });
-        TestProcessor onFailureProcessor3 = new TestProcessor("fail_p3", "mock", ingestDocument -> { });
-        CompoundProcessor onFailureCompoundProcessor = new CompoundProcessor(Collections.singletonList(onFailureProcessor2), Collections.singletonList(onFailureProcessor3));
-
-        Processor compoundProcessor = new CompoundProcessor(Arrays.asList(processor1, processor2), Arrays.asList(onFailureProcessor1, onFailureCompoundProcessor));
-        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
-        executionService.executeVerboseDocument(compoundProcessor, ingestDocument, processorResultList);
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-        assertThat(onFailureProcessor1.getInvokedCounter(), equalTo(1));
-        assertThat(onFailureProcessor2.getInvokedCounter(), equalTo(1));
-        assertThat(onFailureProcessor3.getInvokedCounter(), equalTo(1));
-        assertThat(processorResultList.size(), equalTo(5));
-        assertThat(processorResultList.get(0).getProcessorTag(), equalTo("p1"));
-        assertThat(processorResultList.get(1).getProcessorTag(), equalTo("p2"));
-        assertThat(processorResultList.get(2).getProcessorTag(), equalTo("fail_p1"));
-        assertThat(processorResultList.get(3).getProcessorTag(), equalTo("fail_p2"));
-        assertThat(processorResultList.get(4).getProcessorTag(), equalTo("fail_p3"));
-    }
-
-    public void testExecuteVerboseItem() throws Exception {
-        TestProcessor processor = new TestProcessor("test-id", "mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        assertThat(processor.getInvokedCounter(), equalTo(2));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorTag(), equalTo("test-id"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorTag(), equalTo("test-id"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(),
-            not(sameInstance(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata())));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteItem() throws Exception {
-        TestProcessor processor = new TestProcessor("processor_0", "mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        assertThat(processor.getInvokedCounter(), equalTo(2));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentBaseResult.class));
-        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) actualItemResponse;
-        assertThat(simulateDocumentBaseResult.getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
-    }
-
-    public void testExecuteVerboseItemWithFailure() throws Exception {
-        TestProcessor processor1 = new TestProcessor("processor_0", "mock", ingestDocument -> { throw new RuntimeException("processor failed"); });
-        TestProcessor processor2 = new TestProcessor("processor_1", "mock", ingestDocument -> {});
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(Collections.singletonList(processor1), Collections.singletonList(processor2)));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
-        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorTag(), equalTo("processor_0"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), nullValue());
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorTag(), equalTo("processor_1"));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
-        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
-    }
-
-    public void testExecuteItemWithFailure() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> { throw new RuntimeException("processor failed"); });
-        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
-        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-        assertThat(actualItemResponse, instanceOf(SimulateDocumentBaseResult.class));
-        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) actualItemResponse;
-        assertThat(simulateDocumentBaseResult.getIngestDocument(), nullValue());
-        assertThat(simulateDocumentBaseResult.getFailure(), instanceOf(RuntimeException.class));
-        RuntimeException runtimeException = (RuntimeException) simulateDocumentBaseResult.getFailure();
-        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
deleted file mode 100644
index c0e7d69..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
+++ /dev/null
@@ -1,181 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.ingest.PipelineStore;
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.action.ingest.SimulatePipelineRequest.Fields;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.ID;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.INDEX;
-import static org.elasticsearch.ingest.core.IngestDocument.MetaData.TYPE;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.when;
-
-public class SimulatePipelineRequestParsingTests extends ESTestCase {
-
-    private PipelineStore store;
-
-    @Before
-    public void init() throws IOException {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {});
-        CompoundProcessor pipelineCompoundProcessor = new CompoundProcessor(processor);
-        Pipeline pipeline = new Pipeline(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, null, pipelineCompoundProcessor);
-        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
-        processorRegistry.put("mock_processor", mock(Processor.Factory.class));
-        store = mock(PipelineStore.class);
-        when(store.get(SimulatePipelineRequest.SIMULATED_PIPELINE_ID)).thenReturn(pipeline);
-        when(store.getProcessorFactoryRegistry()).thenReturn(processorRegistry);
-    }
-
-    public void testParseUsingPipelineStore() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parseWithPipelineId(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(1));
-    }
-
-    public void testParseWithProvidedPipeline() throws Exception {
-        int numDocs = randomIntBetween(1, 10);
-
-        Map<String, Object> requestContent = new HashMap<>();
-        List<Map<String, Object>> docs = new ArrayList<>();
-        List<Map<String, Object>> expectedDocs = new ArrayList<>();
-        requestContent.put(Fields.DOCS, docs);
-        for (int i = 0; i < numDocs; i++) {
-            Map<String, Object> doc = new HashMap<>();
-            String index = randomAsciiOfLengthBetween(1, 10);
-            String type = randomAsciiOfLengthBetween(1, 10);
-            String id = randomAsciiOfLengthBetween(1, 10);
-            doc.put(INDEX.getFieldName(), index);
-            doc.put(TYPE.getFieldName(), type);
-            doc.put(ID.getFieldName(), id);
-            String fieldName = randomAsciiOfLengthBetween(1, 10);
-            String fieldValue = randomAsciiOfLengthBetween(1, 10);
-            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            docs.add(doc);
-            Map<String, Object> expectedDoc = new HashMap<>();
-            expectedDoc.put(INDEX.getFieldName(), index);
-            expectedDoc.put(TYPE.getFieldName(), type);
-            expectedDoc.put(ID.getFieldName(), id);
-            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
-            expectedDocs.add(expectedDoc);
-        }
-
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        List<Map<String, Object>> processors = new ArrayList<>();
-        int numProcessors = randomIntBetween(1, 10);
-        for (int i = 0; i < numProcessors; i++) {
-            Map<String, Object> processorConfig = new HashMap<>();
-            List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
-            int numOnFailureProcessors = randomIntBetween(0, 1);
-            for (int j = 0; j < numOnFailureProcessors; j++) {
-                onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
-            }
-            if (numOnFailureProcessors > 0) {
-                processorConfig.put("on_failure", onFailureProcessors);
-            }
-            processors.add(Collections.singletonMap("mock_processor", processorConfig));
-        }
-        pipelineConfig.put("processors", processors);
-
-        List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
-        int numOnFailureProcessors = randomIntBetween(0, 1);
-        for (int i = 0; i < numOnFailureProcessors; i++) {
-            onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
-        }
-        if (numOnFailureProcessors > 0) {
-            pipelineConfig.put("on_failure", onFailureProcessors);
-        }
-
-        requestContent.put(Fields.PIPELINE, pipelineConfig);
-
-        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parse(requestContent, false, store);
-        assertThat(actualRequest.isVerbose(), equalTo(false));
-        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
-        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
-        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
-            Map<String, Object> expectedDocument = expectedDocsIterator.next();
-            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
-            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
-            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
-            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
-            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
-        }
-
-        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
-        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
-        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(numProcessors));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
deleted file mode 100644
index 12a62f0..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class SimulatePipelineResponseTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        boolean isVerbose = randomBoolean();
-        int numResults = randomIntBetween(1, 10);
-        List<SimulateDocumentResult> results = new ArrayList<>(numResults);
-        for (int i = 0; i < numResults; i++) {
-            boolean isFailure = randomBoolean();
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            if (isVerbose) {
-                int numProcessors = randomIntBetween(1, 10);
-                List<SimulateProcessorResult> processorResults = new ArrayList<>(numProcessors);
-                for (int j = 0; j < numProcessors; j++) {
-                    String processorTag = randomAsciiOfLengthBetween(1, 10);
-                    SimulateProcessorResult processorResult;
-                    if (isFailure) {
-                        processorResult = new SimulateProcessorResult(processorTag, new IllegalArgumentException("test"));
-                    } else {
-                        processorResult = new SimulateProcessorResult(processorTag, ingestDocument);
-                    }
-                    processorResults.add(processorResult);
-                }
-                results.add(new SimulateDocumentVerboseResult(processorResults));
-            } else {
-                results.add(new SimulateDocumentBaseResult(ingestDocument));
-                SimulateDocumentBaseResult simulateDocumentBaseResult;
-                if (isFailure) {
-                    simulateDocumentBaseResult = new SimulateDocumentBaseResult(new IllegalArgumentException("test"));
-                } else {
-                    simulateDocumentBaseResult = new SimulateDocumentBaseResult(ingestDocument);
-                }
-                results.add(simulateDocumentBaseResult);
-            }
-        }
-
-        SimulatePipelineResponse response = new SimulatePipelineResponse(randomAsciiOfLengthBetween(1, 10), isVerbose, results);
-        BytesStreamOutput out = new BytesStreamOutput();
-        response.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulatePipelineResponse otherResponse = new SimulatePipelineResponse();
-        otherResponse.readFrom(streamInput);
-
-        assertThat(otherResponse.getPipelineId(), equalTo(response.getPipelineId()));
-        assertThat(otherResponse.getResults().size(), equalTo(response.getResults().size()));
-
-        Iterator<SimulateDocumentResult> expectedResultIterator = response.getResults().iterator();
-        for (SimulateDocumentResult result : otherResponse.getResults()) {
-            if (isVerbose) {
-                SimulateDocumentVerboseResult expectedSimulateDocumentVerboseResult = (SimulateDocumentVerboseResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentVerboseResult.class));
-                SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) result;
-                assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(expectedSimulateDocumentVerboseResult.getProcessorResults().size()));
-                Iterator<SimulateProcessorResult> expectedProcessorResultIterator = expectedSimulateDocumentVerboseResult.getProcessorResults().iterator();
-                for (SimulateProcessorResult simulateProcessorResult : simulateDocumentVerboseResult.getProcessorResults()) {
-                    SimulateProcessorResult expectedProcessorResult = expectedProcessorResultIterator.next();
-                    assertThat(simulateProcessorResult.getProcessorTag(), equalTo(expectedProcessorResult.getProcessorTag()));
-                    assertThat(simulateProcessorResult.getIngestDocument(), equalTo(expectedProcessorResult.getIngestDocument()));
-                    if (expectedProcessorResult.getFailure() == null) {
-                        assertThat(simulateProcessorResult.getFailure(), nullValue());
-                    } else {
-                        assertThat(simulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                        IllegalArgumentException e = (IllegalArgumentException) simulateProcessorResult.getFailure();
-                        assertThat(e.getMessage(), equalTo("test"));
-                    }
-                }
-            } else {
-                SimulateDocumentBaseResult expectedSimulateDocumentBaseResult = (SimulateDocumentBaseResult) expectedResultIterator.next();
-                assertThat(result, instanceOf(SimulateDocumentBaseResult.class));
-                SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) result;
-                assertThat(simulateDocumentBaseResult.getIngestDocument(), equalTo(expectedSimulateDocumentBaseResult.getIngestDocument()));
-                if (expectedSimulateDocumentBaseResult.getFailure() == null) {
-                    assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
-                } else {
-                    assertThat(simulateDocumentBaseResult.getFailure(), instanceOf(IllegalArgumentException.class));
-                    IllegalArgumentException e = (IllegalArgumentException) simulateDocumentBaseResult.getFailure();
-                    assertThat(e.getMessage(), equalTo("test"));
-                }
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
deleted file mode 100644
index 0885475..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class SimulateProcessorResultTests extends ESTestCase {
-
-    public void testSerialization() throws IOException {
-        String processorTag = randomAsciiOfLengthBetween(1, 10);
-        boolean isFailure = randomBoolean();
-        SimulateProcessorResult simulateProcessorResult;
-        if (isFailure) {
-            simulateProcessorResult = new SimulateProcessorResult(processorTag, new IllegalArgumentException("test"));
-        } else {
-            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-            simulateProcessorResult = new SimulateProcessorResult(processorTag, ingestDocument);
-        }
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        simulateProcessorResult.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        SimulateProcessorResult otherSimulateProcessorResult = new SimulateProcessorResult(streamInput);
-        assertThat(otherSimulateProcessorResult.getProcessorTag(), equalTo(simulateProcessorResult.getProcessorTag()));
-        assertThat(otherSimulateProcessorResult.getIngestDocument(), equalTo(simulateProcessorResult.getIngestDocument()));
-        if (isFailure) {
-            assertThat(otherSimulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
-            IllegalArgumentException e = (IllegalArgumentException) otherSimulateProcessorResult.getFailure();
-            assertThat(e.getMessage(), equalTo("test"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
deleted file mode 100644
index 8d3c812..0000000
--- a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.ingest;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-
-public class WriteableIngestDocumentTests extends ESTestCase {
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        WriteableIngestDocument ingestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        WriteableIngestDocument otherIngestDocument = new WriteableIngestDocument(new IngestDocument(otherSourceAndMetadata, otherIngestMetadata));
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            WriteableIngestDocument thirdIngestDocument = new WriteableIngestDocument(new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata)));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testSerialization() throws IOException {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, Object> document = RandomDocumentPicks.randomSource(random());
-        WriteableIngestDocument writeableIngestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
-
-        BytesStreamOutput out = new BytesStreamOutput();
-        writeableIngestDocument.writeTo(out);
-        StreamInput streamInput = StreamInput.wrap(out.bytes());
-        WriteableIngestDocument otherWriteableIngestDocument = new WriteableIngestDocument(streamInput);
-        assertThat(otherWriteableIngestDocument, equalTo(writeableIngestDocument));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
index fc64533..9cef4d4 100644
--- a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.action.search;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESTestCase;
@@ -38,7 +39,7 @@ public class SearchRequestBuilderTests extends ESTestCase {
         //this client will not be hit by any request, but it needs to be a non null proper client
         //that is why we create it but we don't add any transport address to it
         Settings settings = Settings.builder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         client = TransportClient.builder().settings(settings).build();
     }
diff --git a/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java b/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
index f20e540..6d0ae3d 100644
--- a/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
+++ b/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
@@ -60,8 +60,8 @@ public class IndexingMasterFailoverIT extends ESIntegTestCase {
         logger.info("--> start 4 nodes, 3 master, 1 data");
 
         final Settings sharedSettings = Settings.builder()
-                .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // for hitting simulated network failures quickly
-                .put(FaultDetection.SETTING_PING_RETRIES, "1") // for hitting simulated network failures quickly
+                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1s") // for hitting simulated network failures quickly
+                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), "1") // for hitting simulated network failures quickly
                 .put("discovery.zen.join_timeout", "10s")  // still long to induce failures but to long so test won't time out
                 .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
                 .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index 8a18b72..7488141 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -39,11 +39,11 @@ import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.MultiDataPathUpgrader;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.gateway.MetaDataStateFormat;
 import org.elasticsearch.index.IndexSettings;
@@ -143,13 +143,13 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
         Path baseTempDir = createTempDir();
         // start single data path node
         Settings.Builder nodeSettings = Settings.builder()
-            .put("path.data", baseTempDir.resolve("single-path").toAbsolutePath())
+            .put(Environment.PATH_DATA_SETTING.getKey(), baseTempDir.resolve("single-path").toAbsolutePath())
             .put("node.master", false); // workaround for dangling index loading issue when node is master
         InternalTestCluster.Async<String> singleDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
 
         // start multi data path node
         nodeSettings = Settings.builder()
-            .put("path.data", baseTempDir.resolve("multi-path1").toAbsolutePath() + "," + baseTempDir.resolve("multi-path2").toAbsolutePath())
+            .put(Environment.PATH_DATA_SETTING.getKey(), baseTempDir.resolve("multi-path1").toAbsolutePath() + "," + baseTempDir.resolve("multi-path2").toAbsolutePath())
             .put("node.master", false); // workaround for dangling index loading issue when node is master
         InternalTestCluster.Async<String> multiDataPathNode = internalCluster().startNodeAsync(nodeSettings.build());
 
@@ -208,10 +208,6 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
     }
 
     void importIndex(String indexName) throws IOException {
-        final Iterable<NodeEnvironment> instances = internalCluster().getInstances(NodeEnvironment.class);
-        for (NodeEnvironment nodeEnv : instances) { // upgrade multidata path
-            MultiDataPathUpgrader.upgradeMultiDataPath(nodeEnv, logger);
-        }
         // force reloading dangling indices with a cluster state republish
         client().admin().cluster().prepareReroute().get();
         ensureGreen(indexName);
@@ -219,6 +215,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
 
     // randomly distribute the files from src over dests paths
     public static void copyIndex(final ESLogger logger, final Path src, final String indexName, final Path... dests) throws IOException {
+        Path destinationDataPath = dests[randomInt(dests.length - 1)];
         for (Path dest : dests) {
             Path indexDir = dest.resolve(indexName);
             assertFalse(Files.exists(indexDir));
@@ -244,7 +241,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
                 }
 
                 Path relativeFile = src.relativize(file);
-                Path destFile = dests[randomInt(dests.length - 1)].resolve(indexName).resolve(relativeFile);
+                Path destFile = destinationDataPath.resolve(indexName).resolve(relativeFile);
                 logger.trace("--> Moving " + relativeFile.toString() + " to " + destFile.toString());
                 Files.move(file, destFile);
                 assertFalse(Files.exists(file));
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
index a573a83..23163b8 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.bwcompat;
 
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.node.Node;
 
@@ -28,7 +29,7 @@ public class RecoveryWithUnsupportedIndicesIT extends StaticIndexBackwardCompati
         String indexName = "unsupported-0.20.6";
 
         logger.info("Checking static index " + indexName);
-        Settings nodeSettings = prepareBackwardsDataDir(getBwcIndicesPath().resolve(indexName + ".zip"), Node.HTTP_ENABLED, true);
+        Settings nodeSettings = prepareBackwardsDataDir(getBwcIndicesPath().resolve(indexName + ".zip"), NetworkModule.HTTP_ENABLED.getKey(), true);
         try {
             internalCluster().startNode(nodeSettings);
             fail();
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
index 6ad05b3..eabb954 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
@@ -1,4 +1,5 @@
 /*
+/*
  * Licensed to Elasticsearch under one or more contributor
  * license agreements. See the NOTICE file distributed with
  * this work for additional information regarding copyright
@@ -27,6 +28,8 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
 import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.repositories.uri.URLRepository;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.snapshots.AbstractSnapshotIntegTestCase;
 import org.elasticsearch.snapshots.RestoreInfo;
@@ -64,7 +67,7 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
             // Configure using path.repo
             return settingsBuilder()
                     .put(super.nodeSettings(nodeOrdinal))
-                    .put("path.repo", getBwcIndicesPath())
+                    .put(Environment.PATH_REPO_SETTING.getKey(), getBwcIndicesPath())
                     .build();
         } else {
             // Configure using url white list
@@ -72,7 +75,7 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
                 URI repoJarPatternUri = new URI("jar:" + getBwcIndicesPath().toUri().toString() + "*.zip!/repo/");
                 return settingsBuilder()
                         .put(super.nodeSettings(nodeOrdinal))
-                        .putArray("repositories.url.allowed_urls", repoJarPatternUri.toString())
+                        .putArray(URLRepository.ALLOWED_URLS_SETTING.getKey(), repoJarPatternUri.toString())
                         .build();
             } catch (URISyntaxException ex) {
                 throw new IllegalArgumentException(ex);
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
index b814cff..3bdfc1f 100644
--- a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
+++ b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
@@ -48,6 +48,7 @@ import org.elasticsearch.action.search.SearchAction;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportMessage;
@@ -90,7 +91,7 @@ public abstract class AbstractClientHeadersTestCase extends ESTestCase {
     public void initClient() {
         Settings settings = Settings.builder()
                 .put(HEADER_SETTINGS)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         threadPool = new ThreadPool("test-" + getTestName());
         client = buildClient(settings, ACTIONS);
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
index f127ae2..e61dab2 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java
@@ -36,6 +36,7 @@ import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.tasks.TaskManager;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -83,7 +84,7 @@ public class TransportClientHeadersTests extends AbstractClientHeadersTestCase {
                 .put("node.name", "transport_client_" + this.getTestName() + "_1")
                 .put("client.transport.nodes_sampler_interval", "1s")
                 .put(HEADER_SETTINGS)
-                .put("path.home", createTempDir().toString()).build())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build())
             .addPlugin(InternalTransportService.TestPlugin.class)
             .build();
 
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
index f01fdff..8ab432d 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -52,7 +53,7 @@ public class TransportClientIT extends ESIntegTestCase {
         TransportClientNodesService nodeService = client.nodeService();
         Node node = new Node(Settings.builder()
                 .put(internalCluster().getDefaultSettings())
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("node.name", "testNodeVersionIsUpdated")
                 .put("http.enabled", false)
                 .put("node.data", false)
@@ -89,7 +90,10 @@ public class TransportClientIT extends ESIntegTestCase {
     }
 
     public void testThatTransportClientSettingCannotBeChanged() {
-        Settings baseSettings = settingsBuilder().put(Client.CLIENT_TYPE_SETTING, "anything").put("path.home", createTempDir()).build();
+        Settings baseSettings = settingsBuilder()
+            .put(Client.CLIENT_TYPE_SETTING, "anything")
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
+             .build();
         try (TransportClient client = TransportClient.builder().settings(baseSettings).build()) {
             Settings settings = client.injector.getInstance(Settings.class);
             assertThat(settings.get(Client.CLIENT_TYPE_SETTING), is("transport"));
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
index b28fdba..e5367d1 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientRetryIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -57,7 +58,7 @@ public class TransportClientRetryIT extends ESIntegTestCase {
                 .put("node.mode", internalCluster().getNodeMode())
                 .put(ClusterName.SETTING, internalCluster().getClusterName())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
-                .put("path.home", createTempDir());
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir());
 
         try (TransportClient transportClient = TransportClient.builder().settings(builder.build()).build()) {
             transportClient.addTransportAddresses(addresses);
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
index 2d781c8..72d58f7 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
@@ -642,7 +642,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 1)
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "400ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "400ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
index 2d726d9..612f910 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
@@ -77,7 +77,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -189,7 +189,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
                 .put("discovery.zen.minimum_master_nodes", 3)
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "1s")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "1s")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -264,7 +264,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
     public void testDynamicUpdateMinimumMasterNodes() throws Exception {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "400ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "400ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
 
@@ -322,7 +322,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         int nodeCount = scaledRandomIntBetween(1, 5);
         Settings.Builder settings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put("discovery.initial_state_timeout", "500ms");
 
         // set an initial value which is at least quorum to avoid split brains during initial startup
@@ -361,8 +361,8 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
     public void testCanNotPublishWithoutMinMastNodes() throws Exception {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put(FaultDetection.SETTING_PING_TIMEOUT, "1h") // disable it
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1h") // disable it
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
                 .put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), "100ms") // speed things up
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
index 8e5479d..3c71f5f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
@@ -65,7 +65,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.type", "zen")
                 .put("action.auto_create_index", autoCreateIndex)
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "all")
                 .build();
@@ -217,7 +217,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.type", "zen")
                 .put("action.auto_create_index", false)
                 .put("discovery.zen.minimum_master_nodes", 2)
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "write")
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
index 1e9c25e..cf94836 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
@@ -108,7 +108,7 @@ public class AwarenessAllocationIT extends ESIntegTestCase {
                 .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING.getKey() + "zone.values", "a,b")
                 .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey(), "zone")
                 .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 3)
-                .put(ZenDiscovery.SETTING_JOIN_TIMEOUT, "10s")
+                .put(ZenDiscovery.JOIN_TIMEOUT_SETTING.getKey(), "10s")
                 .build();
 
         logger.info("--> starting 4 nodes on different zones");
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
new file mode 100644
index 0000000..a43da9e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.cluster.metadata;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.common.settings.IndexScopedSettings;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.indices.mapper.MapperRegistry;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.VersionUtils;
+
+import java.util.Collections;
+
+public class MetaDataIndexUpgradeServiceTests extends ESTestCase {
+
+    public void testArchiveBrokenIndexSettings() {
+        MetaDataIndexUpgradeService service = new MetaDataIndexUpgradeService(Settings.EMPTY, new MapperRegistry(Collections.emptyMap(), Collections.emptyMap()), IndexScopedSettings.DEFAULT_SCOPED_SETTINGS);
+        IndexMetaData src = newIndexMeta("foo", Settings.EMPTY);
+        IndexMetaData indexMetaData = service.archiveBrokenIndexSettings(src);
+        assertSame(indexMetaData, src);
+
+        src = newIndexMeta("foo", Settings.builder().put("index.refresh_interval", "-200").build());
+        indexMetaData = service.archiveBrokenIndexSettings(src);
+        assertNotSame(indexMetaData, src);
+        assertEquals("-200", indexMetaData.getSettings().get("archived.index.refresh_interval"));
+
+        src = newIndexMeta("foo", Settings.builder().put("index.codec", "best_compression1").build());
+        indexMetaData = service.archiveBrokenIndexSettings(src);
+        assertNotSame(indexMetaData, src);
+        assertEquals("best_compression1", indexMetaData.getSettings().get("archived.index.codec"));
+
+        src = newIndexMeta("foo", Settings.builder().put("index.refresh.interval", "-1").build());
+        indexMetaData = service.archiveBrokenIndexSettings(src);
+        assertNotSame(indexMetaData, src);
+        assertEquals("-1", indexMetaData.getSettings().get("archived.index.refresh.interval"));
+
+        src = newIndexMeta("foo", indexMetaData.getSettings()); // double archive?
+        indexMetaData = service.archiveBrokenIndexSettings(src);
+        assertSame(indexMetaData, src);
+    }
+
+    public void testUpgrade() {
+        MetaDataIndexUpgradeService service = new MetaDataIndexUpgradeService(Settings.EMPTY, new MapperRegistry(Collections.emptyMap(), Collections.emptyMap()), IndexScopedSettings.DEFAULT_SCOPED_SETTINGS);
+        IndexMetaData src = newIndexMeta("foo", Settings.builder().put("index.refresh_interval", "-200").build());
+        assertFalse(service.isUpgraded(src));
+        src = service.upgradeIndexMetaData(src);
+        assertTrue(service.isUpgraded(src));
+        assertEquals("-200", src.getSettings().get("archived.index.refresh_interval"));
+        assertNull(src.getSettings().get("index.refresh_interval"));
+        assertSame(src, service.upgradeIndexMetaData(src)); // no double upgrade
+    }
+
+    public void testIsUpgraded() {
+        MetaDataIndexUpgradeService service = new MetaDataIndexUpgradeService(Settings.EMPTY, new MapperRegistry(Collections.emptyMap(), Collections.emptyMap()), IndexScopedSettings.DEFAULT_SCOPED_SETTINGS);
+        IndexMetaData src = newIndexMeta("foo", Settings.builder().put("index.refresh_interval", "-200").build());
+        assertFalse(service.isUpgraded(src));
+        Version version = VersionUtils.randomVersionBetween(random(), VersionUtils.getFirstVersion(), VersionUtils.getPreviousVersion());
+        src = newIndexMeta("foo", Settings.builder().put(IndexMetaData.SETTING_VERSION_UPGRADED, version).build());
+        assertFalse(service.isUpgraded(src));
+        src = newIndexMeta("foo", Settings.builder().put(IndexMetaData.SETTING_VERSION_UPGRADED, Version.CURRENT).build());
+        assertTrue(service.isUpgraded(src));
+    }
+
+    public static IndexMetaData newIndexMeta(String name, Settings indexSettings) {
+        Settings build = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+            .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)
+            .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
+            .put(IndexMetaData.SETTING_CREATION_DATE, 1)
+            .put(IndexMetaData.SETTING_INDEX_UUID, "BOOM")
+            .put(IndexMetaData.SETTING_VERSION_UPGRADED, Version.V_0_18_1_ID)
+            .put(indexSettings)
+            .build();
+        IndexMetaData metaData = IndexMetaData.builder(name).settings(build).build();
+        return metaData;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java b/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
index 8f9c900..ed8a5cf 100644
--- a/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java
@@ -27,6 +27,7 @@ import org.apache.log4j.spi.LoggingEvent;
 import org.elasticsearch.common.logging.DeprecationLogger;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.After;
 
@@ -53,8 +54,8 @@ public class Log4jESLoggerTests extends ESTestCase {
         Path configDir = getDataPath("config");
         // Need to set custom path.conf so we can use a custom logging.yml file for the test
         Settings settings = Settings.builder()
-                .put("path.conf", configDir.toAbsolutePath())
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_CONF_SETTING.getKey(), configDir.toAbsolutePath())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         LogConfigurator.configure(settings, true);
 
diff --git a/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java b/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
index 2a08dd1..5d90eda 100644
--- a/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java
@@ -54,8 +54,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         try {
             Path configDir = getDataPath("config");
             Settings settings = Settings.builder()
-                    .put("path.conf", configDir.toAbsolutePath())
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_CONF_SETTING.getKey(), configDir.toAbsolutePath())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             LogConfigurator.configure(settings, true);
 
@@ -84,8 +84,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         Files.write(loggingConf, "{\"json\": \"foo\"}".getBytes(StandardCharsets.UTF_8));
         Environment environment = new Environment(
                 Settings.builder()
-                    .put("path.conf", tmpDir.toAbsolutePath())
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_CONF_SETTING.getKey(), tmpDir.toAbsolutePath())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build());
 
         Settings.Builder builder = Settings.builder();
@@ -101,8 +101,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         Files.write(loggingConf, "key: value".getBytes(StandardCharsets.UTF_8));
         Environment environment = new Environment(
                 Settings.builder()
-                    .put("path.conf", tmpDir.toAbsolutePath())
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_CONF_SETTING.getKey(), tmpDir.toAbsolutePath())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build());
 
         Settings.Builder builder = Settings.builder();
@@ -120,8 +120,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         Files.write(loggingConf2, "yaml: bar".getBytes(StandardCharsets.UTF_8));
         Environment environment = new Environment(
                 Settings.builder()
-                    .put("path.conf", tmpDir.toAbsolutePath())
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_CONF_SETTING.getKey(), tmpDir.toAbsolutePath())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build());
 
         Settings.Builder builder = Settings.builder();
@@ -138,8 +138,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         Files.write(invalidSuffix, "yml: bar".getBytes(StandardCharsets.UTF_8));
         Environment environment = new Environment(
                 Settings.builder()
-                    .put("path.conf", invalidSuffix.toAbsolutePath())
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_CONF_SETTING.getKey(), invalidSuffix.toAbsolutePath())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build());
 
         Settings.Builder builder = Settings.builder();
@@ -157,8 +157,8 @@ public class LoggingConfigurationTests extends ESTestCase {
         Files.write(loggingConf, "appender.file.type: file\n".getBytes(StandardCharsets.UTF_8), StandardOpenOption.APPEND);
         Environment environment = InternalSettingsPreparer.prepareEnvironment(
                 Settings.builder()
-                        .put("path.conf", tmpDir.toAbsolutePath())
-                        .put("path.home", createTempDir().toString())
+                        .put(Environment.PATH_CONF_SETTING.getKey(), tmpDir.toAbsolutePath())
+                        .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                         .put("logger.test_resolve_order", "TRACE, console")
                         .put("appender.console.type", "console")
                         .put("appender.console.layout.type", "consolePattern")
@@ -186,8 +186,8 @@ public class LoggingConfigurationTests extends ESTestCase {
                 StandardCharsets.UTF_8);
         Environment environment = InternalSettingsPreparer.prepareEnvironment(
                 Settings.builder()
-                        .put("path.conf", tmpDir.toAbsolutePath())
-                        .put("path.home", createTempDir().toString())
+                        .put(Environment.PATH_CONF_SETTING.getKey(), tmpDir.toAbsolutePath())
+                        .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                         .build(), new CliToolTestCase.MockTerminal());
         LogConfigurator.configure(environment.settings(), false);
         ESLogger esLogger = Log4jESLoggerFactory.getLogger("test_config_not_read");
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
index 82cabf7..b8a21e1 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkModuleTests.java
@@ -143,7 +143,7 @@ public class NetworkModuleTests extends ModuleTestCase {
         }
 
         // not added if http is disabled
-        settings = Settings.builder().put(NetworkModule.HTTP_ENABLED, false).build();
+        settings = Settings.builder().put(NetworkModule.HTTP_ENABLED.getKey(), false).build();
         module = new NetworkModule(new NetworkService(settings), settings, false, null);
         assertNotBound(module, HttpServerTransport.class);
     }
diff --git a/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java b/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java
index cccfa37..6f189bd 100644
--- a/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java
+++ b/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java
@@ -104,13 +104,17 @@ public class SettingTests extends ESTestCase {
         TimeValue defautlValue = TimeValue.timeValueMillis(randomIntBetween(0, 1000000));
         Setting<TimeValue> setting = Setting.positiveTimeSetting("my.time.value", defautlValue, randomBoolean(), Setting.Scope.CLUSTER);
         assertFalse(setting.isGroupSetting());
-        String aDefault = setting.getDefault(Settings.EMPTY);
+        String aDefault = setting.getDefaultRaw(Settings.EMPTY);
         assertEquals(defautlValue.millis() + "ms", aDefault);
         assertEquals(defautlValue.millis(), setting.get(Settings.EMPTY).millis());
+        assertEquals(defautlValue, setting.getDefault(Settings.EMPTY));
 
         Setting<String> secondaryDefault = new Setting<>("foo.bar", (s) -> s.get("old.foo.bar", "some_default"), (s) -> s, randomBoolean(), Setting.Scope.CLUSTER);
         assertEquals("some_default", secondaryDefault.get(Settings.EMPTY));
         assertEquals("42", secondaryDefault.get(Settings.builder().put("old.foo.bar", 42).build()));
+        Setting<String> secondaryDefaultViaSettings = new Setting<>("foo.bar", secondaryDefault, (s) -> s, randomBoolean(), Setting.Scope.CLUSTER);
+        assertEquals("some_default", secondaryDefaultViaSettings.get(Settings.EMPTY));
+        assertEquals("42", secondaryDefaultViaSettings.get(Settings.builder().put("old.foo.bar", 42).build()));
     }
 
     public void testComplexType() {
@@ -298,6 +302,26 @@ public class SettingTests extends ESTestCase {
         for (int i = 0; i < intValues.size(); i++) {
             assertEquals(i, intValues.get(i).intValue());
         }
+
+        Setting<List<String>> settingWithFallback = Setting.listSetting("foo.baz", listSetting, s -> s, true, Setting.Scope.CLUSTER);
+        value = settingWithFallback.get(Settings.EMPTY);
+        assertEquals(1, value.size());
+        assertEquals("foo,bar", value.get(0));
+
+        value = settingWithFallback.get(Settings.builder().putArray("foo.bar", "1", "2").build());
+        assertEquals(2, value.size());
+        assertEquals("1", value.get(0));
+        assertEquals("2", value.get(1));
+
+        value = settingWithFallback.get(Settings.builder().putArray("foo.baz", "3", "4").build());
+        assertEquals(2, value.size());
+        assertEquals("3", value.get(0));
+        assertEquals("4", value.get(1));
+
+        value = settingWithFallback.get(Settings.builder().putArray("foo.baz", "3", "4").putArray("foo.bar", "1", "2").build());
+        assertEquals(2, value.size());
+        assertEquals("3", value.get(0));
+        assertEquals("4", value.get(1));
     }
 
     public void testListSettingAcceptsNumberSyntax() {
diff --git a/core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java b/core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java
index 25c3a13..5d7bbb3 100644
--- a/core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java
@@ -19,6 +19,10 @@
 
 package org.elasticsearch.common.unit;
 
+import com.carrotsearch.randomizedtesting.generators.RandomStrings;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.test.ESTestCase;
 
 import static org.hamcrest.Matchers.closeTo;
@@ -73,4 +77,21 @@ public class DistanceUnitTests extends ESTestCase {
         assertEquals(7, DistanceUnit.MILES.ordinal());
         assertEquals(8, DistanceUnit.METERS.ordinal());
     }
+
+    public void testReadWrite() throws Exception {
+        for (DistanceUnit unit : DistanceUnit.values()) {
+          try (BytesStreamOutput out = new BytesStreamOutput()) {
+              unit.writeTo(out);
+              try (StreamInput in = StreamInput.wrap(out.bytes())) {
+                  assertThat("Roundtrip serialisation failed.", DistanceUnit.readDistanceUnit(in), equalTo(unit));
+              }
+          }
+        }
+    }
+
+    public void testFromString() {
+        for (DistanceUnit unit : DistanceUnit.values()) {
+            assertThat("Roundtrip string parsing failed.", DistanceUnit.fromString(unit.toString()), equalTo(unit));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/util/MultiDataPathUpgraderTests.java b/core/src/test/java/org/elasticsearch/common/util/MultiDataPathUpgraderTests.java
deleted file mode 100644
index 25c765e..0000000
--- a/core/src/test/java/org/elasticsearch/common/util/MultiDataPathUpgraderTests.java
+++ /dev/null
@@ -1,297 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.util;
-
-import org.apache.lucene.util.CollectionUtil;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.AllocationId;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.io.FileSystemUtils;
-import org.elasticsearch.common.util.set.Sets;
-import org.elasticsearch.env.NodeEnvironment;
-import org.elasticsearch.gateway.MetaDataStateFormat;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.index.shard.ShardStateMetaData;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.BufferedWriter;
-import java.io.IOException;
-import java.io.InputStream;
-import java.net.URISyntaxException;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.DirectoryStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-/**
- */
-@LuceneTestCase.SuppressFileSystems("ExtrasFS")
-public class MultiDataPathUpgraderTests extends ESTestCase {
-
-    public void testUpgradeRandomPaths() throws IOException {
-        try (NodeEnvironment nodeEnvironment = newNodeEnvironment()) {
-            final String uuid = Strings.base64UUID();
-            final ShardId shardId = new ShardId("foo", 0);
-            final Path[] shardDataPaths = nodeEnvironment.availableShardPaths(shardId);
-            if (nodeEnvironment.nodeDataPaths().length == 1) {
-                MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-                assertFalse(helper.needsUpgrading(shardId));
-                return;
-            }
-            int numIdxFiles = 0;
-            int numTranslogFiles = 0;
-            int metaStateVersion = 0;
-            for (Path shardPath : shardDataPaths) {
-                final Path translog = shardPath.resolve(ShardPath.TRANSLOG_FOLDER_NAME);
-                final Path idx = shardPath.resolve(ShardPath.INDEX_FOLDER_NAME);
-                Files.createDirectories(translog);
-                Files.createDirectories(idx);
-                int numFiles = randomIntBetween(1, 10);
-                for (int i = 0; i < numFiles; i++, numIdxFiles++) {
-                    String filename = Integer.toString(numIdxFiles);
-                    try (BufferedWriter w = Files.newBufferedWriter(idx.resolve(filename + ".tst"), StandardCharsets.UTF_8)) {
-                        w.write(filename);
-                    }
-                }
-                numFiles = randomIntBetween(1, 10);
-                for (int i = 0; i < numFiles; i++, numTranslogFiles++) {
-                    String filename = Integer.toString(numTranslogFiles);
-                    try (BufferedWriter w = Files.newBufferedWriter(translog.resolve(filename + ".translog"), StandardCharsets.UTF_8)) {
-                        w.write(filename);
-                    }
-                }
-                ++metaStateVersion;
-                ShardStateMetaData.FORMAT.write(new ShardStateMetaData(metaStateVersion, true, uuid, AllocationId.newInitializing()), metaStateVersion, shardDataPaths);
-            }
-            final Path path = randomFrom(shardDataPaths);
-            ShardPath targetPath = new ShardPath(false, path, path, uuid, new ShardId("foo", 0));
-            MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-            helper.upgrade(shardId, targetPath);
-            assertFalse(helper.needsUpgrading(shardId));
-            if (shardDataPaths.length > 1) {
-                for (Path shardPath : shardDataPaths) {
-                    if (shardPath.equals(targetPath.getDataPath())) {
-                        continue;
-                    }
-                    final Path translog = shardPath.resolve(ShardPath.TRANSLOG_FOLDER_NAME);
-                    final Path idx = shardPath.resolve(ShardPath.INDEX_FOLDER_NAME);
-                    final Path state = shardPath.resolve(MetaDataStateFormat.STATE_DIR_NAME);
-                    assertFalse(Files.exists(translog));
-                    assertFalse(Files.exists(idx));
-                    assertFalse(Files.exists(state));
-                    assertFalse(Files.exists(shardPath));
-                }
-            }
-
-            final ShardStateMetaData stateMetaData = ShardStateMetaData.FORMAT.loadLatestState(logger, targetPath.getShardStatePath());
-            assertEquals(metaStateVersion, stateMetaData.version);
-            assertTrue(stateMetaData.primary);
-            assertEquals(uuid, stateMetaData.indexUUID);
-            final Path translog = targetPath.getDataPath().resolve(ShardPath.TRANSLOG_FOLDER_NAME);
-            final Path idx = targetPath.getDataPath().resolve(ShardPath.INDEX_FOLDER_NAME);
-            Files.deleteIfExists(idx.resolve("write.lock"));
-            assertEquals(numTranslogFiles, FileSystemUtils.files(translog).length);
-            assertEquals(numIdxFiles, FileSystemUtils.files(idx).length);
-            final HashSet<Path> translogFiles = Sets.newHashSet(FileSystemUtils.files(translog));
-            for (int i = 0; i < numTranslogFiles; i++) {
-                final String name = Integer.toString(i);
-                translogFiles.contains(translog.resolve(name + ".translog"));
-                byte[] content = Files.readAllBytes(translog.resolve(name + ".translog"));
-                assertEquals(name , new String(content, StandardCharsets.UTF_8));
-            }
-            final HashSet<Path> idxFiles = Sets.newHashSet(FileSystemUtils.files(idx));
-            for (int i = 0; i < numIdxFiles; i++) {
-                final String name = Integer.toString(i);
-                idxFiles.contains(idx.resolve(name + ".tst"));
-                byte[] content = Files.readAllBytes(idx.resolve(name + ".tst"));
-                assertEquals(name , new String(content, StandardCharsets.UTF_8));
-            }
-        }
-    }
-
-    /**
-     * Run upgrade on a real bwc index
-     */
-    public void testUpgradeRealIndex() throws IOException, URISyntaxException {
-        List<Path> indexes = new ArrayList<>();
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(getBwcIndicesPath(), "index-*.zip")) {
-            for (Path path : stream) {
-                indexes.add(path);
-            }
-        }
-        CollectionUtil.introSort(indexes, new Comparator<Path>() {
-            @Override
-            public int compare(Path o1, Path o2) {
-                return o1.getFileName().compareTo(o2.getFileName());
-            }
-        });
-        final ShardId shardId = new ShardId("test", 0);
-        final Path path = randomFrom(indexes);
-        final Path indexFile = path;
-        final String indexName = indexFile.getFileName().toString().replace(".zip", "").toLowerCase(Locale.ROOT);
-        try (NodeEnvironment nodeEnvironment = newNodeEnvironment()) {
-            if (nodeEnvironment.nodeDataPaths().length == 1) {
-                MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-                assertFalse(helper.needsUpgrading(shardId));
-                return;
-            }
-            Path unzipDir = createTempDir();
-            Path unzipDataDir = unzipDir.resolve("data");
-            // decompress the index
-            try (InputStream stream = Files.newInputStream(indexFile)) {
-                TestUtil.unzip(stream, unzipDir);
-            }
-            // check it is unique
-            assertTrue(Files.exists(unzipDataDir));
-            Path[] list = FileSystemUtils.files(unzipDataDir);
-            if (list.length != 1) {
-                throw new IllegalStateException("Backwards index must contain exactly one cluster but was " + list.length);
-            }
-            // the bwc scripts packs the indices under this path
-            Path src = list[0].resolve("nodes/0/indices/" + indexName);
-            assertTrue("[" + indexFile + "] missing index dir: " + src.toString(), Files.exists(src));
-            Path[] multiDataPath = new Path[nodeEnvironment.nodeDataPaths().length];
-            int i = 0;
-            for (NodeEnvironment.NodePath nodePath : nodeEnvironment.nodePaths()) {
-                multiDataPath[i++] = nodePath.indicesPath;
-            }
-            logger.info("--> injecting index [{}] into multiple data paths", indexName);
-            OldIndexBackwardsCompatibilityIT.copyIndex(logger, src, indexName, multiDataPath);
-            final ShardPath shardPath = new ShardPath(false, nodeEnvironment.availableShardPaths(new ShardId(indexName, 0))[0], nodeEnvironment.availableShardPaths(new ShardId(indexName, 0))[0], IndexMetaData.INDEX_UUID_NA_VALUE, new ShardId(indexName, 0));
-
-            logger.info("{}", (Object)FileSystemUtils.files(shardPath.resolveIndex()));
-
-            MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-            helper.upgrade(new ShardId(indexName, 0), shardPath);
-            helper.checkIndex(shardPath);
-            assertFalse(helper.needsUpgrading(new ShardId(indexName, 0)));
-        }
-    }
-
-    public void testNeedsUpgrade() throws IOException {
-        try (NodeEnvironment nodeEnvironment = newNodeEnvironment()) {
-            String uuid = Strings.randomBase64UUID();
-            final ShardId shardId = new ShardId("foo", 0);
-            ShardStateMetaData.FORMAT.write(new ShardStateMetaData(1, true, uuid, AllocationId.newInitializing()), 1, nodeEnvironment.availableShardPaths(shardId));
-            MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-            boolean multiDataPaths = nodeEnvironment.nodeDataPaths().length > 1;
-            boolean needsUpgrading = helper.needsUpgrading(shardId);
-            if (multiDataPaths) {
-                assertTrue(needsUpgrading);
-            } else {
-                assertFalse(needsUpgrading);
-            }
-        }
-    }
-
-    public void testPickTargetShardPath() throws IOException {
-        try (NodeEnvironment nodeEnvironment = newNodeEnvironment()) {
-            final ShardId shard = new ShardId("foo", 0);
-            final Path[] paths = nodeEnvironment.availableShardPaths(shard);
-            if (paths.length == 1) {
-                MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment);
-                try {
-                    helper.pickShardPath(new ShardId("foo", 0));
-                    fail("one path needs no upgrading");
-                } catch (IllegalStateException ex) {
-                    // only one path
-                }
-            } else {
-                final Map<Path, Tuple<Long, Long>> pathToSpace = new HashMap<>();
-                final Path expectedPath;
-                if (randomBoolean()) { // path with most of the file bytes
-                    expectedPath = randomFrom(paths);
-                    long[] used = new long[paths.length];
-                    long sumSpaceUsed = 0;
-                    for (int i = 0; i < used.length; i++) {
-                        long spaceUsed = paths[i] == expectedPath ? randomIntBetween(101, 200) : randomIntBetween(10, 100);
-                        sumSpaceUsed += spaceUsed;
-                        used[i] = spaceUsed;
-                    }
-                    for (int i = 0; i < used.length; i++) {
-                        long availalbe = randomIntBetween((int)(2*sumSpaceUsed-used[i]), 4 * (int)sumSpaceUsed);
-                        pathToSpace.put(paths[i], new Tuple<>(availalbe, used[i]));
-                    }
-                } else { // path with largest available space
-                    expectedPath = randomFrom(paths);
-                    long[] used = new long[paths.length];
-                    long sumSpaceUsed = 0;
-                    for (int i = 0; i < used.length; i++) {
-                        long spaceUsed = randomIntBetween(10, 100);
-                        sumSpaceUsed += spaceUsed;
-                        used[i] = spaceUsed;
-                    }
-
-                    for (int i = 0; i < used.length; i++) {
-                        long availalbe = paths[i] == expectedPath ? randomIntBetween((int)(sumSpaceUsed), (int)(2*sumSpaceUsed)) : randomIntBetween(0, (int)(sumSpaceUsed) - 1) ;
-                        pathToSpace.put(paths[i], new Tuple<>(availalbe, used[i]));
-                    }
-
-                }
-                MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment) {
-                    @Override
-                    protected long getUsabelSpace(NodeEnvironment.NodePath path) throws IOException {
-                        return pathToSpace.get(path.resolve(shard)).v1();
-                    }
-
-                    @Override
-                    protected long getSpaceUsedByShard(Path path) throws IOException {
-                        return  pathToSpace.get(path).v2();
-                    }
-                };
-                String uuid = Strings.randomBase64UUID();
-                ShardStateMetaData.FORMAT.write(new ShardStateMetaData(1, true, uuid, AllocationId.newInitializing()), 1, paths);
-                final ShardPath shardPath = helper.pickShardPath(new ShardId("foo", 0));
-                assertEquals(expectedPath, shardPath.getDataPath());
-                assertEquals(expectedPath, shardPath.getShardStatePath());
-            }
-
-            MultiDataPathUpgrader helper = new MultiDataPathUpgrader(nodeEnvironment) {
-                @Override
-                protected long getUsabelSpace(NodeEnvironment.NodePath path) throws IOException {
-                    return randomIntBetween(0, 10);
-                }
-
-                @Override
-                protected long getSpaceUsedByShard(Path path) throws IOException {
-                    return randomIntBetween(11, 20);
-                }
-            };
-
-            try {
-                helper.pickShardPath(new ShardId("foo", 0));
-                fail("not enough space");
-            } catch (IllegalStateException ex) {
-                // not enough space
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java
index 2a1b146..2b3f918 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryModuleTests.java
@@ -46,7 +46,7 @@ public class DiscoveryModuleTests extends ModuleTestCase {
 
     public void testRegisterMasterElectionService() {
         Settings settings = Settings.builder().put("node.local", false).
-                put(DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_KEY, "custom").build();
+                put(DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING.getKey(), "custom").build();
         DiscoveryModule module = new DiscoveryModule(settings);
         module.addElectMasterService("custom", DummyMasterElectionService.class);
         assertBinding(module, ElectMasterService.class, DummyMasterElectionService.class);
@@ -55,7 +55,7 @@ public class DiscoveryModuleTests extends ModuleTestCase {
 
     public void testLoadUnregisteredMasterElectionService() {
         Settings settings = Settings.builder().put("node.local", false).
-                put(DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_KEY, "foobar").build();
+                put(DiscoveryModule.ZEN_MASTER_SERVICE_TYPE_SETTING.getKey(), "foobar").build();
         DiscoveryModule module = new DiscoveryModule(settings);
         module.addElectMasterService("custom", DummyMasterElectionService.class);
         assertBindingFailure(module, "Unknown master service type [foobar]");
@@ -71,7 +71,7 @@ public class DiscoveryModuleTests extends ModuleTestCase {
     public void testRegisterDiscovery() {
         boolean local = randomBoolean();
         Settings settings = Settings.builder().put("node.local", local).
-                put(DiscoveryModule.DISCOVERY_TYPE_KEY, "custom").build();
+                put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "custom").build();
         DiscoveryModule module = new DiscoveryModule(settings);
         module.addDiscoveryType("custom", DummyDisco.class);
         assertBinding(module, Discovery.class, DummyDisco.class);
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index e9fa8e4..f77bea8 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -45,6 +45,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.math.MathUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
@@ -163,8 +164,8 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
     }
 
     final static Settings DEFAULT_SETTINGS = Settings.builder()
-            .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // for hitting simulated network failures quickly
-            .put(FaultDetection.SETTING_PING_RETRIES, "1") // for hitting simulated network failures quickly
+            .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1s") // for hitting simulated network failures quickly
+            .put(FaultDetection.PING_RETRIES_SETTING.getKey(), "1") // for hitting simulated network failures quickly
             .put("discovery.zen.join_timeout", "10s")  // still long to induce failures but to long so test won't time out
             .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
             .put("http.enabled", false) // just to make test quicker
@@ -465,7 +466,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
                                 logger.info("[{}] Acquired semaphore and it has {} permits left", name, semaphore.availablePermits());
                                 try {
                                     id = Integer.toString(idGenerator.incrementAndGet());
-                                    int shard = Murmur3HashFunction.hash(id) % numPrimaries;
+                                    int shard = MathUtils.mod(Murmur3HashFunction.hash(id), numPrimaries);
                                     logger.trace("[{}] indexing id [{}] through node [{}] targeting shard [{}]", name, id, node, shard);
                                     IndexResponse response = client.prepareIndex("test", "type", id).setSource("{}").setTimeout("1s").get();
                                     assertThat(response.getVersion(), equalTo(1l));
@@ -961,7 +962,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         // don't wait for initial state, wat want to add the disruption while the cluster is forming..
         internalCluster().startNodesAsync(3,
                 Settings.builder()
-                        .put(DiscoveryService.SETTING_INITIAL_STATE_TIMEOUT, "1ms")
+                        .put(DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING.getKey(), "1ms")
                         .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "3s")
                         .build()).get();
 
diff --git a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
index e7a10b0..e3279d2 100644
--- a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
@@ -131,8 +131,8 @@ public class ZenFaultDetectionTests extends ESTestCase {
         Settings.Builder settings = Settings.builder();
         boolean shouldRetry = randomBoolean();
         // make sure we don't ping again after the initial ping
-        settings.put(FaultDetection.SETTING_CONNECT_ON_NETWORK_DISCONNECT, shouldRetry)
-                .put(FaultDetection.SETTING_PING_INTERVAL, "5m");
+        settings.put(FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING.getKey(), shouldRetry)
+                .put(FaultDetection.PING_INTERVAL_SETTING.getKey(), "5m");
         ClusterState clusterState = ClusterState.builder(new ClusterName("test")).nodes(buildNodesForA(true)).build();
         NodesFaultDetection nodesFDA = new NodesFaultDetection(settings.build(), threadPool, serviceA, clusterState.getClusterName());
         nodesFDA.setLocalNode(nodeA);
@@ -179,8 +179,8 @@ public class ZenFaultDetectionTests extends ESTestCase {
         Settings.Builder settings = Settings.builder();
         boolean shouldRetry = randomBoolean();
         // make sure we don't ping
-        settings.put(FaultDetection.SETTING_CONNECT_ON_NETWORK_DISCONNECT, shouldRetry)
-                .put(FaultDetection.SETTING_PING_INTERVAL, "5m");
+        settings.put(FaultDetection.CONNECT_ON_NETWORK_DISCONNECT_SETTING.getKey(), shouldRetry)
+                .put(FaultDetection.PING_INTERVAL_SETTING.getKey(), "5m");
         ClusterName clusterName = new ClusterName(randomAsciiOfLengthBetween(3, 20));
         final ClusterState state = ClusterState.builder(clusterName).nodes(buildNodesForA(false)).build();
         MasterFaultDetection masterFD = new MasterFaultDetection(settings.build(), threadPool, serviceA, clusterName,
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
index 9f9c042..eb17ab2 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
@@ -95,8 +95,8 @@ public class ZenDiscoveryIT extends ESIntegTestCase {
 
     public void testNoShardRelocationsOccurWhenElectedMasterNodeFails() throws Exception {
         Settings defaultSettings = Settings.builder()
-                .put(FaultDetection.SETTING_PING_TIMEOUT, "1s")
-                .put(FaultDetection.SETTING_PING_RETRIES, "1")
+                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1s")
+                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), "1")
                 .put("discovery.type", "zen")
                 .build();
 
@@ -142,8 +142,8 @@ public class ZenDiscoveryIT extends ESIntegTestCase {
     @TestLogging(value = "action.admin.cluster.health:TRACE")
     public void testNodeFailuresAreProcessedOnce() throws ExecutionException, InterruptedException, IOException {
         Settings defaultSettings = Settings.builder()
-                .put(FaultDetection.SETTING_PING_TIMEOUT, "1s")
-                .put(FaultDetection.SETTING_PING_RETRIES, "1")
+                .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1s")
+                .put(FaultDetection.PING_RETRIES_SETTING.getKey(), "1")
                 .put("discovery.type", "zen")
                 .build();
 
diff --git a/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java b/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
index 79f9efb..0a62d28 100644
--- a/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
+++ b/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
@@ -40,8 +40,8 @@ public class EnvironmentTests extends ESTestCase {
     public Environment newEnvironment(Settings settings) throws IOException {
         Settings build = Settings.builder()
                 .put(settings)
-                .put("path.home", createTempDir().toAbsolutePath())
-                .putArray("path.data", tmpPaths()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toAbsolutePath())
+                .putArray(Environment.PATH_DATA_SETTING.getKey(), tmpPaths()).build();
         return new Environment(build);
     }
 
@@ -49,7 +49,7 @@ public class EnvironmentTests extends ESTestCase {
         Environment environment = newEnvironment();
         assertThat(environment.resolveRepoFile("/test/repos/repo1"), nullValue());
         assertThat(environment.resolveRepoFile("test/repos/repo1"), nullValue());
-        environment = newEnvironment(settingsBuilder().putArray("path.repo", "/test/repos", "/another/repos", "/test/repos/../other").build());
+        environment = newEnvironment(settingsBuilder().putArray(Environment.PATH_REPO_SETTING.getKey(), "/test/repos", "/another/repos", "/test/repos/../other").build());
         assertThat(environment.resolveRepoFile("/test/repos/repo1"), notNullValue());
         assertThat(environment.resolveRepoFile("test/repos/repo1"), notNullValue());
         assertThat(environment.resolveRepoFile("/another/repos/repo1"), notNullValue());
diff --git a/core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java b/core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java
index acee455..1ead12f 100644
--- a/core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java
+++ b/core/src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java
@@ -50,7 +50,7 @@ public class NodeEnvironmentTests extends ESTestCase {
         NodeEnvironment env = newNodeEnvironment(Settings.builder()
                 .put("node.max_local_storage_nodes", 1).build());
         Settings settings = env.getSettings();
-        String[] dataPaths = env.getSettings().getAsArray("path.data");
+        List<String> dataPaths = Environment.PATH_DATA_SETTING.get(env.getSettings());
 
         try {
             new NodeEnvironment(settings, new Environment(settings));
@@ -62,10 +62,10 @@ public class NodeEnvironmentTests extends ESTestCase {
 
         // now can recreate and lock it
         env = new NodeEnvironment(settings, new Environment(settings));
-        assertEquals(env.nodeDataPaths().length, dataPaths.length);
+        assertEquals(env.nodeDataPaths().length, dataPaths.size());
 
-        for (int i = 0; i < dataPaths.length; i++) {
-            assertTrue(env.nodeDataPaths()[i].startsWith(PathUtils.get(dataPaths[i])));
+        for (int i = 0; i < dataPaths.size(); i++) {
+            assertTrue(env.nodeDataPaths()[i].startsWith(PathUtils.get(dataPaths.get(i))));
         }
         env.close();
         assertTrue("LockedShards: " + env.lockedShards(), env.lockedShards().isEmpty());
@@ -74,11 +74,11 @@ public class NodeEnvironmentTests extends ESTestCase {
 
     public void testNodeLockMultipleEnvironment() throws IOException {
         final NodeEnvironment first = newNodeEnvironment();
-        String[] dataPaths = first.getSettings().getAsArray("path.data");
+        List<String> dataPaths = Environment.PATH_DATA_SETTING.get(first.getSettings());
         NodeEnvironment second = new NodeEnvironment(first.getSettings(), new Environment(first.getSettings()));
-        assertEquals(first.nodeDataPaths().length, dataPaths.length);
-        assertEquals(second.nodeDataPaths().length, dataPaths.length);
-        for (int i = 0; i < dataPaths.length; i++) {
+        assertEquals(first.nodeDataPaths().length, dataPaths.size());
+        assertEquals(second.nodeDataPaths().length, dataPaths.size());
+        for (int i = 0; i < dataPaths.size(); i++) {
             assertEquals(first.nodeDataPaths()[i].getParent(), second.nodeDataPaths()[i].getParent());
         }
         IOUtils.close(first, second);
@@ -355,25 +355,25 @@ public class NodeEnvironmentTests extends ESTestCase {
     public NodeEnvironment newNodeEnvironment(Settings settings) throws IOException {
         Settings build = Settings.builder()
                 .put(settings)
-                .put("path.home", createTempDir().toAbsolutePath().toString())
-                .putArray("path.data", tmpPaths()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toAbsolutePath().toString())
+                .putArray(Environment.PATH_DATA_SETTING.getKey(), tmpPaths()).build();
         return new NodeEnvironment(build, new Environment(build));
     }
 
     public NodeEnvironment newNodeEnvironment(String[] dataPaths, Settings settings) throws IOException {
         Settings build = Settings.builder()
                 .put(settings)
-                .put("path.home", createTempDir().toAbsolutePath().toString())
-                .putArray("path.data", dataPaths).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toAbsolutePath().toString())
+                .putArray(Environment.PATH_DATA_SETTING.getKey(), dataPaths).build();
         return new NodeEnvironment(build, new Environment(build));
     }
 
     public NodeEnvironment newNodeEnvironment(String[] dataPaths, String sharedDataPath, Settings settings) throws IOException {
         Settings build = Settings.builder()
                 .put(settings)
-                .put("path.home", createTempDir().toAbsolutePath().toString())
-                .put("path.shared_data", sharedDataPath)
-                .putArray("path.data", dataPaths).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toAbsolutePath().toString())
+                .put(Environment.PATH_SHARED_DATA_SETTING.getKey(), sharedDataPath)
+                .putArray(Environment.PATH_DATA_SETTING.getKey(), dataPaths).build();
         return new NodeEnvironment(build, new Environment(build));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
index e93d8fb..e2cb4bc 100644
--- a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
@@ -28,6 +28,7 @@ import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDeci
 import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.indices.recovery.RecoveryState;
@@ -438,11 +439,11 @@ public class RecoveryFromGatewayIT extends ESIntegTestCase {
     public void testRecoveryDifferentNodeOrderStartup() throws Exception {
         // we need different data paths so we make sure we start the second node fresh
 
-        final String node_1 = internalCluster().startNode(settingsBuilder().put("path.data", createTempDir()).build());
+        final String node_1 = internalCluster().startNode(settingsBuilder().put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()).build());
 
         client().prepareIndex("test", "type1", "1").setSource("field", "value").execute().actionGet();
 
-        internalCluster().startNode(settingsBuilder().put("path.data", createTempDir()).build());
+        internalCluster().startNode(settingsBuilder().put(Environment.PATH_DATA_SETTING.getKey(), createTempDir()).build());
 
         ensureGreen();
 
diff --git a/core/src/test/java/org/elasticsearch/http/netty/HttpPublishPortIT.java b/core/src/test/java/org/elasticsearch/http/netty/HttpPublishPortIT.java
index 4d73b52..f227a9a 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/HttpPublishPortIT.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/HttpPublishPortIT.java
@@ -20,6 +20,7 @@ package org.elasticsearch.http.netty;
 
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
@@ -39,7 +40,7 @@ public class HttpPublishPortIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put(Node.HTTP_ENABLED, true)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
                 .put("http.publish_port", 9080)
                 .build();
     }
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java
index cb111a7..f02916c 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyHttpChannelTests.java
@@ -81,7 +81,7 @@ public class NettyHttpChannelTests extends ESTestCase {
     public void testCorsEnabledWithoutAllowOrigins() {
         // Set up a HTTP transport with only the CORS enabled setting
         Settings settings = Settings.builder()
-                .put(NettyHttpServerTransport.SETTING_CORS_ENABLED, true)
+                .put(NettyHttpServerTransport.SETTING_CORS_ENABLED.getKey(), true)
                 .build();
         httpServerTransport = new NettyHttpServerTransport(settings, networkService, bigArrays);
         HttpRequest httpRequest = new TestHttpRequest();
@@ -104,7 +104,7 @@ public class NettyHttpChannelTests extends ESTestCase {
     public void testCorsEnabledWithAllowOrigins() {
         // create a http transport with CORS enabled and allow origin configured
         Settings settings = Settings.builder()
-                .put(NettyHttpServerTransport.SETTING_CORS_ENABLED, true)
+                .put(NettyHttpServerTransport.SETTING_CORS_ENABLED.getKey(), true)
                 .put(NettyHttpServerTransport.SETTING_CORS_ALLOW_ORIGIN, "remote-host")
                 .build();
         httpServerTransport = new NettyHttpServerTransport(settings, networkService, bigArrays);
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
index f4ce375..8f7765d 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIT.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.http.netty;
 
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.http.HttpServerTransport;
@@ -45,7 +46,7 @@ import static org.hamcrest.Matchers.hasSize;
 public class NettyPipeliningDisabledIT extends ESIntegTestCase {
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put(Node.HTTP_ENABLED, true).put("http.pipelining", false).build();
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put(NetworkModule.HTTP_ENABLED.getKey(), true).put("http.pipelining", false).build();
     }
 
     public void testThatNettyHttpServerDoesNotSupportPipelining() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
index 9e5971c..93f54cb 100644
--- a/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
+++ b/core/src/test/java/org/elasticsearch/http/netty/NettyPipeliningEnabledIT.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.http.netty;
 
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.http.HttpServerTransport;
@@ -42,7 +43,7 @@ import static org.hamcrest.Matchers.is;
 public class NettyPipeliningEnabledIT extends ESIntegTestCase {
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put(Node.HTTP_ENABLED, true).put("http.pipelining", true).build();
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put(NetworkModule.HTTP_ENABLED.getKey(), true).put("http.pipelining", true).build();
     }
 
     public void testThatNettyHttpServerSupportsPipelining() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
index eae3e65..2665629 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
@@ -117,7 +117,7 @@ public class IndexModuleTests extends ESTestCase {
     public void setUp() throws Exception {
         super.setUp();
         index = new Index("foo");
-        settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put("path.home", createTempDir().toString()).build();
+        settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
         indexSettings = IndexSettingsModule.newIndexSettings(index, settings);
         environment = new Environment(settings);
         nodeServicesProvider = newNodeServiceProvider(settings, environment, null);
@@ -148,7 +148,12 @@ public class IndexModuleTests extends ESTestCase {
 
     public void testRegisterIndexStore() throws IOException {
         final Index index = new Index("foo");
-        final Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put("path.home", createTempDir().toString()).put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), "foo_store").build();
+        final Settings settings = Settings
+            .builder()
+            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+            .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), "foo_store")
+            .build();
         IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(index, settings);
         IndexModule module = new IndexModule(indexSettings, null, new AnalysisRegistry(null, environment));
         module.addIndexStore("foo_store", FooStore::new);
@@ -210,7 +215,7 @@ public class IndexModuleTests extends ESTestCase {
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .put("index.similarity.my_similarity.type", "test_similarity")
                 .put("index.similarity.my_similarity.key", "there is a key")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
         module.addSimilarity("test_similarity", (string, settings) -> new SimilarityProvider() {
@@ -238,7 +243,7 @@ public class IndexModuleTests extends ESTestCase {
         Settings indexSettings = Settings.settingsBuilder()
                 .put("index.similarity.my_similarity.type", "test_similarity")
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
         try {
@@ -251,7 +256,7 @@ public class IndexModuleTests extends ESTestCase {
     public void testSetupWithoutType() throws IOException {
         Settings indexSettings = Settings.settingsBuilder()
                 .put("index.similarity.my_similarity.foo", "bar")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
@@ -264,7 +269,7 @@ public class IndexModuleTests extends ESTestCase {
 
     public void testCannotRegisterProvidedImplementations() {
         Settings indexSettings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
         try {
@@ -292,7 +297,7 @@ public class IndexModuleTests extends ESTestCase {
     public void testRegisterCustomQueryCache() throws IOException {
         Settings indexSettings = Settings.settingsBuilder()
                 .put(IndexModule.INDEX_QUERY_CACHE_TYPE_SETTING.getKey(), "custom")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
         module.registerQueryCache("custom", (a, b) -> new CustomQueryCache());
@@ -310,7 +315,7 @@ public class IndexModuleTests extends ESTestCase {
 
     public void testDefaultQueryCacheImplIsSelected() throws IOException {
         Settings indexSettings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
         IndexModule module = new IndexModule(IndexSettingsModule.newIndexSettings(new Index("foo"), indexSettings), null, new AnalysisRegistry(null, environment));
         IndexService indexService = module.newIndexService(nodeEnvironment, deleter, nodeServicesProvider, mapperRegistry);
diff --git a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
index 7012ebb..5d54f77 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexWithShadowReplicasIT.java
@@ -36,6 +36,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.ShadowIndexShard;
 import org.elasticsearch.index.translog.TranslogStats;
@@ -86,7 +87,7 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
     private Settings nodeSettings(String dataPath) {
         return Settings.builder()
                 .put("node.add_id_to_custom_path", false)
-                .put("path.shared_data", dataPath)
+                .put(Environment.PATH_SHARED_DATA_SETTING.getKey(), dataPath)
                 .put("index.store.fs.fs_lock", randomFrom("native", "simple"))
                 .build();
     }
@@ -443,7 +444,7 @@ public class IndexWithShadowReplicasIT extends ESIntegTestCase {
         Path dataPath = createTempDir();
         Settings nodeSettings = Settings.builder()
                 .put("node.add_id_to_custom_path", false)
-                .put("path.shared_data", dataPath)
+                .put(Environment.PATH_SHARED_DATA_SETTING.getKey(), dataPath)
                 .build();
 
         String node1 = internalCluster().startNode(nodeSettings);
diff --git a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
index 1f08346..9dfeb44 100644
--- a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
@@ -53,8 +53,8 @@ public class TransportIndexFailuresIT extends ESIntegTestCase {
 
     private static final Settings nodeSettings = Settings.settingsBuilder()
             .put("discovery.type", "zen") // <-- To override the local setting if set externally
-            .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
-            .put(FaultDetection.SETTING_PING_RETRIES, "1") // <-- for hitting simulated network failures quickly
+            .put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
+            .put(FaultDetection.PING_RETRIES_SETTING.getKey(), "1") // <-- for hitting simulated network failures quickly
             .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
             .put("discovery.zen.minimum_master_nodes", 1)
             .build();
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java
index 17bd9d5..ba3f8b2 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.analysis;
 
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -31,7 +32,7 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 public class ASCIIFoldingTokenFilterFactoryTests extends ESTokenStreamTestCase {
     public void testDefault() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_ascii_folding.type", "asciifolding")
                 .build());
         TokenFilterFactory tokenFilter = analysisService.tokenFilter("my_ascii_folding");
@@ -44,7 +45,7 @@ public class ASCIIFoldingTokenFilterFactoryTests extends ESTokenStreamTestCase {
 
     public void testPreserveOriginal() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_ascii_folding.type", "asciifolding")
                 .put("index.analysis.filter.my_ascii_folding.preserve_original", true)
                 .build());
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java
index f844d9a..5da5415 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java
@@ -81,7 +81,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
     private Settings loadFromClasspath(String path) {
         return settingsBuilder().loadFromStream(path, getClass().getResourceAsStream(path))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
 
     }
@@ -106,7 +106,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
         String yaml = "/org/elasticsearch/index/analysis/test1.yml";
         Settings settings2 = settingsBuilder()
                 .loadFromStream(yaml, getClass().getResourceAsStream(yaml))
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_0_90_0)
                 .build();
         AnalysisRegistry newRegistry = getNewRegistry(settings2);
@@ -130,7 +130,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
     private void assertTokenFilter(String name, Class clazz) throws IOException {
         Settings settings = Settings.settingsBuilder()
                                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                               .put("path.home", createTempDir().toString()).build();
+                               .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
         TokenFilterFactory tokenFilter = analysisService.tokenFilter(name);
         Tokenizer tokenizer = new WhitespaceTokenizer();
@@ -215,7 +215,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
 
     public void testWordListPath() throws Exception {
         Settings settings = Settings.builder()
-                               .put("path.home", createTempDir().toString())
+                               .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                                .build();
         Environment env = new Environment(settings);
         String[] words = new String[]{"donau", "dampf", "schiff", "spargel", "creme", "suppe"};
@@ -243,7 +243,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
     public void testUnderscoreInAnalyzerName() throws IOException {
         Settings settings = Settings.builder()
                 .put("index.analysis.analyzer._invalid_name.tokenizer", "keyword")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, "1")
                 .build();
         try {
@@ -258,7 +258,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
         Settings settings = Settings.builder()
                 .put("index.analysis.analyzer.valid_name.tokenizer", "keyword")
                 .put("index.analysis.analyzer.valid_name.alias", "_invalid_name")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, "1")
                 .build();
         try {
@@ -275,7 +275,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
                 .put("index.analysis.analyzer.custom1.position_offset_gap", "128")
                 .put("index.analysis.analyzer.custom2.tokenizer", "standard")
                 .put("index.analysis.analyzer.custom2.position_increment_gap", "256")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(), Version.V_1_0_0,
                         Version.V_1_7_1))
                 .build();
@@ -295,7 +295,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
                 .put("index.analysis.analyzer.custom.tokenizer", "standard")
                 .put("index.analysis.analyzer.custom.position_offset_gap", "128")
                 .put("index.analysis.analyzer.custom.position_increment_gap", "256")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(), Version.V_1_0_0,
                         Version.V_1_7_1))
                 .build();
@@ -312,7 +312,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
         Settings settings = settingsBuilder()
                 .put("index.analysis.analyzer.custom.tokenizer", "standard")
                 .put("index.analysis.analyzer.custom.position_offset_gap", "128")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         try {
@@ -326,7 +326,7 @@ public class AnalysisModuleTests extends ModuleTestCase {
 
     public void testRegisterHunspellDictionary() throws Exception {
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         AnalysisModule module = new AnalysisModule(new Environment(settings));
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisServiceTests.java b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisServiceTests.java
index f467aa2..3dfb097 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisServiceTests.java
@@ -53,7 +53,11 @@ public class AnalysisServiceTests extends ESTestCase {
 
     public void testDefaultAnalyzers() throws IOException {
         Version version = VersionUtils.randomVersion(getRandom());
-        Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, version).put("path.home", createTempDir().toString()).build();
+        Settings settings = Settings
+            .builder()
+            .put(IndexMetaData.SETTING_VERSION_CREATED, version)
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+            .build();
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index("index"), settings);
         AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
         assertThat(analysisService.defaultIndexAnalyzer().analyzer(), instanceOf(StandardAnalyzer.class));
@@ -123,7 +127,7 @@ public class AnalysisServiceTests extends ESTestCase {
 
     public void testConfigureCamelCaseTokenFilter() throws IOException {
         // tests a filter that
-        Settings settings = Settings.builder().put("path.home", createTempDir().toString()).build();
+        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
         Settings indexSettings = settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .put("index.analysis.filter.wordDelimiter.type", "word_delimiter")
@@ -169,7 +173,7 @@ public class AnalysisServiceTests extends ESTestCase {
     }
 
     public void testCameCaseOverride() throws IOException {
-        Settings settings = Settings.builder().put("path.home", createTempDir().toString()).build();
+        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
         Settings indexSettings = settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .put("index.analysis.filter.wordDelimiter.type", "word_delimiter")
@@ -196,7 +200,7 @@ public class AnalysisServiceTests extends ESTestCase {
     }
 
     public void testBuiltInAnalyzersAreCached() throws IOException {
-        Settings settings = Settings.builder().put("path.home", createTempDir().toString()).build();
+        Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();
         Settings indexSettings = settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index("index"), indexSettings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java
index 1404716..7460ddd 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java
@@ -37,7 +37,7 @@ public class AnalysisTestsHelper {
     public static AnalysisService createAnalysisServiceFromClassPath(Path baseDir, String resource) throws IOException {
         Settings settings = Settings.settingsBuilder()
                 .loadFromStream(resource, AnalysisTestsHelper.class.getResourceAsStream(resource))
-                .put("path.home", baseDir.toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), baseDir.toString())
                 .build();
 
         return createAnalysisServiceFromSettings(settings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java b/core/src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java
index 63acbc8..a163d9e 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.index.analysis;
 
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -41,7 +42,7 @@ public class AnalyzerBackwardsCompatTests extends ESTokenStreamTestCase {
                 builder.put(SETTING_VERSION_CREATED, version);
             }
             builder.put("index.analysis.analyzer.foo.type", type);
-            builder.put("path.home", createTempDir().toString());
+            builder.put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString());
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(builder.build());
             NamedAnalyzer analyzer = analysisService.analyzer("foo");
             assertNotNull(analyzer);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java b/core/src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java
index dd08d47..c39c6e7 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java
@@ -40,7 +40,7 @@ public class CharFilterTests extends ESTokenStreamTestCase {
                 .putArray("index.analysis.char_filter.my_mapping.mappings", "ph=>f", "qu=>q")
                 .put("index.analysis.analyzer.custom_with_char_filter.tokenizer", "standard")
                 .putArray("index.analysis.analyzer.custom_with_char_filter.char_filter", "my_mapping")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
         AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
@@ -58,7 +58,7 @@ public class CharFilterTests extends ESTokenStreamTestCase {
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .put("index.analysis.analyzer.custom_with_char_filter.tokenizer", "standard")
                 .putArray("index.analysis.analyzer.custom_with_char_filter.char_filter", "html_strip")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
         AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java b/core/src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java
index a097d55..e00f5f6 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java
@@ -98,7 +98,7 @@ public class CompoundAnalysisTests extends ESTestCase {
         return settingsBuilder()
                 .loadFromStream(json, getClass().getResourceAsStream(json))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
     }
 
@@ -107,7 +107,7 @@ public class CompoundAnalysisTests extends ESTestCase {
         return settingsBuilder()
                 .loadFromStream(yaml, getClass().getResourceAsStream(yaml))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java
index 02c4e1a..51d8b92 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.index.analysis;
 
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -30,8 +31,8 @@ import static org.hamcrest.Matchers.is;
 public class HunspellTokenFilterFactoryTests extends ESTestCase {
     public void testDedup() throws IOException {
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", getDataPath("/indices/analyze/conf_dir"))
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/conf_dir"))
                 .put("index.analysis.filter.en_US.type", "hunspell")
                 .put("index.analysis.filter.en_US.locale", "en_US")
                 .build();
@@ -43,8 +44,8 @@ public class HunspellTokenFilterFactoryTests extends ESTestCase {
         assertThat(hunspellTokenFilter.dedup(), is(true));
 
         settings = settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", getDataPath("/indices/analyze/conf_dir"))
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/conf_dir"))
                 .put("index.analysis.filter.en_US.type", "hunspell")
                 .put("index.analysis.filter.en_US.dedup", false)
                 .put("index.analysis.filter.en_US.locale", "en_US")
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
index 99c936c..a7179da 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.analysis;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 import org.junit.Assert;
 
@@ -41,7 +42,7 @@ public class KeepFilterFactoryTests extends ESTokenStreamTestCase {
 
     public void testLoadOverConfiguredSettings() {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.broken_keep_filter.type", "keep")
                 .put("index.analysis.filter.broken_keep_filter.keep_words_path", "does/not/exists.txt")
                 .put("index.analysis.filter.broken_keep_filter.keep_words", "[\"Hello\", \"worlD\"]")
@@ -57,7 +58,7 @@ public class KeepFilterFactoryTests extends ESTokenStreamTestCase {
 
     public void testKeepWordsPathSettings() {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.non_broken_keep_filter.type", "keep")
                 .put("index.analysis.filter.non_broken_keep_filter.keep_words_path", "does/not/exists.txt")
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/KeepTypesFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/KeepTypesFilterFactoryTests.java
index 1e8a0ba..9111c92 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/KeepTypesFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/KeepTypesFilterFactoryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.analysis;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -32,7 +33,7 @@ import static org.hamcrest.Matchers.instanceOf;
 public class KeepTypesFilterFactoryTests extends ESTokenStreamTestCase {
     public void testKeepTypes() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.keep_numbers.type", "keep_types")
                 .putArray("index.analysis.filter.keep_numbers.types", new String[] {"<NUM>", "<SOMETHINGELSE>"})
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/LimitTokenCountFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/LimitTokenCountFilterFactoryTests.java
index e133ffc..b266be9 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/LimitTokenCountFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/LimitTokenCountFilterFactoryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.analysis;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -31,7 +32,7 @@ public class LimitTokenCountFilterFactoryTests extends ESTokenStreamTestCase {
     public void testDefault() throws IOException {
         Settings settings = Settings.settingsBuilder()
                 .put("index.analysis.filter.limit_default.type", "limit")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
         {
@@ -58,7 +59,7 @@ public class LimitTokenCountFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.filter.limit_1.type", "limit")
                     .put("index.analysis.filter.limit_1.max_token_count", 3)
                     .put("index.analysis.filter.limit_1.consume_all_tokens", true)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("limit_1");
@@ -73,7 +74,7 @@ public class LimitTokenCountFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.filter.limit_1.type", "limit")
                     .put("index.analysis.filter.limit_1.max_token_count", 3)
                     .put("index.analysis.filter.limit_1.consume_all_tokens", false)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("limit_1");
@@ -89,7 +90,7 @@ public class LimitTokenCountFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.filter.limit_1.type", "limit")
                     .put("index.analysis.filter.limit_1.max_token_count", 17)
                     .put("index.analysis.filter.limit_1.consume_all_tokens", true)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("limit_1");
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java b/core/src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java
index 4b7119d..8c6775a 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java
@@ -35,7 +35,7 @@ public class PatternCaptureTokenFilterTests extends ESTokenStreamTestCase {
     public void testPatternCaptureTokenFilter() throws Exception {
         String json = "/org/elasticsearch/index/analysis/pattern_capture.json";
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .loadFromStream(json, getClass().getResourceAsStream(json))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactoryTests.java
index 737a991..37844dc 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactoryTests.java
@@ -25,6 +25,7 @@ import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 import org.elasticsearch.test.VersionUtils;
 
@@ -50,7 +51,7 @@ public class StemmerTokenFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.analyzer.my_english.tokenizer","whitespace")
                     .put("index.analysis.analyzer.my_english.filter","my_english")
                     .put(SETTING_VERSION_CREATED,v)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
 
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -83,7 +84,7 @@ public class StemmerTokenFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.analyzer.my_porter2.tokenizer","whitespace")
                     .put("index.analysis.analyzer.my_porter2.filter","my_porter2")
                     .put(SETTING_VERSION_CREATED,v)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
 
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java b/core/src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java
index 90e55e9..ebaf4cb 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java
@@ -35,7 +35,7 @@ public class StopAnalyzerTests extends ESTokenStreamTestCase {
         String json = "/org/elasticsearch/index/analysis/stop.json";
         Settings settings = settingsBuilder()
             .loadFromStream(json, getClass().getResourceAsStream(json))
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index("index"), settings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/StopTokenFilterTests.java b/core/src/test/java/org/elasticsearch/index/analysis/StopTokenFilterTests.java
index 1dbd9ac..2804f52 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/StopTokenFilterTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/StopTokenFilterTests.java
@@ -28,6 +28,7 @@ import org.apache.lucene.search.suggest.analyzing.SuggestStopFilter;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.Settings.Builder;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -44,7 +45,7 @@ public class StopTokenFilterTests extends ESTokenStreamTestCase {
         if (random().nextBoolean()) {
             builder.put("index.analysis.filter.my_stop.version", "5.0");
         }
-        builder.put("path.home", createTempDir().toString());
+        builder.put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString());
         Settings settings = builder.build();
         try {
             AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -67,7 +68,7 @@ public class StopTokenFilterTests extends ESTokenStreamTestCase {
         } else {
             // don't specify
         }
-        builder.put("path.home", createTempDir().toString());
+        builder.put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString());
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(builder.build());
         TokenFilterFactory tokenFilter = analysisService.tokenFilter("my_stop");
         assertThat(tokenFilter, instanceOf(StopTokenFilterFactory.class));
@@ -86,7 +87,7 @@ public class StopTokenFilterTests extends ESTokenStreamTestCase {
                 .put("index.analysis.filter.my_stop.type", "stop")
                 .put("index.analysis.filter.my_stop.enable_position_increments", false)
                 .put("index.analysis.filter.my_stop.version", "4.3")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
         TokenFilterFactory tokenFilter = analysisService.tokenFilter("my_stop");
@@ -101,7 +102,7 @@ public class StopTokenFilterTests extends ESTokenStreamTestCase {
         Settings settings = Settings.settingsBuilder()
                 .put("index.analysis.filter.my_stop.type", "stop")
                 .put("index.analysis.filter.my_stop.remove_trailing", false)
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
         TokenFilterFactory tokenFilter = analysisService.tokenFilter("my_stop");
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactoryTests.java
index 5481002..a041694 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactoryTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.analysis;
 
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 
 import java.io.IOException;
@@ -31,7 +32,7 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase {
     public void testDefault() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .build());
         TokenFilterFactory tokenFilter = analysisService.tokenFilter("my_word_delimiter");
@@ -44,7 +45,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testCatenateWords() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.catenate_words", "true")
                 .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "false")
@@ -59,7 +60,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testCatenateNumbers() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.generate_number_parts", "false")
                 .put("index.analysis.filter.my_word_delimiter.catenate_numbers", "true")
@@ -74,7 +75,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testCatenateAll() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "false")
                 .put("index.analysis.filter.my_word_delimiter.generate_number_parts", "false")
@@ -90,7 +91,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testSplitOnCaseChange() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.split_on_case_change", "false")
                 .build());
@@ -104,7 +105,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testPreserveOriginal() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.preserve_original", "true")
                 .build());
@@ -118,7 +119,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
 
     public void testStemEnglishPossessive() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.stem_english_possessive", "false")
                 .build());
@@ -133,7 +134,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
     /** Correct offset order when doing both parts and concatenation: PowerShot is a synonym of Power */
     public void testPartsAndCatenate() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.catenate_words", "true")
                 .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true")
@@ -150,7 +151,7 @@ public class WordDelimiterTokenFilterFactoryTests extends ESTokenStreamTestCase
      * old offset order when doing both parts and concatenation: PowerShot is a synonym of Shot */
     public void testDeprecatedPartsAndCatenate() throws IOException {
         AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settingsBuilder()
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .put("index.analysis.filter.my_word_delimiter.type", "word_delimiter")
                 .put("index.analysis.filter.my_word_delimiter.catenate_words", "true")
                 .put("index.analysis.filter.my_word_delimiter.generate_word_parts", "true")
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
index f7c346c..a9d3c88 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
@@ -23,6 +23,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.AnalysisTestsHelper;
 import org.elasticsearch.index.analysis.TokenFilterFactory;
@@ -38,7 +39,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
     public void testDefault() throws IOException {
         Settings settings = Settings.settingsBuilder()
                                 .put("index.analysis.filter.common_grams_default.type", "common_grams")
-                                .put("path.home", createTempDir().toString())
+                                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                                 .build();
 
         try {
@@ -54,7 +55,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_default.type", "common_grams")
                      .putArray("index.analysis.filter.common_grams_default.common_words", "chromosome", "protein")
-                     .put("path.home", createTempDir().toString())
+                     .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                      .build();
 
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -71,7 +72,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_default.type", "common_grams")
                      .put("index.analysis.filter.common_grams_default.query_mode", false)
-                     .put("path.home", createTempDir().toString())
+                     .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                      .putArray("index.analysis.filter.common_grams_default.common_words", "chromosome", "protein")
                      .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -90,7 +91,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_1.type", "common_grams")
                     .put("index.analysis.filter.common_grams_1.ignore_case", true)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .putArray("index.analysis.filter.common_grams_1.common_words", "the", "Or", "Not", "a", "is", "an", "they", "are")
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -104,7 +105,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_2.type", "common_grams")
                     .put("index.analysis.filter.common_grams_2.ignore_case", false)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .putArray("index.analysis.filter.common_grams_2.common_words", "the", "Or", "noT", "a", "is", "an", "they", "are")
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -118,7 +119,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_3.type", "common_grams")
                     .putArray("index.analysis.filter.common_grams_3.common_words", "the", "or", "not", "a", "is", "an", "they", "are")
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("common_grams_3");
@@ -134,7 +135,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         String json = "/org/elasticsearch/index/analysis/commongrams/commongrams.json";
         Settings settings = Settings.settingsBuilder()
                      .loadFromStream(json, getClass().getResourceAsStream(json))
-                     .put("path.home", createHome())
+                     .put(Environment.PATH_HOME_SETTING.getKey(), createHome())
                      .build();
         {
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -158,7 +159,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.filter.common_grams_1.query_mode", true)
                     .putArray("index.analysis.filter.common_grams_1.common_words", "the", "Or", "Not", "a", "is", "an", "they", "are")
                     .put("index.analysis.filter.common_grams_1.ignore_case", true)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("common_grams_1");
@@ -173,7 +174,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
                     .put("index.analysis.filter.common_grams_2.query_mode", true)
                     .putArray("index.analysis.filter.common_grams_2.common_words", "the", "Or", "noT", "a", "is", "an", "they", "are")
                     .put("index.analysis.filter.common_grams_2.ignore_case", false)
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("common_grams_2");
@@ -187,7 +188,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_3.type", "common_grams")
                     .put("index.analysis.filter.common_grams_3.query_mode", true)
                     .putArray("index.analysis.filter.common_grams_3.common_words", "the", "Or", "noT", "a", "is", "an", "they", "are")
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("common_grams_3");
@@ -201,7 +202,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
             Settings settings = Settings.settingsBuilder().put("index.analysis.filter.common_grams_4.type", "common_grams")
                     .put("index.analysis.filter.common_grams_4.query_mode", true)
                     .putArray("index.analysis.filter.common_grams_4.common_words", "the", "or", "not", "a", "is", "an", "they", "are")
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             TokenFilterFactory tokenFilter = analysisService.tokenFilter("common_grams_4");
@@ -217,7 +218,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         String json = "/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json";
         Settings settings = Settings.settingsBuilder()
                 .loadFromStream(json, getClass().getResourceAsStream(json))
-            .put("path.home", createHome())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createHome())
                 .build();
         {
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java
index 3a6adca..c4c664f 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java
@@ -64,7 +64,7 @@ public class SynonymsAnalysisTests extends ESTestCase {
         String json = "/org/elasticsearch/index/analysis/synonyms/synonyms.json";
         Settings settings = settingsBuilder().
             loadFromStream(json, getClass().getResourceAsStream(json))
-                .put("path.home", home)
+                .put(Environment.PATH_HOME_SETTING.getKey(), home)
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
 
         IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index("index"), settings);
diff --git a/core/src/test/java/org/elasticsearch/index/codec/CodecTests.java b/core/src/test/java/org/elasticsearch/index/codec/CodecTests.java
index 7cfe52d..c293237 100644
--- a/core/src/test/java/org/elasticsearch/index/codec/CodecTests.java
+++ b/core/src/test/java/org/elasticsearch/index/codec/CodecTests.java
@@ -106,7 +106,7 @@ public class CodecTests extends ESTestCase {
 
     private static CodecService createCodecService() throws IOException {
         Settings nodeSettings = settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         IndexSettings settings = IndexSettingsModule.newIndexSettings(new Index("_na"), nodeSettings);
         SimilarityService similarityService = new SimilarityService(settings, Collections.emptyMap());
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index 52aa7ea..8c6bfff 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -186,7 +186,7 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
         Version version = randomBoolean() ? Version.CURRENT : VersionUtils.randomVersionBetween(random(), Version.V_2_0_0_beta1, Version.CURRENT);
         Settings settings = Settings.settingsBuilder()
                 .put("name", AbstractQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
                 .build();
         Settings indexSettings = Settings.settingsBuilder()
@@ -218,7 +218,7 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                     @Override
                     protected void configure() {
                         Settings settings = Settings.builder()
-                                .put("path.home", createTempDir())
+                                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                                 // no file watching, so we don't need a ResourceWatcherService
                                 .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
                                 .build();
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java
index b155132..30dbcdf 100644
--- a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTests.java
@@ -280,7 +280,7 @@ public class BoolQueryBuilderTests extends AbstractQueryTestCase<BoolQueryBuilde
                         .minimumNumberShouldMatch("3")
                         .disableCoord(true)
                         .buildAsBytes()).toQuery(createShardContext());
-        assertEquals(0, bq.getMinimumNumberShouldMatch());
+        assertEquals(3, bq.getMinimumNumberShouldMatch());
     }
 
     public void testFromJson() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java
index c3a2d65..911f259 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/NewPathForShardTests.java
@@ -167,8 +167,8 @@ public class NewPathForShardTests extends ESTestCase {
                                        path.resolve("b").toString()};
 
         Settings settings = Settings.builder()
-            .put("path.home", path)
-            .putArray("path.data", paths).build();
+            .put(Environment.PATH_HOME_SETTING.getKey(), path)
+            .putArray(Environment.PATH_DATA_SETTING.getKey(), paths).build();
         NodeEnvironment nodeEnv = new NodeEnvironment(settings, new Environment(settings));
 
         // Make sure all our mocking above actually worked:
diff --git a/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java b/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
index 5a82a89..80d5f4c 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/ShardPathTests.java
@@ -22,6 +22,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.AllocationId;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.IndexSettingsModule;
@@ -118,7 +119,7 @@ public class ShardPathTests extends ESTestCase {
             final Path path = createTempDir();
             final boolean includeNodeId = randomBoolean();
             indexSetttings = indexSettingsBuilder.put(IndexMetaData.SETTING_DATA_PATH, "custom").build();
-            nodeSettings = settingsBuilder().put("path.shared_data", path.toAbsolutePath().toAbsolutePath())
+            nodeSettings = settingsBuilder().put(Environment.PATH_SHARED_DATA_SETTING.getKey(), path.toAbsolutePath().toAbsolutePath())
                     .put(NodeEnvironment.ADD_NODE_ID_TO_CUSTOM_PATH, includeNodeId).build();
             if (includeNodeId) {
                 customPath = path.resolve("custom").resolve("0");
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index e193532..d0ef233 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -896,6 +896,42 @@ public class TranslogTests extends ESTestCase {
         IOUtils.close(writer);
     }
 
+    public void testFailWriterWhileClosing() throws IOException {
+        Path tempDir = createTempDir();
+        final FailSwitch fail = new FailSwitch();
+        fail.failNever();
+        TranslogConfig config = getTranslogConfig(tempDir);
+        try (Translog translog = getFailableTranslog(fail, config)) {
+            final TranslogWriter writer = translog.createWriter(0);
+            final int numOps = randomIntBetween(10, 100);
+            byte[] bytes = new byte[4];
+            ByteArrayDataOutput out = new ByteArrayDataOutput(bytes);
+            for (int i = 0; i < numOps; i++) {
+                out.reset(bytes);
+                out.writeInt(i);
+                writer.add(new BytesArray(bytes));
+            }
+            writer.sync();
+            try {
+                fail.failAlways();
+                writer.closeIntoReader();
+                fail();
+            } catch (MockDirectoryWrapper.FakeIOException ex) {
+            }
+            try (TranslogReader reader = translog.openReader(writer.path(), Checkpoint.read(translog.location().resolve(Translog.CHECKPOINT_FILE_NAME)))) {
+                for (int i = 0; i < numOps; i++) {
+                    ByteBuffer buffer = ByteBuffer.allocate(4);
+                    reader.readBytes(buffer, reader.getFirstOperationOffset() + 4 * i);
+                    buffer.flip();
+                    final int value = buffer.getInt();
+                    assertEquals(i, value);
+                }
+            }
+
+        }
+
+    }
+
     public void testBasicRecovery() throws IOException {
         List<Translog.Location> locations = new ArrayList<>();
         int translogOperations = randomIntBetween(10, 100);
diff --git a/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java b/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
index 722a4eb..c539e2a 100644
--- a/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analyze/HunspellServiceIT.java
@@ -22,6 +22,7 @@ import org.apache.lucene.analysis.hunspell.Dictionary;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.indices.analysis.HunspellService;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -39,9 +40,9 @@ import static org.hamcrest.Matchers.notNullValue;
 public class HunspellServiceIT extends ESIntegTestCase {
     public void testLocaleDirectoryWithNodeLevelConfig() throws Exception {
         Settings settings = Settings.settingsBuilder()
-                .put("path.conf", getDataPath("/indices/analyze/conf_dir"))
-                .put(HUNSPELL_LAZY_LOAD, randomBoolean())
-                .put(HUNSPELL_IGNORE_CASE, true)
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/conf_dir"))
+                .put(HUNSPELL_LAZY_LOAD.getKey(), randomBoolean())
+                .put(HUNSPELL_IGNORE_CASE.getKey(), true)
                 .build();
 
         internalCluster().startNode(settings);
@@ -52,9 +53,9 @@ public class HunspellServiceIT extends ESIntegTestCase {
 
     public void testLocaleDirectoryWithLocaleSpecificConfig() throws Exception {
         Settings settings = Settings.settingsBuilder()
-                .put("path.conf", getDataPath("/indices/analyze/conf_dir"))
-                .put(HUNSPELL_LAZY_LOAD, randomBoolean())
-                .put(HUNSPELL_IGNORE_CASE, true)
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/conf_dir"))
+                .put(HUNSPELL_LAZY_LOAD.getKey(), randomBoolean())
+                .put(HUNSPELL_IGNORE_CASE.getKey(), true)
                 .put("indices.analysis.hunspell.dictionary.en_US.strict_affix_parsing", false)
                 .put("indices.analysis.hunspell.dictionary.en_US.ignore_case", false)
                 .build();
@@ -74,8 +75,8 @@ public class HunspellServiceIT extends ESIntegTestCase {
 
     public void testDicWithNoAff() throws Exception {
         Settings settings = Settings.settingsBuilder()
-                .put("path.conf", getDataPath("/indices/analyze/no_aff_conf_dir"))
-                .put(HUNSPELL_LAZY_LOAD, randomBoolean())
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/no_aff_conf_dir"))
+                .put(HUNSPELL_LAZY_LOAD.getKey(), randomBoolean())
                 .build();
 
         Dictionary dictionary = null;
@@ -92,8 +93,8 @@ public class HunspellServiceIT extends ESIntegTestCase {
 
     public void testDicWithTwoAffs() throws Exception {
         Settings settings = Settings.settingsBuilder()
-                .put("path.conf", getDataPath("/indices/analyze/two_aff_conf_dir"))
-                .put(HUNSPELL_LAZY_LOAD, randomBoolean())
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/indices/analyze/two_aff_conf_dir"))
+                .put(HUNSPELL_LAZY_LOAD.getKey(), randomBoolean())
                 .build();
 
         Dictionary dictionary = null;
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
index 1af04e2..6cdd4cf3 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
@@ -64,11 +64,11 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
         logger.info("--> resetting breaker settings");
         Settings resetSettings = settingsBuilder()
                 .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getDefault(null))
+                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getDefaultRaw(null))
                 .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getDefault(null))
+                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getDefaultRaw(null))
                 .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getDefault(null))
+                        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getDefaultRaw(null))
                 .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.0)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(resetSettings));
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index 4bf7528..65a4d5a 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -171,7 +171,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
     @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/14932")
     public void testDeleteCreateInOneBulk() throws Exception {
         internalCluster().startNodesAsync(2, Settings.builder()
-                .put(DiscoveryModule.DISCOVERY_TYPE_KEY, "zen")
+                .put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "zen")
                 .build()).get();
         assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes("2").get().isTimedOut());
         prepareCreate("test").setSettings(IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS, true).addMapping("type").get();
diff --git a/core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java b/core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java
index 18c03e3..756a9af 100644
--- a/core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/stats/IndexStatsIT.java
@@ -78,7 +78,7 @@ public class IndexStatsIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         //Filter/Query cache is cleaned periodically, default is 60s, so make sure it runs often. Thread.sleep for 60s is bad
         return Settings.settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put(IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL, "1ms")
+                .put(IndicesRequestCache.INDICES_CACHE_REQUEST_CLEAN_INTERVAL.getKey(), "1ms")
                 .put(IndexModule.INDEX_QUERY_CACHE_EVERYTHING_SETTING.getKey(), true)
                 .put(IndexModule.INDEX_QUERY_CACHE_TYPE_SETTING.getKey(), IndexModule.INDEX_QUERY_CACHE)
                 .build();
diff --git a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
index 18d56ee..48f01ab 100644
--- a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
@@ -41,6 +41,7 @@ import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.shard.ShardId;
@@ -82,11 +83,11 @@ import static org.hamcrest.Matchers.equalTo;
 public class IndicesStoreIntegrationIT extends ESIntegTestCase {
     @Override
     protected Settings nodeSettings(int nodeOrdinal) { // simplify this and only use a single data path
-        return Settings.settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put("path.data", "")
+        return Settings.settingsBuilder().put(super.nodeSettings(nodeOrdinal)).put(Environment.PATH_DATA_SETTING.getKey(), "")
                 // by default this value is 1 sec in tests (30 sec in practice) but we adding disruption here
                 // which is between 1 and 2 sec can cause each of the shard deletion requests to timeout.
                 // to prevent this we are setting the timeout here to something highish ie. the default in practice
-                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT, new TimeValue(30, TimeUnit.SECONDS))
+                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT.getKey(), new TimeValue(30, TimeUnit.SECONDS))
                 .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
index 0a51f5d..5aaed6b 100644
--- a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
@@ -63,7 +63,6 @@ import static org.hamcrest.Matchers.nullValue;
  *
  */
 public class SimpleIndexTemplateIT extends ESIntegTestCase {
-
     public void testSimpleIndexTemplateTests() throws Exception {
         // clean all templates setup by the framework.
         client().admin().indices().prepareDeleteTemplate("*").get();
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
deleted file mode 100644
index ae724a5..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
+++ /dev/null
@@ -1,231 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.action.bulk.BulkItemResponse;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.GetPipelineRequest;
-import org.elasticsearch.action.ingest.GetPipelineResponse;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.action.ingest.SimulateDocumentBaseResult;
-import org.elasticsearch.action.ingest.SimulatePipelineRequest;
-import org.elasticsearch.action.ingest.SimulatePipelineResponse;
-import org.elasticsearch.action.ingest.WritePipelineResponse;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.node.NodeModule;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.core.Is.is;
-
-@ESIntegTestCase.ClusterScope(minNumDataNodes = 2)
-public class IngestClientIT extends ESIntegTestCase {
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        // TODO: Remove this method once gets in: https://github.com/elastic/elasticsearch/issues/16019
-        if (nodeOrdinal % 2 == 0) {
-            return Settings.builder().put("node.ingest", false).put(super.nodeSettings(nodeOrdinal)).build();
-        }
-        return super.nodeSettings(nodeOrdinal);
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return pluginList(IngestPlugin.class);
-    }
-
-    public void testSimulate() throws Exception {
-        BytesReference pipelineSource = jsonBuilder().startObject()
-            .field("description", "my_pipeline")
-            .startArray("processors")
-            .startObject()
-            .startObject("test")
-            .endObject()
-            .endObject()
-            .endArray()
-            .endObject().bytes();
-        client().preparePutPipeline("_id", pipelineSource)
-                .get();
-        GetPipelineResponse getResponse = client().prepareGetPipeline("_id")
-                .get();
-        assertThat(getResponse.isFound(), is(true));
-        assertThat(getResponse.pipelines().size(), equalTo(1));
-        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
-
-        BytesReference bytes = jsonBuilder().startObject()
-            .startArray("docs")
-            .startObject()
-            .field("_index", "index")
-            .field("_type", "type")
-            .field("_id", "id")
-            .startObject("_source")
-            .field("foo", "bar")
-            .field("fail", false)
-            .endObject()
-            .endObject()
-            .endArray()
-            .endObject().bytes();
-        SimulatePipelineResponse response;
-        if (randomBoolean()) {
-            response = client().prepareSimulatePipeline(bytes)
-                .setId("_id").get();
-        } else {
-            SimulatePipelineRequest request = new SimulatePipelineRequest(bytes);
-            request.setId("_id");
-            response = client().simulatePipeline(request).get();
-        }
-        assertThat(response.isVerbose(), equalTo(false));
-        assertThat(response.getPipelineId(), equalTo("_id"));
-        assertThat(response.getResults().size(), equalTo(1));
-        assertThat(response.getResults().get(0), instanceOf(SimulateDocumentBaseResult.class));
-        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) response.getResults().get(0);
-        Map<String, Object> source = new HashMap<>();
-        source.put("foo", "bar");
-        source.put("fail", false);
-        source.put("processed", true);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, source);
-        assertThat(simulateDocumentBaseResult.getIngestDocument().getSourceAndMetadata(), equalTo(ingestDocument.getSourceAndMetadata()));
-        assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
-    }
-
-    public void testBulkWithIngestFailures() throws Exception {
-        createIndex("index");
-
-        BytesReference source = jsonBuilder().startObject()
-            .field("description", "my_pipeline")
-            .startArray("processors")
-            .startObject()
-            .startObject("test")
-            .endObject()
-            .endObject()
-            .endArray()
-            .endObject().bytes();
-        PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
-        client().putPipeline(putPipelineRequest).get();
-
-        int numRequests = scaledRandomIntBetween(32, 128);
-        BulkRequest bulkRequest = new BulkRequest();
-        for (int i = 0; i < numRequests; i++) {
-            IndexRequest indexRequest = new IndexRequest("index", "type", Integer.toString(i)).setPipeline("_id");
-            indexRequest.source("field", "value", "fail", i % 2 == 0);
-            bulkRequest.add(indexRequest);
-        }
-
-        BulkResponse response = client().bulk(bulkRequest).actionGet();
-        assertThat(response.getItems().length, equalTo(bulkRequest.requests().size()));
-        for (int i = 0; i < bulkRequest.requests().size(); i++) {
-            BulkItemResponse itemResponse = response.getItems()[i];
-            if (i % 2 == 0) {
-                BulkItemResponse.Failure failure = itemResponse.getFailure();
-                assertThat(failure.getMessage(), equalTo("java.lang.IllegalArgumentException: test processor failed"));
-            } else {
-                IndexResponse indexResponse = itemResponse.getResponse();
-                assertThat(indexResponse.getId(), equalTo(Integer.toString(i)));
-                assertThat(indexResponse.isCreated(), is(true));
-            }
-        }
-    }
-
-    public void test() throws Exception {
-        BytesReference source = jsonBuilder().startObject()
-            .field("description", "my_pipeline")
-            .startArray("processors")
-            .startObject()
-            .startObject("test")
-            .endObject()
-            .endObject()
-            .endArray()
-            .endObject().bytes();
-        PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
-        client().putPipeline(putPipelineRequest).get();
-
-        GetPipelineRequest getPipelineRequest = new GetPipelineRequest("_id");
-        GetPipelineResponse getResponse = client().getPipeline(getPipelineRequest).get();
-        assertThat(getResponse.isFound(), is(true));
-        assertThat(getResponse.pipelines().size(), equalTo(1));
-        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
-
-        client().prepareIndex("test", "type", "1").setPipeline("_id").setSource("field", "value", "fail", false).get();
-
-        Map<String, Object> doc = client().prepareGet("test", "type", "1")
-                .get().getSourceAsMap();
-        assertThat(doc.get("field"), equalTo("value"));
-        assertThat(doc.get("processed"), equalTo(true));
-
-        client().prepareBulk().add(
-                client().prepareIndex("test", "type", "2").setSource("field", "value2", "fail", false).setPipeline("_id")).get();
-        doc = client().prepareGet("test", "type", "2").get().getSourceAsMap();
-        assertThat(doc.get("field"), equalTo("value2"));
-        assertThat(doc.get("processed"), equalTo(true));
-
-        DeletePipelineRequest deletePipelineRequest = new DeletePipelineRequest("_id");
-        WritePipelineResponse response = client().deletePipeline(deletePipelineRequest).get();
-        assertThat(response.isAcknowledged(), is(true));
-
-        getResponse = client().prepareGetPipeline("_id").get();
-        assertThat(getResponse.isFound(), is(false));
-        assertThat(getResponse.pipelines().size(), equalTo(0));
-    }
-
-    @Override
-    protected Collection<Class<? extends Plugin>> getMockPlugins() {
-        return Collections.singletonList(TestSeedPlugin.class);
-    }
-
-    public static class IngestPlugin extends Plugin {
-
-        @Override
-        public String name() {
-            return "ingest";
-        }
-
-        @Override
-        public String description() {
-            return "ingest mock";
-        }
-
-        public void onModule(NodeModule nodeModule) {
-            nodeModule.registerProcessor("test", templateService -> config ->
-                new TestProcessor("id", "test", ingestDocument -> {
-                    ingestDocument.setFieldValue("processed", true);
-                    if (ingestDocument.getFieldValue("fail", Boolean.class)) {
-                        throw new IllegalArgumentException("test processor failed");
-                    }
-                })
-            );
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java b/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java
deleted file mode 100644
index a6cf123..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IngestMetadataTests extends ESTestCase {
-
-    public void testFromXContent() throws IOException {
-        PipelineConfiguration pipeline = new PipelineConfiguration(
-            "1",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
-        );
-        PipelineConfiguration pipeline2 = new PipelineConfiguration(
-            "2",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field1\", \"value\": \"_value1\"}}]}")
-        );
-        Map<String, PipelineConfiguration> map = new HashMap<>();
-        map.put(pipeline.getId(), pipeline);
-        map.put(pipeline2.getId(), pipeline2);
-        IngestMetadata ingestMetadata = new IngestMetadata(map);
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.prettyPrint();
-        builder.startObject();
-        ingestMetadata.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        builder.endObject();
-        String string = builder.string();
-        final XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(string);
-        MetaData.Custom custom = ingestMetadata.fromXContent(parser);
-        assertTrue(custom instanceof IngestMetadata);
-        IngestMetadata m = (IngestMetadata) custom;
-        assertEquals(2, m.getPipelines().size());
-        assertEquals("1", m.getPipelines().get("1").getId());
-        assertEquals("2", m.getPipelines().get("2").getId());
-        assertEquals(pipeline.getConfigAsMap(), m.getPipelines().get("1").getConfigAsMap());
-        assertEquals(pipeline2.getConfigAsMap(), m.getPipelines().get("2").getConfigAsMap());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
deleted file mode 100644
index 9126a51..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
+++ /dev/null
@@ -1,366 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.bulk.BulkRequest;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.ingest.core.CompoundProcessor;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.hamcrest.CustomTypeSafeMatcher;
-import org.junit.Before;
-import org.mockito.ArgumentMatcher;
-import org.mockito.invocation.InvocationOnMock;
-
-import java.util.Collections;
-import java.util.Map;
-import java.util.Objects;
-import java.util.function.BiConsumer;
-import java.util.function.Consumer;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.mockito.Matchers.eq;
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.anyBoolean;
-import static org.mockito.Matchers.anyString;
-import static org.mockito.Matchers.argThat;
-import static org.mockito.Mockito.doAnswer;
-import static org.mockito.Mockito.doThrow;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.never;
-import static org.mockito.Mockito.times;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-
-public class PipelineExecutionServiceTests extends ESTestCase {
-
-    private PipelineStore store;
-    private PipelineExecutionService executionService;
-
-    @Before
-    public void setup() {
-        store = mock(PipelineStore.class);
-        ThreadPool threadPool = mock(ThreadPool.class);
-        when(threadPool.executor(anyString())).thenReturn(Runnable::run);
-        executionService = new PipelineExecutionService(store, threadPool);
-    }
-
-    public void testExecuteIndexPipelineDoesNotExist() {
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        try {
-            executionService.execute(indexRequest, failureHandler, completionHandler);
-            fail("IllegalArgumentException expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("pipeline with id [_id] does not exist"));
-        }
-        verify(failureHandler, never()).accept(any(Throwable.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteBulkPipelineDoesNotExist() {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-        BulkRequest bulkRequest = new BulkRequest();
-
-        IndexRequest indexRequest1 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        bulkRequest.add(indexRequest1);
-        IndexRequest indexRequest2 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("does_not_exist");
-        bulkRequest.add(indexRequest2);
-        @SuppressWarnings("unchecked")
-        BiConsumer<IndexRequest, Throwable> failureHandler = mock(BiConsumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), failureHandler, completionHandler);
-        verify(failureHandler, times(1)).accept(
-            argThat(new CustomTypeSafeMatcher<IndexRequest>("failure handler was not called with the expected arguments") {
-                @Override
-                protected boolean matchesSafely(IndexRequest item) {
-                    return item == indexRequest2;
-                }
-
-            }),
-            argThat(new CustomTypeSafeMatcher<IllegalArgumentException>("failure handler was not called with the expected arguments") {
-                @Override
-                protected boolean matchesSafely(IllegalArgumentException iae) {
-                    return "pipeline with id [does_not_exist] does not exist".equals(iae.getMessage());
-                }
-            })
-        );
-        verify(completionHandler, times(1)).accept(null);
-    }
-
-    public void testExecuteSuccess() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecutePropagateAllMetaDataUpdates() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        doAnswer((InvocationOnMock invocationOnMock) -> {
-            IngestDocument ingestDocument = (IngestDocument) invocationOnMock.getArguments()[0];
-            for (IngestDocument.MetaData metaData : IngestDocument.MetaData.values()) {
-                if (metaData == IngestDocument.MetaData.TTL) {
-                    ingestDocument.setFieldValue(IngestDocument.MetaData.TTL.getFieldName(), "5w");
-                } else {
-                    ingestDocument.setFieldValue(metaData.getFieldName(), "update" + metaData.getFieldName());
-                }
-
-            }
-            return null;
-        }).when(processor).execute(any());
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(any());
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-
-        assertThat(indexRequest.index(), equalTo("update_index"));
-        assertThat(indexRequest.type(), equalTo("update_type"));
-        assertThat(indexRequest.id(), equalTo("update_id"));
-        assertThat(indexRequest.routing(), equalTo("update_routing"));
-        assertThat(indexRequest.parent(), equalTo("update_parent"));
-        assertThat(indexRequest.timestamp(), equalTo("update_timestamp"));
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(3024000000L)));
-    }
-
-    public void testExecuteFailure() throws Exception {
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteSuccessWithOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, never()).accept(any(RuntimeException.class));
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecuteFailureWithOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteFailureWithNestedOnFailure() throws Exception {
-        Processor processor = mock(Processor.class);
-        Processor onFailureProcessor = mock(Processor.class);
-        Processor onFailureOnFailureProcessor = mock(Processor.class);
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor),
-            Collections.singletonList(new CompoundProcessor(Collections.singletonList(onFailureProcessor), Collections.singletonList(onFailureOnFailureProcessor))));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        doThrow(new RuntimeException()).when(onFailureOnFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
-        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteSetTTL() throws Exception {
-        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "5d"));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-
-        assertThat(indexRequest.ttl(), equalTo(TimeValue.parseTimeValue("5d", null, "ttl")));
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testExecuteSetInvalidTTL() throws Exception {
-        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "abc"));
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-        verify(failureHandler, times(1)).accept(any(ElasticsearchParseException.class));
-        verify(completionHandler, never()).accept(anyBoolean());
-    }
-
-    public void testExecuteProvidedTTL() throws Exception {
-        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", mock(CompoundProcessor.class)));
-
-        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id")
-                .source(Collections.emptyMap())
-                .ttl(1000L);
-        Consumer<Throwable> failureHandler = mock(Consumer.class);
-        Consumer<Boolean> completionHandler = mock(Consumer.class);
-        executionService.execute(indexRequest, failureHandler, completionHandler);
-
-        assertThat(indexRequest.ttl(), equalTo(new TimeValue(1000L)));
-        verify(failureHandler, never()).accept(any());
-        verify(completionHandler, times(1)).accept(true);
-    }
-
-    public void testBulkRequestExecutionWithFailures() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        String pipelineId = "_id";
-
-        int numRequest = scaledRandomIntBetween(8, 64);
-        int numIndexRequests = 0;
-        for (int i = 0; i < numRequest; i++) {
-            ActionRequest request;
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    request = new DeleteRequest("_index", "_type", "_id");
-                } else {
-                    request = new UpdateRequest("_index", "_type", "_id");
-                }
-            } else {
-                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline(pipelineId);
-                indexRequest.source("field1", "value1");
-                request = indexRequest;
-                numIndexRequests++;
-            }
-            bulkRequest.add(request);
-        }
-
-        CompoundProcessor processor = mock(CompoundProcessor.class);
-        Exception error = new RuntimeException();
-        doThrow(error).when(processor).execute(any());
-        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, processor));
-
-        BiConsumer<IndexRequest, Throwable> requestItemErrorHandler = mock(BiConsumer.class);
-        Consumer<Throwable> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
-
-        verify(requestItemErrorHandler, times(numIndexRequests)).accept(any(IndexRequest.class), eq(error));
-        verify(completionHandler, times(1)).accept(null);
-    }
-
-    public void testBulkRequestExecution() throws Exception {
-        BulkRequest bulkRequest = new BulkRequest();
-        String pipelineId = "_id";
-
-        int numRequest = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < numRequest; i++) {
-            IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline(pipelineId);
-            indexRequest.source("field1", "value1");
-            bulkRequest.add(indexRequest);
-        }
-
-        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, new CompoundProcessor()));
-
-        @SuppressWarnings("unchecked")
-        BiConsumer<IndexRequest, Throwable> requestItemErrorHandler = mock(BiConsumer.class);
-        @SuppressWarnings("unchecked")
-        Consumer<Throwable> completionHandler = mock(Consumer.class);
-        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
-
-        verify(requestItemErrorHandler, never()).accept(any(), any());
-        verify(completionHandler, times(1)).accept(null);
-    }
-
-    private IngestDocument eqID(String index, String type, String id, Map<String, Object> source) {
-        return argThat(new IngestDocumentMatcher(index, type, id, source));
-    }
-
-    private class IngestDocumentMatcher extends ArgumentMatcher<IngestDocument> {
-
-        private final IngestDocument ingestDocument;
-
-        public IngestDocumentMatcher(String index, String type, String id, Map<String, Object> source) {
-            this.ingestDocument = new IngestDocument(index, type, id, null, null, null, null, source);
-        }
-
-        @Override
-        public boolean matches(Object o) {
-            if (o.getClass() == IngestDocument.class) {
-                IngestDocument otherIngestDocument = (IngestDocument) o;
-                //ingest metadata will not be the same (timestamp differs every time)
-                return Objects.equals(ingestDocument.getSourceAndMetadata(), otherIngestDocument.getSourceAndMetadata());
-            }
-            return false;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
deleted file mode 100644
index 117b95b..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ResourceNotFoundException;
-import org.elasticsearch.action.ingest.DeletePipelineRequest;
-import org.elasticsearch.action.ingest.PutPipelineRequest;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.ingest.core.Pipeline;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.mockito.Mockito.mock;
-
-public class PipelineStoreTests extends ESTestCase {
-
-    private PipelineStore store;
-
-    @Before
-    public void init() throws Exception {
-        store = new PipelineStore(Settings.EMPTY);
-        ProcessorsRegistry registry = new ProcessorsRegistry();
-        registry.registerProcessor("set", (templateService) -> new SetProcessor.Factory(TestTemplateService.instance()));
-        store.buildProcessorFactoryRegistry(registry, null);
-    }
-
-    public void testUpdatePipelines() {
-        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build();
-        store.innerUpdatePipelines(clusterState);
-        assertThat(store.pipelines.size(), is(0));
-
-        PipelineConfiguration pipeline = new PipelineConfiguration(
-            "_id",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
-        );
-        IngestMetadata ingestMetadata = new IngestMetadata(Collections.singletonMap("_id", pipeline));
-        clusterState = ClusterState.builder(clusterState)
-            .metaData(MetaData.builder().putCustom(IngestMetadata.TYPE, ingestMetadata))
-            .build();
-        store.innerUpdatePipelines(clusterState);
-        assertThat(store.pipelines.size(), is(1));
-        assertThat(store.pipelines.get("_id").getId(), equalTo("_id"));
-        assertThat(store.pipelines.get("_id").getDescription(), nullValue());
-        assertThat(store.pipelines.get("_id").getProcessors().size(), equalTo(1));
-        assertThat(store.pipelines.get("_id").getProcessors().get(0).getType(), equalTo("set"));
-    }
-
-    public void testPut() {
-        String id = "_id";
-        Pipeline pipeline = store.get(id);
-        assertThat(pipeline, nullValue());
-        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build();
-
-        // add a new pipeline:
-        PutPipelineRequest putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": []}"));
-        clusterState = store.innerPut(putRequest, clusterState);
-        store.innerUpdatePipelines(clusterState);
-        pipeline = store.get(id);
-        assertThat(pipeline, notNullValue());
-        assertThat(pipeline.getId(), equalTo(id));
-        assertThat(pipeline.getDescription(), nullValue());
-        assertThat(pipeline.getProcessors().size(), equalTo(0));
-
-        // overwrite existing pipeline:
-        putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": [], \"description\": \"_description\"}"));
-        clusterState = store.innerPut(putRequest, clusterState);
-        store.innerUpdatePipelines(clusterState);
-        pipeline = store.get(id);
-        assertThat(pipeline, notNullValue());
-        assertThat(pipeline.getId(), equalTo(id));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(0));
-    }
-
-    public void testDelete() {
-        PipelineConfiguration config = new PipelineConfiguration(
-            "_id",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
-        );
-        IngestMetadata ingestMetadata = new IngestMetadata(Collections.singletonMap("_id", config));
-        ClusterState clusterState = ClusterState.builder(new ClusterName("_name"))
-            .metaData(MetaData.builder().putCustom(IngestMetadata.TYPE, ingestMetadata))
-            .build();
-        store.innerUpdatePipelines(clusterState);
-        assertThat(store.get("_id"), notNullValue());
-
-        // Delete pipeline:
-        DeletePipelineRequest deleteRequest = new DeletePipelineRequest("_id");
-        clusterState = store.innerDelete(deleteRequest, clusterState);
-        store.innerUpdatePipelines(clusterState);
-        assertThat(store.get("_id"), nullValue());
-
-        // Delete existing pipeline:
-        try {
-            store.innerDelete(deleteRequest, clusterState);
-            fail("exception expected");
-        } catch (ResourceNotFoundException e) {
-            assertThat(e.getMessage(), equalTo("pipeline [_id] is missing"));
-        }
-    }
-
-    public void testGetPipelines() {
-        Map<String, PipelineConfiguration> configs = new HashMap<>();
-        configs.put("_id1", new PipelineConfiguration(
-            "_id1", new BytesArray("{\"processors\": []}")
-        ));
-        configs.put("_id2", new PipelineConfiguration(
-            "_id2", new BytesArray("{\"processors\": []}")
-        ));
-
-        assertThat(store.innerGetPipelines(null, "_id1").isEmpty(), is(true));
-
-        IngestMetadata ingestMetadata = new IngestMetadata(configs);
-        List<PipelineConfiguration> pipelines = store.innerGetPipelines(ingestMetadata, "_id1");
-        assertThat(pipelines.size(), equalTo(1));
-        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
-
-        pipelines = store.innerGetPipelines(ingestMetadata, "_id1", "_id2");
-        assertThat(pipelines.size(), equalTo(2));
-        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
-        assertThat(pipelines.get(1).getId(), equalTo("_id2"));
-
-        pipelines = store.innerGetPipelines(ingestMetadata, "_id*");
-        pipelines.sort((o1, o2) -> o1.getId().compareTo(o2.getId()));
-        assertThat(pipelines.size(), equalTo(2));
-        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
-        assertThat(pipelines.get(1).getId(), equalTo("_id2"));
-    }
-
-    public void testCrud() throws Exception {
-        String id = "_id";
-        Pipeline pipeline = store.get(id);
-        assertThat(pipeline, nullValue());
-        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build(); // Start empty
-
-        PutPipelineRequest putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}"));
-        clusterState = store.innerPut(putRequest, clusterState);
-        store.innerUpdatePipelines(clusterState);
-        pipeline = store.get(id);
-        assertThat(pipeline, notNullValue());
-        assertThat(pipeline.getId(), equalTo(id));
-        assertThat(pipeline.getDescription(), nullValue());
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("set"));
-
-        DeletePipelineRequest deleteRequest = new DeletePipelineRequest(id);
-        clusterState = store.innerDelete(deleteRequest, clusterState);
-        store.innerUpdatePipelines(clusterState);
-        pipeline = store.get(id);
-        assertThat(pipeline, nullValue());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java b/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
deleted file mode 100644
index ad18488..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Map;
-import java.util.Set;
-import java.util.function.Function;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class ProcessorsRegistryTests extends ESTestCase {
-
-    public void testAddProcessor() {
-        ProcessorsRegistry processorsRegistry = new ProcessorsRegistry();
-        TestProcessor.Factory factory1 = new TestProcessor.Factory();
-        processorsRegistry.registerProcessor("1", (templateService) -> factory1);
-        TestProcessor.Factory factory2 = new TestProcessor.Factory();
-        processorsRegistry.registerProcessor("2", (templateService) -> factory2);
-        TestProcessor.Factory factory3 = new TestProcessor.Factory();
-        try {
-            processorsRegistry.registerProcessor("1", (templateService) -> factory3);
-            fail("addProcessor should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Processor factory already registered for name [1]"));
-        }
-
-        Set<Map.Entry<String, Function<TemplateService, Processor.Factory<?>>>> entrySet = processorsRegistry.entrySet();
-        assertThat(entrySet.size(), equalTo(2));
-        for (Map.Entry<String, Function<TemplateService, Processor.Factory<?>>> entry : entrySet) {
-            if (entry.getKey().equals("1")) {
-                assertThat(entry.getValue().apply(null), equalTo(factory1));
-            } else if (entry.getKey().equals("2")) {
-                assertThat(entry.getValue().apply(null), equalTo(factory2));
-            } else {
-                fail("unexpected processor id [" + entry.getKey() + "]");
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
deleted file mode 100644
index f21644e..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.processor.AppendProcessor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.elasticsearch.ingest.processor.SplitProcessor;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-public class CompoundProcessorTests extends ESTestCase {
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void init() {
-        ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-    }
-
-    public void testEmpty() throws Exception {
-        CompoundProcessor processor = new CompoundProcessor();
-        assertThat(processor.getProcessors().isEmpty(), is(true));
-        assertThat(processor.getOnFailureProcessors().isEmpty(), is(true));
-        processor.execute(ingestDocument);
-    }
-
-    public void testSingleProcessor() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {});
-        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
-        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
-        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
-        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
-        compoundProcessor.execute(ingestDocument);
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithException() throws Exception {
-        TestProcessor processor = new TestProcessor(ingestDocument -> {throw new RuntimeException("error");});
-        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
-        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
-        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
-        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
-        try {
-            compoundProcessor.execute(ingestDocument);
-            fail("should throw exception");
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("error"));
-        }
-        assertThat(processor.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithOnFailureProcessor() throws Exception {
-        TestProcessor processor1 = new TestProcessor("id", "first", ingestDocument -> {throw new RuntimeException("error");});
-        TestProcessor processor2 = new TestProcessor(ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
-        });
-
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor1), Collections.singletonList(processor2));
-        compoundProcessor.execute(ingestDocument);
-
-        assertThat(processor1.getInvokedCounter(), equalTo(1));
-        assertThat(processor2.getInvokedCounter(), equalTo(1));
-    }
-
-    public void testSingleProcessorWithNestedFailures() throws Exception {
-        TestProcessor processor = new TestProcessor("id", "first", ingestDocument -> {throw new RuntimeException("error");});
-        TestProcessor processorToFail = new TestProcessor("id", "second", ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
-            throw new RuntimeException("error");
-        });
-        TestProcessor lastProcessor = new TestProcessor(ingestDocument -> {
-            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
-            assertThat(ingestMetadata.size(), equalTo(2));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
-            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("second"));
-        });
-        CompoundProcessor compoundOnFailProcessor = new CompoundProcessor(Collections.singletonList(processorToFail), Collections.singletonList(lastProcessor));
-        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(compoundOnFailProcessor));
-        compoundProcessor.execute(ingestDocument);
-
-        assertThat(processorToFail.getInvokedCounter(), equalTo(1));
-        assertThat(lastProcessor.getInvokedCounter(), equalTo(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
deleted file mode 100644
index 958378f..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-
-public class ConfigurationUtilsTests extends ESTestCase {
-    private Map<String, Object> config;
-
-    @Before
-    public void setConfig() {
-        config = new HashMap<>();
-        config.put("foo", "bar");
-        config.put("arr", Arrays.asList("1", "2", "3"));
-        List<Integer> list = new ArrayList<>();
-        list.add(2);
-        config.put("int", list);
-        config.put("ip", "127.0.0.1");
-        Map<String, Object> fizz = new HashMap<>();
-        fizz.put("buzz", "hello world");
-        config.put("fizz", fizz);
-    }
-
-    public void testReadStringProperty() {
-        String val = ConfigurationUtils.readStringProperty(config, "foo");
-        assertThat(val, equalTo("bar"));
-    }
-
-    public void testReadStringPropertyInvalidType() {
-        try {
-            ConfigurationUtils.readStringProperty(config, "arr");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [arr] isn't a string, but of type [java.util.Arrays$ArrayList]"));
-        }
-    }
-
-    // TODO(talevy): Issue with generics. This test should fail, "int" is of type List<Integer>
-    public void testOptional_InvalidType() {
-        List<String> val = ConfigurationUtils.readList(config, "int");
-        assertThat(val, equalTo(Arrays.asList(2)));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java b/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
deleted file mode 100644
index 56d1fa7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
+++ /dev/null
@@ -1,976 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.both;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class IngestDocumentTests extends ESTestCase {
-
-    private IngestDocument ingestDocument;
-
-    @Before
-    public void setIngestDocument() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        document.put("foo", "bar");
-        document.put("int", 123);
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("buzz", "hello world");
-        innerObject.put("foo_null", null);
-        innerObject.put("1", "bar");
-        List<String> innerInnerList = new ArrayList<>();
-        innerInnerList.add("item1");
-        List<Object> innerList = new ArrayList<>();
-        innerList.add(innerInnerList);
-        innerObject.put("list", innerList);
-        document.put("fizz", innerObject);
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-
-        document.put("list", list);
-        ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-    }
-
-    public void testSimpleGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_source.foo", String.class), equalTo("bar"));
-        assertThat(ingestDocument.getFieldValue("_source.int", Integer.class), equalTo(123));
-        assertThat(ingestDocument.getFieldValue("_index", String.class), equalTo("index"));
-        assertThat(ingestDocument.getFieldValue("_type", String.class), equalTo("type"));
-        assertThat(ingestDocument.getFieldValue("_id", String.class), equalTo("id"));
-        assertThat(ingestDocument.getFieldValue("_ingest.timestamp", String.class), both(notNullValue()).and(not(equalTo("bogus_timestamp"))));
-        assertThat(ingestDocument.getFieldValue("_source._ingest.timestamp", String.class), equalTo("bogus_timestamp"));
-    }
-
-    public void testGetSourceObject() {
-        try {
-            ingestDocument.getFieldValue("_source", Object.class);
-            fail("get field value should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testGetIngestObject() {
-        assertThat(ingestDocument.getFieldValue("_ingest", Map.class), notNullValue());
-    }
-
-    public void testGetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.getFieldValue("_source.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("_ingest.", Object.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testGetFieldValueNullValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.foo_null", Object.class), nullValue());
-    }
-
-    public void testSimpleGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("int", String.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [int] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-
-        try {
-            ingestDocument.getFieldValue("foo", Integer.class);
-            fail("getFieldValue should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [foo] of type [java.lang.String] cannot be cast to [java.lang.Integer]"));
-        }
-    }
-
-    public void testNestedGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("fizz.buzz", String.class), equalTo("hello world"));
-        assertThat(ingestDocument.getFieldValue("fizz.1", String.class), equalTo("bar"));
-    }
-
-    public void testNestedGetFieldValueTypeMismatch() {
-        try {
-            ingestDocument.getFieldValue("foo.foo.bar", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testListGetFieldValue() {
-        assertThat(ingestDocument.getFieldValue("list.0.field", String.class), equalTo("value"));
-    }
-
-    public void testListGetFieldValueNull() {
-        assertThat(ingestDocument.getFieldValue("list.1", String.class), nullValue());
-    }
-
-    public void testListGetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.getFieldValue("list.test.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListGetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.getFieldValue("list.10.field", String.class);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testGetFieldValueNotFound() {
-        try {
-            ingestDocument.getFieldValue("not.here", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [not] not present as part of path [not.here]"));
-        }
-    }
-
-    public void testGetFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.getFieldValue("fizz.foo_null.not_there", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testGetFieldValueNull() {
-        try {
-            ingestDocument.getFieldValue(null, String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testGetFieldValueEmpty() {
-        try {
-            ingestDocument.getFieldValue("", String.class);
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasField() {
-        assertTrue(ingestDocument.hasField("fizz"));
-        assertTrue(ingestDocument.hasField("_index"));
-        assertTrue(ingestDocument.hasField("_type"));
-        assertTrue(ingestDocument.hasField("_id"));
-        assertTrue(ingestDocument.hasField("_source.fizz"));
-        assertTrue(ingestDocument.hasField("_ingest.timestamp"));
-    }
-
-    public void testHasFieldNested() {
-        assertTrue(ingestDocument.hasField("fizz.buzz"));
-        assertTrue(ingestDocument.hasField("_source._ingest.timestamp"));
-    }
-
-    public void testListHasField() {
-        assertTrue(ingestDocument.hasField("list.0.field"));
-    }
-
-    public void testListHasFieldNull() {
-        assertTrue(ingestDocument.hasField("list.1"));
-    }
-
-    public void testListHasFieldIndexOutOfBounds() {
-        assertFalse(ingestDocument.hasField("list.10"));
-    }
-
-    public void testListHasFieldIndexNotNumeric() {
-        assertFalse(ingestDocument.hasField("list.test"));
-    }
-
-    public void testNestedHasFieldTypeMismatch() {
-        assertFalse(ingestDocument.hasField("foo.foo.bar"));
-    }
-
-    public void testHasFieldNotFound() {
-        assertFalse(ingestDocument.hasField("not.here"));
-    }
-
-    public void testHasFieldNotFoundNullParent() {
-        assertFalse(ingestDocument.hasField("fizz.foo_null.not_there"));
-    }
-
-    public void testHasFieldNestedNotFound() {
-        assertFalse(ingestDocument.hasField("fizz.doesnotexist"));
-    }
-
-    public void testHasFieldNull() {
-        try {
-            ingestDocument.hasField(null);
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldNullValue() {
-        assertTrue(ingestDocument.hasField("fizz.foo_null"));
-    }
-
-    public void testHasFieldEmpty() {
-        try {
-            ingestDocument.hasField("");
-            fail("has field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testHasFieldSourceObject() {
-        assertThat(ingestDocument.hasField("_source"), equalTo(false));
-    }
-
-    public void testHasFieldIngestObject() {
-        assertThat(ingestDocument.hasField("_ingest"), equalTo(true));
-    }
-
-    public void testHasFieldEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.hasField("_source.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.hasField("_ingest.");
-            fail("has field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testSimpleSetFieldValue() {
-        ingestDocument.setFieldValue("new_field", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), equalTo("foo"));
-        ingestDocument.setFieldValue("_ttl", "ttl");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ttl"), equalTo("ttl"));
-        ingestDocument.setFieldValue("_source.another_field", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("another_field"), equalTo("bar"));
-        ingestDocument.setFieldValue("_ingest.new_field", "new_value");
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(2));
-        assertThat(ingestDocument.getIngestMetadata().get("new_field"), equalTo("new_value"));
-        ingestDocument.setFieldValue("_ingest.timestamp", "timestamp");
-        assertThat(ingestDocument.getIngestMetadata().get("timestamp"), equalTo("timestamp"));
-    }
-
-    public void testSetFieldValueNullValue() {
-        ingestDocument.setFieldValue("new_field", null);
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(true));
-        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), nullValue());
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testNestedSetFieldValue() {
-        ingestDocument.setFieldValue("a.b.c.d", "foo");
-        assertThat(ingestDocument.getSourceAndMetadata().get("a"), instanceOf(Map.class));
-        Map<String, Object> a = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("a");
-        assertThat(a.get("b"), instanceOf(Map.class));
-        Map<String, Object> b = (Map<String, Object>) a.get("b");
-        assertThat(b.get("c"), instanceOf(Map.class));
-        Map<String, Object> c = (Map<String, Object>) b.get("c");
-        assertThat(c.get("d"), instanceOf(String.class));
-        String d = (String) c.get("d");
-        assertThat(d, equalTo("foo"));
-    }
-
-    public void testSetFieldValueOnExistingField() {
-        ingestDocument.setFieldValue("foo", "newbar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("foo"), equalTo("newbar"));
-    }
-
-    @SuppressWarnings("unchecked")
-    public void testSetFieldValueOnExistingParent() {
-        ingestDocument.setFieldValue("fizz.new", "bar");
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        Map<String, Object> innerMap = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(innerMap.get("new"), instanceOf(String.class));
-        String value = (String) innerMap.get("new");
-        assertThat(value, equalTo("bar"));
-    }
-
-    public void testSetFieldValueOnExistingParentTypeMismatch() {
-        try {
-            ingestDocument.setFieldValue("fizz.buzz.new", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [new] with parent object of type [java.lang.String] as part of path [fizz.buzz.new]"));
-        }
-    }
-
-    public void testSetFieldValueOnExistingNullParent() {
-        try {
-            ingestDocument.setFieldValue("fizz.foo_null.test", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [test] with null parent as part of path [fizz.foo_null.test]"));
-        }
-    }
-
-    public void testSetFieldValueNullName() {
-        try {
-            ingestDocument.setFieldValue(null, "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testSetSourceObject() {
-        ingestDocument.setFieldValue("_source", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetIngestObject() {
-        ingestDocument.setFieldValue("_ingest", "value");
-        assertThat(ingestDocument.getSourceAndMetadata().get("_ingest"), equalTo("value"));
-    }
-
-    public void testSetIngestSourceObject() {
-        //test that we don't strip out the _source prefix when _ingest is used
-        ingestDocument.setFieldValue("_ingest._source", "value");
-        assertThat(ingestDocument.getIngestMetadata().get("_source"), equalTo("value"));
-    }
-
-    public void testSetEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.setFieldValue("_source.", "value");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("_ingest.", "_value");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListSetFieldValueNoIndexProvided() {
-        ingestDocument.setFieldValue("list", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(String.class));
-        assertThat(object, equalTo("value"));
-    }
-
-    public void testListAppendFieldValue() {
-        ingestDocument.appendFieldValue("list", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo("new_value"));
-    }
-
-    public void testListAppendFieldValues() {
-        ingestDocument.appendFieldValue("list", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(5));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo("item1"));
-        assertThat(list.get(3), equalTo("item2"));
-        assertThat(list.get(4), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueToNonExistingList() {
-        ingestDocument.appendFieldValue("non_existing_list", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        assertThat(list.get(0), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesToNonExistingList() {
-        ingestDocument.appendFieldValue("non_existing_list", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo("item1"));
-        assertThat(list.get(1), equalTo("item2"));
-        assertThat(list.get(2), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueConvertStringToList() {
-        ingestDocument.appendFieldValue("fizz.buzz", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("buzz");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo("hello world"));
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesConvertStringToList() {
-        ingestDocument.appendFieldValue("fizz.buzz", Arrays.asList("item1", "item2", "item3"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("buzz");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(4));
-        assertThat(list.get(0), equalTo("hello world"));
-        assertThat(list.get(1), equalTo("item1"));
-        assertThat(list.get(2), equalTo("item2"));
-        assertThat(list.get(3), equalTo("item3"));
-    }
-
-    public void testAppendFieldValueConvertIntegerToList() {
-        ingestDocument.appendFieldValue("int", 456);
-        Object object = ingestDocument.getSourceAndMetadata().get("int");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(123));
-        assertThat(list.get(1), equalTo(456));
-    }
-
-    public void testAppendFieldValuesConvertIntegerToList() {
-        ingestDocument.appendFieldValue("int", Arrays.asList(456, 789));
-        Object object = ingestDocument.getSourceAndMetadata().get("int");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(123));
-        assertThat(list.get(1), equalTo(456));
-        assertThat(list.get(2), equalTo(789));
-    }
-
-    public void testAppendFieldValueConvertMapToList() {
-        ingestDocument.appendFieldValue("fizz", Collections.singletonMap("field", "value"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) list.get(0);
-        assertThat(map.size(), equalTo(4));
-        assertThat(list.get(1), equalTo(Collections.singletonMap("field", "value")));
-    }
-
-    public void testAppendFieldValueToNull() {
-        ingestDocument.appendFieldValue("fizz.foo_null", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("foo_null");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), nullValue());
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValueToListElement() {
-        ingestDocument.appendFieldValue("fizz.list.0", "item2");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerList = (List<String>) object;
-        assertThat(innerList.size(), equalTo(2));
-        assertThat(innerList.get(0), equalTo("item1"));
-        assertThat(innerList.get(1), equalTo("item2"));
-    }
-
-    public void testAppendFieldValuesToListElement() {
-        ingestDocument.appendFieldValue("fizz.list.0", Arrays.asList("item2", "item3", "item4"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerList = (List<String>) object;
-        assertThat(innerList.size(), equalTo(4));
-        assertThat(innerList.get(0), equalTo("item1"));
-        assertThat(innerList.get(1), equalTo("item2"));
-        assertThat(innerList.get(2), equalTo("item3"));
-        assertThat(innerList.get(3), equalTo("item4"));
-    }
-
-    public void testAppendFieldValueConvertStringListElementToList() {
-        ingestDocument.appendFieldValue("fizz.list.0.0", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> innerList = (List<Object>) object;
-        object = innerList.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerInnerList = (List<String>) object;
-        assertThat(innerInnerList.size(), equalTo(2));
-        assertThat(innerInnerList.get(0), equalTo("item1"));
-        assertThat(innerInnerList.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValuesConvertStringListElementToList() {
-        ingestDocument.appendFieldValue("fizz.list.0.0", Arrays.asList("item2", "item3", "item4"));
-        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        object = map.get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(1));
-        object = list.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> innerList = (List<Object>) object;
-        object = innerList.get(0);
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> innerInnerList = (List<String>) object;
-        assertThat(innerInnerList.size(), equalTo(4));
-        assertThat(innerInnerList.get(0), equalTo("item1"));
-        assertThat(innerInnerList.get(1), equalTo("item2"));
-        assertThat(innerInnerList.get(2), equalTo("item3"));
-        assertThat(innerInnerList.get(3), equalTo("item4"));
-    }
-
-    public void testAppendFieldValueListElementConvertMapToList() {
-        ingestDocument.appendFieldValue("list.0", Collections.singletonMap("item2", "value2"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), instanceOf(List.class));
-        assertThat(list.get(1), nullValue());
-        list = (List<?>) list.get(0);
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), equalTo(Collections.singletonMap("item2", "value2")));
-    }
-
-    public void testAppendFieldValueToNullListElement() {
-        ingestDocument.appendFieldValue("list.1", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        List<?> list = (List<?>) object;
-        assertThat(list.get(1), instanceOf(List.class));
-        list = (List<?>) list.get(1);
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), nullValue());
-        assertThat(list.get(1), equalTo("new_value"));
-    }
-
-    public void testAppendFieldValueToListOfMaps() {
-        ingestDocument.appendFieldValue("list", Collections.singletonMap("item2", "value2"));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(3));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), nullValue());
-        assertThat(list.get(2), equalTo(Collections.singletonMap("item2", "value2")));
-    }
-
-    public void testListSetFieldValueIndexProvided() {
-        ingestDocument.setFieldValue("list.1", "value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
-        assertThat(list.get(1), equalTo("value"));
-    }
-
-    public void testSetFieldValueListAsPartOfPath() {
-        ingestDocument.setFieldValue("list.0.field", "new_value");
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "new_value")));
-        assertThat(list.get(1), nullValue());
-    }
-
-    public void testListSetFieldValueIndexNotNumeric() {
-        try {
-            ingestDocument.setFieldValue("list.test", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.test.field", "new_value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
-        }
-    }
-
-    public void testListSetFieldValueIndexOutOfBounds() {
-        try {
-            ingestDocument.setFieldValue("list.10", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-
-        try {
-            ingestDocument.setFieldValue("list.10.field", "value");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
-        }
-    }
-
-    public void testSetFieldValueEmptyName() {
-        try {
-            ingestDocument.setFieldValue("", "bar");
-            fail("add field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveField() {
-        ingestDocument.removeField("foo");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("foo"), equalTo(false));
-        ingestDocument.removeField("_index");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(6));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_index"), equalTo(false));
-        ingestDocument.removeField("_source.fizz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(false));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(1));
-        ingestDocument.removeField("_ingest.timestamp");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
-        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(0));
-    }
-
-    public void testRemoveInnerField() {
-        ingestDocument.removeField("fizz.buzz");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
-        assertThat(map.size(), equalTo(3));
-        assertThat(map.containsKey("buzz"), equalTo(false));
-
-        ingestDocument.removeField("fizz.foo_null");
-        assertThat(map.size(), equalTo(2));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-
-        ingestDocument.removeField("fizz.1");
-        assertThat(map.size(), equalTo(1));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-
-        ingestDocument.removeField("fizz.list");
-        assertThat(map.size(), equalTo(0));
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
-    }
-
-    public void testRemoveNonExistingField() {
-        try {
-            ingestDocument.removeField("does_not_exist");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [does_not_exist] not present as part of path [does_not_exist]"));
-        }
-    }
-
-    public void testRemoveExistingParentTypeMismatch() {
-        try {
-            ingestDocument.removeField("foo.foo.bar");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
-        }
-    }
-
-    public void testRemoveSourceObject() {
-        try {
-            ingestDocument.removeField("_source");
-            fail("remove field should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
-        }
-    }
-
-    public void testRemoveIngestObject() {
-        ingestDocument.removeField("_ingest");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_ingest"), equalTo(false));
-    }
-
-    public void testRemoveEmptyPathAfterStrippingOutPrefix() {
-        try {
-            ingestDocument.removeField("_source.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
-        }
-
-        try {
-            ingestDocument.removeField("_ingest.");
-            fail("set field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
-        }
-    }
-
-    public void testListRemoveField() {
-        ingestDocument.removeField("list.0.field");
-        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
-        assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-        Object object = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(object, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<Object> list = (List<Object>) object;
-        assertThat(list.size(), equalTo(2));
-        object = list.get(0);
-        assertThat(object, instanceOf(Map.class));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> map = (Map<String, Object>) object;
-        assertThat(map.size(), equalTo(0));
-        ingestDocument.removeField("list.0");
-        assertThat(list.size(), equalTo(1));
-        assertThat(list.get(0), nullValue());
-    }
-
-    public void testRemoveFieldValueNotFoundNullParent() {
-        try {
-            ingestDocument.removeField("fizz.foo_null.not_there");
-            fail("get field value should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [not_there] from null as part of path [fizz.foo_null.not_there]"));
-        }
-    }
-
-    public void testNestedRemoveFieldTypeMismatch() {
-        try {
-            ingestDocument.removeField("fizz.1.bar");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot remove [bar] from object of type [java.lang.String] as part of path [fizz.1.bar]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexNotNumeric() {
-        try {
-            ingestDocument.removeField("list.test");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
-        }
-    }
-
-    public void testListRemoveFieldIndexOutOfBounds() {
-        try {
-            ingestDocument.removeField("list.10");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
-        }
-    }
-
-    public void testRemoveNullField() {
-        try {
-            ingestDocument.removeField((String) null);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testRemoveEmptyField() {
-        try {
-            ingestDocument.removeField("");
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
-        }
-    }
-
-    public void testEqualsAndHashcode() throws Exception {
-        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
-        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-        for (int i = 0; i < numFields; i++) {
-            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-        }
-        Map<String, String> ingestMetadata = new HashMap<>();
-        numFields = randomIntBetween(1, 5);
-        for (int i = 0; i < numFields; i++) {
-            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-        }
-        IngestDocument ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
-
-        boolean changed = false;
-        Map<String, Object> otherSourceAndMetadata;
-        if (randomBoolean()) {
-            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
-            changed = true;
-        } else {
-            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
-        }
-        if (randomBoolean()) {
-            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
-            for (int i = 0; i < numFields; i++) {
-                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        }
-
-        Map<String, String> otherIngestMetadata;
-        if (randomBoolean()) {
-            otherIngestMetadata = new HashMap<>();
-            numFields = randomIntBetween(1, 5);
-            for (int i = 0; i < numFields; i++) {
-                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
-            }
-            changed = true;
-        } else {
-            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
-        }
-
-        IngestDocument otherIngestDocument = new IngestDocument(otherSourceAndMetadata, otherIngestMetadata);
-        if (changed) {
-            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
-            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
-        } else {
-            assertThat(ingestDocument, equalTo(otherIngestDocument));
-            assertThat(otherIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
-            IngestDocument thirdIngestDocument = new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata));
-            assertThat(thirdIngestDocument, equalTo(ingestDocument));
-            assertThat(ingestDocument, equalTo(thirdIngestDocument));
-            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
-        }
-    }
-
-    public void testIngestMetadataTimestamp() throws Exception {
-        long before = System.currentTimeMillis();
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        long after = System.currentTimeMillis();
-        String timestampString = ingestDocument.getIngestMetadata().get("timestamp");
-        assertThat(timestampString, notNullValue());
-        assertThat(timestampString, endsWith("+0000"));
-        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
-        Date timestamp = df.parse(timestampString);
-        assertThat(timestamp.getTime(), greaterThanOrEqualTo(before));
-        assertThat(timestamp.getTime(), lessThanOrEqualTo(after));
-    }
-
-    public void testCopyConstructor() {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        IngestDocument copy = new IngestDocument(ingestDocument);
-        assertThat(ingestDocument.getSourceAndMetadata(), not(sameInstance(copy.getSourceAndMetadata())));
-        assertThat(ingestDocument.getSourceAndMetadata(), equalTo(copy.getSourceAndMetadata()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
deleted file mode 100644
index 2292903..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.TestProcessor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-
-public class PipelineFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        Map<String, Object> processorConfig0 = new HashMap<>();
-        Map<String, Object> processorConfig1 = new HashMap<>();
-        processorConfig0.put(AbstractProcessorFactory.TAG_KEY, "first-processor");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
-        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Arrays.asList(Collections.singletonMap("test", processorConfig0), Collections.singletonMap("test", processorConfig1)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(2));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
-        assertThat(pipeline.getProcessors().get(0).getTag(), equalTo("first-processor"));
-        assertThat(pipeline.getProcessors().get(1).getType(), equalTo("test-processor"));
-        assertThat(pipeline.getProcessors().get(1).getTag(), nullValue());
-    }
-
-    public void testCreateWithPipelineOnFailure() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
-        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        pipelineConfig.put(Pipeline.ON_FAILURE_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
-        assertThat(pipeline.getOnFailureProcessors().size(), equalTo(1));
-        assertThat(pipeline.getOnFailureProcessors().get(0).getType(), equalTo("test-processor"));
-    }
-
-    public void testCreateUnusedProcessorOptions() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put("unused", "value");
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
-        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        try {
-            factory.create("_id", pipelineConfig, processorRegistry);
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
-        }
-    }
-
-    public void testCreateProcessorsWithOnFailureProperties() throws Exception {
-        Map<String, Object> processorConfig = new HashMap<>();
-        processorConfig.put(Pipeline.ON_FAILURE_KEY, Collections.singletonList(Collections.singletonMap("test", new HashMap<>())));
-
-        Map<String, Object> pipelineConfig = new HashMap<>();
-        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
-        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
-        Pipeline.Factory factory = new Pipeline.Factory();
-        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
-        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
-        assertThat(pipeline.getId(), equalTo("_id"));
-        assertThat(pipeline.getDescription(), equalTo("_description"));
-        assertThat(pipeline.getProcessors().size(), equalTo(1));
-        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("compound"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
deleted file mode 100644
index f2aa9f3..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.core;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class ValueSourceTests extends ESTestCase {
-
-    public void testDeepCopy() {
-        int iterations = scaledRandomIntBetween(8, 64);
-        for (int i = 0; i < iterations; i++) {
-            Map<String, Object> map = RandomDocumentPicks.randomSource(random());
-            ValueSource valueSource = ValueSource.wrap(map, TestTemplateService.instance());
-            Object copy = valueSource.copyAndResolve(Collections.emptyMap());
-            assertThat("iteration: " + i, copy, equalTo(map));
-            assertThat("iteration: " + i, copy, not(sameInstance(map)));
-        }
-    }
-
-    public void testCopyDoesNotChangeProvidedMap() {
-        Map<String, Object> myPreciousMap = new HashMap<>();
-        myPreciousMap.put("field2", "value2");
-
-        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousMap, TestTemplateService.instance()));
-        ingestDocument.removeField("field1.field2");
-
-        assertThat(myPreciousMap.size(), equalTo(1));
-        assertThat(myPreciousMap.get("field2"), equalTo("value2"));
-    }
-
-    public void testCopyDoesNotChangeProvidedList() {
-        List<String> myPreciousList = new ArrayList<>();
-        myPreciousList.add("value");
-
-        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
-        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousList, TestTemplateService.instance()));
-        ingestDocument.removeField("field1.0");
-
-        assertThat(myPreciousList.size(), equalTo(1));
-        assertThat(myPreciousList.get(0), equalTo("value"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java b/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
deleted file mode 100644
index 1113a4b..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class AbstractStringProcessorTestCase extends ESTestCase {
-
-    protected abstract AbstractStringProcessor newProcessor(String field);
-
-    protected String modifyInput(String input) {
-        return input;
-    }
-
-    protected abstract String expectedResult(String input);
-
-    public void testProcessor() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldValue = RandomDocumentPicks.randomString(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, modifyInput(fieldValue));
-        Processor processor = newProcessor(fieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult(fieldValue)));
-    }
-
-    public void testFieldNotFound() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testNullValue() throws Exception {
-        Processor processor = newProcessor("field");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot process it."));
-        }
-    }
-
-    public void testNonStringValue() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = newProcessor(fieldName);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        try {
-            processor.execute(ingestDocument);
-            fail("processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
deleted file mode 100644
index b72c144..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class AppendProcessorFactoryTests extends ESTestCase {
-
-    private AppendProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new AppendProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        Object value;
-        if (randomBoolean()) {
-            value = "value1";
-        } else {
-            value = Arrays.asList("value1", "value2", "value3");
-        }
-        config.put("value", value);
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        AppendProcessor appendProcessor = factory.create(config);
-        assertThat(appendProcessor.getTag(), equalTo(processorTag));
-        assertThat(appendProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-        assertThat(appendProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo(value));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("value", "value1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoValuePresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-    public void testCreateNullValue() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", null);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
deleted file mode 100644
index 4a78ba6..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.not;
-import static org.hamcrest.CoreMatchers.sameInstance;
-
-public class AppendProcessorTests extends ESTestCase {
-
-    public void testAppendValuesToExistingList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        List<Object> list = new ArrayList<>();
-        int size = randomIntBetween(0, 10);
-        for (int i = 0; i < size; i++) {
-            list.add(scalar.randomValue());
-        }
-        List<Object> checkList = new ArrayList<>(list);
-        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, list);
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        Object fieldValue = ingestDocument.getFieldValue(field, Object.class);
-        assertThat(fieldValue, sameInstance(list));
-        assertThat(list.size(), equalTo(size + values.size()));
-        for (int i = 0; i < size; i++) {
-            assertThat(list.get(i), equalTo(checkList.get(i)));
-        }
-        for (int i = size; i < size + values.size(); i++) {
-            assertThat(list.get(i), equalTo(values.get(i - size)));
-        }
-    }
-
-    public void testAppendValuesToNonExistingList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String field = RandomDocumentPicks.randomFieldName(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        List list = ingestDocument.getFieldValue(field, List.class);
-        assertThat(list, not(sameInstance(values)));
-        assertThat(list, equalTo(values));
-    }
-
-    public void testConvertScalarToList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Scalar scalar = randomFrom(Scalar.values());
-        Object initialValue = scalar.randomValue();
-        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, initialValue);
-        List<Object> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            Object value = scalar.randomValue();
-            values.add(value);
-            appendProcessor = createAppendProcessor(field, value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(scalar.randomValue());
-            }
-            appendProcessor = createAppendProcessor(field, values);
-        }
-        appendProcessor.execute(ingestDocument);
-        List fieldValue = ingestDocument.getFieldValue(field, List.class);
-        assertThat(fieldValue.size(), equalTo(values.size() + 1));
-        assertThat(fieldValue.get(0), equalTo(initialValue));
-        for (int i = 1; i < values.size() + 1; i++) {
-            assertThat(fieldValue.get(i), equalTo(values.get(i - 1)));
-        }
-    }
-
-    public void testAppendMetadata() throws Exception {
-        //here any metadata field value becomes a list, which won't make sense in most of the cases,
-        // but support for append is streamlined like for set so we test it
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        List<String> values = new ArrayList<>();
-        Processor appendProcessor;
-        if (randomBoolean()) {
-            String value = randomAsciiOfLengthBetween(1, 10);
-            values.add(value);
-            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), value);
-        } else {
-            int valuesSize = randomIntBetween(0, 10);
-            for (int i = 0; i < valuesSize; i++) {
-                values.add(randomAsciiOfLengthBetween(1, 10));
-            }
-            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), values);
-        }
-
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Object initialValue = ingestDocument.getSourceAndMetadata().get(randomMetaData.getFieldName());
-        appendProcessor.execute(ingestDocument);
-        List list = ingestDocument.getFieldValue(randomMetaData.getFieldName(), List.class);
-        if (initialValue == null) {
-            assertThat(list, equalTo(values));
-        } else {
-            assertThat(list.size(), equalTo(values.size() + 1));
-            assertThat(list.get(0), equalTo(initialValue));
-            for (int i = 1; i < list.size(); i++) {
-                assertThat(list.get(i), equalTo(values.get(i - 1)));
-            }
-        }
-    }
-
-    private static Processor createAppendProcessor(String fieldName, Object fieldValue) {
-        TemplateService templateService = TestTemplateService.instance();
-        return new AppendProcessor(randomAsciiOfLength(10), templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
-    }
-
-    private enum Scalar {
-        INTEGER {
-            @Override
-            Object randomValue() {
-                return randomInt();
-            }
-        }, DOUBLE {
-            @Override
-            Object randomValue() {
-                return randomDouble();
-            }
-        }, FLOAT {
-            @Override
-            Object randomValue() {
-                return randomFloat();
-            }
-        }, BOOLEAN {
-            @Override
-            Object randomValue() {
-                return randomBoolean();
-            }
-        }, STRING {
-            @Override
-            Object randomValue() {
-                return randomAsciiOfLengthBetween(1, 10);
-            }
-        }, MAP {
-            @Override
-            Object randomValue() {
-                int numItems = randomIntBetween(1, 10);
-                Map<String, Object> map = new HashMap<>(numItems);
-                for (int i = 0; i < numItems; i++) {
-                    map.put(randomAsciiOfLengthBetween(1, 10), randomFrom(Scalar.values()).randomValue());
-                }
-                return map;
-            }
-        }, NULL {
-            @Override
-            Object randomValue() {
-                return null;
-            }
-        };
-
-        abstract Object randomValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
deleted file mode 100644
index 7064331..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class ConvertProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        ConvertProcessor.Type type = randomFrom(ConvertProcessor.Type.values());
-        config.put("field", "field1");
-        config.put("type", type.toString());
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        ConvertProcessor convertProcessor = factory.create(config);
-        assertThat(convertProcessor.getTag(), equalTo(processorTag));
-        assertThat(convertProcessor.getField(), equalTo("field1"));
-        assertThat(convertProcessor.getConvertType(), equalTo(type));
-    }
-
-    public void testCreateUnsupportedType() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("field", "field1");
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("type [" + type + "] not supported, cannot convert field."));
-        }
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
-        config.put("type", type);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoTypePresent() throws Exception {
-        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), Matchers.equalTo("required property [type] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
deleted file mode 100644
index 1350eba..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
+++ /dev/null
@@ -1,268 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.elasticsearch.ingest.processor.ConvertProcessor.Type;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ConvertProcessorTests extends ESTestCase {
-
-    public void testConvertInt() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int randomInt = randomInt();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomInt);
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Integer.class), equalTo(randomInt));
-    }
-
-    public void testConvertIntList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Integer> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            int randomInt = randomInt();
-            fieldValue.add(Integer.toString(randomInt));
-            expectedList.add(randomInt);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertIntError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to integer"));
-        }
-    }
-
-    public void testConvertFloat() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Map<String, Float> expectedResult = new HashMap<>();
-        float randomFloat = randomFloat();
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomFloat);
-        expectedResult.put(fieldName, randomFloat);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Float.class), equalTo(randomFloat));
-    }
-
-    public void testConvertFloatList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Float> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            float randomFloat = randomFloat();
-            fieldValue.add(Float.toString(randomFloat));
-            expectedList.add(randomFloat);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertFloatError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
-        ingestDocument.setFieldValue(fieldName, value);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to float"));
-        }
-    }
-
-    public void testConvertBoolean() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        boolean randomBoolean = randomBoolean();
-        String booleanString = Boolean.toString(randomBoolean);
-        if (randomBoolean) {
-            booleanString = booleanString.toUpperCase(Locale.ROOT);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, booleanString);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, Boolean.class), equalTo(randomBoolean));
-    }
-
-    public void testConvertBooleanList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<String> fieldValue = new ArrayList<>();
-        List<Boolean> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            boolean randomBoolean = randomBoolean();
-            String booleanString = Boolean.toString(randomBoolean);
-            if (randomBoolean) {
-                booleanString = booleanString.toUpperCase(Locale.ROOT);
-            }
-            fieldValue.add(booleanString);
-            expectedList.add(randomBoolean);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertBooleanError() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        String fieldValue;
-        if (randomBoolean()) {
-            fieldValue = "string-" + randomAsciiOfLengthBetween(1, 10);
-        } else {
-            //verify that only proper boolean values are supported and we are strict about it
-            fieldValue = randomFrom("on", "off", "yes", "no", "0", "1");
-        }
-        ingestDocument.setFieldValue(fieldName, fieldValue);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(Exception e) {
-            assertThat(e.getMessage(), equalTo("[" + fieldValue + "] is not a boolean value, cannot convert to boolean"));
-        }
-    }
-
-    public void testConvertString() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        Object fieldValue;
-        String expectedFieldValue;
-        switch(randomIntBetween(0, 2)) {
-            case 0:
-                float randomFloat = randomFloat();
-                fieldValue = randomFloat;
-                expectedFieldValue = Float.toString(randomFloat);
-                break;
-            case 1:
-                int randomInt = randomInt();
-                fieldValue = randomInt;
-                expectedFieldValue = Integer.toString(randomInt);
-                break;
-            case 2:
-                boolean randomBoolean = randomBoolean();
-                fieldValue = randomBoolean;
-                expectedFieldValue = Boolean.toString(randomBoolean);
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedFieldValue));
-    }
-
-    public void testConvertStringList() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        List<Object> fieldValue = new ArrayList<>();
-        List<String> expectedList = new ArrayList<>();
-        for (int j = 0; j < numItems; j++) {
-            Object randomValue;
-            String randomValueString;
-            switch(randomIntBetween(0, 2)) {
-                case 0:
-                    float randomFloat = randomFloat();
-                    randomValue = randomFloat;
-                    randomValueString = Float.toString(randomFloat);
-                    break;
-                case 1:
-                    int randomInt = randomInt();
-                    randomValue = randomInt;
-                    randomValueString = Integer.toString(randomInt);
-                    break;
-                case 2:
-                    boolean randomBoolean = randomBoolean();
-                    randomValue = randomBoolean;
-                    randomValueString = Boolean.toString(randomBoolean);
-                    break;
-                default:
-                    throw new UnsupportedOperationException();
-            }
-            fieldValue.add(randomValue);
-            expectedList.add(randomValueString);
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.STRING);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
-    }
-
-    public void testConvertNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testConvertNullField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Type type = randomFrom(Type.values());
-        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), "field", type);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Field [field] is null, cannot be converted to type [" + type + "]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
deleted file mode 100644
index 401dd44..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.time.Instant;
-import java.time.ZoneId;
-import java.time.format.DateTimeFormatter;
-import java.util.Locale;
-import java.util.function.Function;
-
-import static org.hamcrest.core.IsEqual.equalTo;
-
-public class DateFormatTests extends ESTestCase {
-
-    public void testParseJoda() {
-        Function<String, DateTime> jodaFunction = DateFormat.Joda.getFunction("MMM dd HH:mm:ss Z", DateTimeZone.forOffsetHours(-8), Locale.ENGLISH);
-        assertThat(Instant.ofEpochMilli(jodaFunction.apply("Nov 24 01:29:01 -0800").getMillis())
-                        .atZone(ZoneId.of("GMT-8"))
-                        .format(DateTimeFormatter.ofPattern("MM dd HH:mm:ss", Locale.ENGLISH)),
-                equalTo("11 24 01:29:01"));
-    }
-
-    public void testParseUnixMs() {
-        assertThat(DateFormat.UnixMs.getFunction(null, DateTimeZone.UTC, null).apply("1000500").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseUnix() {
-        assertThat(DateFormat.Unix.getFunction(null, DateTimeZone.UTC, null).apply("1000.5").getMillis(), equalTo(1000500L));
-    }
-
-    public void testParseISO8601() {
-        assertThat(DateFormat.Iso8601.getFunction(null, DateTimeZone.UTC, null).apply("2001-01-01T00:00:00-0800").getMillis(), equalTo(978336000000L));
-    }
-
-    public void testParseISO8601Failure() {
-        Function<String, DateTime> function = DateFormat.Iso8601.getFunction(null, DateTimeZone.UTC, null);
-        try {
-            function.apply("2001-01-0:00-0800");
-            fail("parse should have failed");
-        } catch(IllegalArgumentException e) {
-            //all good
-        }
-    }
-
-    public void testTAI64NParse() {
-        String input = "4000000050d506482dbdf024";
-        String expected = "2012-12-22T03:00:46.767+02:00";
-        assertThat(DateFormat.Tai64n.getFunction(null, DateTimeZone.forOffsetHours(2), null).apply((randomBoolean() ? "@" : "") + input).toString(), equalTo(expected));
-    }
-
-    public void testFromString() {
-        assertThat(DateFormat.fromString("UNIX_MS"), equalTo(DateFormat.UnixMs));
-        assertThat(DateFormat.fromString("unix_ms"), equalTo(DateFormat.Joda));
-        assertThat(DateFormat.fromString("UNIX"), equalTo(DateFormat.Unix));
-        assertThat(DateFormat.fromString("unix"), equalTo(DateFormat.Joda));
-        assertThat(DateFormat.fromString("ISO8601"), equalTo(DateFormat.Iso8601));
-        assertThat(DateFormat.fromString("iso8601"), equalTo(DateFormat.Joda));
-        assertThat(DateFormat.fromString("TAI64N"), equalTo(DateFormat.Tai64n));
-        assertThat(DateFormat.fromString("tai64n"), equalTo(DateFormat.Joda));
-        assertThat(DateFormat.fromString("prefix-" + randomAsciiOfLengthBetween(1, 10)), equalTo(DateFormat.Joda));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
deleted file mode 100644
index a145a7c..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTimeZone;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class DateProcessorFactoryTests extends ESTestCase {
-
-    public void testBuildDefaults() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTag(), equalTo(processorTag));
-        assertThat(processor.getMatchField(), equalTo(sourceField));
-        assertThat(processor.getTargetField(), equalTo(DateProcessor.DEFAULT_TARGET_FIELD));
-        assertThat(processor.getMatchFormats(), equalTo(Collections.singletonList("dd/MM/yyyyy")));
-        assertThat(processor.getLocale(), equalTo(Locale.ENGLISH));
-        assertThat(processor.getTimezone(), equalTo(DateTimeZone.UTC));
-    }
-
-    public void testMatchFieldIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("target_field", targetField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_field] is missing"));
-        }
-    }
-
-    public void testMatchFormatsIsMandatory() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("required property [match_formats] is missing"));
-        }
-    }
-
-    public void testParseLocale() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        Locale locale = randomLocale(random());
-        config.put("locale", locale.toLanguageTag());
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getLocale().toLanguageTag(), equalTo(locale.toLanguageTag()));
-    }
-
-    public void testParseInvalidLocale() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        config.put("locale", "invalid_locale");
-        try {
-            factory.create(config);
-            fail("should fail with invalid locale");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Invalid language tag specified: invalid_locale"));
-        }
-    }
-
-    public void testParseTimezone() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-
-        DateTimeZone timezone = randomTimezone();
-        config.put("timezone", timezone.getID());
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTimezone(), equalTo(timezone));
-    }
-
-    public void testParseInvalidTimezone() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
-        config.put("timezone", "invalid_timezone");
-        try {
-            factory.create(config);
-            fail("invalid timezone should fail");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("The datetime zone id 'invalid_timezone' is not recognised"));
-        }
-    }
-
-    //we generate a timezone out of the available ones in joda, some available in the jdk are not available in joda by default
-    private static DateTimeZone randomTimezone() {
-        List<String> ids = new ArrayList<>(DateTimeZone.getAvailableIDs());
-        Collections.sort(ids);
-        return DateTimeZone.forID(randomFrom(ids));
-    }
-
-
-    public void testParseMatchFormats() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getMatchFormats(), equalTo(Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy")));
-    }
-
-    public void testParseMatchFormatsFailure() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("match_formats", "dd/MM/yyyy");
-
-        try {
-            factory.create(config);
-            fail("processor creation should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("property [match_formats] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-
-    public void testParseTargetField() throws Exception {
-        DateProcessor.Factory factory = new DateProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        String sourceField = randomAsciiOfLengthBetween(1, 10);
-        String targetField = randomAsciiOfLengthBetween(1, 10);
-        config.put("match_field", sourceField);
-        config.put("target_field", targetField);
-        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
-
-        DateProcessor processor = factory.create(config);
-        assertThat(processor.getTargetField(), equalTo(targetField));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
deleted file mode 100644
index 5daab95..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
+++ /dev/null
@@ -1,147 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.test.ESTestCase;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.containsString;
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class DateProcessorTests extends ESTestCase {
-
-    public void testJodaPattern() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("yyyy dd MM hh:mm:ss"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06 11:05:15");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T11:05:15.000+02:00"));
-    }
-
-    public void testJodaPatternMultipleFormats() {
-        List<String> matchFormats = new ArrayList<>();
-        matchFormats.add("yyyy dd MM");
-        matchFormats.add("dd/MM/yyyy");
-        matchFormats.add("dd-MM-yyyy");
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", matchFormats, "date_as_date");
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12/06/2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "12-06-2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-
-        document = new HashMap<>();
-        document.put("date_as_string", "2010");
-        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        try {
-            dateProcessor.execute(ingestDocument);
-            fail("processor should have failed due to not supported date format");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("unable to parse date [2010]"));
-        }
-    }
-
-    public void testInvalidJodaPattern() {
-        try {
-            new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList("invalid pattern"), "date_as_date");
-            fail("date processor initialization should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("Illegal pattern component: i"));
-        }
-    }
-
-    public void testJodaPatternLocale() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ITALIAN,
-                "date_as_string", Collections.singletonList("yyyy dd MMM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "2010 12 giugno");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testJodaPatternDefaultYear() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
-                "date_as_string", Collections.singletonList("dd/MM"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "12/06");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo(DateTime.now().getYear() + "-06-12T00:00:00.000+02:00"));
-    }
-
-    public void testTAI64N() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forOffsetHours(2), randomLocale(random()),
-                "date_as_string", Collections.singletonList("TAI64N"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        String dateAsString = (randomBoolean() ? "@" : "") + "4000000050d506482dbdf024";
-        document.put("date_as_string", dateAsString);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2012-12-22T03:00:46.767+02:00"));
-    }
-
-    public void testUnixMs() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList("UNIX_MS"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000500");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-
-    public void testUnix() {
-        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
-                "date_as_string", Collections.singletonList("UNIX"), "date_as_date");
-        Map<String, Object> document = new HashMap<>();
-        document.put("date_as_string", "1000.5");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        dateProcessor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
deleted file mode 100644
index 63eee56..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class DeDotProcessorFactoryTests extends ESTestCase {
-
-    private DeDotProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new DeDotProcessor.Factory();
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "_");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        DeDotProcessor deDotProcessor = factory.create(config);
-        assertThat(deDotProcessor.getSeparator(), equalTo("_"));
-        assertThat(deDotProcessor.getTag(), equalTo(processorTag));
-    }
-
-    public void testCreateMissingSeparatorField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        DeDotProcessor deDotProcessor = factory.create(config);
-        assertThat(deDotProcessor.getSeparator(), equalTo(DeDotProcessor.DEFAULT_SEPARATOR));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
deleted file mode 100644
index a0c87d7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class DeDotProcessorTests extends ESTestCase {
-
-    public void testSimple() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        source.put("a.b", "hello world!");
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        String separator = randomUnicodeOfCodepointLengthBetween(1, 10);
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getSourceAndMetadata().get("a" + separator + "b" ), equalTo("hello world!"));
-    }
-
-    public void testSimpleMap() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        Map<String, Object> subField = new HashMap<>();
-        subField.put("b.c", "hello world!");
-        source.put("a", subField);
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
-        processor.execute(ingestDocument);
-
-        IngestDocument expectedDocument = new IngestDocument(
-            Collections.singletonMap("a", Collections.singletonMap("b_c", "hello world!")),
-            Collections.emptyMap());
-        assertThat(ingestDocument, equalTo(expectedDocument));
-    }
-
-    public void testSimpleList() throws Exception {
-        Map<String, Object> source = new HashMap<>();
-        Map<String, Object> subField = new HashMap<>();
-        subField.put("b.c", "hello world!");
-        source.put("a", Arrays.asList(subField));
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
-        processor.execute(ingestDocument);
-
-        IngestDocument expectedDocument = new IngestDocument(
-            Collections.singletonMap("a",
-                Collections.singletonList(Collections.singletonMap("b_c", "hello world!"))),
-            Collections.emptyMap());
-        assertThat(ingestDocument, equalTo(expectedDocument));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
deleted file mode 100644
index 993c7cc..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class FailProcessorFactoryTests extends ESTestCase {
-
-    private FailProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new FailProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("message", "error");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        FailProcessor failProcessor = factory.create(config);
-        assertThat(failProcessor.getTag(), equalTo(processorTag));
-        assertThat(failProcessor.getMessage().execute(Collections.emptyMap()), equalTo("error"));
-    }
-
-    public void testCreateMissingMessageField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [message] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
deleted file mode 100644
index 3fdc207..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.test.ESTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class FailProcessorTests extends ESTestCase {
-
-    public void test() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String message = randomAsciiOfLength(10);
-        Processor processor = new FailProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(message));
-        try {
-            processor.execute(ingestDocument);
-            fail("fail processor should throw an exception");
-        } catch (FailProcessorException e) {
-            assertThat(e.getMessage(), equalTo(message));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
deleted file mode 100644
index fd62f6c..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class GsubProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        GsubProcessor gsubProcessor = factory.create(config);
-        assertThat(gsubProcessor.getTag(), equalTo(processorTag));
-        assertThat(gsubProcessor.getField(), equalTo("field1"));
-        assertThat(gsubProcessor.getPattern().toString(), equalTo("\\."));
-        assertThat(gsubProcessor.getReplacement(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("pattern", "\\.");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoPatternPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("replacement", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [pattern] is missing"));
-        }
-    }
-
-    public void testCreateNoReplacementPresent() throws Exception {
-        GsubProcessor.Factory factory = new GsubProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("pattern", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [replacement] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
deleted file mode 100644
index fe44f33..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.regex.Pattern;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class GsubProcessorTests extends ESTestCase {
-
-    public void testGsub() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo("127-0-0-1"));
-    }
-
-    public void testGsubNotAStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, 123);
-        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testGsubFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testGsubNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new GsubProcessor(randomAsciiOfLength(10), "field", Pattern.compile("\\."), "-");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execution should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot match pattern."));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
deleted file mode 100644
index 2af2b09..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class JoinProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "-");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        JoinProcessor joinProcessor = factory.create(config);
-        assertThat(joinProcessor.getTag(), equalTo(processorTag));
-        assertThat(joinProcessor.getField(), equalTo("field1"));
-        assertThat(joinProcessor.getSeparator(), equalTo("-"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "-");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        JoinProcessor.Factory factory = new JoinProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
deleted file mode 100644
index 2aa3ac2..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class JoinProcessorTests extends ESTestCase {
-
-    private static final String[] SEPARATORS = new String[]{"-", "_", "."};
-
-    public void testJoinStrings() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<String> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            String value = randomAsciiOfLengthBetween(1, 10);
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinIntegers() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        int numItems = randomIntBetween(1, 10);
-        String separator = randomFrom(SEPARATORS);
-        List<Integer> fieldValue = new ArrayList<>(numItems);
-        String expectedResult = "";
-        for (int j = 0; j < numItems; j++) {
-            int value = randomInt();
-            fieldValue.add(value);
-            expectedResult += value;
-            if (j < numItems - 1) {
-                expectedResult += separator;
-            }
-        }
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
-        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, separator);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
-    }
-
-    public void testJoinNonListField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomAsciiOfLengthBetween(1, 10));
-        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.String] cannot be cast to [java.util.List]"));
-        }
-    }
-
-    public void testJoinNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testJoinNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new JoinProcessor(randomAsciiOfLength(10), "field", "-");
-        try {
-            processor.execute(ingestDocument);
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot join."));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
deleted file mode 100644
index 6a4a67e..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class LowercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        LowercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
deleted file mode 100644
index 77e22b0..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-public class LowercaseProcessorTests extends AbstractStringProcessorTestCase {
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new LowercaseProcessor(randomAsciiOfLength(10), field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toLowerCase(Locale.ROOT);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
deleted file mode 100644
index 0b03150..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RemoveProcessorFactoryTests extends ESTestCase {
-
-    private RemoveProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new RemoveProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        RemoveProcessor removeProcessor = factory.create(config);
-        assertThat(removeProcessor.getTag(), equalTo(processorTag));
-        assertThat(removeProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
deleted file mode 100644
index d134b02..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class RemoveProcessorTests extends ESTestCase {
-
-    public void testRemoveFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String field = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RemoveProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(field));
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(field), equalTo(false));
-    }
-
-    public void testRemoveNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RemoveProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(fieldName));
-        try {
-            processor.execute(ingestDocument);
-            fail("remove field should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
deleted file mode 100644
index 21f5c66..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class RenameProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        config.put("to", "new_field");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        RenameProcessor renameProcessor = factory.create(config);
-        assertThat(renameProcessor.getTag(), equalTo(processorTag));
-        assertThat(renameProcessor.getOldFieldName(), equalTo("old_field"));
-        assertThat(renameProcessor.getNewFieldName(), equalTo("new_field"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("to", "new_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoToPresent() throws Exception {
-        RenameProcessor.Factory factory = new RenameProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "old_field");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [to] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
deleted file mode 100644
index 1f9bdda..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-
-public class RenameProcessorTests extends ESTestCase {
-
-    public void testRename() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = ingestDocument.getFieldValue(fieldName, Object.class);
-        String newFieldName;
-        do {
-            newFieldName = RandomDocumentPicks.randomFieldName(random());
-        } while (RandomDocumentPicks.canAddField(newFieldName, ingestDocument) == false || newFieldName.equals(fieldName));
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testRenameArrayElement() throws Exception {
-        Map<String, Object> document = new HashMap<>();
-        List<String> list = new ArrayList<>();
-        list.add("item1");
-        list.add("item2");
-        list.add("item3");
-        document.put("list", list);
-        List<Map<String, String>> one = new ArrayList<>();
-        one.add(Collections.singletonMap("one", "one"));
-        one.add(Collections.singletonMap("two", "two"));
-        document.put("one", one);
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list.0", "item");
-        processor.execute(ingestDocument);
-        Object actualObject = ingestDocument.getSourceAndMetadata().get("list");
-        assertThat(actualObject, instanceOf(List.class));
-        @SuppressWarnings("unchecked")
-        List<String> actualList = (List<String>) actualObject;
-        assertThat(actualList.size(), equalTo(2));
-        assertThat(actualList.get(0), equalTo("item2"));
-        assertThat(actualList.get(1), equalTo("item3"));
-        actualObject = ingestDocument.getSourceAndMetadata().get("item");
-        assertThat(actualObject, instanceOf(String.class));
-        assertThat(actualObject, equalTo("item1"));
-
-        processor = new RenameProcessor(randomAsciiOfLength(10), "list.0", "list.3");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("[3] is out of bounds for array with length [2] as part of path [list.3]"));
-            assertThat(actualList.size(), equalTo(2));
-            assertThat(actualList.get(0), equalTo("item2"));
-            assertThat(actualList.get(1), equalTo("item3"));
-        }
-    }
-
-    public void testRenameNonExistingField() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, RandomDocumentPicks.randomFieldName(random()));
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] doesn't exist"));
-        }
-    }
-
-    public void testRenameNewFieldAlreadyExists() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument), fieldName);
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] already exists"));
-        }
-    }
-
-    public void testRenameExistingFieldNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, null);
-        String newFieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, newFieldName);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(false));
-        assertThat(ingestDocument.hasField(newFieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), nullValue());
-    }
-
-    public void testRenameAtomicOperationSetFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            @Override
-            public Object put(String key, Object value) {
-                if (key.equals("new_field")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.put(key, value);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-
-    public void testRenameAtomicOperationRemoveFails() throws Exception {
-        Map<String, Object> source = new HashMap<String, Object>() {
-            @Override
-            public Object remove(Object key) {
-                if (key.equals("list")) {
-                    throw new UnsupportedOperationException();
-                }
-                return super.remove(key);
-            }
-        };
-        source.put("list", Collections.singletonList("item"));
-
-        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
-        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list", "new_field");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch (UnsupportedOperationException e) {
-            //the set failed, the old field has not been removed
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
-            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
deleted file mode 100644
index a58ee49..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SetProcessorFactoryTests extends ESTestCase {
-
-    private SetProcessor.Factory factory;
-
-    @Before
-    public void init() {
-        factory = new SetProcessor.Factory(TestTemplateService.instance());
-    }
-
-    public void testCreate() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", "value1");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        SetProcessor setProcessor = factory.create(config);
-        assertThat(setProcessor.getTag(), equalTo(processorTag));
-        assertThat(setProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
-        assertThat(setProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo("value1"));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("value", "value1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoValuePresent() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-    public void testCreateNullValue() throws Exception {
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("value", null);
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
deleted file mode 100644
index 283825c..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.ingest.TestTemplateService;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-import org.hamcrest.Matchers;
-
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class SetProcessorTests extends ESTestCase {
-
-    public void testSetExistingFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        Processor processor = createSetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetNewFields() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        //used to verify that there are no conflicts between subsequent fields going to be added
-        IngestDocument testIngestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), testIngestDocument, fieldValue);
-        Processor processor = createSetProcessor(fieldName, fieldValue);
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
-        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
-    }
-
-    public void testSetFieldsTypeMismatch() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        ingestDocument.setFieldValue("field", "value");
-        Processor processor = createSetProcessor("field.inner", "value");
-        try {
-            processor.execute(ingestDocument);
-            fail("processor execute should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("cannot set [inner] with parent object of type [java.lang.String] as part of path [field.inner]"));
-        }
-    }
-
-    public void testSetMetadata() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value"));
-    }
-
-    private static Processor createSetProcessor(String fieldName, Object fieldValue) {
-        TemplateService templateService = TestTemplateService.instance();
-        return new SetProcessor(randomAsciiOfLength(10), templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
deleted file mode 100644
index 7267544..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class SplitProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        config.put("separator", "\\.");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        SplitProcessor splitProcessor = factory.create(config);
-        assertThat(splitProcessor.getTag(), equalTo(processorTag));
-        assertThat(splitProcessor.getField(), equalTo("field1"));
-        assertThat(splitProcessor.getSeparator(), equalTo("\\."));
-    }
-
-    public void testCreateNoFieldPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("separator", "\\.");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-
-    public void testCreateNoSeparatorPresent() throws Exception {
-        SplitProcessor.Factory factory = new SplitProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
deleted file mode 100644
index e1c8a62..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SplitProcessorTests extends ESTestCase {
-
-    public void testSplit() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
-        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
-        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(Arrays.asList("127", "0", "0", "1")));
-    }
-
-    public void testSplitFieldNotFound() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
-        }
-    }
-
-    public void testSplitNullValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
-        Processor processor = new SplitProcessor(randomAsciiOfLength(10), "field", "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [field] is null, cannot split."));
-        }
-    }
-
-    public void testSplitNonStringValue() throws Exception {
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        ingestDocument.setFieldValue(fieldName, randomInt());
-        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
-        try {
-            processor.execute(ingestDocument);
-            fail("split processor should have failed");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testSplitAppendable() throws Exception {
-        Map<String, Object> splitConfig = new HashMap<>();
-        splitConfig.put("field", "flags");
-        splitConfig.put("separator", "\\|");
-        Processor splitProcessor = (new SplitProcessor.Factory()).create(splitConfig);
-        Map<String, Object> source = new HashMap<>();
-        source.put("flags", "new|hot|super|fun|interesting");
-        IngestDocument ingestDocument = new IngestDocument(source, new HashMap<>());
-        splitProcessor.execute(ingestDocument);
-        @SuppressWarnings("unchecked")
-        List<String> flags = (List<String>)ingestDocument.getFieldValue("flags", List.class);
-        assertThat(flags, equalTo(Arrays.asList("new", "hot", "super", "fun", "interesting")));
-        ingestDocument.appendFieldValue("flags", "additional_flag");
-        assertThat(ingestDocument.getFieldValue("flags", List.class), equalTo(Arrays.asList("new", "hot", "super", "fun", "interesting", "additional_flag")));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
deleted file mode 100644
index 350aaa6..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class TrimProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        TrimProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        TrimProcessor.Factory factory = new TrimProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
deleted file mode 100644
index a0e5fde..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-public class TrimProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new TrimProcessor(randomAsciiOfLength(10), field);
-    }
-
-    @Override
-    protected String modifyInput(String input) {
-        String updatedFieldValue = "";
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        updatedFieldValue += input;
-        updatedFieldValue = addWhitespaces(updatedFieldValue);
-        return updatedFieldValue;
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.trim();
-    }
-
-    private static String addWhitespaces(String input) {
-        int prefixLength = randomIntBetween(0, 10);
-        for (int i = 0; i < prefixLength; i++) {
-            input += ' ';
-        }
-        return input;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
deleted file mode 100644
index 2220438..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-
-public class UppercaseProcessorFactoryTests extends ESTestCase {
-
-    public void testCreate() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field1");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        UppercaseProcessor uppercaseProcessor = factory.create(config);
-        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
-        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
-    }
-
-    public void testCreateMissingField() throws Exception {
-        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
-        Map<String, Object> config = new HashMap<>();
-        try {
-            factory.create(config);
-            fail("factory create should have failed");
-        } catch(IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
deleted file mode 100644
index 4ab61f7..0000000
--- a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.processor;
-
-import java.util.Locale;
-
-public class UppercaseProcessorTests extends AbstractStringProcessorTestCase {
-
-    @Override
-    protected AbstractStringProcessor newProcessor(String field) {
-        return new UppercaseProcessor(randomAsciiOfLength(10), field);
-    }
-
-    @Override
-    protected String expectedResult(String input) {
-        return input.toUpperCase(Locale.ROOT);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
index 4dda068..442b1af 100644
--- a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
+++ b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
@@ -48,7 +48,7 @@ public class InternalSettingsPreparerTests extends ESTestCase {
     @Before
     public void createBaseEnvSettings() {
         baseEnvSettings = settingsBuilder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             .build();
     }
 
@@ -68,7 +68,7 @@ public class InternalSettingsPreparerTests extends ESTestCase {
         assertNotNull(settings.get("name")); // a name was set
         assertNotNull(settings.get(ClusterName.SETTING)); // a cluster name was set
         assertEquals(settings.toString(), size + 1 /* path.home is in the base settings */, settings.names().size());
-        String home = baseEnvSettings.get("path.home");
+        String home = Environment.PATH_HOME_SETTING.get(baseEnvSettings);
         String configDir = env.configFile().toString();
         assertTrue(configDir, configDir.startsWith(home));
     }
diff --git a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsDisabledIT.java b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsDisabledIT.java
index a0751df..2a121be 100644
--- a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsDisabledIT.java
+++ b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsDisabledIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.options.detailederrors;
 
 import org.apache.http.impl.client.HttpClients;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.http.netty.NettyHttpServerTransport;
@@ -43,8 +44,8 @@ public class DetailedErrorsDisabledIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put(Node.HTTP_ENABLED, true)
-                .put(NettyHttpServerTransport.SETTING_HTTP_DETAILED_ERRORS_ENABLED, false)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
+                .put(NettyHttpServerTransport.SETTING_HTTP_DETAILED_ERRORS_ENABLED.getKey(), false)
                 .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
index 935b4e2..4333d81 100644
--- a/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
+++ b/core/src/test/java/org/elasticsearch/options/detailederrors/DetailedErrorsEnabledIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.options.detailederrors;
 
 import org.apache.http.impl.client.HttpClients;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.node.Node;
@@ -42,7 +43,7 @@ public class DetailedErrorsEnabledIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put(Node.HTTP_ENABLED, true)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
                 .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
index cb8ffb8..9378eaa 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
@@ -28,6 +28,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.support.XContentMapValues;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
 
@@ -113,7 +114,7 @@ public class PercolatorBackwardsCompatibilityIT extends ESIntegTestCase {
         }
 
         Settings.Builder nodeSettings = Settings.builder()
-            .put("path.data", dataDir);
+            .put(Environment.PATH_DATA_SETTING.getKey(), dataDir);
         internalCluster().startNode(nodeSettings.build());
         ensureGreen(INDEX_NAME);
     }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
index deaff46..37a0f4e 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginInfoTests.java
@@ -40,17 +40,13 @@ public class PluginInfoTests extends ESTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
         PluginInfo info = PluginInfo.readFromProperties(pluginDir);
         assertEquals("my_plugin", info.getName());
         assertEquals("fake desc", info.getDescription());
         assertEquals("1.0", info.getVersion());
         assertEquals("FakePlugin", info.getClassname());
-        assertTrue(info.isJvm());
         assertTrue(info.isIsolated());
-        assertFalse(info.isSite());
-        assertNull(info.getUrl());
     }
 
     public void testReadFromPropertiesNameMissing() throws Exception {
@@ -94,27 +90,12 @@ public class PluginInfoTests extends ESTestCase {
         }
     }
 
-    public void testReadFromPropertiesJvmAndSiteMissing() throws Exception {
-        Path pluginDir = createTempDir().resolve("fake-plugin");
-        PluginTestUtil.writeProperties(pluginDir,
-            "description", "fake desc",
-            "version", "1.0",
-            "name", "my_plugin");
-        try {
-            PluginInfo.readFromProperties(pluginDir);
-            fail("expected jvm or site exception");
-        } catch (IllegalArgumentException e) {
-            assertTrue(e.getMessage().contains("must be at least a jvm or site plugin"));
-        }
-    }
-
     public void testReadFromPropertiesElasticsearchVersionMissing() throws Exception {
         Path pluginDir = createTempDir().resolve("fake-plugin");
         PluginTestUtil.writeProperties(pluginDir,
             "description", "fake desc",
             "name", "my_plugin",
-            "version", "1.0",
-            "jvm", "true");
+            "version", "1.0");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected missing elasticsearch version exception");
@@ -129,8 +110,7 @@ public class PluginInfoTests extends ESTestCase {
             "description", "fake desc",
             "name", "my_plugin",
             "elasticsearch.version", Version.CURRENT.toString(),
-            "version", "1.0",
-            "jvm", "true");
+            "version", "1.0");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected missing java version exception");
@@ -148,8 +128,7 @@ public class PluginInfoTests extends ESTestCase {
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", "1000000.0",
             "classname", "FakePlugin",
-            "version", "1.0",
-            "jvm", "true");
+            "version", "1.0");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected incompatible java version exception");
@@ -167,8 +146,7 @@ public class PluginInfoTests extends ESTestCase {
                 "elasticsearch.version", Version.CURRENT.toString(),
                 "java.version", "1.7.0_80",
                 "classname", "FakePlugin",
-                "version", "1.0",
-                "jvm", "true");
+                "version", "1.0");
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected bad java version format exception");
@@ -182,7 +160,6 @@ public class PluginInfoTests extends ESTestCase {
         PluginTestUtil.writeProperties(pluginDir,
             "description", "fake desc",
             "version", "1.0",
-            "jvm", "true",
             "name", "my_plugin",
             "elasticsearch.version", "bogus");
         try {
@@ -199,7 +176,6 @@ public class PluginInfoTests extends ESTestCase {
             "description", "fake desc",
             "name", "my_plugin",
             "version", "1.0",
-            "jvm", "true",
             "elasticsearch.version", Version.V_1_7_0.toString());
         try {
             PluginInfo.readFromProperties(pluginDir);
@@ -216,8 +192,7 @@ public class PluginInfoTests extends ESTestCase {
             "name", "my_plugin",
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
-            "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true");
+            "java.version", System.getProperty("java.specification.version"));
         try {
             PluginInfo.readFromProperties(pluginDir);
             fail("expected old elasticsearch version exception");
@@ -226,42 +201,13 @@ public class PluginInfoTests extends ESTestCase {
         }
     }
 
-    public void testReadFromPropertiesSitePlugin() throws Exception {
-        Path pluginDir = createTempDir().resolve("fake-plugin");
-        Files.createDirectories(pluginDir.resolve("_site"));
-        PluginTestUtil.writeProperties(pluginDir,
-            "description", "fake desc",
-            "name", "my_plugin",
-            "version", "1.0",
-            "site", "true");
-        PluginInfo info = PluginInfo.readFromProperties(pluginDir);
-        assertTrue(info.isSite());
-        assertFalse(info.isJvm());
-        assertEquals("NA", info.getClassname());
-    }
-
-    public void testReadFromPropertiesSitePluginWithoutSite() throws Exception {
-        Path pluginDir = createTempDir().resolve("fake-plugin");
-        PluginTestUtil.writeProperties(pluginDir,
-                "description", "fake desc",
-                "name", "my_plugin",
-                "version", "1.0",
-                "site", "true");
-        try {
-            PluginInfo.readFromProperties(pluginDir);
-            fail("didn't get expected exception");
-        } catch (IllegalArgumentException e) {
-            assertTrue(e.getMessage().contains("site plugin but has no '_site"));
-        }
-    }
-
     public void testPluginListSorted() {
         PluginsAndModules pluginsInfo = new PluginsAndModules();
-        pluginsInfo.addPlugin(new PluginInfo("c", "foo", true, "dummy", true, "dummyclass", true));
-        pluginsInfo.addPlugin(new PluginInfo("b", "foo", true, "dummy", true, "dummyclass", true));
-        pluginsInfo.addPlugin(new PluginInfo("e", "foo", true, "dummy", true, "dummyclass", true));
-        pluginsInfo.addPlugin(new PluginInfo("a", "foo", true, "dummy", true, "dummyclass", true));
-        pluginsInfo.addPlugin(new PluginInfo("d", "foo", true, "dummy", true, "dummyclass", true));
+        pluginsInfo.addPlugin(new PluginInfo("c", "foo", "dummy", "dummyclass", true));
+        pluginsInfo.addPlugin(new PluginInfo("b", "foo", "dummy", "dummyclass", true));
+        pluginsInfo.addPlugin(new PluginInfo("e", "foo", "dummy", "dummyclass", true));
+        pluginsInfo.addPlugin(new PluginInfo("a", "foo", "dummy", "dummyclass", true));
+        pluginsInfo.addPlugin(new PluginInfo("d", "foo", "dummy", "dummyclass", true));
 
         final List<PluginInfo> infos = pluginsInfo.getPluginInfos();
         List<String> names = infos.stream().map((input) -> input.getName()).collect(Collectors.toList());
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java
index 5d8605a..d3c1f1b 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginsServiceTests.java
@@ -88,7 +88,7 @@ public class PluginsServiceTests extends ESTestCase {
 
     public void testAdditionalSettings() {
         Settings settings = Settings.builder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             .put("my.setting", "test")
             .put(IndexModule.INDEX_STORE_TYPE_SETTING.getKey(), IndexModule.Type.SIMPLEFS.getSettingsKey()).build();
         PluginsService service = newPluginsService(settings, AdditionalSettingsPlugin1.class);
@@ -100,7 +100,7 @@ public class PluginsServiceTests extends ESTestCase {
 
     public void testAdditionalSettingsClash() {
         Settings settings = Settings.builder()
-            .put("path.home", createTempDir()).build();
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
         PluginsService service = newPluginsService(settings, AdditionalSettingsPlugin1.class, AdditionalSettingsPlugin2.class);
         try {
             service.updatedSettings();
@@ -115,7 +115,7 @@ public class PluginsServiceTests extends ESTestCase {
 
     public void testOnModuleExceptionsArePropagated() {
         Settings settings = Settings.builder()
-                .put("path.home", createTempDir()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
         PluginsService service = newPluginsService(settings, FailOnModule.class);
         try {
             service.processModule(new BrokenModule());
diff --git a/core/src/test/java/org/elasticsearch/plugins/SitePluginIT.java b/core/src/test/java/org/elasticsearch/plugins/SitePluginIT.java
deleted file mode 100644
index e2df251..0000000
--- a/core/src/test/java/org/elasticsearch/plugins/SitePluginIT.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugins;
-
-import org.apache.http.client.config.RequestConfig;
-import org.apache.http.impl.client.CloseableHttpClient;
-import org.apache.http.impl.client.HttpClients;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.http.HttpServerTransport;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
-import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
-import org.elasticsearch.test.rest.client.http.HttpResponse;
-
-import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.rest.RestStatus.FORBIDDEN;
-import static org.elasticsearch.rest.RestStatus.MOVED_PERMANENTLY;
-import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
-import static org.elasticsearch.rest.RestStatus.OK;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasStatus;
-import static org.hamcrest.Matchers.containsString;
-
-/**
- * We want to test site plugins
- */
-@ClusterScope(scope = Scope.SUITE, numDataNodes = 1)
-public class SitePluginIT extends ESIntegTestCase {
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Path pluginDir = getDataPath("/org/elasticsearch/test_plugins");
-        return settingsBuilder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("path.plugins", pluginDir.toAbsolutePath())
-                .put("force.http.enabled", true)
-                .build();
-    }
-
-    @Override
-    public HttpRequestBuilder httpClient() {
-        RequestConfig.Builder builder = RequestConfig.custom().setRedirectsEnabled(false);
-        CloseableHttpClient httpClient = HttpClients.custom().setDefaultRequestConfig(builder.build()).build();
-        return new HttpRequestBuilder(httpClient).httpTransport(internalCluster().getDataNodeInstance(HttpServerTransport.class));
-    }
-
-    public void testRedirectSitePlugin() throws Exception {
-        // We use an HTTP Client to test redirection
-        HttpResponse response = httpClient().method("GET").path("/_plugin/dummy").execute();
-        assertThat(response, hasStatus(MOVED_PERMANENTLY));
-        assertThat(response.getBody(), containsString("/_plugin/dummy/"));
-
-        // We test the real URL
-        response = httpClient().method("GET").path("/_plugin/dummy/").execute();
-        assertThat(response, hasStatus(OK));
-        assertThat(response.getBody(), containsString("<title>Dummy Site Plugin</title>"));
-    }
-
-    /**
-     * Test direct access to an existing file (index.html)
-     */
-    public void testAnyPage() throws Exception {
-        HttpResponse response = httpClient().path("/_plugin/dummy/index.html").execute();
-        assertThat(response, hasStatus(OK));
-        assertThat(response.getBody(), containsString("<title>Dummy Site Plugin</title>"));
-    }
-
-    /**
-     * Test normalizing of path
-     */
-    public void testThatPathsAreNormalized() throws Exception {
-        // more info: https://www.owasp.org/index.php/Path_Traversal
-        List<String> notFoundUris = new ArrayList<>();
-        notFoundUris.add("/_plugin/dummy/../../../../../log4j.properties");
-        notFoundUris.add("/_plugin/dummy/../../../../../%00log4j.properties");
-        notFoundUris.add("/_plugin/dummy/..%c0%af..%c0%af..%c0%af..%c0%af..%c0%aflog4j.properties");
-        notFoundUris.add("/_plugin/dummy/%2E%2E/%2E%2E/%2E%2E/%2E%2E/index.html");
-        notFoundUris.add("/_plugin/dummy/%2e%2e/%2e%2e/%2e%2e/%2e%2e/index.html");
-        notFoundUris.add("/_plugin/dummy/%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2findex.html");
-        notFoundUris.add("/_plugin/dummy/%2E%2E/%2E%2E/%2E%2E/%2E%2E/index.html");
-        notFoundUris.add("/_plugin/dummy/..%5C..%5C..%5C..%5C..%5Clog4j.properties");
-
-        for (String uri : notFoundUris) {
-            HttpResponse response = httpClient().path(uri).execute();
-            String message = String.format(Locale.ROOT, "URI [%s] expected to be not found", uri);
-            assertThat(message, response, hasStatus(NOT_FOUND));
-        }
-
-        // using relative path inside of the plugin should work
-        HttpResponse response = httpClient().path("/_plugin/dummy/dir1/../dir1/../index.html").execute();
-        assertThat(response, hasStatus(OK));
-        assertThat(response.getBody(), containsString("<title>Dummy Site Plugin</title>"));
-    }
-
-    /**
-     * Test case for #4845: https://github.com/elasticsearch/elasticsearch/issues/4845
-     * Serving _site plugins do not pick up on index.html for sub directories
-     */
-    public void testWelcomePageInSubDirs() throws Exception {
-        HttpResponse response = httpClient().path("/_plugin/subdir/dir/").execute();
-        assertThat(response, hasStatus(OK));
-        assertThat(response.getBody(), containsString("<title>Dummy Site Plugin (subdir)</title>"));
-
-        response = httpClient().path("/_plugin/subdir/dir_without_index/").execute();
-        assertThat(response, hasStatus(FORBIDDEN));
-
-        response = httpClient().path("/_plugin/subdir/dir_without_index/page.html").execute();
-        assertThat(response, hasStatus(OK));
-        assertThat(response.getBody(), containsString("<title>Dummy Site Plugin (page)</title>"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigIT.java b/core/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigIT.java
deleted file mode 100644
index 1cde90d..0000000
--- a/core/src/test/java/org/elasticsearch/plugins/SitePluginRelativePathConfigIT.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.plugins;
-
-import org.apache.http.impl.client.CloseableHttpClient;
-import org.apache.http.impl.client.HttpClients;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.http.HttpServerTransport;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
-import org.elasticsearch.test.rest.client.http.HttpResponse;
-
-import java.nio.file.Path;
-
-import static org.apache.lucene.util.Constants.WINDOWS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.rest.RestStatus.OK;
-import static org.elasticsearch.test.ESIntegTestCase.Scope.SUITE;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasStatus;
-
-@ClusterScope(scope = SUITE, numDataNodes = 1)
-public class SitePluginRelativePathConfigIT extends ESIntegTestCase {
-    private final Path root = PathUtils.get(".").toAbsolutePath().getRoot();
-
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        String cwdToRoot = getRelativePath(PathUtils.get(".").toAbsolutePath());
-        Path pluginDir = PathUtils.get(cwdToRoot, relativizeToRootIfNecessary(getDataPath("/org/elasticsearch/test_plugins")).toString());
-
-        Path tempDir = createTempDir();
-        boolean useRelativeInMiddleOfPath = randomBoolean();
-        if (useRelativeInMiddleOfPath) {
-            pluginDir = PathUtils.get(tempDir.toString(), getRelativePath(tempDir), pluginDir.toString());
-        }
-
-        return settingsBuilder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put("path.plugins", pluginDir)
-                .put("force.http.enabled", true)
-                .build();
-    }
-
-    public void testThatRelativePathsDontAffectPlugins() throws Exception {
-        HttpResponse response = httpClient().method("GET").path("/_plugin/dummy/").execute();
-        assertThat(response, hasStatus(OK));
-    }
-
-    private Path relativizeToRootIfNecessary(Path path) {
-        if (WINDOWS) {
-            return root.relativize(path);
-        }
-        return path;
-    }
-
-    private String getRelativePath(Path path) {
-        StringBuilder sb = new StringBuilder();
-        for (int i = 0; i < path.getNameCount(); i++) {
-            sb.append("..");
-            sb.append(path.getFileSystem().getSeparator());
-        }
-
-        return sb.toString();
-    }
-
-    @Override
-    public HttpRequestBuilder httpClient() {
-        CloseableHttpClient httpClient = HttpClients.createDefault();
-        return new HttpRequestBuilder(httpClient).httpTransport(internalCluster().getDataNodeInstance(HttpServerTransport.class));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java b/core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java
index 3b61edf..663d951 100644
--- a/core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/FullRollingRestartIT.java
@@ -57,7 +57,7 @@ public class FullRollingRestartIT extends ESIntegTestCase {
     }
 
     public void testFullRollingRestart() throws Exception {
-        Settings settings = Settings.builder().put(ZenDiscovery.SETTING_JOIN_TIMEOUT, "30s").build();
+        Settings settings = Settings.builder().put(ZenDiscovery.JOIN_TIMEOUT_SETTING.getKey(), "30s").build();
         internalCluster().startNode(settings);
         createIndex("test");
 
diff --git a/core/src/test/java/org/elasticsearch/rest/CorsRegexDefaultIT.java b/core/src/test/java/org/elasticsearch/rest/CorsRegexDefaultIT.java
index 2b7533c..f2ce16a 100644
--- a/core/src/test/java/org/elasticsearch/rest/CorsRegexDefaultIT.java
+++ b/core/src/test/java/org/elasticsearch/rest/CorsRegexDefaultIT.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.rest;
 
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -35,7 +36,7 @@ public class CorsRegexDefaultIT extends ESIntegTestCase {
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
-            .put(Node.HTTP_ENABLED, true)
+            .put(NetworkModule.HTTP_ENABLED.getKey(), true)
             .put(super.nodeSettings(nodeOrdinal)).build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/rest/CorsRegexIT.java b/core/src/test/java/org/elasticsearch/rest/CorsRegexIT.java
index 3828ae0..1c624f9 100644
--- a/core/src/test/java/org/elasticsearch/rest/CorsRegexIT.java
+++ b/core/src/test/java/org/elasticsearch/rest/CorsRegexIT.java
@@ -20,6 +20,7 @@ package org.elasticsearch.rest;
 
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -47,9 +48,9 @@ public class CorsRegexIT extends ESIntegTestCase {
         return Settings.settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
                 .put(SETTING_CORS_ALLOW_ORIGIN, "/https?:\\/\\/localhost(:[0-9]+)?/")
-                .put(SETTING_CORS_ALLOW_CREDENTIALS, true)
-                .put(SETTING_CORS_ENABLED, true)
-                .put(Node.HTTP_ENABLED, true)
+                .put(SETTING_CORS_ALLOW_CREDENTIALS.getKey(), true)
+                .put(SETTING_CORS_ENABLED.getKey(), true)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
                 .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
index 37f2ebb..8ef7bcc 100644
--- a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -39,7 +39,7 @@ public class FileScriptTests extends ESTestCase {
         Path mockscript = scriptsDir.resolve("script1.mockscript");
         Files.write(mockscript, "1".getBytes("UTF-8"));
         settings = Settings.builder()
-            .put("path.home", homeDir)
+            .put(Environment.PATH_HOME_SETTING.getKey(), homeDir)
                 // no file watching, so we don't need a ResourceWatcherService
             .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
             .put(settings)
@@ -63,8 +63,7 @@ public class FileScriptTests extends ESTestCase {
             .put("script.engine." + MockScriptEngine.NAME + ".file.aggs", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.search", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.mapping", false)
-            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false)
-            .put("script.engine." + MockScriptEngine.NAME + ".file.ingest", false).build();
+            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false).build();
         ScriptService scriptService = makeScriptService(settings);
         Script script = new Script("script1", ScriptService.ScriptType.FILE, MockScriptEngine.NAME, null);
         for (ScriptContext context : ScriptContext.Standard.values()) {
diff --git a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
index 47adeab..0e8d477 100644
--- a/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
@@ -50,7 +50,7 @@ public class NativeScriptTests extends ESTestCase {
         ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
         Settings settings = Settings.settingsBuilder()
                 .put("name", "testNativeScript")
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         ScriptModule scriptModule = new ScriptModule(settings);
         scriptModule.registerScript("my", MyNativeScriptFactory.class);
@@ -78,7 +78,7 @@ public class NativeScriptTests extends ESTestCase {
             String scriptContext = randomFrom(ScriptContext.Standard.values()).getKey();
             builder.put(ScriptModes.SCRIPT_SETTINGS_PREFIX + scriptContext, randomFrom(ScriptMode.values()));
         }
-        Settings settings = builder.put("path.home", createTempDir()).build();
+        Settings settings = builder.put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
         Environment environment = new Environment(settings);
         ResourceWatcherService resourceWatcherService = new ResourceWatcherService(settings, null);
         Map<String, NativeScriptFactory> nativeScriptFactoryMap = new HashMap<>();
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java b/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
index 019eb7c..36865f2 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
@@ -38,7 +38,7 @@ public class ScriptContextTests extends ESTestCase {
 
     ScriptService makeScriptService() throws Exception {
         Settings settings = Settings.builder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             // no file watching, so we don't need a ResourceWatcherService
             .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
             .put("script." + PLUGIN_NAME + "_custom_globally_disabled_op", false)
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
index 3c939e7..2028605 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
@@ -67,8 +67,8 @@ public class ScriptServiceTests extends ESTestCase {
     public void setup() throws IOException {
         Path genericConfigFolder = createTempDir();
         baseSettings = settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", genericConfigFolder)
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+                .put(Environment.PATH_CONF_SETTING.getKey(), genericConfigFolder)
                 .build();
         resourceWatcherService = new ResourceWatcherService(baseSettings, null);
         scriptEngineService = new TestEngineService();
@@ -378,7 +378,7 @@ public class ScriptServiceTests extends ESTestCase {
     public void testCompilationStatsOnCacheHit() throws IOException {
         ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
         Settings.Builder builder = Settings.builder();
-        builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING, 1);
+        builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING.getKey(), 1);
         buildScriptService(builder.build());
         scriptService.executable(new Script("1+1", ScriptType.INLINE, "test", null), randomFrom(scriptContexts), contextAndHeaders, Collections.emptyMap());
         scriptService.executable(new Script("1+1", ScriptType.INLINE, "test", null), randomFrom(scriptContexts), contextAndHeaders, Collections.emptyMap());
@@ -403,7 +403,7 @@ public class ScriptServiceTests extends ESTestCase {
     public void testCacheEvictionCountedInCacheEvictionsStats() throws IOException {
         ContextAndHeaderHolder contextAndHeaders = new ContextAndHeaderHolder();
         Settings.Builder builder = Settings.builder();
-        builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING, 1);
+        builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING.getKey(), 1);
         buildScriptService(builder.build());
         scriptService.executable(new Script("1+1", ScriptType.INLINE, "test", null), randomFrom(scriptContexts), contextAndHeaders, Collections.emptyMap());
         scriptService.executable(new Script("2+2", ScriptType.INLINE, "test", null), randomFrom(scriptContexts), contextAndHeaders, Collections.emptyMap());
diff --git a/core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java b/core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java
index addfe14..9ea5ec9 100644
--- a/core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java
+++ b/core/src/test/java/org/elasticsearch/search/StressSearchServiceReaperIT.java
@@ -40,7 +40,7 @@ public class StressSearchServiceReaperIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         // very frequent checks
         return Settings.builder().put(super.nodeSettings(nodeOrdinal))
-                .put(SearchService.KEEPALIVE_INTERVAL_KEY, TimeValue.timeValueMillis(1)).build();
+                .put(SearchService.KEEPALIVE_INTERVAL_SETTING.getKey(), TimeValue.timeValueMillis(1)).build();
     }
 
     // see issue #5165 - this test fails each time without the fix in pull #5170
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
index bb22361..75c0fc2 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java
@@ -61,7 +61,7 @@ public class DateHistogramOffsetIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put(AssertingLocalTransport.ASSERTING_TRANSPORT_MIN_VERSION_KEY, Version.V_1_4_0_Beta1).build();
+                .put(AssertingLocalTransport.ASSERTING_TRANSPORT_MIN_VERSION_KEY.getKey(), Version.V_1_4_0_Beta1.toString()).build();
     }
 
     @Before
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index 65d5fba..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -109,7 +109,7 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
      */
-    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder<?> metric) {
+    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
         if (metric instanceof MinBuilder) {
             double accumulator = Double.POSITIVE_INFINITY;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index 6184cb9..90d4437 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -77,7 +77,7 @@ public class MovAvgIT extends ESIntegTestCase {
     static int period;
     static HoltWintersModel.SeasonalityType seasonalityType;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceMetricsAggregationBuilder<?> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -864,7 +864,7 @@ public class MovAvgIT extends ESIntegTestCase {
 
     public void testHoltWintersNotEnoughData() {
         try {
-            client()
+            SearchResponse response = client()
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
@@ -1003,7 +1003,7 @@ public class MovAvgIT extends ESIntegTestCase {
 
     public void testBadModelParams() {
         try {
-            client()
+            SearchResponse response = client()
                     .prepareSearch("idx").setTypes("type")
                     .addAggregation(
                             histogram("histo").field(INTERVAL_FIELD).interval(interval)
@@ -1248,7 +1248,7 @@ public class MovAvgIT extends ESIntegTestCase {
 
         for (MovAvgModelBuilder builder : builders) {
             try {
-                client()
+                SearchResponse response = client()
                         .prepareSearch("idx").setTypes("type")
                         .addAggregation(
                                 histogram("histo").field(INTERVAL_FIELD).interval(interval)
@@ -1265,10 +1265,14 @@ public class MovAvgIT extends ESIntegTestCase {
                 // All good
             }
         }
+
+
+
+
     }
 
 
-    private void assertValidIterators(Iterator<?> expectedBucketIter, Iterator<?> expectedCountsIter, Iterator<?> expectedValuesIter) {
+    private void assertValidIterators(Iterator expectedBucketIter, Iterator expectedCountsIter, Iterator expectedValuesIter) {
         if (!expectedBucketIter.hasNext()) {
             fail("`expectedBucketIter` iterator ended before `actual` iterator, size mismatch");
         }
@@ -1351,7 +1355,7 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceMetricsAggregationBuilder<?> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index 145587a..aebd6a7 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -60,7 +60,7 @@ public class SerialDiffIT extends ESIntegTestCase {
     static int numBuckets;
     static int lag;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceMetricsAggregationBuilder<?> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -80,7 +80,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceMetricsAggregationBuilder<?> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
@@ -95,7 +95,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private void assertValidIterators(Iterator<?> expectedBucketIter, Iterator<?> expectedCountsIter, Iterator<?> expectedValuesIter) {
+    private void assertValidIterators(Iterator expectedBucketIter, Iterator expectedCountsIter, Iterator expectedValuesIter) {
         if (!expectedBucketIter.hasNext()) {
             fail("`expectedBucketIter` iterator ended before `actual` iterator, size mismatch");
         }
diff --git a/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java b/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
index 28874d2..35dbde2 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
@@ -29,7 +29,6 @@ import org.elasticsearch.test.ESIntegTestCase;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 
-
 /**
  * This test basically verifies that search with a single shard active (cause we indexed to it) and other
  * shards possibly not active at all (cause they haven't allocated) will still work.
@@ -58,39 +57,44 @@ public class SearchWhileCreatingIndexIT extends ESIntegTestCase {
         int shardsNo = numberOfReplicas + 1;
         int neededNodes = shardsNo <= 2 ? 1 : shardsNo / 2 + 1;
         internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(neededNodes, shardsNo));
-        for (int i = 0; i < 20; i++) {
-            logger.info("running iteration {}", i);
-            if (createIndex) {
-                createIndex("test");
-            }
-            client().prepareIndex("test", "type1", randomAsciiOfLength(5)).setSource("field", "test").execute().actionGet();
-            RefreshResponse refreshResponse = client().admin().indices().prepareRefresh("test").execute().actionGet();
-            assertThat(refreshResponse.getSuccessfulShards(), greaterThanOrEqualTo(1)); // at least one shard should be successful when refreshing
 
-            // we want to make sure that while recovery happens, and a replica gets recovered, its properly refreshed
-            ClusterHealthStatus status = ClusterHealthStatus.RED;
-            while (status != ClusterHealthStatus.GREEN) {
-                // first, verify that search on the primary search works
-                SearchResponse searchResponse = client().prepareSearch("test").setPreference("_primary").setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                assertHitCount(searchResponse, 1);
-                // now, let it go to primary or replica, though in a randomized re-creatable manner
-                String preference = randomAsciiOfLength(5);
-                Client client = client();
-                searchResponse = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                if (searchResponse.getHits().getTotalHits() != 1) {
-                    refresh();
-                    SearchResponse searchResponseAfterRefresh = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                    logger.info("hits count mismatch on any shard search failed, post explicit refresh hits are {}", searchResponseAfterRefresh.getHits().getTotalHits());
-                    ensureGreen();
-                    SearchResponse searchResponseAfterGreen = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                    logger.info("hits count mismatch on any shard search failed, post explicit wait for green hits are {}", searchResponseAfterGreen.getHits().getTotalHits());
-                    assertHitCount(searchResponse, 1);
-                }
+        String id = randomAsciiOfLength(5);
+        // we will go the primary or the replica, but in a
+        // randomized re-creatable manner
+        int counter = 0;
+        String preference = randomAsciiOfLength(5);
+
+        logger.info("running iteration for id {}, preference {}", id, preference);
+
+        if (createIndex) {
+            createIndex("test");
+        }
+        client().prepareIndex("test", "type1", id).setSource("field", "test").execute().actionGet();
+        RefreshResponse refreshResponse = client().admin().indices().prepareRefresh("test").execute().actionGet();
+        assertThat(refreshResponse.getSuccessfulShards(), greaterThanOrEqualTo(1)); // at least one shard should be successful when refreshing
+
+        logger.info("using preference {}", preference);
+        // we want to make sure that while recovery happens, and a replica gets recovered, its properly refreshed
+        ClusterHealthStatus status = ClusterHealthStatus.RED;
+        while (status != ClusterHealthStatus.GREEN) {
+            // first, verify that search on the primary search works
+            SearchResponse searchResponse = client().prepareSearch("test").setPreference("_primary").setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+            assertHitCount(searchResponse, 1);
+            Client client = client();
+            searchResponse = client.prepareSearch("test").setPreference(preference + Integer.toString(counter++)).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+            if (searchResponse.getHits().getTotalHits() != 1) {
+                refresh();
+                SearchResponse searchResponseAfterRefresh = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+                logger.info("hits count mismatch on any shard search failed, post explicit refresh hits are {}", searchResponseAfterRefresh.getHits().getTotalHits());
+                ensureGreen();
+                SearchResponse searchResponseAfterGreen = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+                logger.info("hits count mismatch on any shard search failed, post explicit wait for green hits are {}", searchResponseAfterGreen.getHits().getTotalHits());
                 assertHitCount(searchResponse, 1);
-                status = client().admin().cluster().prepareHealth("test").get().getStatus();
-                internalCluster().ensureAtLeastNumDataNodes(numberOfReplicas + 1);
             }
-            cluster().wipeIndices("test");
+            assertHitCount(searchResponse, 1);
+            status = client().admin().cluster().prepareHealth("test").get().getStatus();
+            internalCluster().ensureAtLeastNumDataNodes(numberOfReplicas + 1);
         }
+        cluster().wipeIndices("test");
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 5a1b99f..9597af9 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -44,6 +44,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.query.AbstractQueryTestCase;
 import org.elasticsearch.index.query.EmptyQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
@@ -81,7 +82,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
     public static void init() throws IOException {
         Settings settings = Settings.settingsBuilder()
                 .put("name", SearchSourceBuilderTests.class.toString())
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         namedWriteableRegistry = new NamedWriteableRegistry();
         injector = new ModulesBuilder().add(
diff --git a/core/src/test/java/org/elasticsearch/search/sort/SortOrderTests.java b/core/src/test/java/org/elasticsearch/search/sort/SortOrderTests.java
new file mode 100644
index 0000000..e505ec6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/sort/SortOrderTests.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.sort;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.test.ESTestCase;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class SortOrderTests extends ESTestCase {
+
+    /** Check that ordinals remain stable as we rely on them for serialisation. */
+    public void testDistanceUnitNames() {
+        assertEquals(0, SortOrder.ASC.ordinal());
+        assertEquals(1, SortOrder.DESC.ordinal());
+    }
+
+    public void testReadWrite() throws Exception {
+        for (SortOrder unit : SortOrder.values()) {
+          try (BytesStreamOutput out = new BytesStreamOutput()) {
+              unit.writeTo(out);
+              try (StreamInput in = StreamInput.wrap(out.bytes())) {
+                  assertThat("Roundtrip serialisation failed.", SortOrder.readOrderFrom(in), equalTo(unit));
+              }
+          }
+        }
+    }
+
+    public void testFromString() {
+        for (SortOrder unit : SortOrder.values()) {
+            assertThat("Roundtrip string parsing failed.", SortOrder.fromString(unit.toString()), equalTo(unit));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
index 1543433..fac7f71 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
@@ -230,7 +230,7 @@ public class CompletionSuggestSearchIT extends ESIntegTestCase {
         SuggestResponse suggestResponse = client().suggest(request).get();
         assertThat(suggestResponse.getSuccessfulShards(), equalTo(0));
         for (ShardOperationFailedException exception : suggestResponse.getShardFailures()) {
-            assertThat(exception.reason(), containsString("ParsingException[[completion] failed to parse field [payload]]; nested: IllegalStateException[Can't get text on a START_OBJECT"));
+            assertThat(exception.reason(), containsString("ParsingException[[completion] failed to parse field [payload]]; nested: IllegalStateException[expected value but got [START_OBJECT]]"));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
index ad554b7..22bef61 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
@@ -100,7 +100,7 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
         logger.info("--> start 2 nodes");
         Settings nodeSettings = settingsBuilder()
                 .put("discovery.type", "zen")
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
                 .build();
         internalCluster().startNode(nodeSettings);
diff --git a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
index 60f1bad..ea225d9 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -186,7 +187,7 @@ public class SimpleThreadPoolIT extends ESIntegTestCase {
     public void testThreadPoolLeakingThreadsWithTribeNode() {
         Settings settings = Settings.builder()
                 .put("node.name", "thread_pool_leaking_threads_tribe_node")
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("tribe.t1.cluster.name", "non_existing_cluster")
                         //trigger initialization failure of one of the tribes (doesn't require starting the node)
                 .put("tribe.t1.plugin.mandatory", "non_existing").build();
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index 2fe11b5..09653c1 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -29,7 +29,6 @@ import org.elasticsearch.threadpool.ThreadPool.Names;
 import java.lang.reflect.Field;
 import java.util.Arrays;
 import java.util.HashSet;
-import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executor;
@@ -48,7 +47,6 @@ import static org.hamcrest.Matchers.sameInstance;
 /**
  */
 public class UpdateThreadPoolSettingsTests extends ESTestCase {
-
     public void testCorrectThreadPoolTypePermittedInSettings() throws InterruptedException {
         String threadPoolName = randomThreadPoolName();
         ThreadPool.ThreadPoolType correctThreadPoolType = ThreadPool.THREAD_POOL_TYPES.get(threadPoolName);
@@ -454,10 +452,11 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         Set<ThreadPool.ThreadPoolType> set = new HashSet<>();
         set.addAll(Arrays.asList(ThreadPool.ThreadPoolType.values()));
         set.remove(ThreadPool.THREAD_POOL_TYPES.get(threadPoolName));
-        return randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
+        ThreadPool.ThreadPoolType invalidThreadPoolType = randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
+        return invalidThreadPoolType;
     }
 
     private String randomThreadPool(ThreadPool.ThreadPoolType type) {
-        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(Map.Entry::getKey).collect(Collectors.toList()));
+        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(t -> t.getKey()).collect(Collectors.toList()));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
index ee49012..f4dccc7 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportMultiPortIntegrationIT.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.ESIntegTestCase.Scope;
@@ -73,7 +74,7 @@ public class NettyTransportMultiPortIntegrationIT extends ESIntegTestCase {
         Settings settings = settingsBuilder()
                 .put("cluster.name", internalCluster().getClusterName())
                 .put(NetworkModule.TRANSPORT_TYPE_KEY, "netty")
-                .put("path.home", createTempDir().toString())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                 .build();
         try (TransportClient transportClient = TransportClient.builder().settings(settings).build()) {
             transportClient.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("127.0.0.1"), randomPort));
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index da01ca9..260c625 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -32,6 +32,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
@@ -79,7 +80,7 @@ public class TribeIT extends ESIntegTestCase {
         NodeConfigurationSource nodeConfigurationSource = new NodeConfigurationSource() {
             @Override
             public Settings nodeSettings(int nodeOrdinal) {
-                return Settings.builder().put(Node.HTTP_ENABLED, false).build();
+                return Settings.builder().put(NetworkModule.HTTP_ENABLED.getKey(), false).build();
             }
 
             @Override
@@ -135,8 +136,8 @@ public class TribeIT extends ESIntegTestCase {
             tribe2Defaults.put("tribe.t2." + entry.getKey(), entry.getValue());
         }
         // give each tribe it's unicast hosts to connect to
-        tribe1Defaults.putArray("tribe.t1." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS, getUnicastHosts(internalCluster().client()));
-        tribe1Defaults.putArray("tribe.t2." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS, getUnicastHosts(cluster2.client()));
+        tribe1Defaults.putArray("tribe.t1." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING.getKey(), getUnicastHosts(internalCluster().client()));
+        tribe1Defaults.putArray("tribe.t2." + UnicastZenPing.DISCOVERY_ZEN_PING_UNICAST_HOSTS_SETTING.getKey(), getUnicastHosts(cluster2.client()));
 
         Settings merged = Settings.builder()
                 .put("tribe.t1.cluster.name", internalCluster().getClusterName())
diff --git a/core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java b/core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java
index fe36b74..6c6c45e 100644
--- a/core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/watcher/ResourceWatcherServiceTests.java
@@ -79,7 +79,7 @@ public class ResourceWatcherServiceTests extends ESTestCase {
         };
 
         // checking default freq
-        WatcherHandle<?> handle = service.add(watcher);
+        WatcherHandle handle = service.add(watcher);
         assertThat(handle, notNullValue());
         assertThat(handle.frequency(), equalTo(ResourceWatcherService.Frequency.MEDIUM));
         assertThat(service.lowMonitor.watchers.size(), is(0));
diff --git a/distribution/src/main/packaging/systemd/elasticsearch.service b/distribution/src/main/packaging/systemd/elasticsearch.service
index 4a280a0..301586c 100644
--- a/distribution/src/main/packaging/systemd/elasticsearch.service
+++ b/distribution/src/main/packaging/systemd/elasticsearch.service
@@ -26,11 +26,8 @@ ExecStart=/usr/share/elasticsearch/bin/elasticsearch \
                                                 -Des.default.path.data=${DATA_DIR} \
                                                 -Des.default.path.conf=${CONF_DIR}
 
-# Connects standard output to /dev/null
-StandardOutput=null
-
-# Connects standard error to journal
-StandardError=journal
+StandardOutput=journal
+StandardError=inherit
 
 # Specifies the maximum file descriptor number that can be opened by this process
 LimitNOFILE=65535
diff --git a/distribution/src/main/resources/bin/elasticsearch.in.bat b/distribution/src/main/resources/bin/elasticsearch.in.bat
index 6f6550d..7138cf5 100644
--- a/distribution/src/main/resources/bin/elasticsearch.in.bat
+++ b/distribution/src/main/resources/bin/elasticsearch.in.bat
@@ -93,7 +93,7 @@ set JAVA_OPTS=%JAVA_OPTS% -Djna.nosys=true
 
 REM check in case a user was using this mechanism
 if "%ES_CLASSPATH%" == "" (
-set ES_CLASSPATH=%ES_HOME%/lib/elasticsearch-${project.version}.jar;%ES_HOME%/lib/*
+set ES_CLASSPATH=!ES_HOME!/lib/elasticsearch-${project.version}.jar;!ES_HOME!/lib/*
 ) else (
 ECHO Error: Don't modify the classpath with ES_CLASSPATH, Best is to add 1>&2
 ECHO additional elements via the plugin mechanism, or if code must really be 1>&2
diff --git a/distribution/src/main/resources/bin/service.bat b/distribution/src/main/resources/bin/service.bat
index 5b5fbff..f423bb9 100644
--- a/distribution/src/main/resources/bin/service.bat
+++ b/distribution/src/main/resources/bin/service.bat
@@ -1,5 +1,5 @@
 @echo off
-SETLOCAL
+SETLOCAL enabledelayedexpansion
 
 TITLE Elasticsearch Service ${project.version}
 
diff --git a/docs/plugins/authors.asciidoc b/docs/plugins/authors.asciidoc
index 9461ba8..6f63eab 100644
--- a/docs/plugins/authors.asciidoc
+++ b/docs/plugins/authors.asciidoc
@@ -3,8 +3,6 @@
 
 The Elasticsearch repository contains examples of:
 
-* a https://github.com/elastic/elasticsearch/tree/master/plugins/site-example[site plugin]
-  for serving static HTML, JavaScript, and CSS.
 * a https://github.com/elastic/elasticsearch/tree/master/plugins/jvm-example[Java plugin]
   which contains Java code.
 
@@ -12,20 +10,6 @@ These examples provide the bare bones needed to get started.  For more
 information about how to write a plugin, we recommend looking at the plugins
 listed in this documentation for inspiration.
 
-[NOTE]
-.Site plugins
-====================================
-
-The example site plugin mentioned above contains all of the scaffolding needed
-for integrating with Gradle builds.  If you don't plan on using Gradle, then all
-you really need in your plugin is:
-
-* The `plugin-descriptor.properties` file
-* The `_site/` directory
-* The `_site/index.html` file
-
-====================================
-
 [float]
 === Plugin descriptor file
 
@@ -43,7 +27,7 @@ instance, see
 https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/build.gradle[`/plugins/site-example/build.gradle`].
 
 [float]
-==== Mandatory elements for all plugins
+==== Mandatory elements for plugins
 
 
 [cols="<,<,<",options="header",]
@@ -56,23 +40,6 @@ https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/build.
 
 |`name`                     |String  | the plugin name
 
-|=======================================================================
-
-
-
-[float]
-==== Mandatory elements for Java plugins
-
-
-[cols="<,<,<",options="header",]
-|=======================================================================
-|Element                    | Type   | Description
-
-|`jvm`                      |Boolean | true if the `classname` class should be loaded
-from jar files in the root directory of the plugin.
-Note that only jar files in the root directory are added to the classpath for the plugin!
-If you need other resources, package them into a resources jar.
-
 |`classname`                |String  | the name of the class to load, fully-qualified.
 
 |`java.version`             |String  | version of java the code is built against.
@@ -83,6 +50,9 @@ of nonnegative decimal integers separated by "."'s and may have leading zeros.
 
 |=======================================================================
 
+Note that only jar files in the root directory are added to the classpath for the plugin!
+If you need other resources, package them into a resources jar.
+
 [IMPORTANT]
 .Plugin release lifecycle
 ==============================================
@@ -95,20 +65,6 @@ in the presence of plugins with the incorrect `elasticsearch.version`.
 
 
 [float]
-==== Mandatory elements for Site plugins
-
-
-[cols="<,<,<",options="header",]
-|=======================================================================
-|Element                    | Type   | Description
-
-|`site`                     |Boolean | true to indicate contents of the `_site/`
-directory in the root of the plugin should be served.
-
-|=======================================================================
-
-
-[float]
 === Testing your plugin
 
 When testing a Java plugin, it will only be auto-loaded if it is in the
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
index 567c80a..27d6d08 100644
--- a/docs/plugins/discovery-ec2.asciidoc
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -113,6 +113,7 @@ The available values are:
 * `ap-southeast-1`
 * `ap-southeast-2`
 * `ap-northeast` (`ap-northeast-1`)
+* `ap-northeast-2` (`ap-northeast-2`)
 * `eu-west` (`eu-west-1`)
 * `eu-central` (`eu-central-1`)
 * `sa-east` (`sa-east-1`)
diff --git a/docs/plugins/ingest.asciidoc b/docs/plugins/ingest.asciidoc
deleted file mode 100644
index 7233608..0000000
--- a/docs/plugins/ingest.asciidoc
+++ /dev/null
@@ -1,1023 +0,0 @@
-[[ingest]]
-== Ingest Plugin
-
-The ingest plugin can be used to pre-process documents before the actual indexing takes place.
-This pre-processing happens by the ingest plugin that intercepts bulk and index requests, applies the
-transformations and then passes the documents back to the index or bulk APIs.
-
-The ingest plugin is disabled by default. In order to enable the ingest plugin the following
-setting should be configured in the elasticsearch.yml file:
-
-[source,yaml]
---------------------------------------------------
-node.ingest: true
---------------------------------------------------
-
-The ingest plugin can be installed and enabled on any node. It is possible to run ingest
-on an master and or data node or have dedicated client nodes that run with ingest.
-
-In order to pre-process document before indexing the `pipeline` parameter should be used
-on an index or bulk request to tell the ingest plugin what pipeline is going to be used.
-
-[source,js]
---------------------------------------------------
-PUT /my-index/my-type/my-id?pipeline=my_pipeline_id
-{
-  ...
-}
---------------------------------------------------
-// AUTOSENSE
-
-=== Processors
-
-==== Set processor
-Sets one field and associates it with the specified value. If the field already exists,
-its value will be replaced with the provided one.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "field1",
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-==== Append processor
-Appends one or more values to an existing array if the field already exists and it is an array.
-Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar.
-Creates an array containing the provided values if the fields doesn't exist.
-Accepts a single value or an array of values.
-
-[source,js]
---------------------------------------------------
-{
-  "append": {
-    "field": "field1"
-    "value": ["item2", "item3", "item4"]
-  }
-}
---------------------------------------------------
-
-==== Remove processor
-Removes an existing field. If the field doesn't exist, an exception will be thrown
-
-[source,js]
---------------------------------------------------
-{
-  "remove": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Rename processor
-Renames an existing field. If the field doesn't exist, an exception will be thrown. Also, the new field
-name must not exist.
-
-[source,js]
---------------------------------------------------
-{
-  "rename": {
-    "field": "foo",
-    "to": "foobar"
-  }
-}
---------------------------------------------------
-
-
-==== Convert processor
-Converts an existing field's value to a different type, like turning a string to an integer.
-If the field value is an array, all members will be converted.
-
-The supported types include: `integer`, `float`, `string`, and `boolean`.
-
-`boolean` will set the field to true if its string value is equal to `true` (ignore case), to
-false if its string value is equal to `false` (ignore case) and it will throw exception otherwise.
-
-[source,js]
---------------------------------------------------
-{
-  "convert": {
-    "field" : "foo"
-    "type": "integer"
-  }
-}
---------------------------------------------------
-
-==== Gsub processor
-Converts a string field by applying a regular expression and a replacement.
-If the field is not a string, the processor will throw an exception.
-
-This configuration takes a `field` for the field name, `pattern` for the
-pattern to be replaced, and `replacement` for the string to replace the matching patterns with.
-
-
-[source,js]
---------------------------------------------------
-{
-  "gsub": {
-    "field": "field1",
-    "pattern": "\.",
-    "replacement": "-"
-  }
-}
---------------------------------------------------
-
-==== Join processor
-Joins each element of an array into a single string using a separator character between each element.
-Throws error when the field is not an array.
-
-[source,js]
---------------------------------------------------
-{
-  "join": {
-    "field": "joined_array_field",
-    "separator": "-"
-  }
-}
---------------------------------------------------
-
-==== Split processor
-Split a field to an array using a separator character. Only works on string fields.
-
-[source,js]
---------------------------------------------------
-{
-  "split": {
-    "field": ","
-  }
-}
---------------------------------------------------
-
-==== Lowercase processor
-Converts a string to its lowercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "lowercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Uppercase processor
-Converts a string to its uppercase equivalent.
-
-[source,js]
---------------------------------------------------
-{
-  "uppercase": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Trim processor
-Trims whitespace from field. NOTE: this only works on leading and trailing whitespaces.
-
-[source,js]
---------------------------------------------------
-{
-  "trim": {
-    "field": "foo"
-  }
-}
---------------------------------------------------
-
-==== Grok Processor
-
-The Grok Processor extracts structured fields out of a single text field within a document. You choose which field to
-extract matched fields from, as well as the Grok Pattern you expect will match. A Grok Pattern is like a regular
-expression that supports aliased expressions that can be reused.
-
-This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format
-that is generally written for humans and not computer consumption.
-
-The processor comes packaged with over 120 reusable patterns that are located at `$ES_HOME/config/ingest/grok/patterns`.
-Here, you can add your own custom grok pattern files with custom grok expressions to be used by the processor.
-
-If you need help building patterns to match your logs, you will find the <http://grokdebug.herokuapp.com> and
-<http://grokconstructor.appspot.com/> applications quite useful!
-
-===== Grok Basics
-
-Grok sits on top of regular expressions, so any regular expressions are valid in grok as well.
-The regular expression library is Oniguruma, and you can see the full supported regexp syntax
-https://github.com/kkos/oniguruma/blob/master/doc/RE[on the Onigiruma site].
-
-Grok works by leveraging this regular expression language to allow naming existing patterns and combining them into more
-complex patterns that match your fields.
-
-The syntax for re-using a grok pattern comes in three forms: `%{SYNTAX:SEMANTIC}`, `%{SYNTAX}`, `%{SYNTAX:SEMANTIC:TYPE}`.
-
-The `SYNTAX` is the name of the pattern that will match your text. For example, `3.44` will be matched by the `NUMBER`
-pattern and `55.3.244.1` will be matched by the `IP` pattern. The syntax is how you match. `NUMBER` and `IP` are both
-patterns that are provided within the default patterns set.
-
-The `SEMANTIC` is the identifier you give to the piece of text being matched. For example, `3.44` could be the
-duration of an event, so you could call it simply `duration`. Further, a string `55.3.244.1` might identify
-the `client` making a request.
-
-The `TYPE` is the type you wish to cast your named field. `int` and `float` are currently the only types supported for coercion.
-
-For example, here is a grok pattern that would match the above example given. We would like to match a text with the following
-contents:
-
-[source,js]
---------------------------------------------------
-3.44 55.3.244.1
---------------------------------------------------
-
-We may know that the above message is a number followed by an IP-address. We can match this text with the following
-Grok expression.
-
-[source,js]
---------------------------------------------------
-%{NUMBER:duration} %{IP:client}
---------------------------------------------------
-
-===== Custom Patterns and Pattern Files
-
-The Grok Processor comes pre-packaged with a base set of pattern files. These patterns may not always have
-what you are looking for. These pattern files have a very basic format. Each line describes a named pattern with
-the following format:
-
-[source,js]
---------------------------------------------------
-NAME ' '+ PATTERN '\n'
---------------------------------------------------
-
-You can add this pattern to an existing file, or add your own file in the patterns directory here: `$ES_HOME/config/ingest/grok/patterns`.
-The Ingest Plugin will pick up files in this directory to be loaded into the grok processor's known patterns. These patterns are loaded
-at startup, so you will need to do a restart your ingest node if you wish to update these files while running.
-
-Example snippet of pattern definitions found in the `grok-patterns` patterns file:
-
-[source,js]
---------------------------------------------------
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
---------------------------------------------------
-
-===== Using Grok Processor in a Pipeline
-
-[[grok-options]]
-.Grok Options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to use for grok expression parsing
-| `match_pattern`        | yes       | -                   | The grok expression to match and extract named captures with
-| `pattern_definitions`  | no        | -                   | A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.
-|======
-
-Here is an example of using the provided patterns to extract out and name structured fields from a string field in
-a document.
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043"
-}
---------------------------------------------------
-
-The pattern for this could be
-
-[source]
---------------------------------------------------
-%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
---------------------------------------------------
-
-An example pipeline for processing the above document using Grok:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-This pipeline will insert these named captures as new fields within the document, like so:
-
-[source,js]
---------------------------------------------------
-{
-  "message": "55.3.244.1 GET /index.html 15824 0.043",
-  "client": "55.3.244.1",
-  "method": "GET",
-  "request": "/index.html",
-  "bytes": 15824,
-  "duration": "0.043"
-}
---------------------------------------------------
-
-An example of a pipeline specifying custom pattern definitions:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors": [
-    {
-      "grok": {
-        "match_field": "message",
-        "match_pattern": "my %{FAVORITE_DOG:dog} is colored %{RGB:color}"
-        "pattern_definitions" : {
-          "FAVORITE_DOG" : "beagle",
-          "RGB" : "RED|GREEN|BLUE"
-        }
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-
-==== Geoip processor
-
-The GeoIP processor adds information about the geographical location of IP addresses, based on data from the Maxmind databases.
-This processor adds this information by default under the `geoip` field.
-
-The ingest plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
-under the CCA-ShareAlike 3.0 license. For more details see, http://dev.maxmind.com/geoip/geoip2/geolite2/
-
-The GeoIP processor can run with other geoip2 databases from Maxmind. The files must be copied into the geoip config directory
-and the `database_file` option should be used to specify the filename of the custom database. The geoip config directory
-is located at `$ES_HOME/config/ingest/geoip` and holds the shipped databases too.
-
-[[geoip-options]]
-.Geoip options
-[options="header"]
-|======
-| Name                   | Required  | Default                                                                            | Description
-| `source_field`         | yes       | -                                                                                  | The field to get the ip address or hostname from for the geographical lookup.
-| `target_field`         | no        | geoip                                                                              | The field that will hold the geographical information looked up from the Maxmind database.
-| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
-| `fields`               | no        | [`continent_name`, `country_iso_code`, `region_name`, `city_name`, `location`] <1> | Controls what properties are added to the `target_field` based on the geoip lookup.
-|======
-
-<1> Depends on what is available in `database_field`:
-* If the GeoLite2 City database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name`, `continent_name`, `region_name`, `city_name`, `timezone`, `latitude`, `longitude`
-and `location`. The fields actually added depend on what has been found and which fields were configured in `fields`.
-* If the GeoLite2 Country database is used then the following fields may be added under the `target_field`: `ip`,
-`country_iso_code`, `country_name` and `continent_name`.The fields actually added depend on what has been found and which fields were configured in `fields`.
-
-An example that uses the default city database and adds the geographical information to the `geoip` field based on the `ip` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-An example that uses the default country database and add the geographical information to the `geo` field based on the `ip` field`:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "geoip" : {
-        "source_field" : "ip",
-        "target_field" : "geo",
-        "database_file" : "GeoLite2-Country.mmdb"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Date processor
-
-The date processor is used for parsing dates from fields, and then using that date or timestamp as the timestamp for that document.
-The date processor adds by default the parsed date as a new field called `@timestamp`, configurable by setting the `target_field`
-configuration parameter. Multiple date formats are supported as part of the same date processor definition. They will be used
-sequentially to attempt parsing the date field, in the same order they were defined as part of the processor definition.
-
-[[date-options]]
-.Date options
-[options="header"]
-|======
-| Name                   | Required  | Default             | Description
-| `match_field`          | yes       | -                   | The field to get the date from.
-| `target_field`         | no        | @timestamp          | The field that will hold the parsed date.
-| `match_formats`        | yes       | -                   | Array of the expected date formats. Can be a joda pattern or one of the following formats: ISO8601, UNIX, UNIX_MS, TAI64N.
-| `timezone`             | no        | UTC                 | The timezone to use when parsing the date.
-| `locale`               | no        | ENGLISH             | The locale to use when parsing the date, relevant when parsing month names or week days.
-|======
-
-An example that adds the parsed date to the `timestamp` field based on the `initial_date` field:
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "...",
-  "processors" : [
-    {
-      "date" : {
-        "match_field" : "initial_date",
-        "target_field" : "timestamp",
-        "match_formats" : ["dd/MM/yyyy hh:mm:ss"],
-        "timezone" : "Europe/Amsterdam"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-==== Fail processor
-The Fail Processor is used to raise an exception. This is useful for when
-a user expects a pipeline to fail and wishes to relay a specific message
-to the requester.
-
-[source,js]
---------------------------------------------------
-{
-  "fail": {
-    "message": "an error message"
-  }
-}
---------------------------------------------------
-
-==== DeDot Processor
-The DeDot Processor is used to remove dots (".") from field names and
-replace them with a specific `separator` string.
-
-[source,js]
---------------------------------------------------
-{
-  "dedot": {
-    "separator": "_"
-  }
-}
---------------------------------------------------
-
-
-=== Accessing data in pipelines
-
-Processors in pipelines have read and write access to documents that pass through the pipeline.
-The fields in the source of a document and its metadata fields are accessible.
-
-Accessing a field in the source is straightforward and one can refer to fields by
-their name. For example:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "my_field"
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-On top of this fields from the source are always accessible via the `_source` prefix:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_source.my_field"
-    "value": 582.1
-  }
-}
---------------------------------------------------
-
-Metadata fields can also be accessed in the same way as fields from the source. This
-is possible because Elasticsearch doesn't allow fields in the source that have the
-same name as metadata fields.
-
-The following example sets the id of a document to `1`:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_id"
-    "value": "1"
-  }
-}
---------------------------------------------------
-
-The following metadata fields are accessible by a processor: `_index`, `_type`, `_id`, `_routing`, `_parent`,
-`_timestamp` and `_ttl`.
-
-Beyond metadata fields and source fields, the ingest plugin also adds ingest metadata to documents being processed.
-These metadata properties are accessible under the `_ingest` key. Currently the ingest plugin adds the ingest timestamp
-under `_ingest.timestamp` key to the ingest metadata, which is the time the ingest plugin received the index or bulk
-request to pre-process. But any processor is free to add more ingest related metadata to it. Ingest metadata is transient
-and is lost after a document has been processed by the pipeline and thus ingest metadata won't be indexed.
-
-The following example adds a field with the name `received` and the value is the ingest timestamp:
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "received"
-    "value": "{{_ingest.timestamp}}"
-  }
-}
---------------------------------------------------
-
-As opposed to Elasticsearch metadata fields, the ingest metadata field name _ingest can be used as a valid field name
-in the source of a document. Use _source._ingest to refer to it, otherwise _ingest will be interpreted as ingest
-metadata fields by the ingest plugin.
-
-A number of processor settings also support templating. Settings that support templating can have zero or more
-template snippets. A template snippet begins with `{{` and ends with `}}`.
-Accessing fields and metafields in templates is exactly the same as via regular processor field settings.
-
-In this example a field by the name `field_c` is added and its value is a concatenation of
-the values of `field_a` and `field_b`.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "field_c"
-    "value": "{{field_a}} {{field_b}}"
-  }
-}
---------------------------------------------------
-
-The following example changes the index a document is going to be indexed into. The index a document will be redirected
-to depends on the field in the source with name `geoip.country_iso_code`.
-
-[source,js]
---------------------------------------------------
-{
-  "set": {
-    "field": "_index"
-    "value": "{{geoip.country_iso_code}}"
-  }
-}
---------------------------------------------------
-
-==== Handling Failure in Pipelines
-
-In its simplest case, pipelines describe a list of processors which 
-are executed sequentially and processing halts at the first exception. This 
-may not be desirable when failures are expected. For example, not all your logs 
-may match a certain grok expression and you may wish to index such documents into 
-a separate index.
-
-To enable this behavior, you can utilize the `on_failure` parameter. `on_failure` 
-defines a list of processors to be executed immediately following the failed processor.
-This parameter can be supplied at the pipeline level, as well as at the processor 
-level. If a processor has an `on_failure` configuration option provided, whether 
-it is empty or not, any exceptions that are thrown by it will be caught and the 
-pipeline will continue executing the proceeding processors defined. Since further processors
-are defined within the scope of an `on_failure` statement, failure handling can be nested.
-
-Example: In the following example we define a pipeline that hopes to rename documents with 
-a field named `foo` to `bar`. If the document does not contain the `foo` field, we 
-go ahead and attach an error message within the document for later analysis within 
-Elasticsearch.
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "my first pipeline with handled exceptions",
-  "processors" : [
-    {
-      "rename" : {
-        "field" : "foo",
-        "to" : "bar",
-        "on_failure" : [
-          {
-            "set" : {
-              "field" : "error",
-              "value" : "field \"foo\" does not exist, cannot rename to \"bar\""
-            }
-          }
-        ]
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-Example: Here we define an `on_failure` block on a whole pipeline to change 
-the index for which failed documents get sent.
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "my first pipeline with handled exceptions",
-  "processors" : [ ... ],
-  "on_failure" : [
-    {
-      "set" : {
-        "field" : "_index",
-        "value" : "failed-{{ _index }}"
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-
-===== Accessing Error Metadata From Processors Handling Exceptions
-
-Sometimes you may want to retrieve the actual error message that was thrown 
-by a failed processor. To do so you can access metadata fields called 
-`on_failure_message` and `on_failure_processor`. These fields are only accessible 
-from within the context of an `on_failure` block. Here is an updated version of 
-our first example which leverages these fields to provide the error message instead 
-of manually setting it.
-
-[source,js]
---------------------------------------------------
-{
-  "description" : "my first pipeline with handled exceptions",
-  "processors" : [
-    {
-      "rename" : {
-        "field" : "foo",
-        "to" : "bar",
-        "on_failure" : [
-          {
-            "set" : {
-              "field" : "error",
-              "value" : "{{ _ingest.on_failure_message }}"
-            }
-          }
-        ]
-      }
-    }
-  ]
-}
---------------------------------------------------
-
-
-=== Ingest APIs
-
-==== Put pipeline API
-
-The put pipeline api adds pipelines and updates existing pipelines in the cluster.
-
-[source,js]
---------------------------------------------------
-PUT _ingest/pipeline/my-pipeline-id
-{
-  "description" : "describe pipeline",
-  "processors" : [
-    {
-      "simple" : {
-        // settings
-      }
-    },
-    // other processors
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-NOTE: The put pipeline api also instructs all ingest nodes to reload their in-memory representation of pipelines, so that
-      pipeline changes take immediately in effect.
-
-==== Get pipeline API
-
-The get pipeline api returns pipelines based on id. This api always returns a local reference of the pipeline.
-
-[source,js]
---------------------------------------------------
-GET _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
-
-Example response:
-
-[source,js]
---------------------------------------------------
-{
-   "my-pipeline-id": {
-      "_source" : {
-        "description": "describe pipeline",
-        "processors": [
-          {
-            "simple" : {
-              // settings
-            }
-          },
-          // other processors
-        ]
-      },
-      "_version" : 0
-   }
-}
---------------------------------------------------
-
-For each returned pipeline the source and the version is returned.
-The version is useful for knowing what version of the pipeline the node has.
-Multiple ids can be provided at the same time. Also wildcards are supported.
-
-==== Delete pipeline API
-
-The delete pipeline api deletes pipelines by id.
-
-[source,js]
---------------------------------------------------
-DELETE _ingest/pipeline/my-pipeline-id
---------------------------------------------------
-// AUTOSENSE
-
-==== Simulate pipeline API
-
-The simulate pipeline api executes a specific pipeline against
-the set of documents provided in the body of the request.
-
-A simulate request may call upon an existing pipeline to be executed
-against the provided documents, or supply a pipeline definition in
-the body of the request.
-
-Here is the structure of a simulate request with a provided pipeline:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate
-{
-  "pipeline" : {
-    // pipeline definition here
-  },
-  "docs" : [
-    { /** first document **/ },
-    { /** second document **/ },
-    // ...
-  ]
-}
---------------------------------------------------
-
-Here is the structure of a simulate request against a pre-existing pipeline:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/my-pipeline-id/_simulate
-{
-  "docs" : [
-    { /** first document **/ },
-    { /** second document **/ },
-    // ...
-  ]
-}
---------------------------------------------------
-
-
-Here is an example simulate request with a provided pipeline and its response:
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate
-{
-  "pipeline" :
-  {
-    "description": "_description",
-    "processors": [
-      {
-        "set" : {
-          "field" : "field2",
-          "value" : "_value"
-        }
-      }
-    ]
-  },
-  "docs": [
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "bar"
-      }
-    },
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "rab"
-      }
-    }
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-response:
-
-[source,js]
---------------------------------------------------
-{
-   "docs": [
-      {
-         "doc": {
-            "_id": "id",
-            "_ttl": null,
-            "_parent": null,
-            "_index": "index",
-            "_routing": null,
-            "_type": "type",
-            "_timestamp": null,
-            "_source": {
-               "field2": "_value",
-               "foo": "bar"
-            },
-            "_ingest": {
-               "timestamp": "2016-01-04T23:53:27.186+0000"
-            }
-         }
-      },
-      {
-         "doc": {
-            "_id": "id",
-            "_ttl": null,
-            "_parent": null,
-            "_index": "index",
-            "_routing": null,
-            "_type": "type",
-            "_timestamp": null,
-            "_source": {
-               "field2": "_value",
-               "foo": "rab"
-            },
-            "_ingest": {
-               "timestamp": "2016-01-04T23:53:27.186+0000"
-            }
-         }
-      }
-   ]
-}
---------------------------------------------------
-
-It is often useful to see how each processor affects the ingest document
-as it is passed through the pipeline. To see the intermediate results of
-each processor in the simulat request, a `verbose` parameter may be added
-to the request
-
-Here is an example verbose request and its response:
-
-
-[source,js]
---------------------------------------------------
-POST _ingest/pipeline/_simulate?verbose
-{
-  "pipeline" :
-  {
-    "description": "_description",
-    "processors": [
-      {
-        "set" : {
-          "field" : "field2",
-          "value" : "_value2"
-        }
-      },
-      {
-        "set" : {
-          "field" : "field3",
-          "value" : "_value3"
-        }
-      }
-    ]
-  },
-  "docs": [
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "bar"
-      }
-    },
-    {
-      "_index": "index",
-      "_type": "type",
-      "_id": "id",
-      "_source": {
-        "foo": "rab"
-      }
-    }
-  ]
-}
---------------------------------------------------
-// AUTOSENSE
-
-response:
-
-[source,js]
---------------------------------------------------
-{
-   "docs": [
-      {
-         "processor_results": [
-            {
-               "processor_id": "processor[set]-0",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field2": "_value2",
-                     "foo": "bar"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.383+0000"
-                  }
-               }
-            },
-            {
-               "processor_id": "processor[set]-1",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field3": "_value3",
-                     "field2": "_value2",
-                     "foo": "bar"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.383+0000"
-                  }
-               }
-            }
-         ]
-      },
-      {
-         "processor_results": [
-            {
-               "processor_id": "processor[set]-0",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field2": "_value2",
-                     "foo": "rab"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.384+0000"
-                  }
-               }
-            },
-            {
-               "processor_id": "processor[set]-1",
-               "doc": {
-                  "_id": "id",
-                  "_ttl": null,
-                  "_parent": null,
-                  "_index": "index",
-                  "_routing": null,
-                  "_type": "type",
-                  "_timestamp": null,
-                  "_source": {
-                     "field3": "_value3",
-                     "field2": "_value2",
-                     "foo": "rab"
-                  },
-                  "_ingest": {
-                     "timestamp": "2016-01-05T00:02:51.384+0000"
-                  }
-               }
-            }
-         ]
-      }
-   ]
-}
---------------------------------------------------
diff --git a/docs/plugins/repository-s3.asciidoc b/docs/plugins/repository-s3.asciidoc
index faaa873..7c89b0e 100644
--- a/docs/plugins/repository-s3.asciidoc
+++ b/docs/plugins/repository-s3.asciidoc
@@ -116,6 +116,7 @@ The available values are:
 * `ap-southeast-1`
 * `ap-southeast-2`
 * `ap-northeast` (`ap-northeast-1`)
+* `ap-northeast-2` (`ap-northeast-2`)
 * `eu-west` (`eu-west-1`)
 * `eu-central` (`eu-central-1`)
 * `sa-east` (`sa-east-1`)
diff --git a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
index 5afff0c..29ba5e4 100644
--- a/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/datehistogram-aggregation.asciidoc
@@ -105,8 +105,7 @@ that bucketing should use a different time zone.
 
 Time zones may either be specified as an ISO 8601 UTC offset (e.g. `+01:00` or
 `-08:00`)  or as a timezone id, an identifier used in the TZ database like
-`America\Los_Angeles` (which would need to be escaped in JSON as
-`"America\\Los_Angeles"`).
+`America/Los_Angeles`.
 
 Consider the following example:
 
diff --git a/docs/reference/docs/index_.asciidoc b/docs/reference/docs/index_.asciidoc
index 3aacdc7..5f79efb 100644
--- a/docs/reference/docs/index_.asciidoc
+++ b/docs/reference/docs/index_.asciidoc
@@ -33,6 +33,7 @@ The result of the above index operation is:
 --------------------------------------------------
 
 The `_shards` header provides information about the replication process of the index operation.
+
 * `total` - Indicates to how many shard copies (primary and replica shards) the index operation should be executed on.
 * `successful`- Indicates the number of shard copies the index operation succeeded on.
 * `failures` - An array that contains replication related errors in the case an index operation failed on a replica shard.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 2b6daac..76b1ddb 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -16,6 +16,7 @@ your application to Elasticsearch 3.0.
 * <<breaking_30_thread_pool>>
 * <<breaking_30_allocation>>
 * <<breaking_30_percolator>>
+* <<breaking_30_packaging>>
 
 [[breaking_30_search_changes]]
 === Warmers
@@ -644,3 +645,17 @@ When percolating an existing document then specifying a document in the source o
 any more.
 
 Percolator documents are no longer excluded from the search response.
+
+[[breaking_30_packaging]]
+=== Packaging
+
+==== Default logging using systemd (since Elasticsearch 2.2.0)
+
+In previous versions of Elasticsearch, the default logging
+configuration routed standard output to /dev/null and standard error to
+the journal. However, there are often critical error messages at
+startup that are logged to standard output rather than standard error
+and these error messages would be lost to the nether. The default has
+changed to now route standard output to the journal and standard error
+to inherit this setting (these are the defaults for systemd). These
+settings can be modified by editing the elasticsearch.service file.
diff --git a/docs/reference/query-dsl/function-score-query.asciidoc b/docs/reference/query-dsl/function-score-query.asciidoc
index 08e5e57..39fdae8 100644
--- a/docs/reference/query-dsl/function-score-query.asciidoc
+++ b/docs/reference/query-dsl/function-score-query.asciidoc
@@ -300,9 +300,9 @@ location field. You want to compute a decay function depending on how
 far the hotel is from a given location. You might not immediately see
 what scale to choose for the gauss function, but you can say something
 like: "At a distance of 2km from the desired location, the score should
-be reduced by one third."
+be reduced to one third."
 The parameter "scale" will then be adjusted automatically to assure that
-the score function computes a score of 0.5 for hotels that are 2km away
+the score function computes a score of 0.33 for hotels that are 2km away
 from the desired location.
 
 
diff --git a/docs/reference/search/suggesters/term-suggest.asciidoc b/docs/reference/search/suggesters/term-suggest.asciidoc
index 55fce63..965a487 100644
--- a/docs/reference/search/suggesters/term-suggest.asciidoc
+++ b/docs/reference/search/suggesters/term-suggest.asciidoc
@@ -9,70 +9,70 @@ suggest text is analyzed before terms are suggested. The suggested terms
 are provided per analyzed suggest text token. The `term` suggester
 doesn't take the query into account that is part of request.
 
-==== Common suggest options: 
+==== Common suggest options:
 
 [horizontal]
-`text`:: 
+`text`::
     The suggest text. The suggest text is a required option that
     needs to be set globally or per suggestion.
 
-`field`:: 
+`field`::
     The field to fetch the candidate suggestions from. This is
     an required option that either needs to be set globally or per
-    suggestion. 
+    suggestion.
 
-`analyzer`:: 
+`analyzer`::
     The analyzer to analyse the suggest text with. Defaults
-    to the search analyzer of the suggest field. 
+    to the search analyzer of the suggest field.
 
-`size`:: 
+`size`::
     The maximum corrections to be returned per suggest text
-    token. 
+    token.
 
-`sort`:: 
+`sort`::
     Defines how suggestions should be sorted per suggest text
     term. Two possible values:
 +
-    ** `score`:     Sort by score first, then document frequency and 
-                    then the term itself. 
+    ** `score`:     Sort by score first, then document frequency and
+                    then the term itself.
     ** `frequency`: Sort by document frequency first, then similarity
-                    score and then the term itself. 
+                    score and then the term itself.
 +
-`suggest_mode`:: 
+`suggest_mode`::
     The suggest mode controls what suggestions are
     included or controls for what suggest text terms, suggestions should be
-    suggested. Three possible values can be specified: 
-+    
+    suggested. Three possible values can be specified:
++
      ** `missing`:  Only provide suggestions for suggest text terms that are
-                    not in the index. This is the default. 
+                    not in the index. This is the default.
      ** `popular`:  Only suggest suggestions that occur in more docs then
-                    the original suggest text term. 
+                    the original suggest text term.
      ** `always`:   Suggest any matching suggestions based on terms in the
                     suggest text.
 
-==== Other term suggest options: 
+==== Other term suggest options:
 
 [horizontal]
-`lowercase_terms`:: 
-    Lower cases the suggest text terms after text analysis. 
+`lowercase_terms`::
+    Lower cases the suggest text terms after text analysis.
 
-`max_edits`:: 
+`max_edits`::
     The maximum edit distance candidate suggestions can
     have in order to be considered as a suggestion. Can only be a value
     between 1 and 2. Any other value result in an bad request error being
-    thrown. Defaults to 2. 
+    thrown. Defaults to 2.
 
-`prefix_length`:: 
+`prefix_length`::
     The number of minimal prefix characters that must
     match in order be a candidate suggestions. Defaults to 1. Increasing
     this number improves spellcheck performance. Usually misspellings don't
-    occur in the beginning of terms. (Old name "prefix_len" is deprecated) 
+    occur in the beginning of terms. (Old name "prefix_len" is deprecated)
 
-`min_word_length`:: 
+`min_word_length`::
     The minimum length a suggest text term must have in
     order to be included. Defaults to 4. (Old name "min_word_len" is deprecated)
 
-`shard_size`:: 
+`shard_size`::
     Sets the maximum number of suggestions to be retrieved
     from each individual shard. During the reduce phase only the top N
     suggestions are returned based on the `size` option. Defaults to the
@@ -81,24 +81,24 @@ doesn't take the query into account that is part of request.
     corrections at the cost of performance. Due to the fact that terms are
     partitioned amongst shards, the shard level document frequencies of
     spelling corrections may not be precise. Increasing this will make these
-    document frequencies more precise. 
+    document frequencies more precise.
 
-`max_inspections`:: 
+`max_inspections`::
     A factor that is used to multiply with the
     `shards_size` in order to inspect more candidate spell corrections on
     the shard level. Can improve accuracy at the cost of performance.
-    Defaults to 5. 
+    Defaults to 5.
 
-`min_doc_freq`:: 
+`min_doc_freq`::
     The minimal threshold in number of documents a
     suggestion should appear in. This can be specified as an absolute number
     or as a relative percentage of number of documents. This can improve
     quality by only suggesting high frequency terms. Defaults to 0f and is
     not enabled. If a value higher than 1 is specified then the number
     cannot be fractional. The shard level document frequencies are used for
-    this option. 
+    this option.
 
-`max_term_freq`:: 
+`max_term_freq`::
     The maximum threshold in number of documents a
     suggest text token can exist in order to be included. Can be a relative
     percentage number (e.g 0.4) or an absolute number to represent document
@@ -108,3 +108,15 @@ doesn't take the query into account that is part of request.
     usually spelled correctly on top of this also improves the spellcheck
     performance. The shard level document frequencies are used for this
     option.
+
+`string_distance`::
+    Which string distance implementation to use for comparing how similar
+    suggested terms are. Five possible values can be specfied:
+    `internal` - The default based on damerau_levenshtein but highly optimized
+    for comparing string distancee for terms inside the index.
+    `damerau_levenshtein` - String distance algorithm based on
+    Damerau-Levenshtein algorithm.
+    `levenstein` - String distance algorithm based on Levenstein edit distance
+    algorithm.
+    `jarowinkler` - String distance algorithm based on Jaro-Winkler algorithm.
+    `ngram` - String distance algorithm based on character n-grams.
diff --git a/docs/reference/setup/upgrade.asciidoc b/docs/reference/setup/upgrade.asciidoc
index 894f82a..05b6358 100644
--- a/docs/reference/setup/upgrade.asciidoc
+++ b/docs/reference/setup/upgrade.asciidoc
@@ -27,7 +27,7 @@ consult this table:
 |2.x            |3.x            |<<restart-upgrade,Full cluster restart>>
 |=======================================================================
 
-TIP: Take plugins into consideration as well when upgrading. Most plugins will have to be upgraded alongside Elasticsearch, although some plugins accessed primarily through the browser (`_site` plugins) may continue to work given that API changes are compatible.
+TIP: Take plugins into consideration as well when upgrading. Plugins must be upgraded alongside Elasticsearch.
 
 include::backup.asciidoc[]
 
diff --git a/modules/build.gradle b/modules/build.gradle
index 41f7a88..4b88dfd 100644
--- a/modules/build.gradle
+++ b/modules/build.gradle
@@ -39,8 +39,5 @@ subprojects {
     if (esplugin.isolated == false) {
       throw new InvalidModelException("Modules cannot disable isolation")
     }
-    if (esplugin.jvm == false) {
-      throw new InvalidModelException("Modules must be jvm plugins")
-    }
   }
 }
diff --git a/modules/ingest-grok/build.gradle b/modules/ingest-grok/build.gradle
deleted file mode 100644
index 2672234..0000000
--- a/modules/ingest-grok/build.gradle
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-    description 'Ingest processor that uses grok patterns to split text'
-    classname 'org.elasticsearch.ingest.grok.IngestGrokPlugin'
-}
-
-dependencies {
-    compile 'org.jruby.joni:joni:2.1.6'
-    // joni dependencies:
-    compile 'org.jruby.jcodings:jcodings:1.0.12'
-}
-
-compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
-compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
-
-thirdPartyAudit.excludes = [
-        // joni has AsmCompilerSupport, but that isn't being used:
-        'org.objectweb.asm.ClassWriter',
-        'org.objectweb.asm.MethodVisitor',
-        'org.objectweb.asm.Opcodes',
-]
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1 b/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
deleted file mode 100644
index b097e32..0000000
--- a/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-6bc17079fcaa8823ea8cd0d4c66516335b558db8
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-LICENSE.txt b/modules/ingest-grok/licenses/jcodings-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/modules/ingest-grok/licenses/jcodings-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-NOTICE.txt b/modules/ingest-grok/licenses/jcodings-NOTICE.txt
deleted file mode 100644
index f6c4948..0000000
--- a/modules/ingest-grok/licenses/jcodings-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-JCodings is released under the MIT License.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1 b/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
deleted file mode 100644
index 48abe13..0000000
--- a/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-0f23c95a06eaecbc8c74c7458a8bfd13e4fd2d3a
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-LICENSE.txt b/modules/ingest-grok/licenses/joni-LICENSE.txt
deleted file mode 100644
index a3fdf73..0000000
--- a/modules/ingest-grok/licenses/joni-LICENSE.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
-of the Software, and to permit persons to whom the Software is furnished to do
-so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-NOTICE.txt b/modules/ingest-grok/licenses/joni-NOTICE.txt
deleted file mode 100644
index 45bc517..0000000
--- a/modules/ingest-grok/licenses/joni-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
-Joni is released under the MIT License.
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
deleted file mode 100644
index abed841..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.jcodings.specific.UTF8Encoding;
-import org.joni.Matcher;
-import org.joni.NameEntry;
-import org.joni.Option;
-import org.joni.Regex;
-import org.joni.Region;
-import org.joni.Syntax;
-import org.joni.exception.ValueException;
-
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Locale;
-import java.util.Map;
-
-final class Grok {
-
-    private static final String NAME_GROUP = "name";
-    private static final String SUBNAME_GROUP = "subname";
-    private static final String PATTERN_GROUP = "pattern";
-    private static final String DEFINITION_GROUP = "definition";
-    private static final String GROK_PATTERN =
-            "%\\{" +
-            "(?<name>" +
-            "(?<pattern>[A-z0-9]+)" +
-            "(?::(?<subname>[A-z0-9_:.-]+))?" +
-            ")" +
-            "(?:=(?<definition>" +
-            "(?:" +
-            "(?:[^{}]+|\\.+)+" +
-            ")+" +
-            ")" +
-            ")?" + "\\}";
-    private static final Regex GROK_PATTERN_REGEX = new Regex(GROK_PATTERN.getBytes(StandardCharsets.UTF_8), 0, GROK_PATTERN.getBytes(StandardCharsets.UTF_8).length, Option.NONE, UTF8Encoding.INSTANCE, Syntax.DEFAULT);
-    private final Map<String, String> patternBank;
-    private final boolean namedCaptures;
-    private final Regex compiledExpression;
-    private final String expression;
-
-
-    public Grok(Map<String, String> patternBank, String grokPattern) {
-        this(patternBank, grokPattern, true);
-    }
-
-    @SuppressWarnings("unchecked")
-    public Grok(Map<String, String> patternBank, String grokPattern, boolean namedCaptures) {
-        this.patternBank = patternBank;
-        this.namedCaptures = namedCaptures;
-
-        this.expression = toRegex(grokPattern);
-        byte[] expressionBytes = expression.getBytes(StandardCharsets.UTF_8);
-        this.compiledExpression = new Regex(expressionBytes, 0, expressionBytes.length, Option.DEFAULT, UTF8Encoding.INSTANCE);
-    }
-
-
-    public String groupMatch(String name, Region region, String pattern) {
-        try {
-            int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region);
-            int begin = region.beg[number];
-            int end = region.end[number];
-            return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8);
-        } catch (StringIndexOutOfBoundsException e) {
-            return null;
-        } catch (ValueException e) {
-            return null;
-        }
-    }
-
-    /**
-     * converts a grok expression into a named regex expression
-     *
-     * @return named regex expression
-     */
-    public String toRegex(String grokPattern) {
-        byte[] grokPatternBytes = grokPattern.getBytes(StandardCharsets.UTF_8);
-        Matcher matcher = GROK_PATTERN_REGEX.matcher(grokPatternBytes);
-
-        int result = matcher.search(0, grokPatternBytes.length, Option.NONE);
-        if (result != -1) {
-            Region region = matcher.getEagerRegion();
-            String namedPatternRef = groupMatch(NAME_GROUP, region, grokPattern);
-            String subName = groupMatch(SUBNAME_GROUP, region, grokPattern);
-            // TODO(tal): Support definitions
-            String definition = groupMatch(DEFINITION_GROUP, region, grokPattern);
-            String patternName = groupMatch(PATTERN_GROUP, region, grokPattern);
-            String pattern = patternBank.get(patternName);
-
-            String grokPart;
-            if (namedCaptures && subName != null) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", namedPatternRef, pattern);
-            } else if (!namedCaptures) {
-                grokPart = String.format(Locale.US, "(?<%s>%s)", patternName + "_" + String.valueOf(result), pattern);
-            } else {
-                grokPart = String.format(Locale.US, "(?:%s)", pattern);
-            }
-
-            String start = new String(grokPatternBytes, 0, result, StandardCharsets.UTF_8);
-            String rest = new String(grokPatternBytes, region.end[0], grokPatternBytes.length - region.end[0], StandardCharsets.UTF_8);
-            return start + toRegex(grokPart + rest);
-        }
-
-        return grokPattern;
-    }
-
-    public boolean match(String text) {
-        Matcher matcher = compiledExpression.matcher(text.getBytes(StandardCharsets.UTF_8));
-        int result = matcher.search(0, text.length(), Option.DEFAULT);
-        return (result != -1);
-    }
-
-    public Map<String, Object> captures(String text) {
-        byte[] textAsBytes = text.getBytes(StandardCharsets.UTF_8);
-        Map<String, Object> fields = new HashMap<>();
-        Matcher matcher = compiledExpression.matcher(textAsBytes);
-        int result = matcher.search(0, textAsBytes.length, Option.DEFAULT);
-        if (result != -1 && compiledExpression.numberOfNames() > 0) {
-            Region region = matcher.getEagerRegion();
-            for (Iterator<NameEntry> entry = compiledExpression.namedBackrefIterator(); entry.hasNext();) {
-                NameEntry e = entry.next();
-                int number = e.getBackRefs()[0];
-
-                String groupName = new String(e.name, e.nameP, e.nameEnd - e.nameP, StandardCharsets.UTF_8);
-                String matchValue = null;
-                if (region.beg[number] >= 0) {
-                    matchValue = new String(textAsBytes, region.beg[number], region.end[number] - region.beg[number], StandardCharsets.UTF_8);
-                }
-                GrokMatchGroup match = new GrokMatchGroup(groupName, matchValue);
-                fields.put(match.getName(), match.getValue());
-            }
-            return fields;
-        } else if (result != -1) {
-            return fields;
-        }
-        return null;
-    }
-}
-
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
deleted file mode 100644
index 2cebf62..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-final class GrokMatchGroup {
-    private static final String DEFAULT_TYPE = "string";
-    private final String patternName;
-    private final String fieldName;
-    private final String type;
-    private final String groupValue;
-
-    public GrokMatchGroup(String groupName, String groupValue) {
-        String[] parts = groupName.split(":");
-        patternName = parts[0];
-        if (parts.length >= 2) {
-            fieldName = parts[1];
-        } else {
-            fieldName = null;
-        }
-
-        if (parts.length == 3) {
-            type = parts[2];
-        } else {
-            type = DEFAULT_TYPE;
-        }
-        this.groupValue = groupValue;
-    }
-
-    public String getName() {
-        return (fieldName == null) ? patternName : fieldName;
-    }
-
-    public Object getValue() {
-        if (groupValue == null) { return null; }
-
-        switch(type) {
-            case "int":
-                return Integer.parseInt(groupValue);
-            case "float":
-                return Float.parseFloat(groupValue);
-            default:
-                return groupValue;
-        }
-    }
-}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
deleted file mode 100644
index 4df8d67..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.ConfigurationUtils;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public final class GrokProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "grok";
-
-    private final String matchField;
-    private final Grok grok;
-
-    public GrokProcessor(String tag, Grok grok, String matchField) {
-        super(tag);
-        this.matchField = matchField;
-        this.grok = grok;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        String fieldValue = ingestDocument.getFieldValue(matchField, String.class);
-        Map<String, Object> matches = grok.captures(fieldValue);
-        if (matches != null) {
-            matches.forEach((k, v) -> ingestDocument.setFieldValue(k, v));
-        } else {
-            throw new IllegalArgumentException("Grok expression does not match field value: [" + fieldValue + "]");
-        }
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getMatchField() {
-        return matchField;
-    }
-
-    Grok getGrok() {
-        return grok;
-    }
-
-    public final static class Factory extends AbstractProcessorFactory<GrokProcessor> {
-
-        private final Map<String, String> builtinPatterns;
-
-        public Factory(Map<String, String> builtinPatterns) {
-            this.builtinPatterns = builtinPatterns;
-        }
-
-        @Override
-        public GrokProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String matchField = ConfigurationUtils.readStringProperty(config, "field");
-            String matchPattern = ConfigurationUtils.readStringProperty(config, "pattern");
-            Map<String, String> customPatternBank = ConfigurationUtils.readOptionalMap(config, "pattern_definitions");
-            Map<String, String> patternBank = new HashMap<>(builtinPatterns);
-            if (customPatternBank != null) {
-                patternBank.putAll(customPatternBank);
-            }
-
-            Grok grok = new Grok(patternBank, matchPattern);
-            return new GrokProcessor(processorTag, grok, matchField);
-        }
-
-    }
-
-}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
deleted file mode 100644
index 54800ac..0000000
--- a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.node.NodeModule;
-import org.elasticsearch.plugins.Plugin;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.nio.charset.StandardCharsets;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IngestGrokPlugin extends Plugin {
-
-    private static final String[] PATTERN_NAMES = new String[] {
-        "aws", "bacula", "bro", "exim", "firewalls", "grok-patterns", "haproxy",
-        "java", "junos", "linux-syslog", "mcollective-patterns", "mongodb", "nagios",
-        "postgresql", "rails", "redis", "ruby"
-    };
-
-    private final Map<String, String> builtinPatterns;
-
-    public IngestGrokPlugin() throws IOException {
-        this.builtinPatterns = loadBuiltinPatterns();
-    }
-
-    @Override
-    public String name() {
-        return "ingest-grok";
-    }
-
-    @Override
-    public String description() {
-        return "Ingest processor that uses grok patterns to split text";
-    }
-
-    public void onModule(NodeModule nodeModule) {
-        nodeModule.registerProcessor(GrokProcessor.TYPE, (templateService) -> new GrokProcessor.Factory(builtinPatterns));
-    }
-
-    static Map<String, String> loadBuiltinPatterns() throws IOException {
-        Map<String, String> builtinPatterns = new HashMap<>();
-        for (String pattern : PATTERN_NAMES) {
-            try(InputStream is = IngestGrokPlugin.class.getResourceAsStream("/patterns/" + pattern)) {
-                loadPatterns(builtinPatterns, is);
-            }
-        }
-        return Collections.unmodifiableMap(builtinPatterns);
-    }
-
-    private static void loadPatterns(Map<String, String> patternBank, InputStream inputStream) throws IOException {
-        String line;
-        BufferedReader br = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));
-        while ((line = br.readLine()) != null) {
-            String trimmedLine = line.replaceAll("^\\s+", "");
-            if (trimmedLine.startsWith("#") || trimmedLine.length() == 0) {
-                continue;
-            }
-
-            String[] parts = trimmedLine.split("\\s+", 2);
-            if (parts.length == 2) {
-                patternBank.put(parts[0], parts[1]);
-            }
-        }
-    }
-}
diff --git a/modules/ingest-grok/src/main/resources/patterns/aws b/modules/ingest-grok/src/main/resources/patterns/aws
deleted file mode 100644
index 71edbc9..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/aws
+++ /dev/null
@@ -1,11 +0,0 @@
-S3_REQUEST_LINE (?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-S3_ACCESS_LOG %{WORD:owner} %{NOTSPACE:bucket} \[%{HTTPDATE:timestamp}\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:"%{S3_REQUEST_LINE}"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:"?%{QS:agent}"?|-) (?:-|%{NOTSPACE:version_id})
-
-ELB_URIPATHPARAM %{URIPATH:path}(?:%{URIPARAM:params})?
-
-ELB_URI %{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?
-
-ELB_REQUEST_LINE (?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
-
-ELB_ACCESS_LOG %{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} "%{ELB_REQUEST_LINE}"
diff --git a/modules/ingest-grok/src/main/resources/patterns/bacula b/modules/ingest-grok/src/main/resources/patterns/bacula
deleted file mode 100644
index d80dfe5..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/bacula
+++ /dev/null
@@ -1,50 +0,0 @@
-BACULA_TIMESTAMP %{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}
-BACULA_HOST [a-zA-Z0-9-]+
-BACULA_VOLUME %{USER}
-BACULA_DEVICE %{USER}
-BACULA_DEVICEPATH %{UNIXPATH}
-BACULA_CAPACITY %{INT}{1,3}(,%{INT}{3})*
-BACULA_VERSION %{USER}
-BACULA_JOB %{USER}
-
-BACULA_LOG_MAX_CAPACITY User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_END_VOLUME End of medium on Volume \"%{BACULA_VOLUME:volume}\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NEW_VOLUME Created new Volume \"%{BACULA_VOLUME:volume}\" in catalog.
-BACULA_LOG_NEW_LABEL Labeled new Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\).
-BACULA_LOG_WROTE_LABEL Wrote label to prelabeled Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE}\" \(%{BACULA_DEVICEPATH}\)
-BACULA_LOG_NEW_MOUNT New volume \"%{BACULA_VOLUME:volume}\" mounted on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
-BACULA_LOG_NOOPEN \s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOOPENDIR \s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOSTAT \s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}
-BACULA_LOG_NOJOBS There are no more Jobs associated with Volume \"%{BACULA_VOLUME:volume}\". Marking it purged.
-BACULA_LOG_ALL_RECORDS_PRUNED All records pruned from Volume \"%{BACULA_VOLUME:volume}\"; marking it \"Purged\"
-BACULA_LOG_BEGIN_PRUNE_JOBS Begin pruning Jobs older than %{INT} month %{INT} days .
-BACULA_LOG_BEGIN_PRUNE_FILES Begin pruning Files.
-BACULA_LOG_PRUNED_JOBS Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_PRUNED_FILES Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
-BACULA_LOG_ENDPRUNE End auto prune.
-BACULA_LOG_STARTJOB Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}
-BACULA_LOG_STARTRESTORE Start Restore Job %{BACULA_JOB:job}
-BACULA_LOG_USEDEVICE Using Device \"%{BACULA_DEVICE:device}\"
-BACULA_LOG_DIFF_FS \s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it.
-BACULA_LOG_JOBEND Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second
-BACULA_LOG_NOPRUNE_JOBS No Jobs found to prune.
-BACULA_LOG_NOPRUNE_FILES No Files found to prune.
-BACULA_LOG_VOLUME_PREVWRITTEN Volume \"%{BACULA_VOLUME:volume}\" previously written, moving to end of data.
-BACULA_LOG_READYAPPEND Ready to append to end of Volume \"%{BACULA_VOLUME:volume}\" size=%{INT}
-BACULA_LOG_CANCELLING Cancelling duplicate JobId=%{INT}.
-BACULA_LOG_MARKCANCEL JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled.
-BACULA_LOG_CLIENT_RBJ shell command: run ClientRunBeforeJob \"%{GREEDYDATA:runjob}\"
-BACULA_LOG_VSS (Generate )?VSS (Writer)?
-BACULA_LOG_MAXSTART Fatal error: Job canceled because max start delay time exceeded.
-BACULA_LOG_DUPLICATE Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed.
-BACULA_LOG_NOJOBSTAT Fatal error: No Job status returned from FD.
-BACULA_LOG_FATAL_CONN Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_CONNECT Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
-BACULA_LOG_NO_AUTH Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:
-BACULA_LOG_NOSUIT No prior or suitable Full backup found in catalog. Doing FULL backup.
-BACULA_LOG_NOPRIOR No prior Full backup Job record found.
-
-BACULA_LOG_JOB (Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \(%{BACULA_VERSION}\):
-
-BACULA_LOGLINE %{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})
diff --git a/modules/ingest-grok/src/main/resources/patterns/bro b/modules/ingest-grok/src/main/resources/patterns/bro
deleted file mode 100644
index 31b138b..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/bro
+++ /dev/null
@@ -1,13 +0,0 @@
-# https://www.bro.org/sphinx/script-reference/log-files.html
-
-# http.log
-BRO_HTTP %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{INT:trans_depth}\t%{GREEDYDATA:method}\t%{GREEDYDATA:domain}\t%{GREEDYDATA:uri}\t%{GREEDYDATA:referrer}\t%{GREEDYDATA:user_agent}\t%{NUMBER:request_body_len}\t%{NUMBER:response_body_len}\t%{GREEDYDATA:status_code}\t%{GREEDYDATA:status_msg}\t%{GREEDYDATA:info_code}\t%{GREEDYDATA:info_msg}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:bro_tags}\t%{GREEDYDATA:username}\t%{GREEDYDATA:password}\t%{GREEDYDATA:proxied}\t%{GREEDYDATA:orig_fuids}\t%{GREEDYDATA:orig_mime_types}\t%{GREEDYDATA:resp_fuids}\t%{GREEDYDATA:resp_mime_types}
-
-# dns.log
-BRO_DNS %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{INT:trans_id}\t%{GREEDYDATA:query}\t%{GREEDYDATA:qclass}\t%{GREEDYDATA:qclass_name}\t%{GREEDYDATA:qtype}\t%{GREEDYDATA:qtype_name}\t%{GREEDYDATA:rcode}\t%{GREEDYDATA:rcode_name}\t%{GREEDYDATA:AA}\t%{GREEDYDATA:TC}\t%{GREEDYDATA:RD}\t%{GREEDYDATA:RA}\t%{GREEDYDATA:Z}\t%{GREEDYDATA:answers}\t%{GREEDYDATA:TTLs}\t%{GREEDYDATA:rejected}
-
-# conn.log
-BRO_CONN %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{GREEDYDATA:service}\t%{NUMBER:duration}\t%{NUMBER:orig_bytes}\t%{NUMBER:resp_bytes}\t%{GREEDYDATA:conn_state}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:missed_bytes}\t%{GREEDYDATA:history}\t%{GREEDYDATA:orig_pkts}\t%{GREEDYDATA:orig_ip_bytes}\t%{GREEDYDATA:resp_pkts}\t%{GREEDYDATA:resp_ip_bytes}\t%{GREEDYDATA:tunnel_parents}
-
-# files.log
-BRO_FILES %{NUMBER:ts}\t%{NOTSPACE:fuid}\t%{IP:tx_hosts}\t%{IP:rx_hosts}\t%{NOTSPACE:conn_uids}\t%{GREEDYDATA:source}\t%{GREEDYDATA:depth}\t%{GREEDYDATA:analyzers}\t%{GREEDYDATA:mime_type}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:duration}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:is_orig}\t%{GREEDYDATA:seen_bytes}\t%{GREEDYDATA:total_bytes}\t%{GREEDYDATA:missing_bytes}\t%{GREEDYDATA:overflow_bytes}\t%{GREEDYDATA:timedout}\t%{GREEDYDATA:parent_fuid}\t%{GREEDYDATA:md5}\t%{GREEDYDATA:sha1}\t%{GREEDYDATA:sha256}\t%{GREEDYDATA:extracted}
diff --git a/modules/ingest-grok/src/main/resources/patterns/exim b/modules/ingest-grok/src/main/resources/patterns/exim
deleted file mode 100644
index 68c4e5c..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/exim
+++ /dev/null
@@ -1,13 +0,0 @@
-EXIM_MSGID [0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}
-EXIM_FLAGS (<=|[-=>*]>|[*]{2}|==)
-EXIM_DATE %{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}
-EXIM_PID \[%{POSINT}\]
-EXIM_QT ((\d+y)?(\d+w)?(\d+d)?(\d+h)?(\d+m)?(\d+s)?)
-EXIM_EXCLUDE_TERMS (Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)
-EXIM_REMOTE_HOST (H=(%{NOTSPACE:remote_hostname} )?(\(%{NOTSPACE:remote_heloname}\) )?\[%{IP:remote_host}\])
-EXIM_INTERFACE (I=\[%{IP:exim_interface}\](:%{NUMBER:exim_interface_port}))
-EXIM_PROTOCOL (P=%{NOTSPACE:protocol})
-EXIM_MSG_SIZE (S=%{NUMBER:exim_msg_size})
-EXIM_HEADER_ID (id=%{NOTSPACE:exim_header_id})
-EXIM_SUBJECT (T=%{QS:exim_subject})
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/firewalls b/modules/ingest-grok/src/main/resources/patterns/firewalls
deleted file mode 100644
index 03c3e5a..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/firewalls
+++ /dev/null
@@ -1,86 +0,0 @@
-# NetScreen firewall logs
-NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
-
-#== Cisco ASA ==
-CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:
-CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
-CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
-# Common Particles
-CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
-CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
-CISCO_DIRECTION Inbound|inbound|Outbound|outbound
-CISCO_INTERVAL first hit|%{INT}-second interval
-CISCO_XLATE_TYPE static|dynamic
-# ASA-1-104001
-CISCOFW104001 \((?:Primary|Secondary)\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}
-# ASA-1-104002
-CISCOFW104002 \((?:Primary|Secondary)\) Switching to STANDBY - %{GREEDYDATA:switch_reason}
-# ASA-1-104003
-CISCOFW104003 \((?:Primary|Secondary)\) Switching to FAILED\.
-# ASA-1-104004
-CISCOFW104004 \((?:Primary|Secondary)\) Switching to OK\.
-# ASA-1-105003
-CISCOFW105003 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting
-# ASA-1-105004
-CISCOFW105004 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal
-# ASA-1-105005
-CISCOFW105005 \((?:Primary|Secondary)\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105008
-CISCOFW105008 \((?:Primary|Secondary)\) Testing [Ii]nterface %{GREEDYDATA:interface_name}
-# ASA-1-105009
-CISCOFW105009 \((?:Primary|Secondary)\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)
-# ASA-2-106001
-CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
-# ASA-2-106006, ASA-2-106007, ASA-2-106010
-CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
-# ASA-3-106014
-CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
-# ASA-6-106015
-CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
-# ASA-1-106021
-CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
-# ASA-4-106023
-CISCOFW106023 %{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group "?%{DATA:policy_id}"? \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-4-106100, ASA-4-106102, ASA-4-106103
-CISCOFW106100_2_3 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\) -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-5-106100
-CISCOFW106100 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-6-110002
-CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-302010
-CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
-# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
-CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
-# ASA-6-302020, ASA-6-302021
-CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
-# ASA-6-305011
-CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
-# ASA-3-313001, ASA-3-313004, ASA-3-313008
-CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
-# ASA-4-313005
-CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
-# ASA-5-321001
-CISCOFW321001 Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system
-# ASA-4-402117
-CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
-# ASA-4-402119
-CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
-# ASA-4-419001
-CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
-# ASA-4-419002
-CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
-# ASA-4-500004
-CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-602303, ASA-6-602304
-CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
-# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
-CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-713172
-CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
-# ASA-4-733100
-CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
-#== End Cisco ASA ==
-
-# Shorewall firewall logs
-SHOREWALL (%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)
-#== End Shorewall
diff --git a/modules/ingest-grok/src/main/resources/patterns/grok-patterns b/modules/ingest-grok/src/main/resources/patterns/grok-patterns
deleted file mode 100644
index cb4c3ff..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/grok-patterns
+++ /dev/null
@@ -1,102 +0,0 @@
-USERNAME [a-zA-Z0-9._-]+
-USER %{USERNAME}
-EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+
-EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME}
-HTTPDUSER %{EMAILADDRESS}|%{USER}
-INT (?:[+-]?(?:[0-9]+))
-BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
-NUMBER (?:%{BASE10NUM})
-BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
-BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
-
-POSINT \b(?:[1-9][0-9]*)\b
-NONNEGINT \b(?:[0-9]+)\b
-WORD \b\w+\b
-NOTSPACE \S+
-SPACE \s*
-DATA .*?
-GREEDYDATA .*
-QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
-UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
-
-# Networking
-MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
-CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
-WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
-COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
-IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
-IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])
-IP (?:%{IPV6}|%{IPV4})
-HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
-IPORHOST (?:%{IP}|%{HOSTNAME})
-HOSTPORT %{IPORHOST}:%{POSINT}
-
-# paths
-PATH (?:%{UNIXPATH}|%{WINPATH})
-UNIXPATH (/([\w_%!$@:.,~-]+|\\.)*)+
-TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
-WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
-URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
-URIHOST %{IPORHOST}(?::%{POSINT:port})?
-# uripath comes loosely from RFC1738, but mostly from what Firefox
-# doesn't turn into %XX
-URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
-#URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
-URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]<>]*
-URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
-URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
-
-# Months: January, Feb, 3, 03, 12, December
-MONTH \b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\b
-MONTHNUM (?:0?[1-9]|1[0-2])
-MONTHNUM2 (?:0[1-9]|1[0-2])
-MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
-
-# Days: Monday, Tue, Thu, etc...
-DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
-
-# Years?
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-# '60' is a leap second in most time standards and thus is valid.
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
-# datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
-DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
-DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
-ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
-ISO8601_SECOND (?:%{SECOND}|60)
-ISO8601_HOUR (?:2[0123]|[01][0-9])
-TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{ISO8601_HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
-DATE %{DATE_US}|%{DATE_EU}
-DATESTAMP %{DATE}[- ]%{TIME}
-TZ (?:[PMCE][SD]T|UTC)
-DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
-DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
-DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
-DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
-HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}
-
-# Syslog Dates: Month Day HH:MM:SS
-SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
-PROG [\x21-\x5a\x5c\x5e-\x7e]+
-SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
-SYSLOGHOST %{IPORHOST}
-SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
-HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
-
-# Shortcuts
-QS %{QUOTEDSTRING}
-
-# Log formats
-SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
-COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
-HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:errormsg}
-HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}:tid %{NUMBER:tid}\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_errormessage}:)?( \[client %{IPORHOST:client}:%{POSINT:clientport}\])? %{DATA:errorcode}: %{GREEDYDATA:message}
-HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}
-
-
-# Log Levels
-LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/modules/ingest-grok/src/main/resources/patterns/haproxy b/modules/ingest-grok/src/main/resources/patterns/haproxy
deleted file mode 100644
index ddabd19..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/haproxy
+++ /dev/null
@@ -1,39 +0,0 @@
-## These patterns were tested w/ haproxy-1.4.15
-
-## Documentation of the haproxy log formats can be found at the following links:
-## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
-## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
-
-HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
-HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
-
-# Override these default patterns to parse out what is captured in your haproxy.cfg
-HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
-HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
-
-# Example:
-#  These haproxy config lines will add data to the logs that are captured
-#  by the patterns below. Place them in your custom patterns directory to
-#  override the defaults.
-#
-#  capture request header Host len 40
-#  capture request header X-Forwarded-For len 50
-#  capture request header Accept-Language len 50
-#  capture request header Referer len 200
-#  capture request header User-Agent len 200
-#
-#  capture response header Content-Type len 30
-#  capture response header Content-Encoding len 10
-#  capture response header Cache-Control len 200
-#  capture response header Last-Modified len 200
-#
-# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
-# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
-
-# parse a haproxy 'httplog' line
-HAPROXYHTTPBASE %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
-
-HAPROXYHTTP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}
-
-# parse a haproxy 'tcplog' line
-HAPROXYTCP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/modules/ingest-grok/src/main/resources/patterns/java b/modules/ingest-grok/src/main/resources/patterns/java
deleted file mode 100644
index e968006..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/java
+++ /dev/null
@@ -1,20 +0,0 @@
-JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
-#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
-JAVAFILE (?:[A-Za-z0-9_. -]+)
-#Allow special <init> method
-JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
-#Line number is optional in special cases 'Native method' or 'Unknown source'
-JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
-# Java Logs
-JAVATHREAD (?:[A-Z]{2}-Processor[\d]+)
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$]+
-JAVAFILE (?:[A-Za-z0-9_.-]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
-JAVALOGMESSAGE (.*)
-# MMM dd, yyyy HH:mm:ss eg: Jan 9, 2014 7:13:13 AM
-CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)
-# yyyy-MM-dd HH:mm:ss,SSS ZZZ eg: 2014-01-09 17:32:25,527 -0800
-TOMCAT_DATESTAMP 20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}
-CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}
-# 2014-01-09 20:03:28,269 -0800 | ERROR | com.example.service.ExampleService - something compeletely unexpected happened...
-TOMCATLOG %{TOMCAT_DATESTAMP:timestamp} \| %{LOGLEVEL:level} \| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}
diff --git a/modules/ingest-grok/src/main/resources/patterns/junos b/modules/ingest-grok/src/main/resources/patterns/junos
deleted file mode 100644
index 4eea59d..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/junos
+++ /dev/null
@@ -1,9 +0,0 @@
-# JUNOS 11.4 RT_FLOW patterns
-RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
-
-RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
-
-RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
-
-RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/linux-syslog b/modules/ingest-grok/src/main/resources/patterns/linux-syslog
deleted file mode 100644
index dcffb41..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/linux-syslog
+++ /dev/null
@@ -1,16 +0,0 @@
-SYSLOG5424PRINTASCII [!-~]+
-
-SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)
-SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
-
-CRON_ACTION [A-Z ]+
-CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
-
-SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
-
-# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
-SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
-SYSLOG5424SD \[%{DATA}\]+
-SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
-
-SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns b/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
deleted file mode 100644
index bb2f7f9..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
+++ /dev/null
@@ -1,4 +0,0 @@
-# Remember, these can be multi-line events.
-MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
-
-MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/modules/ingest-grok/src/main/resources/patterns/mongodb b/modules/ingest-grok/src/main/resources/patterns/mongodb
deleted file mode 100644
index 78a4300..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/mongodb
+++ /dev/null
@@ -1,7 +0,0 @@
-MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
-MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
-MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
-MONGO_WORDDASH \b[\w-]+\b
-MONGO3_SEVERITY \w
-MONGO3_COMPONENT %{WORD}|-
-MONGO3_LOG %{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\[%{DATA:context}\])? %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/main/resources/patterns/nagios b/modules/ingest-grok/src/main/resources/patterns/nagios
deleted file mode 100644
index f4a98bf..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/nagios
+++ /dev/null
@@ -1,124 +0,0 @@
-##################################################################################
-##################################################################################
-# Chop Nagios log files to smithereens!
-#
-# A set of GROK filters to process logfiles generated by Nagios.
-# While it does not, this set intends to cover all possible Nagios logs.
-#
-# Some more work needs to be done to cover all External Commands:
-# http://old.nagios.org/developerinfo/externalcommands/commandlist.php
-#
-# If you need some support on these rules please contact:
-# Jelle Smet http://smetj.net
-#
-#################################################################################
-#################################################################################
-
-NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
-
-###############################################
-######## Begin nagios log types
-###############################################
-NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
-NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
-
-NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
-NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
-
-NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
-NAGIOS_TYPE_HOST_ALERT HOST ALERT
-
-NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
-NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
-
-NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
-NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
-
-NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
-NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
-
-NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
-NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
-
-NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
-NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
-###############################################
-######## End nagios log types
-###############################################
-
-###############################################
-######## Begin external check types
-###############################################
-NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
-NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
-NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
-NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
-NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
-NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
-NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
-NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
-NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS DISABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS ENABLE_HOST_SVC_NOTIFICATIONS
-NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS DISABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS ENABLE_HOST_NOTIFICATIONS
-NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS DISABLE_SVC_NOTIFICATIONS
-NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS ENABLE_SVC_NOTIFICATIONS
-###############################################
-######## End external check types
-###############################################
-NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
-
-NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-
-NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}
-
-####################
-#### External checks
-####################
-
-#Disable host & service check
-NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Enable host & service check
-NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Process host & service check
-NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-
-#Disable host & service notifications
-NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Enable host & service notifications
-NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
-NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
-
-#Schedule host & service downtime
-NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
-
-#End matching line
-NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})
diff --git a/modules/ingest-grok/src/main/resources/patterns/postgresql b/modules/ingest-grok/src/main/resources/patterns/postgresql
deleted file mode 100644
index c5b3e90..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/postgresql
+++ /dev/null
@@ -1,3 +0,0 @@
-# Default postgresql pg_log format pattern
-POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/rails b/modules/ingest-grok/src/main/resources/patterns/rails
deleted file mode 100644
index 68a50c7..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/rails
+++ /dev/null
@@ -1,13 +0,0 @@
-RUUID \h{32}
-# rails controller with action
-RCONTROLLER (?<controller>[^#]+)#(?<action>\w+)
-
-# this will often be the only line:
-RAILS3HEAD (?m)Started %{WORD:verb} "%{URIPATHPARAM:request}" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})
-# for some a strange reason, params are stripped of {} - not sure that's a good idea.
-RPROCESSING \W*Processing by %{RCONTROLLER} as (?<format>\S+)(?:\W*Parameters: {%{DATA:params}}\W*)?
-RAILS3FOOT Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}
-RAILS3PROFILE (?:\(Views: %{NUMBER:viewms}ms \| ActiveRecord: %{NUMBER:activerecordms}ms|\(ActiveRecord: %{NUMBER:activerecordms}ms)?
-
-# putting it all together
-RAILS3 %{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\n)*)(?:%{RAILS3FOOT})?
diff --git a/modules/ingest-grok/src/main/resources/patterns/redis b/modules/ingest-grok/src/main/resources/patterns/redis
deleted file mode 100644
index 8655c4f..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/redis
+++ /dev/null
@@ -1,3 +0,0 @@
-REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
-REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
-
diff --git a/modules/ingest-grok/src/main/resources/patterns/ruby b/modules/ingest-grok/src/main/resources/patterns/ruby
deleted file mode 100644
index b1729cd..0000000
--- a/modules/ingest-grok/src/main/resources/patterns/ruby
+++ /dev/null
@@ -1,2 +0,0 @@
-RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
-RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
deleted file mode 100644
index f6bed13..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-public class GrokProcessorFactoryTests extends ESTestCase {
-
-    public void testBuild() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory(Collections.emptyMap());
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "(?<foo>\\w+)");
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getTag(), equalTo(processorTag));
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-    }
-
-    public void testCreateWithCustomPatterns() throws Exception {
-        GrokProcessor.Factory factory = new GrokProcessor.Factory(Collections.emptyMap());
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "_field");
-        config.put("pattern", "%{MY_PATTERN:name}!");
-        config.put("pattern_definitions", Collections.singletonMap("MY_PATTERN", "foo"));
-        GrokProcessor processor = factory.create(config);
-        assertThat(processor.getMatchField(), equalTo("_field"));
-        assertThat(processor.getGrok(), notNullValue());
-        assertThat(processor.getGrok().match("foo!"), equalTo(true));
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
deleted file mode 100644
index 840cf95..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.grok.Grok;
-import org.elasticsearch.ingest.grok.GrokProcessor;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Collections;
-import java.util.HashMap;
-
-import static org.hamcrest.Matchers.equalTo;
-
-
-public class GrokProcessorTests extends ESTestCase {
-
-    public void testMatch() throws Exception {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "1");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
-        processor.execute(doc);
-        assertThat(doc.getFieldValue("one", String.class), equalTo("1"));
-    }
-
-    public void testNoMatch() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, "23");
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("Grok expression does not match field value: [23]"));
-        }
-    }
-
-    public void testMatchWithoutCaptures() throws Exception {
-        String fieldName = "value";
-        IngestDocument originalDoc = new IngestDocument(new HashMap<>(), new HashMap<>());
-        originalDoc.setFieldValue(fieldName, fieldName);
-        IngestDocument doc = new IngestDocument(originalDoc);
-        Grok grok = new Grok(Collections.emptyMap(), fieldName);
-        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
-        processor.execute(doc);
-        assertThat(doc, equalTo(originalDoc));
-    }
-
-    public void testNotStringField() {
-        String fieldName = RandomDocumentPicks.randomFieldName(random());
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        doc.setFieldValue(fieldName, 1);
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
-        }
-    }
-
-    public void testMissingField() {
-        String fieldName = "foo.bar";
-        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
-        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
-        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
-        try {
-            processor.execute(doc);
-            fail();
-        } catch (Exception e) {
-            assertThat(e.getMessage(), equalTo("field [foo] not present as part of path [foo.bar]"));
-        }
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
deleted file mode 100644
index 21ca17a..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
+++ /dev/null
@@ -1,285 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.nullValue;
-
-
-public class GrokTests extends ESTestCase {
-    private Map<String, String> basePatterns;
-
-    @Before
-    public void setup() throws IOException {
-        basePatterns = IngestGrokPlugin.loadBuiltinPatterns();
-    }
-
-    public void testMatchWithoutCaptures() {
-        String line = "value";
-        Grok grok = new Grok(basePatterns, "value");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals(0, matches.size());
-    }
-
-    public void testSimpleSyslogLine() {
-        String line = "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]";
-        Grok grok = new Grok(basePatterns, "%{SYSLOGLINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("evita", matches.get("logsource"));
-        assertEquals("Mar 16 00:01:25", matches.get("timestamp"));
-        assertEquals("connect from camomile.cloud9.net[168.100.1.3]", matches.get("message"));
-        assertEquals("postfix/smtpd", matches.get("program"));
-        assertEquals("1713", matches.get("pid"));
-    }
-
-    public void testSyslog5424Line() {
-        String line = "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"] Hello, syslog.";
-        Grok grok = new Grok(basePatterns, "%{SYSLOG5424LINE}");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("191", matches.get("syslog5424_pri"));
-        assertEquals("1", matches.get("syslog5424_ver"));
-        assertEquals("2009-06-30T18:30:00+02:00", matches.get("syslog5424_ts"));
-        assertEquals("paxton.local", matches.get("syslog5424_host"));
-        assertEquals("grokdebug", matches.get("syslog5424_app"));
-        assertEquals("4123", matches.get("syslog5424_proc"));
-        assertEquals(null, matches.get("syslog5424_msgid"));
-        assertEquals("[id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"]", matches.get("syslog5424_sd"));
-        assertEquals("Hello, syslog.", matches.get("syslog5424_msg"));
-    }
-
-    public void testDatePattern() {
-        String line = "fancy 12-12-12 12:12:12";
-        Grok grok = new Grok(basePatterns, "(?<timestamp>%{DATE_EU} %{TIME})");
-        Map<String, Object> matches = grok.captures(line);
-        assertEquals("12-12-12 12:12:12", matches.get("timestamp"));
-    }
-
-    public void testNilCoercedValues() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration:float}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals(28.4f, matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testNilWithNoCoercion() {
-        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration}ms)");
-        Map<String, Object> matches = grok.captures("test 28.4ms");
-        assertEquals("28.4", matches.get("duration"));
-        matches = grok.captures("test N/A");
-        assertEquals(null, matches.get("duration"));
-    }
-
-    public void testUnicodeSyslog() {
-        Grok grok = new Grok(basePatterns, "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}");
-        Map<String, Object> matches = grok.captures("<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 �all'): Envelope-from: email@domain.no");
-        assertThat(matches.get("syslog_pri"), equalTo("22"));
-        assertThat(matches.get("syslog_program"), equalTo("postfix/policy-spf"));
-        assertThat(matches.get("tags"), nullValue());
-    }
-
-    public void testNamedFieldsWithWholeTextMatch() {
-        Grok grok = new Grok(basePatterns, "%{DATE_EU:stimestamp}");
-        Map<String, Object> matches = grok.captures("11/01/01");
-        assertThat(matches.get("stimestamp"), equalTo("11/01/01"));
-    }
-
-    public void testWithOniguramaNamedCaptures() {
-        Grok grok = new Grok(basePatterns, "(?<foo>\\w+)");
-        Map<String, Object> matches = grok.captures("hello world");
-        assertThat(matches.get("foo"), equalTo("hello"));
-    }
-
-    public void testISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-01-01T00:00:00",
-                "1974-03-02T04:09:09",
-                "2010-05-03T08:18:18+00:00",
-                "2004-07-04T12:27:27-00:00",
-                "2001-09-05T16:36:36+0000",
-                "2001-11-06T20:45:45-0000",
-                "2001-12-07T23:54:54Z",
-                "2001-01-01T00:00:00.123456",
-                "1974-03-02T04:09:09.123456",
-                "2010-05-03T08:18:18.123456+00:00",
-                "2004-07-04T12:27:27.123456-00:00",
-                "2001-09-05T16:36:36.123456+0000",
-                "2001-11-06T20:45:45.123456-0000",
-                "2001-12-07T23:54:54.123456Z",
-                "2001-12-07T23:54:60.123456Z" // '60' second is a leap second.
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(true));
-        }
-    }
-
-    public void testNotISO8601() {
-        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
-        List<String> timeMessages = Arrays.asList(
-                "2001-13-01T00:00:00", // invalid month
-                "2001-00-01T00:00:00", // invalid month
-                "2001-01-00T00:00:00", // invalid day
-                "2001-01-32T00:00:00", // invalid day
-                "2001-01-aT00:00:00", // invalid day
-                "2001-01-1aT00:00:00", // invalid day
-                "2001-01-01Ta0:00:00", // invalid hour
-                "2001-01-01T0:00:00", // invalid hour
-                "2001-01-01T25:00:00", // invalid hour
-                "2001-01-01T01:60:00", // invalid minute
-                "2001-01-01T00:aa:00", // invalid minute
-                "2001-01-01T00:00:aa", // invalid second
-                "2001-01-01T00:00:-1", // invalid second
-                "2001-01-01T00:00:61", // invalid second
-                "2001-01-01T00:00:00A", // invalid timezone
-                "2001-01-01T00:00:00+", // invalid timezone
-                "2001-01-01T00:00:00+25", // invalid timezone
-                "2001-01-01T00:00:00+2500", // invalid timezone
-                "2001-01-01T00:00:00+25:00", // invalid timezone
-                "2001-01-01T00:00:00-25", // invalid timezone
-                "2001-01-01T00:00:00-2500", // invalid timezone
-                "2001-01-01T00:00:00-00:61" // invalid timezone
-        );
-        for (String msg : timeMessages) {
-            assertThat(grok.match(msg), is(false));
-        }
-    }
-
-    public void testNoNamedCaptures() {
-        Map<String, String> bank = new HashMap<>();
-
-        bank.put("NAME", "Tal");
-        bank.put("EXCITED_NAME", "!!!%{NAME:name}!!!");
-        bank.put("TEST", "hello world");
-
-        String text = "wowza !!!Tal!!! - Tal";
-        String pattern = "%{EXCITED_NAME} - %{NAME}";
-        Grok g = new Grok(bank, pattern, false);
-
-        assertEquals("(?<EXCITED_NAME_0>!!!(?<NAME_21>Tal)!!!) - (?<NAME_22>Tal)", g.toRegex(pattern));
-        assertEquals(true, g.match(text));
-
-        Object actual = g.captures(text);
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("EXCITED_NAME_0", "!!!Tal!!!");
-        expected.put("NAME_21", "Tal");
-        expected.put("NAME_22", "Tal");
-        assertEquals(expected, actual);
-    }
-
-    public void testNumericCapturesCoercion() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-
-        String pattern = "%{NUMBER:bytes:float} %{NUMBER:status} %{NUMBER}";
-        Grok g = new Grok(bank, pattern);
-
-        String text = "12009.34 200 9032";
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("bytes", 12009.34f);
-        expected.put("status", "200");
-        Map<String, Object> actual = g.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testApacheLog() {
-        String logLine = "31.184.238.164 - - [24/Jul/2014:05:35:37 +0530] \"GET /logs/access.log HTTP/1.0\" 200 69849 \"http://8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\" \"www.dlwindianrailways.com\"";
-        Grok grok = new Grok(basePatterns, "%{COMBINEDAPACHELOG}");
-        Map<String, Object> matches = grok.captures(logLine);
-
-        assertEquals("31.184.238.164", matches.get("clientip"));
-        assertEquals("-", matches.get("ident"));
-        assertEquals("-", matches.get("auth"));
-        assertEquals("24/Jul/2014:05:35:37 +0530", matches.get("timestamp"));
-        assertEquals("GET", matches.get("verb"));
-        assertEquals("/logs/access.log", matches.get("request"));
-        assertEquals("1.0", matches.get("httpversion"));
-        assertEquals("200", matches.get("response"));
-        assertEquals("69849", matches.get("bytes"));
-        assertEquals("\"http://8rursodiol.enjin.com\"", matches.get("referrer"));
-        assertEquals(null, matches.get("port"));
-        assertEquals("\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\"", matches.get("agent"));
-    }
-
-    public void testComplete() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        bank.put("MONTH", "\\b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|ä)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\\b");
-        bank.put("MINUTE", "(?:[0-5][0-9])");
-        bank.put("YEAR", "(?>\\d\\d){1,2}");
-        bank.put("HOUR", "(?:2[0123]|[01]?[0-9])");
-        bank.put("SECOND", "(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)");
-        bank.put("TIME", "(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])");
-        bank.put("INT", "(?:[+-]?(?:[0-9]+))");
-        bank.put("HTTPDATE", "%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}");
-        bank.put("WORD", "\\b\\w+\\b");
-        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
-        bank.put("NUMBER", "(?:%{BASE10NUM})");
-        bank.put("IPV6", "((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?");
-        bank.put("IPV4", "(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])");
-        bank.put("IP", "(?:%{IPV6}|%{IPV4})");
-        bank.put("HOSTNAME", "\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)");
-        bank.put("IPORHOST", "(?:%{IP}|%{HOSTNAME})");
-        bank.put("USER", "[a-zA-Z0-9._-]+");
-        bank.put("DATA", ".*?");
-        bank.put("QS", "(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))");
-
-        String text = "83.149.9.216 - - [19/Jul/2015:08:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1\" 200 171717 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"";
-        String pattern = "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}";
-
-        Grok grok = new Grok(bank, pattern);
-
-        Map<String, Object> expected = new HashMap<>();
-        expected.put("clientip", "83.149.9.216");
-        expected.put("ident", "-");
-        expected.put("auth", "-");
-        expected.put("timestamp", "19/Jul/2015:08:13:42 +0000");
-        expected.put("verb", "GET");
-        expected.put("request", "/presentations/logstash-monitorama-2013/images/kibana-dashboard3.png");
-        expected.put("httpversion", "1.1");
-        expected.put("response", 200);
-        expected.put("bytes", 171717);
-        expected.put("referrer", "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"");
-        expected.put("agent", "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"");
-
-        Map<String, Object> actual = grok.captures(text);
-
-        assertEquals(expected, actual);
-    }
-
-    public void testNoMatch() {
-        Map<String, String> bank = new HashMap<>();
-        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
-        Grok grok = new Grok(bank, "%{MONTHDAY:greatday}");
-        assertThat(grok.captures("nomatch"), nullValue());
-    }
-}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
deleted file mode 100644
index 3f4bdf1..0000000
--- a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.grok;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.ingest.grok.IngestGrokPlugin;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class IngestGrokRestIT extends ESRestTestCase {
-
-    public IngestGrokRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
deleted file mode 100644
index 68d1fc6..0000000
--- a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
+++ /dev/null
@@ -1,12 +0,0 @@
-"Ingest grok installed":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.modules.0.name: ingest-grok  }
-    - match:  { nodes.$master.modules.0.jvm: true  }
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
deleted file mode 100644
index f88136d..0000000
--- a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
+++ /dev/null
@@ -1,109 +0,0 @@
----
-"Test Grok Pipeline":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "123.42 400 <foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.val: 123.42 }
-  - match: { _source.status: 400 }
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{MY_PATTERN:msg}>",
-                  "pattern_definitions" : {
-                    "MY_PATTERN" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
-
----
-"Test Grok Pipeline With Custom Pattern Sharing Same Name As Another":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "grok" : {
-                  "field" : "field1",
-                  "pattern" : "<%{NUMBER:msg}>",
-                  "pattern_definitions" : {
-                    "NUMBER" : "foo"
-                  }
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "<foo>"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.msg: "foo" }
diff --git a/modules/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml b/modules/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml
index 1550f2a7..cc777bd 100644
--- a/modules/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml
+++ b/modules/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.modules.0.name: lang-expression  }
-    - match:  { nodes.$master.modules.0.jvm: true  }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 2a6be52..fc19a95 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -38,6 +38,7 @@ import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.FilterClient;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.index.query.BoolQueryBuilder;
@@ -69,7 +70,6 @@ import java.util.Locale;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.Node.HTTP_ENABLED;
 import static org.elasticsearch.rest.RestStatus.OK;
 import static org.elasticsearch.test.ESIntegTestCase.Scope.SUITE;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -92,7 +92,7 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
         return settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
                 .put("script.indexed", "on")
-                .put(HTTP_ENABLED, true)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
                 .build();
     }
 
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ScriptedMetricTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ScriptedMetricTests.java
index 7e6dbd6..fb57c54 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ScriptedMetricTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ScriptedMetricTests.java
@@ -23,6 +23,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
@@ -123,7 +124,7 @@ public class ScriptedMetricTests extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings settings = Settings.settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", getDataPath("/org/elasticsearch/messy/tests/conf"))
+                .put(Environment.PATH_CONF_SETTING.getKey(), getDataPath("/org/elasticsearch/messy/tests/conf"))
                 .build();
         return settings;
     }
diff --git a/modules/lang-groovy/src/test/resources/rest-api-spec/test/lang_groovy/10_basic.yaml b/modules/lang-groovy/src/test/resources/rest-api-spec/test/lang_groovy/10_basic.yaml
index c276bab..d5044bb 100644
--- a/modules/lang-groovy/src/test/resources/rest-api-spec/test/lang_groovy/10_basic.yaml
+++ b/modules/lang-groovy/src/test/resources/rest-api-spec/test/lang_groovy/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.modules.0.name: lang-groovy  }
-    - match:  { nodes.$master.modules.0.jvm: true  }
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/CustomReflectionObjectHandler.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/CustomReflectionObjectHandler.java
new file mode 100644
index 0000000..45d3d8c
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/CustomReflectionObjectHandler.java
@@ -0,0 +1,157 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.reflect.ReflectionObjectHandler;
+import org.elasticsearch.common.util.iterable.Iterables;
+
+import java.lang.reflect.Array;
+import java.util.AbstractMap;
+import java.util.Collection;
+import java.util.Set;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.HashMap;
+
+final class CustomReflectionObjectHandler extends ReflectionObjectHandler {
+
+    @Override
+    public Object coerce(Object object) {
+        if (object == null) {
+            return null;
+        }
+
+        if (object.getClass().isArray()) {
+            return new ArrayMap(object);
+        } else if (object instanceof Collection) {
+            @SuppressWarnings("unchecked")
+            Collection<Object> collection = (Collection<Object>) object;
+            return new CollectionMap(collection);
+        } else {
+            return super.coerce(object);
+        }
+    }
+
+    final static class ArrayMap extends AbstractMap<Object, Object> implements Iterable<Object> {
+
+        private final Object array;
+        private final int length;
+
+        public ArrayMap(Object array) {
+            this.array = array;
+            this.length = Array.getLength(array);
+        }
+
+        @Override
+        public Object get(Object key) {
+            if ("size".equals(key)) {
+                return size();
+            } else if (key instanceof Number) {
+                return Array.get(array, ((Number) key).intValue());
+            }
+            try {
+                int index = Integer.parseInt(key.toString());
+                return Array.get(array, index);
+            } catch (NumberFormatException nfe) {
+                // if it's not a number it is as if the key doesn't exist
+                return null;
+            }
+        }
+
+        @Override
+        public boolean containsKey(Object key) {
+            return get(key) != null;
+        }
+
+        @Override
+        public Set<Entry<Object, Object>> entrySet() {
+            Map<Object, Object> map = new HashMap<>(length);
+            for (int i = 0; i < length; i++) {
+                map.put(i, Array.get(array, i));
+            }
+            return map.entrySet();
+        }
+
+        @Override
+        public Iterator<Object> iterator() {
+            return new Iterator<Object>() {
+
+                int index = 0;
+
+                @Override
+                public boolean hasNext() {
+                    return index < length;
+                }
+
+                @Override
+                public Object next() {
+                    return Array.get(array, index++);
+                }
+            };
+        }
+
+    }
+
+    final static class CollectionMap extends AbstractMap<Object, Object> implements Iterable<Object> {
+
+        private final Collection<Object> col;
+
+        public CollectionMap(Collection<Object> col) {
+            this.col = col;
+        }
+
+        @Override
+        public Object get(Object key) {
+            if ("size".equals(key)) {
+                return col.size();
+            } else if (key instanceof Number) {
+                return Iterables.get(col, ((Number) key).intValue());
+            }
+            try {
+                int index = Integer.parseInt(key.toString());
+                return Iterables.get(col, index);
+            } catch (NumberFormatException nfe) {
+                // if it's not a number it is as if the key doesn't exist
+                return null;
+            }
+        }
+
+        @Override
+        public boolean containsKey(Object key) {
+            return get(key) != null;
+        }
+
+        @Override
+        public Set<Entry<Object, Object>> entrySet() {
+            Map<Object, Object> map = new HashMap<>(col.size());
+            int i = 0;
+            for (Object item : col) {
+                map.put(i++, item);
+            }
+            return map.entrySet();
+        }
+
+        @Override
+        public Iterator<Object> iterator() {
+            return col.iterator();
+        }
+    }
+
+}
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
index 7734d03..38d48b9 100644
--- a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
@@ -28,13 +28,12 @@ import java.io.Writer;
 /**
  * A MustacheFactory that does simple JSON escaping.
  */
-public final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
-    
+final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
+
     @Override
     public void encode(String value, Writer writer) {
         try {
-            JsonStringEncoder utils = new JsonStringEncoder();
-            writer.write(utils.quoteAsString(value));;
+            writer.write(JsonStringEncoder.getInstance().quoteAsString(value));
         } catch (IOException e) {
             throw new MustacheException("Failed to encode value: " + value);
         }
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
index 41f8924..685ba6a 100644
--- a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
@@ -22,6 +22,7 @@ import java.lang.ref.SoftReference;
 import java.security.AccessController;
 import java.security.PrivilegedAction;
 import java.util.Collections;
+import java.io.Reader;
 import java.util.Map;
 
 import org.elasticsearch.SpecialPermission;
@@ -40,6 +41,7 @@ import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.lookup.SearchLookup;
 
 import com.github.mustachejava.Mustache;
+import com.github.mustachejava.DefaultMustacheFactory;
 
 /**
  * Main entry point handling template registration, compilation and
@@ -49,9 +51,12 @@ import com.github.mustachejava.Mustache;
  * process: First compile the string representing the template, the resulting
  * {@link Mustache} object can then be re-used for subsequent executions.
  */
-public class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
+public final class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
 
     public static final String NAME = "mustache";
+    static final String CONTENT_TYPE_PARAM = "content_type";
+    static final String JSON_CONTENT_TYPE = "application/json";
+    static final String PLAIN_TEXT_CONTENT_TYPE = "text/plain";
 
     /** Thread local UTF8StreamWriter to store template execution results in, thread local to save object creation.*/
     private static ThreadLocal<SoftReference<UTF8StreamWriter>> utf8StreamWriter = new ThreadLocal<>();
@@ -86,8 +91,21 @@ public class MustacheScriptEngineService extends AbstractComponent implements Sc
      * */
     @Override
     public Object compile(String template, Map<String, String> params) {
-        /** Factory to generate Mustache objects from. */
-        return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
+        String contentType = params.getOrDefault(CONTENT_TYPE_PARAM, JSON_CONTENT_TYPE);
+        final DefaultMustacheFactory mustacheFactory;
+        switch (contentType){
+            case PLAIN_TEXT_CONTENT_TYPE:
+                mustacheFactory = new NoneEscapingMustacheFactory();
+                break;
+            case JSON_CONTENT_TYPE:
+            default:
+                // assume that the default is json encoding:
+                mustacheFactory = new JsonEscapingMustacheFactory();
+                break;
+        }
+        mustacheFactory.setObjectHandler(new CustomReflectionObjectHandler());
+        Reader reader = new FastStringReader(template);
+        return mustacheFactory.compile(reader, "query-template");
     }
 
     @Override
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/NoneEscapingMustacheFactory.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/NoneEscapingMustacheFactory.java
new file mode 100644
index 0000000..3539402
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/NoneEscapingMustacheFactory.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.MustacheException;
+
+import java.io.IOException;
+import java.io.Writer;
+
+/**
+ * A MustacheFactory that does no string escaping.
+ */
+final class NoneEscapingMustacheFactory extends DefaultMustacheFactory {
+
+    @Override
+    public void encode(String value, Writer writer) {
+        try {
+            writer.write(value);
+        } catch (IOException e) {
+            throw new MustacheException("Failed to encode value: " + value);
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 485e687..d1275f6 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -34,6 +34,7 @@ import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.FilterClient;
+import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -63,7 +64,6 @@ import java.util.Map;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.Node.HTTP_ENABLED;
 import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
 import static org.elasticsearch.test.ESIntegTestCase.Scope.SUITE;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -87,7 +87,7 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
         return settingsBuilder()
                 .put(super.nodeSettings(nodeOrdinal))
                 .put("script.indexed", "on")
-                .put(HTTP_ENABLED, true)
+                .put(NetworkModule.HTTP_ENABLED.getKey(), true)
                 .build();
     }
 
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
index 4b3d3f3..39b2f29 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
@@ -25,6 +25,7 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
@@ -68,7 +69,7 @@ public class RenderSearchTemplateTests extends ESIntegTestCase {
             throw new RuntimeException(e);
         }
         return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", configDir).build();
+                .put(Environment.PATH_CONF_SETTING.getKey(), configDir).build();
     }
 
     public void testInlineTemplate() {
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
index 9d13680..e005ca5 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
@@ -88,8 +88,8 @@ public class TemplateQueryParserTests extends ESTestCase {
     @Before
     public void setup() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", this.getDataPath("config"))
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
+                .put(Environment.PATH_CONF_SETTING.getKey(), this.getDataPath("config"))
                 .put("name", getClass().getName())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
index 7029826..0914fd6 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.TemplateQueryBuilder;
 import org.elasticsearch.index.query.TemplateQueryParser;
@@ -85,7 +86,7 @@ public class TemplateQueryTests extends ESIntegTestCase {
     @Override
     public Settings nodeSettings(int nodeOrdinal) {
         return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config")).build();
+                .put(Environment.PATH_CONF_SETTING.getKey(), this.getDataPath("config")).build();
     }
 
     public void testTemplateInBody() throws IOException {
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
index 8e8c898..b388f8a 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
@@ -48,12 +48,13 @@ public class MustacheScriptEngineTests extends ESTestCase {
     }
 
     public void testSimpleParameterReplace() {
+        Map<String, String> compileParams = Collections.singletonMap("content_type", "application/json");
         {
             String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
             Map<String, Object> vars = new HashMap<>();
             vars.put("boost_val", "0.3");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template, Collections.emptyMap())), vars).run();
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template, compileParams)), vars).run();
             assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
                     new String(o.toBytes(), Charset.forName("UTF-8")));
@@ -64,7 +65,7 @@ public class MustacheScriptEngineTests extends ESTestCase {
             Map<String, Object> vars = new HashMap<>();
             vars.put("boost_val", "0.3");
             vars.put("body_val", "\"quick brown\"");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template, Collections.emptyMap())), vars).run();
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template, compileParams)), vars).run();
             assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
                     + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
                     new String(o.toBytes(), Charset.forName("UTF-8")));
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
index d8cf773..1bbae2b 100644
--- a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
@@ -18,36 +18,163 @@
  */
 package org.elasticsearch.script.mustache;
 
-import com.github.mustachejava.DefaultMustacheFactory;
 import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheFactory;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.test.ESTestCase;
 
-import java.io.StringReader;
-import java.io.StringWriter;
+import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import static java.util.Collections.singleton;
+import static java.util.Collections.singletonMap;
+import static org.elasticsearch.script.mustache.MustacheScriptEngineService.CONTENT_TYPE_PARAM;
+import static org.elasticsearch.script.mustache.MustacheScriptEngineService.JSON_CONTENT_TYPE;
+import static org.elasticsearch.script.mustache.MustacheScriptEngineService.PLAIN_TEXT_CONTENT_TYPE;
+import static org.hamcrest.Matchers.both;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.notNullValue;
 
-/**
- * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
- * */
 public class MustacheTests extends ESTestCase {
-    public void test() {
-        HashMap<String, Object> scopes = new HashMap<>();
-        scopes.put("boost_val", "0.2");
 
+    private ScriptEngineService engine = new MustacheScriptEngineService(Settings.EMPTY);
+
+    public void testBasics() {
         String template = "GET _search {\"query\": " + "{\"boosting\": {"
-                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
-                + "}}, \"negative_boost\": {{boost_val}} } }}";
-        MustacheFactory f = new DefaultMustacheFactory();
-        Mustache mustache = f.compile(new StringReader(template), "example");
-        StringWriter writer = new StringWriter();
-        mustache.execute(writer, scopes);
-        writer.flush();
+            + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+            + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
+            + "}}, \"negative_boost\": {{boost_val}} } }}";
+        Map<String, Object> params = Collections.singletonMap("boost_val", "0.2");
+
+        Mustache mustache = (Mustache) engine.compile(template, Collections.emptyMap());
+        CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "my-name", "mustache", mustache);
+        ExecutableScript result = engine.executable(compiledScript, params);
         assertEquals(
                 "Mustache templating broken",
                 "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
                         + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
-                writer.toString());
+                ((BytesReference) result.run()).toUtf8()
+        );
+    }
+
+    public void testArrayAccess() throws Exception {
+        String template = "{{data.0}} {{data.1}}";
+        CompiledScript mustache = new CompiledScript(ScriptService.ScriptType.INLINE, "inline", "mustache", engine.compile(template, Collections.emptyMap()));
+        Map<String, Object> vars = new HashMap<>();
+        Object data = randomFrom(
+            new String[] { "foo", "bar" },
+            Arrays.asList("foo", "bar"));
+        vars.put("data", data);
+        Object output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+        BytesReference bytes = (BytesReference) output;
+        assertThat(bytes.toUtf8(), equalTo("foo bar"));
+
+        // Sets can come out in any order
+        Set<String> setData = new HashSet<>();
+        setData.add("foo");
+        setData.add("bar");
+        vars.put("data", setData);
+        output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+        bytes = (BytesReference) output;
+        assertThat(bytes.toUtf8(), both(containsString("foo")).and(containsString("bar")));
+    }
+
+    public void testArrayInArrayAccess() throws Exception {
+        String template = "{{data.0.0}} {{data.0.1}}";
+        CompiledScript mustache = new CompiledScript(ScriptService.ScriptType.INLINE, "inline", "mustache", engine.compile(template, Collections.emptyMap()));
+        Map<String, Object> vars = new HashMap<>();
+        Object data = randomFrom(
+            new String[][] { new String[] { "foo", "bar" }},
+            Collections.singletonList(new String[] { "foo", "bar" }),
+            singleton(new String[] { "foo", "bar" })
+        );
+        vars.put("data", data);
+        Object output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+        BytesReference bytes = (BytesReference) output;
+        assertThat(bytes.toUtf8(), equalTo("foo bar"));
+    }
+
+    public void testMapInArrayAccess() throws Exception {
+        String template = "{{data.0.key}} {{data.1.key}}";
+        CompiledScript mustache = new CompiledScript(ScriptService.ScriptType.INLINE, "inline", "mustache", engine.compile(template, Collections.emptyMap()));
+        Map<String, Object> vars = new HashMap<>();
+        Object data = randomFrom(
+            new Object[] { singletonMap("key", "foo"), singletonMap("key", "bar") },
+            Arrays.asList(singletonMap("key", "foo"), singletonMap("key", "bar")));
+        vars.put("data", data);
+        Object output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+        BytesReference bytes = (BytesReference) output;
+        assertThat(bytes.toUtf8(), equalTo("foo bar"));
+
+        // HashSet iteration order isn't fixed
+        Set<Object> setData = new HashSet<>();
+        setData.add(singletonMap("key", "foo"));
+        setData.add(singletonMap("key", "bar"));
+        vars.put("data", setData);
+        output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+        bytes = (BytesReference) output;
+        assertThat(bytes.toUtf8(), both(containsString("foo")).and(containsString("bar")));
+    }
+
+    public void testEscaping() {
+        // json string escaping enabled:
+        Map<String, String> params = randomBoolean() ? Collections.emptyMap() : Collections.singletonMap(CONTENT_TYPE_PARAM, JSON_CONTENT_TYPE);
+        Mustache mustache = (Mustache) engine.compile("{ \"field1\": \"{{value}}\"}", Collections.emptyMap());
+        CompiledScript compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "name", "mustache", mustache);
+        ExecutableScript executableScript = engine.executable(compiledScript, Collections.singletonMap("value", "a \"value\""));
+        BytesReference rawResult = (BytesReference) executableScript.run();
+        String result = rawResult.toUtf8();
+        assertThat(result, equalTo("{ \"field1\": \"a \\\"value\\\"\"}"));
+
+        // json string escaping disabled:
+        mustache = (Mustache) engine.compile("{ \"field1\": \"{{value}}\"}", Collections.singletonMap(CONTENT_TYPE_PARAM, PLAIN_TEXT_CONTENT_TYPE));
+        compiledScript = new CompiledScript(ScriptService.ScriptType.INLINE, "name", "mustache", mustache);
+        executableScript = engine.executable(compiledScript, Collections.singletonMap("value", "a \"value\""));
+        rawResult = (BytesReference) executableScript.run();
+        result = rawResult.toUtf8();
+        assertThat(result, equalTo("{ \"field1\": \"a \"value\"\"}"));
+    }
+
+    public void testSizeAccessForCollectionsAndArrays() throws Exception {
+        String[] randomArrayValues = generateRandomStringArray(10, 20, false);
+        List<String> randomList = Arrays.asList(generateRandomStringArray(10, 20, false));
+
+        String template = "{{data.array.size}} {{data.list.size}}";
+        CompiledScript mustache = new CompiledScript(ScriptService.ScriptType.INLINE, "inline", "mustache", engine.compile(template, Collections.emptyMap()));
+        Map<String, Object> data = new HashMap<>();
+        data.put("array", randomArrayValues);
+        data.put("list", randomList);
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("data", data);
+
+        Object output = engine.executable(mustache, vars).run();
+        assertThat(output, notNullValue());
+        assertThat(output, instanceOf(BytesReference.class));
+
+        BytesReference bytes = (BytesReference) output;
+        String expectedString = String.format(Locale.ROOT, "%s %s", randomArrayValues.length, randomList.size());
+        assertThat(bytes.toUtf8(), equalTo(expectedString));
     }
 }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
index 9bfea28..195eea7 100644
--- a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
@@ -11,7 +11,6 @@
         nodes.info: {}
 
     - match:  { nodes.$master.modules.0.name: lang-mustache  }
-    - match:  { nodes.$master.modules.0.jvm: true  }
 
 ---
 "Indexed template":
diff --git a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java
index d4b2530..efd6042 100644
--- a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java
+++ b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.analysis;
 
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -32,7 +33,7 @@ import static org.hamcrest.Matchers.instanceOf;
 public class SimpleIcuAnalysisTests extends ESTestCase {
     public void testDefaultsIcuAnalysis() throws IOException {
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
         AnalysisService analysisService = createAnalysisService(settings);
 
         TokenizerFactory tokenizerFactory = analysisService.tokenizer("icu_tokenizer");
diff --git a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuCollationTokenFilterTests.java b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuCollationTokenFilterTests.java
index 33c1f33..632f3f5 100644
--- a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuCollationTokenFilterTests.java
+++ b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuCollationTokenFilterTests.java
@@ -27,6 +27,7 @@ import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -45,7 +46,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testBasicUsage() throws Exception {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "tr")
                 .put("index.analysis.filter.myCollator.strength", "primary")
@@ -61,7 +62,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testNormalization() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "tr")
                 .put("index.analysis.filter.myCollator.strength", "primary")
@@ -78,7 +79,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testSecondaryStrength() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.strength", "secondary")
@@ -96,7 +97,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testIgnorePunctuation() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.strength", "primary")
@@ -114,7 +115,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testIgnoreWhitespace() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.strength", "primary")
@@ -135,7 +136,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testNumerics() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.numeric", "true")
@@ -152,7 +153,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testIgnoreAccentsButNotCase() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.strength", "primary")
@@ -173,7 +174,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
     */
     public void testUpperCaseFirst() throws IOException {
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.language", "en")
                 .put("index.analysis.filter.myCollator.strength", "tertiary")
@@ -203,7 +204,7 @@ public class SimpleIcuCollationTokenFilterTests extends ESTestCase {
         String tailoredRules = tailoredCollator.getRules();
 
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myCollator.type", "icu_collation")
                 .put("index.analysis.filter.myCollator.rules", tailoredRules)
                 .put("index.analysis.filter.myCollator.strength", "primary")
diff --git a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuNormalizerCharFilterTests.java b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuNormalizerCharFilterTests.java
index acdbd9d..7ebb783 100644
--- a/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuNormalizerCharFilterTests.java
+++ b/plugins/analysis-icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuNormalizerCharFilterTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.analysis;
 import com.ibm.icu.text.Normalizer2;
 import org.apache.lucene.analysis.CharFilter;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.StringReader;
@@ -34,7 +35,7 @@ import static org.elasticsearch.index.analysis.AnalysisTestUtils.createAnalysisS
 public class SimpleIcuNormalizerCharFilterTests extends ESTestCase {
     public void testDefaultSetting() throws Exception {
         Settings settings = Settings.settingsBuilder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             .put("index.analysis.char_filter.myNormalizerChar.type", "icu_normalizer")
             .build();
         AnalysisService analysisService = createAnalysisService(settings);
@@ -57,7 +58,7 @@ public class SimpleIcuNormalizerCharFilterTests extends ESTestCase {
 
     public void testNameAndModeSetting() throws Exception {
         Settings settings = Settings.settingsBuilder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             .put("index.analysis.char_filter.myNormalizerChar.type", "icu_normalizer")
             .put("index.analysis.char_filter.myNormalizerChar.name", "nfkc")
             .put("index.analysis.char_filter.myNormalizerChar.mode", "decompose")
diff --git a/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java b/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
index 6312284..0160538 100644
--- a/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
+++ b/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
@@ -199,7 +199,7 @@ public class KuromojiAnalysisTests extends ESTestCase {
 
         String json = "/org/elasticsearch/index/analysis/kuromoji_analysis.json";
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", home)
+                .put(Environment.PATH_HOME_SETTING.getKey(), home)
                 .loadFromStream(json, getClass().getResourceAsStream(json))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
diff --git a/plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/SimplePhoneticAnalysisTests.java b/plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/SimplePhoneticAnalysisTests.java
index b0a93f1..6dd3413 100644
--- a/plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/SimplePhoneticAnalysisTests.java
+++ b/plugins/analysis-phonetic/src/test/java/org/elasticsearch/index/analysis/SimplePhoneticAnalysisTests.java
@@ -48,7 +48,7 @@ public class SimplePhoneticAnalysisTests extends ESTestCase {
         String yaml = "/org/elasticsearch/index/analysis/phonetic-1.yml";
         Settings settings = settingsBuilder().loadFromStream(yaml, getClass().getResourceAsStream(yaml))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         AnalysisService analysisService = testSimpleConfiguration(settings);
         TokenFilterFactory filterFactory = analysisService.tokenFilter("phonetic");
diff --git a/plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/SimpleSmartChineseAnalysisTests.java b/plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/SimpleSmartChineseAnalysisTests.java
index 55c0912..d33d36d 100644
--- a/plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/SimpleSmartChineseAnalysisTests.java
+++ b/plugins/analysis-smartcn/src/test/java/org/elasticsearch/index/analysis/SimpleSmartChineseAnalysisTests.java
@@ -47,7 +47,7 @@ public class SimpleSmartChineseAnalysisTests extends ESTestCase {
     public void testDefaultsIcuAnalysis() throws IOException {
         Index index = new Index("test");
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
         AnalysisModule analysisModule = new AnalysisModule(new Environment(settings));
diff --git a/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/PolishAnalysisTests.java b/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/PolishAnalysisTests.java
index 02fcbd0..05c7252 100644
--- a/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/PolishAnalysisTests.java
+++ b/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/PolishAnalysisTests.java
@@ -50,7 +50,7 @@ public class PolishAnalysisTests extends ESTestCase {
     public void testDefaultsPolishAnalysis() throws IOException {
         Index index = new Index("test");
         Settings settings = settingsBuilder()
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
 
diff --git a/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/SimplePolishTokenFilterTests.java b/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/SimplePolishTokenFilterTests.java
index e091b0a..306a835 100644
--- a/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/SimplePolishTokenFilterTests.java
+++ b/plugins/analysis-stempel/src/test/java/org/elasticsearch/index/analysis/SimplePolishTokenFilterTests.java
@@ -59,7 +59,7 @@ public class SimplePolishTokenFilterTests extends ESTestCase {
         Index index = new Index("test");
         Settings settings = Settings.settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .put("index.analysis.filter.myStemmer.type", "polish_stem")
                 .build();
         AnalysisService analysisService = createAnalysisService(index, settings);
@@ -81,7 +81,7 @@ public class SimplePolishTokenFilterTests extends ESTestCase {
         Index index = new Index("test");
         Settings settings = Settings.settingsBuilder()
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .build();
         AnalysisService analysisService = createAnalysisService(index, settings);
 
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java
index 5215b90..d48eed9 100644
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java
@@ -30,6 +30,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.azure.AzureDiscovery;
 
 /**
@@ -73,7 +74,7 @@ public class AzureDiscoveryModule extends AbstractModule {
      */
     public static boolean isDiscoveryReady(Settings settings, ESLogger logger) {
         // User set discovery.type: azure
-        if (!AzureDiscovery.AZURE.equalsIgnoreCase(settings.get("discovery.type"))) {
+        if (!AzureDiscovery.AZURE.equalsIgnoreCase(DiscoveryModule.DISCOVERY_TYPE_SETTING.get(settings))) {
             logger.trace("discovery.type not set to {}", AzureDiscovery.AZURE);
             return false;
         }
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
index c79a745..de2343d 100644
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/management/AzureComputeService.java
@@ -20,6 +20,11 @@
 package org.elasticsearch.cloud.azure.management;
 
 import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
+import org.elasticsearch.common.settings.Setting;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.discovery.azure.AzureUnicastHostsProvider;
+
+import java.util.Locale;
 
 /**
  *
@@ -39,9 +44,11 @@ public interface AzureComputeService {
     }
 
     static public final class Discovery {
-        public static final String REFRESH = "discovery.azure.refresh_interval";
+        public static final Setting<TimeValue> REFRESH_SETTING = Setting.positiveTimeSetting("discovery.azure.refresh_interval", TimeValue.timeValueSeconds(0), false, Setting.Scope.CLUSTER);
+
+        public static final Setting<AzureUnicastHostsProvider.HostType> HOST_TYPE_SETTING = new Setting<>("discovery.azure.host.type",
+            AzureUnicastHostsProvider.HostType.PRIVATE_IP.name(), AzureUnicastHostsProvider.HostType::fromString, false, Setting.Scope.CLUSTER);
 
-        public static final String HOST_TYPE = "discovery.azure.host.type";
         public static final String ENDPOINT_NAME = "discovery.azure.endpoint.name";
         public static final String DEPLOYMENT_NAME = "discovery.azure.deployment.name";
         public static final String DEPLOYMENT_SLOT = "discovery.azure.deployment.slot";
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
index 690ab62..aac167f 100644
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureUnicastHostsProvider.java
@@ -45,7 +45,6 @@ import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Locale;
 
 /**
  *
@@ -68,7 +67,7 @@ public class AzureUnicastHostsProvider extends AbstractComponent implements Unic
                     return hostType;
                 }
             }
-            return null;
+            throw new IllegalArgumentException("invalid value for host type [" + type + "]");
         }
     }
 
@@ -118,16 +117,9 @@ public class AzureUnicastHostsProvider extends AbstractComponent implements Unic
         this.networkService = networkService;
         this.version = version;
 
-        this.refreshInterval = settings.getAsTime(Discovery.REFRESH, TimeValue.timeValueSeconds(0));
+        this.refreshInterval = Discovery.REFRESH_SETTING.get(settings);
 
-        String strHostType = settings.get(Discovery.HOST_TYPE, HostType.PRIVATE_IP.name()).toUpperCase(Locale.ROOT);
-        HostType tmpHostType = HostType.fromString(strHostType);
-        if (tmpHostType == null) {
-            logger.warn("wrong value for [{}]: [{}]. falling back to [{}]...", Discovery.HOST_TYPE,
-                    strHostType, HostType.PRIVATE_IP.name().toLowerCase(Locale.ROOT));
-            tmpHostType = HostType.PRIVATE_IP;
-        }
-        this.hostType = tmpHostType;
+        this.hostType = Discovery.HOST_TYPE_SETTING.get(settings);
         this.publicEndpointName = settings.get(Discovery.ENDPOINT_NAME, "elasticsearch");
 
         // Deployment name could be set with discovery.azure.deployment.name
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
index e61a82a..418bd12 100644
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/plugin/discovery/azure/AzureDiscoveryPlugin.java
@@ -20,10 +20,12 @@
 package org.elasticsearch.plugin.discovery.azure;
 
 import org.elasticsearch.cloud.azure.AzureDiscoveryModule;
+import org.elasticsearch.cloud.azure.management.AzureComputeService;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsModule;
 import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.azure.AzureDiscovery;
 import org.elasticsearch.discovery.azure.AzureUnicastHostsProvider;
@@ -66,4 +68,9 @@ public class AzureDiscoveryPlugin extends Plugin {
             discoveryModule.addUnicastHostProvider(AzureUnicastHostsProvider.class);
         }
     }
+
+    public void onModule(SettingsModule settingsModule) {
+        settingsModule.registerSetting(AzureComputeService.Discovery.REFRESH_SETTING);
+        settingsModule.registerSetting(AzureComputeService.Discovery.HOST_TYPE_SETTING);
+    }
 }
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
index bf1ba94..9babfb4 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureComputeServiceTestCase.java
@@ -48,7 +48,7 @@ public abstract class AbstractAzureComputeServiceTestCase extends ESIntegTestCas
 
         // We add a fake subscription_id to start mock compute service
         builder.put(Management.SUBSCRIPTION_ID, "fake")
-            .put(Discovery.REFRESH, "5s")
+            .put(Discovery.REFRESH_SETTING.getKey(), "5s")
             .put(Management.KEYSTORE_PATH, "dummy")
             .put(Management.KEYSTORE_PASSWORD, "dummy")
             .put(Management.SERVICE_NAME, "dummy");
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
index 9747543..ad7140f 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTestCase.java
@@ -23,6 +23,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -54,7 +55,7 @@ public abstract class AbstractAzureTestCase extends ESIntegTestCase {
 
     protected Settings readSettingsFromFile() {
         Settings.Builder settings = Settings.builder();
-        settings.put("path.home", createTempDir());
+        settings.put(Environment.PATH_HOME_SETTING.getKey(), createTempDir());
 
         // if explicit, just load it and don't load from env
         try {
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
index 19d6d03..0f2d965 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureMinimumMasterNodesTests.java
@@ -53,8 +53,8 @@ public class AzureMinimumMasterNodesTests extends AbstractAzureComputeServiceTes
                 .put(super.nodeSettings(nodeOrdinal))
                 .put("discovery.zen.minimum_master_nodes", 2)
                 // Make the test run faster
-                .put(ZenDiscovery.SETTING_JOIN_TIMEOUT, "50ms")
-                .put(ZenDiscovery.SETTING_PING_TIMEOUT, "10ms")
+                .put(ZenDiscovery.JOIN_TIMEOUT_SETTING.getKey(), "50ms")
+                .put(ZenDiscovery.PING_TIMEOUT_SETTING.getKey(), "10ms")
                 .put("discovery.initial_state_timeout", "100ms");
         return builder.build();
     }
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
index cc4021f..6a6ef09 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureSimpleTests.java
@@ -26,6 +26,7 @@ import org.elasticsearch.cloud.azure.management.AzureComputeService.Management;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.test.ESIntegTestCase;
 
+import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.notNullValue;
 
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST,
@@ -40,7 +41,7 @@ public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
     public void testOneNodeDhouldRunUsingPrivateIp() {
         Settings.Builder settings = Settings.settingsBuilder()
                 .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
+                .put(Discovery.HOST_TYPE_SETTING.getKey(), "private_ip");
 
         logger.info("--> start one node");
         internalCluster().startNode(settings);
@@ -53,7 +54,7 @@ public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
     public void testOneNodeShouldRunUsingPublicIp() {
         Settings.Builder settings = Settings.settingsBuilder()
                 .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
+                .put(Discovery.HOST_TYPE_SETTING.getKey(), "public_ip");
 
         logger.info("--> start one node");
         internalCluster().startNode(settings);
@@ -66,13 +67,14 @@ public class AzureSimpleTests extends AbstractAzureComputeServiceTestCase {
     public void testOneNodeShouldRunUsingWrongSettings() {
         Settings.Builder settings = Settings.settingsBuilder()
                 .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "do_not_exist");
+                .put(Discovery.HOST_TYPE_SETTING.getKey(), "do_not_exist");
 
         logger.info("--> start one node");
-        internalCluster().startNode(settings);
-        assertThat(client().admin().cluster().prepareState().setMasterNodeTimeout("1s").execute().actionGet().getState().nodes().masterNodeId(), notNullValue());
-
-        // We expect having 1 node as part of the cluster, let's test that
-        checkNumberOfNodes(1);
+        try {
+            internalCluster().startNode(settings);
+            fail("Expected IllegalArgumentException on startup");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("invalid value for host type [do_not_exist]"));
+        }
     }
 }
diff --git a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
index 2d134d0..880c05e 100644
--- a/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
+++ b/plugins/discovery-azure/src/test/java/org/elasticsearch/discovery/azure/AzureTwoStartedNodesTests.java
@@ -42,7 +42,7 @@ public class AzureTwoStartedNodesTests extends AbstractAzureComputeServiceTestCa
     public void testTwoNodesShouldRunUsingPrivateIp() {
         Settings.Builder settings = Settings.settingsBuilder()
                 .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "private_ip");
+                .put(Discovery.HOST_TYPE_SETTING.getKey(), "private_ip");
 
         logger.info("--> start first node");
         internalCluster().startNode(settings);
@@ -60,7 +60,7 @@ public class AzureTwoStartedNodesTests extends AbstractAzureComputeServiceTestCa
     public void testTwoNodesShouldRunUsingPublicIp() {
         Settings.Builder settings = Settings.settingsBuilder()
                 .put(Management.SERVICE_NAME, "dummy")
-                .put(Discovery.HOST_TYPE, "public_ip");
+                .put(Discovery.HOST_TYPE_SETTING.getKey(), "public_ip");
 
         logger.info("--> start first node");
         internalCluster().startNode(settings);
diff --git a/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml b/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml
index 51ba41e..7a5acd1 100644
--- a/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml
+++ b/plugins/discovery-azure/src/test/resources/rest-api-spec/test/discovery_azure/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: discovery-azure  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
index 4830945..51dfd55 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
@@ -181,6 +181,8 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
                 endpoint = "ec2.ap-southeast-2.amazonaws.com";
             } else if (region.equals("ap-northeast") || region.equals("ap-northeast-1")) {
                 endpoint = "ec2.ap-northeast-1.amazonaws.com";
+            } else if (region.equals("ap-northeast-2")) {
+                endpoint = "ec2.ap-northeast-2.amazonaws.com";
             } else if (region.equals("eu-west") || region.equals("eu-west-1")) {
                 endpoint = "ec2.eu-west-1.amazonaws.com";
             } else if (region.equals("eu-central") || region.equals("eu-central-1")) {
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java
index 09a0116..4aac319 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/Ec2Module.java
@@ -22,6 +22,7 @@ package org.elasticsearch.cloud.aws;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.ec2.Ec2Discovery;
 
 public class Ec2Module extends AbstractModule {
@@ -37,7 +38,7 @@ public class Ec2Module extends AbstractModule {
      */
     public static boolean isEc2DiscoveryActive(Settings settings, ESLogger logger) {
         // User set discovery.type: ec2
-        if (!Ec2Discovery.EC2.equalsIgnoreCase(settings.get("discovery.type"))) {
+        if (!Ec2Discovery.EC2.equalsIgnoreCase(DiscoveryModule.DISCOVERY_TYPE_SETTING.get(settings))) {
             logger.trace("discovery.type not set to {}", Ec2Discovery.EC2);
             return false;
         }
diff --git a/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
index ec9155c..e5931dc 100644
--- a/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
+++ b/plugins/discovery-ec2/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
@@ -23,6 +23,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugin.discovery.ec2.Ec2DiscoveryPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
@@ -40,7 +41,7 @@ public abstract class AbstractAwsTestCase extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
                 Settings.Builder settings = Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .extendArray("plugin.types", Ec2DiscoveryPlugin.class.getName())
                 .put("cloud.aws.test.random", randomInt())
                 .put("cloud.aws.test.write_failures", 0.1)
diff --git a/plugins/discovery-ec2/src/test/resources/rest-api-spec/test/discovery_ec2/10_basic.yaml b/plugins/discovery-ec2/src/test/resources/rest-api-spec/test/discovery_ec2/10_basic.yaml
index e3b7844..d612c75 100644
--- a/plugins/discovery-ec2/src/test/resources/rest-api-spec/test/discovery_ec2/10_basic.yaml
+++ b/plugins/discovery-ec2/src/test/resources/rest-api-spec/test/discovery_ec2/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: discovery-ec2  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
index 496a1df..97e637a 100644
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/plugin/discovery/gce/GceDiscoveryPlugin.java
@@ -115,7 +115,7 @@ public class GceDiscoveryPlugin extends Plugin {
      */
     public static boolean isDiscoveryAlive(Settings settings, ESLogger logger) {
         // User set discovery.type: gce
-        if (GceDiscovery.GCE.equalsIgnoreCase(settings.get("discovery.type")) == false) {
+        if (GceDiscovery.GCE.equalsIgnoreCase(DiscoveryModule.DISCOVERY_TYPE_SETTING.get(settings)) == false) {
             logger.debug("discovery.type not set to {}", GceDiscovery.GCE);
             return false;
         }
diff --git a/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
index 8f5fbdc..6f48aa6 100644
--- a/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
+++ b/plugins/discovery-gce/src/test/resources/rest-api-spec/test/discovery_gce/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: discovery-gce  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/discovery-multicast/src/test/resources/rest-api-spec/test/discovery_multicast/10_basic.yaml b/plugins/discovery-multicast/src/test/resources/rest-api-spec/test/discovery_multicast/10_basic.yaml
index 4c11023..36172fa 100644
--- a/plugins/discovery-multicast/src/test/resources/rest-api-spec/test/discovery_multicast/10_basic.yaml
+++ b/plugins/discovery-multicast/src/test/resources/rest-api-spec/test/discovery_multicast/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: discovery-multicast  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/ingest-geoip/build.gradle b/plugins/ingest-geoip/build.gradle
deleted file mode 100644
index 7eee668..0000000
--- a/plugins/ingest-geoip/build.gradle
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-  description 'Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database'
-  classname 'org.elasticsearch.ingest.geoip.IngestGeoIpPlugin'
-}
-
-dependencies {
-  compile ('com.maxmind.geoip2:geoip2:2.4.0')
-  // geoip2 dependencies:
-  compile('com.fasterxml.jackson.core:jackson-annotations:2.5.0')
-  compile('com.fasterxml.jackson.core:jackson-databind:2.5.3')
-  compile('com.maxmind.db:maxmind-db:1.0.1')
-
-  testCompile 'org.elasticsearch:geolite2-databases:20151029'
-}
-
-task copyDefaultGeoIp2DatabaseFiles(type: Copy) {
-  from { zipTree(configurations.testCompile.files.find { it.name.contains('geolite2-databases')}) }
-  into "${project.buildDir}/ingest-geoip"
-  include "*.mmdb"
-}
-
-project.bundlePlugin.dependsOn(copyDefaultGeoIp2DatabaseFiles)
-
-compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
-compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
-
-bundlePlugin {
-  from("${project.buildDir}/ingest-geoip") {
-    into 'config/'
-  }
-}
-
-thirdPartyAudit.excludes = [
-  // geoip WebServiceClient needs Google http client, but we're not using WebServiceClient:
-  'com.google.api.client.http.HttpTransport',
-  'com.google.api.client.http.GenericUrl',
-  'com.google.api.client.http.HttpResponse',
-  'com.google.api.client.http.HttpRequestFactory',
-  'com.google.api.client.http.HttpRequest',
-  'com.google.api.client.http.HttpHeaders',
-  'com.google.api.client.http.HttpResponseException',
-  'com.google.api.client.http.javanet.NetHttpTransport',
-  'com.google.api.client.http.javanet.NetHttpTransport',
-]
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1 b/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
deleted file mode 100644
index 485286f..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ad40667ae87138e0aed075d2c15884497fa64acc
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt b/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
deleted file mode 100644
index 7a4a3ea..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt b/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
deleted file mode 100644
index 448b71d..0000000
--- a/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2013 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
deleted file mode 100644
index 862ac6f..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE b/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE b/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
deleted file mode 100644
index cdc6695..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-LICENSE b/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
deleted file mode 100644
index f5f45d2..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
+++ /dev/null
@@ -1,8 +0,0 @@
-This copy of Jackson JSON processor streaming parser/generator is licensed under the
-Apache (Software) License, version 2.0 ("the License").
-See the License for details about distribution rights, and the
-specific rights regarding derivate works.
-
-You may obtain a copy of the License at:
-
-http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-NOTICE b/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
deleted file mode 100644
index 4c976b7..0000000
--- a/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
+++ /dev/null
@@ -1,20 +0,0 @@
-# Jackson JSON processor
-
-Jackson is a high-performance, Free/Open Source JSON processing library.
-It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
-been in development since 2007.
-It is currently developed by a community of developers, as well as supported
-commercially by FasterXML.com.
-
-## Licensing
-
-Jackson core and extension components may licensed under different licenses.
-To find the details that apply to this artifact see the accompanying LICENSE file.
-For more information, including possible other licensing options, contact
-FasterXML.com (http://fasterxml.com).
-
-## Credits
-
-A list of contributors may be found from CREDITS file, which is included
-in some artifacts (usually source distributions); but is always available
-from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1 b/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
deleted file mode 100644
index 6cb749e..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-305429b84dbcd1cc3d393686f412cdcaec9cdbe6
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt b/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt b/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
deleted file mode 100644
index 1ebe2b0..0000000
--- a/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is Copyright (c) 2014 by MaxMind, Inc.
-
-This is free software, licensed under the Apache License, Version 2.0.
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
deleted file mode 100644
index b1c25f5..0000000
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import com.maxmind.geoip2.exception.AddressNotFoundException;
-import com.maxmind.geoip2.model.CityResponse;
-import com.maxmind.geoip2.model.CountryResponse;
-import com.maxmind.geoip2.record.City;
-import com.maxmind.geoip2.record.Continent;
-import com.maxmind.geoip2.record.Country;
-import com.maxmind.geoip2.record.Location;
-import com.maxmind.geoip2.record.Subdivision;
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.SpecialPermission;
-import org.elasticsearch.common.network.InetAddresses;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.ingest.core.AbstractProcessor;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.net.InetAddress;
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import static org.elasticsearch.ingest.core.ConfigurationUtils.readOptionalList;
-import static org.elasticsearch.ingest.core.ConfigurationUtils.readStringProperty;
-
-public final class GeoIpProcessor extends AbstractProcessor {
-
-    public static final String TYPE = "geoip";
-
-    private final String sourceField;
-    private final String targetField;
-    private final DatabaseReader dbReader;
-    private final Set<Field> fields;
-
-    GeoIpProcessor(String tag, String sourceField, DatabaseReader dbReader, String targetField, Set<Field> fields) throws IOException {
-        super(tag);
-        this.sourceField = sourceField;
-        this.targetField = targetField;
-        this.dbReader = dbReader;
-        this.fields = fields;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) {
-        String ip = ingestDocument.getFieldValue(sourceField, String.class);
-        final InetAddress ipAddress = InetAddresses.forString(ip);
-
-        Map<String, Object> geoData;
-        switch (dbReader.getMetadata().getDatabaseType()) {
-            case "GeoLite2-City":
-                try {
-                    geoData = retrieveCityGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            case "GeoLite2-Country":
-                try {
-                    geoData = retrieveCountryGeoData(ipAddress);
-                } catch (AddressNotFoundRuntimeException e) {
-                    geoData = Collections.emptyMap();
-                }
-                break;
-            default:
-                throw new IllegalStateException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]");
-        }
-        ingestDocument.setFieldValue(targetField, geoData);
-    }
-
-    @Override
-    public String getType() {
-        return TYPE;
-    }
-
-    String getSourceField() {
-        return sourceField;
-    }
-
-    String getTargetField() {
-        return targetField;
-    }
-
-    DatabaseReader getDbReader() {
-        return dbReader;
-    }
-
-    Set<Field> getFields() {
-        return fields;
-    }
-
-    private Map<String, Object> retrieveCityGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CityResponse response = AccessController.doPrivileged((PrivilegedAction<CityResponse>) () -> {
-            try {
-                return dbReader.city(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        City city = response.getCity();
-        Location location = response.getLocation();
-        Continent continent = response.getContinent();
-        Subdivision subdivision = response.getMostSpecificSubdivision();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-                case REGION_NAME:
-                    geoData.put("region_name", subdivision.getName());
-                    break;
-                case CITY_NAME:
-                    geoData.put("city_name", city.getName());
-                    break;
-                case TIMEZONE:
-                    geoData.put("timezone", location.getTimeZone());
-                    break;
-                case LOCATION:
-                    Map<String, Object> locationObject = new HashMap<>();
-                    locationObject.put("lat", location.getLatitude());
-                    locationObject.put("lon", location.getLongitude());
-                    geoData.put("location", locationObject);
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    private Map<String, Object> retrieveCountryGeoData(InetAddress ipAddress) {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        CountryResponse response = AccessController.doPrivileged((PrivilegedAction<CountryResponse>) () -> {
-            try {
-                return dbReader.country(ipAddress);
-            } catch (AddressNotFoundException e) {
-                throw new AddressNotFoundRuntimeException(e);
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-        Country country = response.getCountry();
-        Continent continent = response.getContinent();
-
-        Map<String, Object> geoData = new HashMap<>();
-        for (Field field : fields) {
-            switch (field) {
-                case IP:
-                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
-                    break;
-                case COUNTRY_ISO_CODE:
-                    geoData.put("country_iso_code", country.getIsoCode());
-                    break;
-                case COUNTRY_NAME:
-                    geoData.put("country_name", country.getName());
-                    break;
-                case CONTINENT_NAME:
-                    geoData.put("continent_name", continent.getName());
-                    break;
-            }
-        }
-        return geoData;
-    }
-
-    public static final class Factory extends AbstractProcessorFactory<GeoIpProcessor> implements Closeable {
-
-        static final Set<Field> DEFAULT_FIELDS = EnumSet.of(
-                Field.CONTINENT_NAME, Field.COUNTRY_ISO_CODE, Field.REGION_NAME, Field.CITY_NAME, Field.LOCATION
-        );
-
-        private final Map<String, DatabaseReader> databaseReaders;
-
-        public Factory(Map<String, DatabaseReader> databaseReaders) {
-            this.databaseReaders = databaseReaders;
-        }
-
-        @Override
-        public GeoIpProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            String ipField = readStringProperty(config, "source_field");
-            String targetField = readStringProperty(config, "target_field", "geoip");
-            String databaseFile = readStringProperty(config, "database_file", "GeoLite2-City.mmdb");
-            List<String> fieldNames = readOptionalList(config, "fields");
-
-            final Set<Field> fields;
-            if (fieldNames != null) {
-                fields = EnumSet.noneOf(Field.class);
-                for (String fieldName : fieldNames) {
-                    try {
-                        fields.add(Field.parse(fieldName));
-                    } catch (Exception e) {
-                        throw new IllegalArgumentException("illegal field option [" + fieldName +"]. valid values are [" + Arrays.toString(Field.values()) +"]", e);
-                    }
-                }
-            } else {
-                fields = DEFAULT_FIELDS;
-            }
-
-            DatabaseReader databaseReader = databaseReaders.get(databaseFile);
-            if (databaseReader == null) {
-                throw new IllegalArgumentException("database file [" + databaseFile + "] doesn't exist");
-            }
-            return new GeoIpProcessor(processorTag, ipField, databaseReader, targetField, fields);
-        }
-
-        @Override
-        public void close() throws IOException {
-            IOUtils.close(databaseReaders.values());
-        }
-    }
-
-    // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code
-    // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow
-    // it with an unchecked exception.
-    private final static class AddressNotFoundRuntimeException extends RuntimeException {
-
-        public AddressNotFoundRuntimeException(Throwable cause) {
-            super(cause);
-        }
-    }
-
-    public enum Field {
-
-        IP,
-        COUNTRY_ISO_CODE,
-        COUNTRY_NAME,
-        CONTINENT_NAME,
-        REGION_NAME,
-        CITY_NAME,
-        TIMEZONE,
-        LATITUDE,
-        LONGITUDE,
-        LOCATION;
-
-        public static Field parse(String value) {
-            return valueOf(value.toUpperCase(Locale.ROOT));
-        }
-    }
-
-}
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
deleted file mode 100644
index f92cb7b..0000000
--- a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import org.elasticsearch.node.NodeModule;
-import org.elasticsearch.plugins.Plugin;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.PathMatcher;
-import java.nio.file.StandardOpenOption;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.stream.Stream;
-
-public class IngestGeoIpPlugin extends Plugin {
-
-    @Override
-    public String name() {
-        return "ingest-geoip";
-    }
-
-    @Override
-    public String description() {
-        return "Ingest processor that adds information about the geographical location of ip addresses";
-    }
-
-    public void onModule(NodeModule nodeModule) throws IOException {
-        Path geoIpConfigDirectory = nodeModule.getNode().getEnvironment().configFile().resolve("ingest-geoip");
-        Map<String, DatabaseReader> databaseReaders = loadDatabaseReaders(geoIpConfigDirectory);
-        nodeModule.registerProcessor(GeoIpProcessor.TYPE, (templateService) -> new GeoIpProcessor.Factory(databaseReaders));
-    }
-
-    static Map<String, DatabaseReader> loadDatabaseReaders(Path geoIpConfigDirectory) throws IOException {
-        if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) {
-            throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory  + "] containing databases doesn't exist");
-        }
-
-        Map<String, DatabaseReader> databaseReaders = new HashMap<>();
-        try (Stream<Path> databaseFiles = Files.list(geoIpConfigDirectory)) {
-            PathMatcher pathMatcher = geoIpConfigDirectory.getFileSystem().getPathMatcher("glob:**.mmdb");
-            // Use iterator instead of forEach otherwise IOException needs to be caught twice...
-            Iterator<Path> iterator = databaseFiles.iterator();
-            while (iterator.hasNext()) {
-                Path databasePath = iterator.next();
-                if (Files.isRegularFile(databasePath) && pathMatcher.matches(databasePath)) {
-                    try (InputStream inputStream = Files.newInputStream(databasePath, StandardOpenOption.READ)) {
-                        databaseReaders.put(databasePath.getFileName().toString(), new DatabaseReader.Builder(inputStream).build());
-                    }
-                }
-            }
-        }
-        return Collections.unmodifiableMap(databaseReaders);
-    }
-}
diff --git a/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy b/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
deleted file mode 100644
index f49d15d..0000000
--- a/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-grant {
-  // needed because jackson-databind is using Class#getDeclaredConstructors(), Class#getDeclaredMethods() and
-  // Class#getDeclaredAnnotations() to find all public, private, protected, package protected and
-  // private constructors, methods or annotations. Just locating all public constructors, methods and annotations
-  // should be enough, so this permission wouldn't then be needed. Unfortunately this is not what jackson-databind does
-  // or can be configured to do.
-  permission java.lang.RuntimePermission "accessDeclaredMembers";
-};
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
deleted file mode 100644
index b59242e..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.StreamsUtils;
-import org.junit.BeforeClass;
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.sameInstance;
-
-public class GeoIpProcessorFactoryTests extends ESTestCase {
-
-    private static Map<String, DatabaseReader> databaseReaders;
-
-    @BeforeClass
-    public static void loadDatabaseReaders() throws IOException {
-        Path configDir = createTempDir();
-        Path geoIpConfigDir = configDir.resolve("ingest-geoip");
-        Files.createDirectories(geoIpConfigDir);
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-City.mmdb")), geoIpConfigDir.resolve("GeoLite2-City.mmdb"));
-        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-Country.mmdb")), geoIpConfigDir.resolve("GeoLite2-Country.mmdb"));
-        databaseReaders = IngestGeoIpPlugin.loadDatabaseReaders(geoIpConfigDir);
-    }
-
-    public void testBuildDefaults() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-
-        String processorTag = randomAsciiOfLength(10);
-        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
-
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getTag(), equalTo(processorTag));
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-City"));
-        assertThat(processor.getFields(), sameInstance(GeoIpProcessor.Factory.DEFAULT_FIELDS));
-    }
-
-    public void testBuildTargetField() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("target_field", "_field");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("_field"));
-    }
-
-    public void testBuildDbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getTargetField(), equalTo("geoip"));
-        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-Country"));
-    }
-
-    public void testBuildNonExistingDbFile() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("database_file", "does-not-exist.mmdb");
-        try {
-            factory.create(config);
-            fail("Exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("database file [does-not-exist.mmdb] doesn't exist"));
-        }
-    }
-
-    public void testBuildFields() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-
-        Set<GeoIpProcessor.Field> fields = EnumSet.noneOf(GeoIpProcessor.Field.class);
-        List<String> fieldNames = new ArrayList<>();
-        int numFields = scaledRandomIntBetween(1, GeoIpProcessor.Field.values().length);
-        for (int i = 0; i < numFields; i++) {
-            GeoIpProcessor.Field field = GeoIpProcessor.Field.values()[i];
-            fields.add(field);
-            fieldNames.add(field.name().toLowerCase(Locale.ROOT));
-        }
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", fieldNames);
-        GeoIpProcessor processor = factory.create(config);
-        assertThat(processor.getSourceField(), equalTo("_field"));
-        assertThat(processor.getFields(), equalTo(fields));
-    }
-
-    public void testBuildIllegalFieldOption() throws Exception {
-        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
-
-        Map<String, Object> config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", Collections.singletonList("invalid"));
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("illegal field option [invalid]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]"));
-        }
-
-        config = new HashMap<>();
-        config.put("source_field", "_field");
-        config.put("fields", "invalid");
-        try {
-            factory.create(config);
-            fail("exception expected");
-        } catch (IllegalArgumentException e) {
-            assertThat(e.getMessage(), equalTo("property [fields] isn't a list, but of type [java.lang.String]"));
-        }
-    }
-}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
deleted file mode 100644
index 4351798..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.maxmind.geoip2.DatabaseReader;
-import org.elasticsearch.ingest.RandomDocumentPicks;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.InputStream;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class GeoIpProcessorTests extends ESTestCase {
-
-    public void testCity() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(8));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-        assertThat(geoData.get("region_name"), equalTo("North Holland"));
-        assertThat(geoData.get("city_name"), equalTo("Amsterdam"));
-        assertThat(geoData.get("timezone"), equalTo("Europe/Amsterdam"));
-        Map<String, Object> location = new HashMap<>();
-        location.put("lat", 52.374d);
-        location.put("lon", 4.8897d);
-        assertThat(geoData.get("location"), equalTo(location));
-    }
-
-    public void testCountry() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-Country.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "82.170.213.79");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-
-        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(4));
-        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
-        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
-        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
-        assertThat(geoData.get("continent_name"), equalTo("Europe"));
-    }
-
-    public void testAddressIsNotInTheDatabase() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "202.45.11.11");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        processor.execute(ingestDocument);
-        @SuppressWarnings("unchecked")
-        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
-        assertThat(geoData.size(), equalTo(0));
-    }
-
-    /** Don't silently do DNS lookups or anything trappy on bogus data */
-    public void testInvalid() throws Exception {
-        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
-        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
-
-        Map<String, Object> document = new HashMap<>();
-        document.put("source_field", "www.google.com");
-        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
-        try {
-            processor.execute(ingestDocument);
-            fail("did not get expected exception");
-        } catch (IllegalArgumentException expected) {
-            assertNotNull(expected.getMessage());
-            assertThat(expected.getMessage(), containsString("not an IP string literal"));
-        }
-    }
-
-}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
deleted file mode 100644
index 0e4d1ee..0000000
--- a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest.geoip;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-import java.util.Collection;
-
-public class IngestGeoIpRestIT extends ESRestTestCase {
-
-    public IngestGeoIpRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
deleted file mode 100644
index b522cb7..0000000
--- a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-"Ingest plugin installed":
-    - do:
-        cluster.stats: {}
-
-    - match:  { nodes.plugins.0.name: ingest-geoip  }
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
deleted file mode 100644
index 704f288..0000000
--- a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
+++ /dev/null
@@ -1,124 +0,0 @@
----
-"Test geoip processor with defaults":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 5 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.location.lon: -93.2166 }
-  - match: { _source.geoip.location.lat: 44.9759 }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with fields":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "fields" : ["city_name", "country_iso_code", "ip", "latitude", "longitude", "location", "timezone", "country_name", "region_name", "continent_name"]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 8 }
-  - match: { _source.geoip.city_name: "Minneapolis" }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.ip: "128.101.101.101" }
-  - match: { _source.geoip.location.lon: -93.2166 }
-  - match: { _source.geoip.location.lat: 44.9759 }
-  - match: { _source.geoip.timezone: "America/Chicago" }
-  - match: { _source.geoip.country_name: "United States" }
-  - match: { _source.geoip.region_name: "Minnesota" }
-  - match: { _source.geoip.continent_name: "North America" }
-
----
-"Test geoip processor with different database file":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "geoip" : {
-                  "source_field" : "field1",
-                  "database_file" : "GeoLite2-Country.mmdb"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "128.101.101.101"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "128.101.101.101" }
-  - length: { _source.geoip: 2 }
-  - match: { _source.geoip.country_iso_code: "US" }
-  - match: { _source.geoip.continent_name: "North America" }
diff --git a/plugins/jvm-example/src/test/resources/rest-api-spec/test/jvm_example/10_basic.yaml b/plugins/jvm-example/src/test/resources/rest-api-spec/test/jvm_example/10_basic.yaml
index 169b924..c671fe2 100644
--- a/plugins/jvm-example/src/test/resources/rest-api-spec/test/jvm_example/10_basic.yaml
+++ b/plugins/jvm-example/src/test/resources/rest-api-spec/test/jvm_example/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: jvm-example  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
index 6259780..04a5a7a 100644
--- a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: lang-plan-a }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
index 9b7d8af..81a8282 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
@@ -22,6 +22,7 @@ package org.elasticsearch.mapper.attachments;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Before;
@@ -39,7 +40,7 @@ public class AttachmentUnitTestCase extends ESTestCase {
     @Before
     public void createSettings() throws Exception {
       testSettings = Settings.builder()
-                             .put("path.home", createTempDir())
+                             .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                              .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.id)
                              .build();
     }
diff --git a/plugins/mapper-attachments/src/test/resources/rest-api-spec/test/mapper_attachments/00_basic.yaml b/plugins/mapper-attachments/src/test/resources/rest-api-spec/test/mapper_attachments/00_basic.yaml
index 819478d..9654535 100644
--- a/plugins/mapper-attachments/src/test/resources/rest-api-spec/test/mapper_attachments/00_basic.yaml
+++ b/plugins/mapper-attachments/src/test/resources/rest-api-spec/test/mapper_attachments/00_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: mapper-attachments  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperUpgradeTests.java b/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperUpgradeTests.java
index b3ad01b..fe12cb0 100644
--- a/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperUpgradeTests.java
+++ b/plugins/mapper-murmur3/src/test/java/org/elasticsearch/index/mapper/murmur3/Murmur3FieldMapperUpgradeTests.java
@@ -23,6 +23,7 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.plugin.mapper.MapperMurmur3Plugin;
 import org.elasticsearch.plugins.Plugin;
@@ -62,7 +63,7 @@ public class Murmur3FieldMapperUpgradeTests extends ESIntegTestCase {
 
         Path dataPath = createTempDir();
         Settings settings = Settings.builder()
-                .put("path.data", dataPath)
+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)
                 .build();
         final String node = internalCluster().startDataOnlyNode(settings); // workaround for dangling index loading issue when node is master
         Path[] nodePaths = internalCluster().getInstance(NodeEnvironment.class, node).nodeDataPaths();
diff --git a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java
index 4529111..a2af6df 100644
--- a/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java
+++ b/plugins/mapper-size/src/test/java/org/elasticsearch/index/mapper/size/SizeFieldMapperUpgradeTests.java
@@ -23,6 +23,7 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.plugin.mapper.MapperSizePlugin;
 import org.elasticsearch.plugins.Plugin;
@@ -63,7 +64,7 @@ public class SizeFieldMapperUpgradeTests extends ESIntegTestCase {
 
         Path dataPath = createTempDir();
         Settings settings = Settings.builder()
-                .put("path.data", dataPath)
+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)
                 .build();
         final String node = internalCluster().startDataOnlyNode(settings); // workaround for dangling index loading issue when node is master
         Path[] nodePaths = internalCluster().getInstance(NodeEnvironment.class, node).nodeDataPaths();
diff --git a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
index cab2f98..1818a5e 100644
--- a/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
+++ b/plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureSnapshotRestoreTests.java
@@ -35,6 +35,7 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.repositories.RepositoryMissingException;
 import org.elasticsearch.repositories.RepositoryVerificationException;
 import org.elasticsearch.repositories.azure.AzureRepository.Repository;
@@ -77,7 +78,7 @@ public class AzureSnapshotRestoreTests extends AbstractAzureWithThirdPartyTestCa
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder().put(super.nodeSettings(nodeOrdinal))
                 // In snapshot tests, we explicitly disable cloud discovery
-                .put("discovery.type", "local")
+                .put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "local")
                 .build();
     }
 
diff --git a/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml b/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml
index a77304b..fb929f1 100644
--- a/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml
+++ b/plugins/repository-azure/src/test/resources/rest-api-spec/test/repository_azure/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: repository-azure  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/repository-hdfs/src/test/resources/rest-api-spec/test/hdfs_repository/10_basic.yaml b/plugins/repository-hdfs/src/test/resources/rest-api-spec/test/hdfs_repository/10_basic.yaml
index 7c56940..6fbbfc8 100644
--- a/plugins/repository-hdfs/src/test/resources/rest-api-spec/test/hdfs_repository/10_basic.yaml
+++ b/plugins/repository-hdfs/src/test/resources/rest-api-spec/test/hdfs_repository/10_basic.yaml
@@ -13,7 +13,6 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: repository-hdfs  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
 ---
 #
 # Check that we can't use file:// repositories or anything like that
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index 90b79fd..a897cf6 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -200,6 +200,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             return "s3-ap-southeast-2.amazonaws.com";
         } else if ("ap-northeast".equals(region) || "ap-northeast-1".equals(region)) {
             return "s3-ap-northeast-1.amazonaws.com";
+        } else if ("ap-northeast-2".equals(region)) {
+            return "s3-ap-northeast-2.amazonaws.com";
         } else if ("eu-west".equals(region) || "eu-west-1".equals(region)) {
             return "s3-eu-west-1.amazonaws.com";
         } else if ("eu-central".equals(region) || "eu-central-1".equals(region)) {
diff --git a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
index e823e8e..bc37062 100644
--- a/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
+++ b/plugins/repository-s3/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTestCase.java
@@ -23,6 +23,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.plugin.repository.s3.S3RepositoryPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
@@ -40,7 +41,7 @@ public abstract class AbstractAwsTestCase extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
                 Settings.Builder settings = Settings.builder()
                 .put(super.nodeSettings(nodeOrdinal))
-                .put("path.home", createTempDir())
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
                 .extendArray("plugin.types", S3RepositoryPlugin.class.getName(), TestAwsS3Service.TestPlugin.class.getName())
                 .put("cloud.aws.test.random", randomInt())
                 .put("cloud.aws.test.write_failures", 0.1)
diff --git a/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/10_basic.yaml b/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/10_basic.yaml
index 811ff88..5fcc812 100644
--- a/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/10_basic.yaml
+++ b/plugins/repository-s3/src/test/resources/rest-api-spec/test/repository_s3/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: repository-s3  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/site-example/build.gradle b/plugins/site-example/build.gradle
deleted file mode 100644
index d222812..0000000
--- a/plugins/site-example/build.gradle
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-esplugin {
-  description 'Demonstrates how to serve resources via elasticsearch.'
-  jvm false
-  site true
-}
-
-// no unit tests
-test.enabled = false
diff --git a/plugins/site-example/src/site/_site/index.html b/plugins/site-example/src/site/_site/index.html
deleted file mode 100644
index bc6343f..0000000
--- a/plugins/site-example/src/site/_site/index.html
+++ /dev/null
@@ -1,6 +0,0 @@
-<html>
-  <head>
-    <title>Page title</title>
-  </head>
-  <body>Page body</body>
-</html>
diff --git a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java b/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java
deleted file mode 100644
index c92a0ba..0000000
--- a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteContentsIT.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.example;
-
-import org.apache.http.impl.client.CloseableHttpClient;
-import org.apache.http.impl.client.HttpClients;
-import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
-import org.elasticsearch.common.network.NetworkAddress;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.ExternalTestCluster;
-import org.elasticsearch.test.TestCluster;
-import org.elasticsearch.test.rest.client.RestResponse;
-import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
-
-import java.net.InetSocketAddress;
-import java.util.concurrent.TimeUnit;
-
-/**
- * verifies content is actually served for the site plugin
- */
-public class SiteContentsIT extends ESIntegTestCase {
-
-    // TODO: probably a better way to test, but we don't want to really
-    // define a fake rest spec or anything?
-    public void test() throws Exception {
-        TestCluster cluster = cluster();
-        assumeTrue("this test will not work from an IDE unless you pass tests.cluster pointing to a running instance", cluster instanceof ExternalTestCluster);
-        ExternalTestCluster externalCluster = (ExternalTestCluster) cluster;
-        try (CloseableHttpClient httpClient = HttpClients.createMinimal(new PoolingHttpClientConnectionManager(15, TimeUnit.SECONDS))) {
-            for (InetSocketAddress address :  externalCluster.httpAddresses()) {
-                RestResponse restResponse = new RestResponse(
-                        new HttpRequestBuilder(httpClient)
-                        .host(NetworkAddress.formatAddress(address.getAddress())).port(address.getPort())
-                        .path("/_plugin/site-example/")
-                        .method("GET").execute());
-                assertEquals(200, restResponse.getStatusCode());
-                String body = restResponse.getBodyAsString();
-                assertTrue("unexpected body contents: " + body, body.contains("<body>Page body</body>"));
-            }
-        }
-    }
-}
diff --git a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteRestIT.java b/plugins/site-example/src/test/java/org/elasticsearch/example/SiteRestIT.java
deleted file mode 100644
index e3df9ce..0000000
--- a/plugins/site-example/src/test/java/org/elasticsearch/example/SiteRestIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.example;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class SiteRestIT extends ESRestTestCase {
-
-    public SiteRestIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-}
-
diff --git a/plugins/site-example/src/test/resources/rest-api-spec/test/example/10_basic.yaml b/plugins/site-example/src/test/resources/rest-api-spec/test/example/10_basic.yaml
deleted file mode 100644
index a66ce5c..0000000
--- a/plugins/site-example/src/test/resources/rest-api-spec/test/example/10_basic.yaml
+++ /dev/null
@@ -1,15 +0,0 @@
-# Integration tests for Example site plugin
-#
-"Example site loaded":
-    - do:
-        cluster.state: {}
-
-    # Get master node id
-    - set: { master_node: master }
-
-    - do:
-        nodes.info: {}
-
-    - match:  { nodes.$master.plugins.0.name: site-example  }
-    - match:  { nodes.$master.plugins.0.jvm: false  }
-    - match:  { nodes.$master.plugins.0.site: true  }
diff --git a/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml b/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml
index 155a39b..a210fd4 100644
--- a/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml
+++ b/plugins/store-smb/src/test/resources/rest-api-spec/test/store_smb/10_basic.yaml
@@ -11,4 +11,3 @@
         nodes.info: {}
 
     - match:  { nodes.$master.plugins.0.name: store-smb  }
-    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/bootstrap/EvilSecurityTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/bootstrap/EvilSecurityTests.java
index 695d2a4..c213f18 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/bootstrap/EvilSecurityTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/bootstrap/EvilSecurityTests.java
@@ -36,14 +36,14 @@ import java.util.Set;
 
 @SuppressForbidden(reason = "modifies system properties and attempts to create symbolic links intentionally")
 public class EvilSecurityTests extends ESTestCase {
-    
+
     /** test generated permissions */
     public void testGeneratedPermissions() throws Exception {
         Path path = createTempDir();
         // make a fake ES home and ensure we only grant permissions to that.
         Path esHome = path.resolve("esHome");
         Settings.Builder settingsBuilder = Settings.builder();
-        settingsBuilder.put("path.home", esHome.toString());
+        settingsBuilder.put(Environment.PATH_HOME_SETTING.getKey(), esHome.toString());
         Settings settings = settingsBuilder.build();
 
         Path fakeTmpDir = createTempDir();
@@ -56,7 +56,7 @@ public class EvilSecurityTests extends ESTestCase {
         } finally {
             System.setProperty("java.io.tmpdir", realTmpDir);
         }
-      
+
         // the fake es home
         assertNoPermissions(esHome, permissions);
         // its parent
@@ -74,14 +74,14 @@ public class EvilSecurityTests extends ESTestCase {
         Path esHome = path.resolve("esHome");
 
         Settings.Builder settingsBuilder = Settings.builder();
-        settingsBuilder.put("path.home", esHome.resolve("home").toString());
-        settingsBuilder.put("path.conf", esHome.resolve("conf").toString());
-        settingsBuilder.put("path.scripts", esHome.resolve("scripts").toString());
-        settingsBuilder.put("path.plugins", esHome.resolve("plugins").toString());
-        settingsBuilder.putArray("path.data", esHome.resolve("data1").toString(), esHome.resolve("data2").toString());
-        settingsBuilder.put("path.shared_data", esHome.resolve("custom").toString());
-        settingsBuilder.put("path.logs", esHome.resolve("logs").toString());
-        settingsBuilder.put("pidfile", esHome.resolve("test.pid").toString());
+        settingsBuilder.put(Environment.PATH_HOME_SETTING.getKey(), esHome.resolve("home").toString());
+        settingsBuilder.put(Environment.PATH_CONF_SETTING.getKey(), esHome.resolve("conf").toString());
+        settingsBuilder.put(Environment.PATH_SCRIPTS_SETTING.getKey(), esHome.resolve("scripts").toString());
+        settingsBuilder.put(Environment.PATH_PLUGINS_SETTING.getKey(), esHome.resolve("plugins").toString());
+        settingsBuilder.putArray(Environment.PATH_DATA_SETTING.getKey(), esHome.resolve("data1").toString(), esHome.resolve("data2").toString());
+        settingsBuilder.put(Environment.PATH_SHARED_DATA_SETTING.getKey(), esHome.resolve("custom").toString());
+        settingsBuilder.put(Environment.PATH_LOGS_SETTING.getKey(), esHome.resolve("logs").toString());
+        settingsBuilder.put(Environment.PIDFILE_SETTING.getKey(), esHome.resolve("test.pid").toString());
         Settings settings = settingsBuilder.build();
 
         Path fakeTmpDir = createTempDir();
@@ -104,7 +104,7 @@ public class EvilSecurityTests extends ESTestCase {
         assertNoPermissions(esHome.getParent().resolve("other"), permissions);
         // double check we overwrote java.io.tmpdir correctly for the test
         assertNoPermissions(PathUtils.get(realTmpDir), permissions);
- 
+
         // check that all directories got permissions:
 
         // bin file: ro
@@ -135,10 +135,10 @@ public class EvilSecurityTests extends ESTestCase {
         // PID file: delete only (for the shutdown hook)
         assertExactPermissions(new FilePermission(environment.pidFile().toString(), "delete"), permissions);
     }
-    
+
     public void testEnsureSymlink() throws IOException {
         Path p = createTempDir();
-        
+
         Path exists = p.resolve("exists");
         Files.createDirectory(exists);
 
@@ -154,7 +154,7 @@ public class EvilSecurityTests extends ESTestCase {
         Security.ensureDirectoryExists(linkExists);
         Files.createTempFile(linkExists, null, null);
     }
-    
+
     public void testEnsureBrokenSymlink() throws IOException {
         Path p = createTempDir();
 
@@ -199,7 +199,7 @@ public class EvilSecurityTests extends ESTestCase {
         assertExactPermissions(new FilePermission(target.resolve("foo").toString(), "read"), permissions);
     }
 
-    /** 
+    /**
      * checks exact file permissions, meaning those and only those for that path.
      */
     static void assertExactPermissions(FilePermission expected, PermissionCollection actual) {
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java
index 8633511..45f3df2 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/common/cli/CheckFileCommandTests.java
@@ -124,7 +124,7 @@ public class CheckFileCommandTests extends ESTestCase {
         try (FileSystem fs = Jimfs.newFileSystem(configuration)) {
             Path path = fs.getPath(randomAsciiOfLength(10));
             Settings settings = Settings.builder()
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             new CreateFileCommand(captureOutputTerminal, path).execute(settings, new Environment(settings));
             assertThat(Files.exists(path), is(true));
@@ -141,7 +141,7 @@ public class CheckFileCommandTests extends ESTestCase {
             Files.write(path, "anything".getBytes(StandardCharsets.UTF_8));
 
             Settings settings = Settings.builder()
-                    .put("path.home", createTempDir().toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())
                     .build();
             new DeleteFileCommand(captureOutputTerminal, path).execute(settings, new Environment(settings));
             assertThat(Files.exists(path), is(false));
@@ -173,7 +173,7 @@ public class CheckFileCommandTests extends ESTestCase {
             this.fs = fs;
             this.paths = new Path[] { writePath(fs, "p1", "anything"), writePath(fs, "p2", "anything"), writePath(fs, "p3", "anything") };
             Settings settings = Settings.settingsBuilder()
-                    .put("path.home", baseDir.toString())
+                    .put(Environment.PATH_HOME_SETTING.getKey(), baseDir.toString())
                     .build();
             return super.execute(Settings.EMPTY, new Environment(settings));
         }
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java
index 3789c27..d2c8ccf 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/node/internal/EvilInternalSettingsPreparerTests.java
@@ -72,7 +72,7 @@ public class EvilInternalSettingsPreparerTests extends ESTestCase {
     @Before
     public void createBaseEnvSettings() {
         baseEnvSettings = settingsBuilder()
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             .build();
     }
 
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
index 0eebc97..5e70cf7 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerPermissionTests.java
@@ -70,13 +70,13 @@ public class PluginManagerPermissionTests extends ESTestCase {
     @Before
     public void setup() {
         Path tempDir = createTempDir();
-        Settings.Builder settingsBuilder = settingsBuilder().put("path.home", tempDir);
+        Settings.Builder settingsBuilder = settingsBuilder().put(Environment.PATH_HOME_SETTING.getKey(), tempDir);
         if (randomBoolean()) {
-            settingsBuilder.put("path.plugins", createTempDir());
+            settingsBuilder.put(Environment.PATH_PLUGINS_SETTING.getKey(), createTempDir());
         }
 
         if (randomBoolean()) {
-            settingsBuilder.put("path.conf", createTempDir());
+            settingsBuilder.put(Environment.PATH_CONF_SETTING.getKey(), createTempDir());
         }
 
         environment = new Environment(settingsBuilder.build());
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
index bc92f89..24055d9 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
@@ -108,7 +108,7 @@ public class PluginManagerTests extends ESIntegTestCase {
     @Before
     public void setup() throws Exception {
         environment = buildInitialSettings();
-        System.setProperty("es.default.path.home", environment.settings().get("path.home"));
+        System.setProperty("es.default.path.home", Environment.PATH_HOME_SETTING.get(environment.settings()));
         Path binDir = environment.binFile();
         if (!Files.exists(binDir)) {
             Files.createDirectories(binDir);
@@ -196,7 +196,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
         assertStatus("install", USAGE);
     }
@@ -216,7 +215,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
 
         Path binDir = environment.binFile();
@@ -260,7 +258,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
 
         Path pluginConfigDir = environment.configFile().resolve(pluginName);
@@ -296,7 +293,6 @@ public class PluginManagerTests extends ESIntegTestCase {
                 "version", "2.0",
                 "elasticsearch.version", Version.CURRENT.toString(),
                 "java.version", System.getProperty("java.specification.version"),
-                "jvm", "true",
                 "classname", "FakePlugin");
 
         assertStatusOk(String.format(Locale.ROOT, "install %s --verbose", pluginUrl));
@@ -361,7 +357,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
 
         Path binDir = environment.binFile();
@@ -392,16 +387,13 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
         System.err.println("install " + pluginUrl + " --verbose");
         ExitStatus status = new PluginManagerCliParser(terminal).execute(args("install " + pluginUrl + " --verbose"));
         assertThat("Terminal output was: " + terminal.getTerminalOutput(), status, is(ExitStatus.OK));
         assertThat(terminal.getTerminalOutput(), hasItem(containsString("Name: fake-plugin")));
         assertThat(terminal.getTerminalOutput(), hasItem(containsString("Description: fake desc")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("Site: false")));
         assertThat(terminal.getTerminalOutput(), hasItem(containsString("Version: 1.0")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("JVM: true")));
         assertThatPluginIsListed(pluginName);
     }
 
@@ -414,7 +406,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
         ExitStatus status = new PluginManagerCliParser(terminal).execute(args("install " + pluginUrl));
         assertThat("Terminal output was: " + terminal.getTerminalOutput(), status, is(ExitStatus.OK));
@@ -447,7 +438,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
             "isolated", "false",
-            "jvm", "true",
             "classname", "FakePlugin");
 
         // install
@@ -465,63 +455,20 @@ public class PluginManagerTests extends ESIntegTestCase {
         assertTrue(foundExpectedMessage);
     }
 
-    public void testInstallSitePluginVerbose() throws IOException {
-        String pluginName = "fake-plugin";
-        Path pluginDir = createTempDir().resolve(pluginName);
-        Files.createDirectories(pluginDir.resolve("_site"));
-        Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
-        String pluginUrl = createPlugin(pluginDir,
-                "description", "fake desc",
-                "name", pluginName,
-                "version", "1.0",
-                "site", "true");
-        ExitStatus status = new PluginManagerCliParser(terminal).execute(args("install " + pluginUrl + " --verbose"));
-        assertThat("Terminal output was: " + terminal.getTerminalOutput(), status, is(ExitStatus.OK));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("Name: fake-plugin")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("Description: fake desc")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("Site: true")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("Version: 1.0")));
-        assertThat(terminal.getTerminalOutput(), hasItem(containsString("JVM: false")));
-        assertThatPluginIsListed(pluginName);
-        // We want to check that Plugin Manager moves content to _site
-        assertFileExists(environment.pluginsFile().resolve(pluginName).resolve("_site"));
-    }
-
-    public void testInstallSitePlugin() throws IOException {
+    public void testInstallPluginWithBadChecksum() throws IOException {
         String pluginName = "fake-plugin";
         Path pluginDir = createTempDir().resolve(pluginName);
-        Files.createDirectories(pluginDir.resolve("_site"));
-        Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
-        String pluginUrl = createPlugin(pluginDir,
+        String pluginUrl = createPluginWithBadChecksum(pluginDir,
             "description", "fake desc",
             "name", pluginName,
             "version", "1.0",
-            "site", "true");
-        ExitStatus status = new PluginManagerCliParser(terminal).execute(args("install " + pluginUrl));
-        assertThat("Terminal output was: " + terminal.getTerminalOutput(), status, is(ExitStatus.OK));
-        assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("Name: fake-plugin"))));
-        assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("Description:"))));
-        assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("Site:"))));
-        assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("Version:"))));
-        assertThat(terminal.getTerminalOutput(), not(hasItem(containsString("JVM:"))));
-        assertThatPluginIsListed(pluginName);
-        // We want to check that Plugin Manager moves content to _site
-        assertFileExists(environment.pluginsFile().resolve(pluginName).resolve("_site"));
-    }
-
-    public void testInstallPluginWithBadChecksum() throws IOException {
-        String pluginName = "fake-plugin";
-        Path pluginDir = createTempDir().resolve(pluginName);
-        Files.createDirectories(pluginDir.resolve("_site"));
-        Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
-        String pluginUrl = createPluginWithBadChecksum(pluginDir,
-                "description", "fake desc",
-                "version", "1.0",
-                "site", "true");
+            "elasticsearch.version", Version.CURRENT.toString(),
+            "java.version", System.getProperty("java.specification.version"),
+            "classname", "FakePlugin");
         assertStatus(String.format(Locale.ROOT, "install %s --verbose", pluginUrl),
                 ExitStatus.IO_ERROR);
         assertThatPluginIsNotListed(pluginName);
-        assertFileNotExists(environment.pluginsFile().resolve(pluginName).resolve("_site"));
+        assertFileNotExists(environment.pluginsFile().resolve(pluginName));
     }
 
     private void singlePluginInstallAndRemove(String pluginDescriptor, String pluginName, String pluginCoordinates) throws IOException {
@@ -606,7 +553,6 @@ public class PluginManagerTests extends ESIntegTestCase {
             "version", "1.0.0",
             "elasticsearch.version", Version.CURRENT.toString(),
             "java.version", System.getProperty("java.specification.version"),
-            "jvm", "true",
             "classname", "FakePlugin");
 
         // We want to remove plugin with plugin short name
@@ -750,7 +696,7 @@ public class PluginManagerTests extends ESIntegTestCase {
     private Environment buildInitialSettings() throws IOException {
         Settings settings = settingsBuilder()
                 .put("http.enabled", true)
-                .put("path.home", createTempDir()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
         return InternalSettingsPreparer.prepareEnvironment(settings, null);
     }
 
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
index 266b44e..49edcc7 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
@@ -56,8 +56,8 @@ public class PluginManagerUnitTests extends ESTestCase {
         Path genericConfigFolder = createTempDir();
 
         Settings settings = settingsBuilder()
-                .put("path.conf", genericConfigFolder)
-                .put("path.home", homeFolder)
+                .put(Environment.PATH_CONF_SETTING.getKey(), genericConfigFolder)
+                .put(Environment.PATH_HOME_SETTING.getKey(), homeFolder)
                 .build();
         Environment environment = new Environment(settings);
 
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
index 1ad972e..24560aa 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
@@ -25,6 +25,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -56,21 +57,21 @@ public class TribeUnitTests extends ESTestCase {
         Settings baseSettings = Settings.builder()
             .put("http.enabled", false)
             .put("node.mode", NODE_MODE)
-            .put("path.home", createTempDir()).build();
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).build();
 
         tribe1 = new TribeClientNode(
             Settings.builder()
                 .put(baseSettings)
                 .put("cluster.name", "tribe1")
                 .put("name", "tribe1_node")
-                .put(DiscoveryService.SETTING_DISCOVERY_SEED, random().nextLong())
+                .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), random().nextLong())
                 .build()).start();
         tribe2 = new TribeClientNode(
             Settings.builder()
                 .put(baseSettings)
                 .put("cluster.name", "tribe2")
                 .put("name", "tribe2_node")
-                .put(DiscoveryService.SETTING_DISCOVERY_SEED, random().nextLong())
+                .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), random().nextLong())
                 .build()).start();
     }
 
@@ -102,7 +103,11 @@ public class TribeUnitTests extends ESTestCase {
 
     public void testThatTribeClientsIgnoreGlobalConfig() throws Exception {
         Path pathConf = getDataPath("elasticsearch.yml").getParent();
-        Settings settings = Settings.builder().put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true).put("path.conf", pathConf).build();
+        Settings settings = Settings
+            .builder()
+            .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
+            .put(Environment.PATH_CONF_SETTING.getKey(), pathConf)
+            .build();
         assertTribeNodeSuccesfullyCreated(settings);
     }
 
@@ -111,7 +116,7 @@ public class TribeUnitTests extends ESTestCase {
         //they can find their corresponding tribes using the proper transport
         Settings settings = Settings.builder().put("http.enabled", false).put("node.name", "tribe_node")
                 .put("tribe.t1.node.mode", NODE_MODE).put("tribe.t2.node.mode", NODE_MODE)
-                .put("path.home", createTempDir()).put(extraSettings).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir()).put(extraSettings).build();
 
         try (Node node = new Node(settings).start()) {
             try (Client client = node.client()) {
diff --git a/qa/ingest-disabled/build.gradle b/qa/ingest-disabled/build.gradle
deleted file mode 100644
index ca71697..0000000
--- a/qa/ingest-disabled/build.gradle
+++ /dev/null
@@ -1,26 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-integTest {
-    cluster {
-        systemProperty 'es.node.ingest', 'false'
-    }
-}
diff --git a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java b/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
deleted file mode 100644
index e162807..0000000
--- a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestDisabledIT extends ESRestTestCase {
-
-    public IngestDisabledIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml b/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
deleted file mode 100644
index 01d6740..0000000
--- a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
+++ /dev/null
@@ -1,122 +0,0 @@
----
-"Test ingest CRUD APIS work fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { pipelines.0.id: "my_pipeline" }
-  - match: { pipelines.0.config.description: "_description" }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { acknowledged: true }
-
----
-"Test ingest simulate API works fine when node.ingest is set to false":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test index api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
----
-"Test bulk api with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      bulk:
-        pipeline: "my_pipeline_1"
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-          - f1: v2
-
----
-"Test bulk api that contains a single index call with pipeline id fails when node.ingest is set to false":
-  - do:
-      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
-      bulk:
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-              pipeline: my_pipeline_1
-          - f1: v2
-
diff --git a/qa/ingest-with-mustache/build.gradle b/qa/ingest-with-mustache/build.gradle
deleted file mode 100644
index e5ca482..0000000
--- a/qa/ingest-with-mustache/build.gradle
+++ /dev/null
@@ -1,24 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-apply plugin: 'elasticsearch.rest-test'
-
-dependencies {
-    testCompile project(path: ':modules:lang-mustache', configuration: 'runtime')
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
deleted file mode 100644
index 57165e6..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.ingest.InternalTemplateService;
-import org.elasticsearch.ingest.core.TemplateService;
-import org.elasticsearch.script.ScriptContextRegistry;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.util.Collections;
-
-public abstract class AbstractMustacheTests extends ESTestCase {
-
-    protected TemplateService templateService;
-
-    @Before
-    public void init() throws Exception {
-        Settings settings = Settings.builder()
-            .put("path.home", createTempDir())
-            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
-            .build();
-        MustacheScriptEngineService mustache = new MustacheScriptEngineService(settings);
-        ScriptContextRegistry registry = new ScriptContextRegistry(Collections.emptyList());
-        ScriptService scriptService = new ScriptService(
-            settings, new Environment(settings), Collections.singleton(mustache), null, registry
-        );
-        templateService = new InternalTemplateService(scriptService);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
deleted file mode 100644
index 1b080fe..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class IngestDocumentMustacheIT extends AbstractMustacheTests {
-
-    public void testAccessMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("foo", "bar");
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 bar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 bar"));
-    }
-
-    public void testAccessMapMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> innerObject = new HashMap<>();
-        innerObject.put("bar", "hello bar");
-        innerObject.put("baz", "hello baz");
-        innerObject.put("qux", Collections.singletonMap("fubar", "hello qux and fubar"));
-        document.put("foo", innerObject);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo.bar}} {{foo.baz}} {{foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 hello bar hello baz hello qux and fubar"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo.bar}} {{_source.foo.baz}} {{_source.foo.qux.fubar}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 hello bar hello baz hello qux and fubar"));
-    }
-
-    public void testAccessListMetaDataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        document.put("list1", Arrays.asList("foo", "bar", null));
-        List<Map<String, Object>> list = new ArrayList<>();
-        Map<String, Object> value = new HashMap<>();
-        value.put("field", "value");
-        list.add(value);
-        list.add(null);
-        document.put("list2", list);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        // TODO: fix index based lookups in lists:
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{list1}} {{list2}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 [foo, bar, null] [{field=value}, null]"));
-
-        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.list1}} {{_source.list2}}", templateService));
-        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 [foo, bar, null] [{field=value}, null]"));
-    }
-
-    public void testAccessIngestMetadataViaTemplate() {
-        Map<String, Object> document = new HashMap<>();
-        Map<String, Object> ingestMap = new HashMap<>();
-        ingestMap.put("timestamp", "bogus_timestamp");
-        document.put("_ingest", ingestMap);
-        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
-        ingestDocument.setFieldValue(templateService.compile("ingest_timestamp"), ValueSource.wrap("{{_ingest.timestamp}} and {{_source._ingest.timestamp}}", templateService));
-        assertThat(ingestDocument.getFieldValue("ingest_timestamp", String.class), equalTo(ingestDocument.getIngestMetadata().get("timestamp") + " and bogus_timestamp"));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
deleted file mode 100644
index e94765a..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.processor.RemoveProcessor;
-import org.hamcrest.CoreMatchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-public class IngestMustacheRemoveProcessorIT extends AbstractMustacheTests {
-
-    public void testRemoveProcessorMustacheExpression() throws Exception {
-        RemoveProcessor.Factory factory = new RemoveProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", "field{{var}}");
-        RemoveProcessor processor = factory.create(config);
-        assertThat(processor.getField().execute(Collections.singletonMap("var", "_value")), CoreMatchers.equalTo("field_value"));
-    }
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
deleted file mode 100644
index 6846679..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-import org.elasticsearch.ingest.core.Processor;
-import org.elasticsearch.ingest.processor.SetProcessor;
-import org.hamcrest.Matchers;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IngestMustacheSetProcessorIT extends AbstractMustacheTests {
-
-    public void testExpression() throws Exception {
-        SetProcessor processor = createSetProcessor("_index", "text {{var}}");
-        assertThat(processor.getValue(), instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(processor.getValue().copyAndResolve(Collections.singletonMap("var", "_value")), equalTo("text _value"));
-    }
-
-    public void testSetMetadataWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
-        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value {{field}}");
-        IngestDocument ingestDocument = createIngestDocument(Collections.singletonMap("field", "value"));
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value value"));
-    }
-
-    public void testSetWithTemplates() throws Exception {
-        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.INDEX, IngestDocument.MetaData.TYPE, IngestDocument.MetaData.ID);
-        Processor processor = createSetProcessor("field{{_type}}", "_value {{" + randomMetaData.getFieldName() + "}}");
-        IngestDocument ingestDocument = createIngestDocument(new HashMap<>());
-        processor.execute(ingestDocument);
-        assertThat(ingestDocument.getFieldValue("field_type", String.class), Matchers.equalTo("_value " + ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class)));
-    }
-
-    private SetProcessor createSetProcessor(String fieldName, Object fieldValue) throws Exception {
-        SetProcessor.Factory factory = new SetProcessor.Factory(templateService);
-        Map<String, Object> config = new HashMap<>();
-        config.put("field", fieldName);
-        config.put("value", fieldValue);
-        return factory.create(config);
-    }
-
-    private IngestDocument createIngestDocument(Map<String, Object> source) {
-        return new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
deleted file mode 100644
index 1d1579f..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public class TemplateServiceIT extends AbstractMustacheTests {
-
-    public void testTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("fielda", "value1");
-        model.put("fieldb", Collections.singletonMap("fieldc", "value3"));
-
-        TemplateService.Template template = templateService.compile("{{fielda}}/{{fieldb}}/{{fieldb.fieldc}}");
-        assertThat(template.execute(model), equalTo("value1/{fieldc=value3}/value3"));
-    }
-
-    public void testWrongTemplateUsage() {
-        Map<String, Object> model = Collections.emptyMap();
-        TemplateService.Template template = templateService.compile("value");
-        assertThat(template.execute(model), equalTo("value"));
-
-        template = templateService.compile("value {{");
-        assertThat(template.execute(model), equalTo("value {{"));
-        template = templateService.compile("value {{abc");
-        assertThat(template.execute(model), equalTo("value {{abc"));
-        template = templateService.compile("value }}");
-        assertThat(template.execute(model), equalTo("value }}"));
-        template = templateService.compile("value }} {{");
-        assertThat(template.execute(model), equalTo("value }} {{"));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
deleted file mode 100644
index 18085b94..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.ValueSource;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ValueSourceMustacheIT extends AbstractMustacheTests {
-
-    public void testValueSourceWithTemplates() {
-        Map<String, Object> model = new HashMap<>();
-        model.put("field1", "value1");
-        model.put("field2", Collections.singletonMap("field3", "value3"));
-
-        ValueSource valueSource = ValueSource.wrap("{{field1}}/{{field2}}/{{field2.field3}}", templateService);
-        assertThat(valueSource, instanceOf(ValueSource.TemplatedValue.class));
-        assertThat(valueSource.copyAndResolve(model), equalTo("value1/{field3=value3}/value3"));
-
-        valueSource = ValueSource.wrap(Arrays.asList("_value", "{{field1}}"), templateService);
-        assertThat(valueSource, instanceOf(ValueSource.ListValue.class));
-        List<String> result = (List<String>) valueSource.copyAndResolve(model);
-        assertThat(result.size(), equalTo(2));
-        assertThat(result.get(0), equalTo("_value"));
-        assertThat(result.get(1), equalTo("value1"));
-
-        Map<String, Object> map = new HashMap<>();
-        map.put("field1", "{{field1}}");
-        map.put("field2", Collections.singletonMap("field3", "{{field2.field3}}"));
-        map.put("field4", "_value");
-        valueSource = ValueSource.wrap(map, templateService);
-        assertThat(valueSource, instanceOf(ValueSource.MapValue.class));
-        Map<String, Object> resultMap = (Map<String, Object>) valueSource.copyAndResolve(model);
-        assertThat(resultMap.size(), equalTo(3));
-        assertThat(resultMap.get("field1"), equalTo("value1"));
-        assertThat(((Map) resultMap.get("field2")).size(), equalTo(1));
-        assertThat(((Map) resultMap.get("field2")).get("field3"), equalTo("value3"));
-        assertThat(resultMap.get("field4"), equalTo("_value"));
-    }
-
-    public void testAccessSourceViaTemplate() {
-        IngestDocument ingestDocument = new IngestDocument("marvel", "type", "id", null, null, null, null, new HashMap<>());
-        assertThat(ingestDocument.hasField("marvel"), is(false));
-        ingestDocument.setFieldValue(templateService.compile("{{_index}}"), ValueSource.wrap("{{_index}}", templateService));
-        assertThat(ingestDocument.getFieldValue("marvel", String.class), equalTo("marvel"));
-        ingestDocument.removeField(templateService.compile("{{marvel}}"));
-        assertThat(ingestDocument.hasField("index"), is(false));
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
deleted file mode 100644
index 73f64d4..0000000
--- a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.smoketest;
-
-import com.carrotsearch.randomizedtesting.annotations.Name;
-import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
-import org.elasticsearch.test.rest.ESRestTestCase;
-import org.elasticsearch.test.rest.RestTestCandidate;
-import org.elasticsearch.test.rest.parser.RestTestParseException;
-
-import java.io.IOException;
-
-public class IngestWithMustacheIT extends ESRestTestCase {
-
-    public IngestWithMustacheIT(@Name("yaml") RestTestCandidate testCandidate) {
-        super(testCandidate);
-    }
-
-    @ParametersFactory
-    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
-        return ESRestTestCase.createParameters(0, 1);
-    }
-
-}
diff --git a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml b/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
deleted file mode 100644
index 9e64477..0000000
--- a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
+++ /dev/null
@@ -1,220 +0,0 @@
----
-"Test metadata templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "index_type_id",
-                  "value": "{{_index}}/{{_type}}/{{_id}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{_index}}", "{{_type}}", "{{_id}}"]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.index_type_id: "test/test/1" }
-  - match: { _source.metadata: ["test", "test", "1"] }
-
----
-"Test templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field4",
-                  "value": "{{field1}}/{{field2}}/{{field3}}"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "metadata",
-                  "value": ["{{field1}}", "{{field2}}", "{{field3}}"]
-                }
-              }
-
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_2"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "{{field1}}",
-                  "value": "value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline_3"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "field" : "{{field_to_remove}}"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_1"
-        body: {
-          metadata: "0",
-          field1: "1",
-          field2: "2",
-          field3: "3"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 5 }
-  - match: { _source.field1: "1" }
-  - match: { _source.field2: "2" }
-  - match: { _source.field3: "3" }
-  - match: { _source.field4: "1/2/3" }
-  - match: { _source.metadata: ["0","1","2","3"] }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_2"
-        body: {
-          field1: "field2"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.field1: "field2" }
-  - match: { _source.field2: "value" }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline_3"
-        body: {
-          field_to_remove: "field2",
-          field2: "2",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field_to_remove: "field2" }
-
----
-"Test on_failure metadata context templating":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_handled_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "remove" : {
-                  "field" : "field_to_remove",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "error",
-                        "value" : "processor [{{ _ingest.on_failure_processor }}]: {{ _ingest.on_failure_message }}"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_handled_pipeline"
-        body: {
-          do_nothing: "foo",
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - length: { _source: 2 }
-  - match: { _source.do_nothing: "foo" }
-  - match: { _source.error: "processor [remove]: field [field_to_remove] not present as part of path [field_to_remove]" }
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
index 227936b..3fea664 100644
--- a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.junit.After;
 import org.junit.AfterClass;
@@ -79,7 +80,7 @@ public abstract class ESSmokeClientTestCase extends LuceneTestCase {
                 .put("name", "qa_smoke_client_" + counter.getAndIncrement())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // prevents any settings to be replaced by system properties.
                 .put("client.transport.ignore_cluster_name", true)
-                .put("path.home", tempDir)
+                .put(Environment.PATH_HOME_SETTING.getKey(), tempDir)
                 .put("node.mode", "network").build(); // we require network here!
 
         TransportClient.Builder transportClientBuilder = TransportClient.builder().settings(clientSettings);
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index 54978b3..da5790d 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -263,12 +263,6 @@ fi
     install_and_check_plugin repository s3 aws-java-sdk-core-*.jar
 }
 
-@test "[$GROUP] install site example" {
-    # Doesn't use install_and_check_plugin because this is a site plugin
-    install_plugin site-example $(readlink -m site-example-*.zip)
-    assert_file_exist "$ESHOME/plugins/site-example/_site/index.html"
-}
-
 @test "[$GROUP] install store-smb plugin" {
     install_and_check_plugin store smb
 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
index 590054b..577a03f 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
@@ -40,10 +40,6 @@
         "fields": {
           "type": "list",
           "description" : "Default comma-separated list of fields to return in the response for updates"
-        },
-        "pipeline" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
index 5c13f67..1b8f714 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
@@ -65,10 +65,6 @@
           "type" : "enum",
           "options" : ["internal", "external", "external_gte", "force"],
           "description" : "Specific version type"
-        },
-        "pipeline" : {
-          "type" : "string",
-          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
deleted file mode 100644
index 1c515e4..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "ingest.delete_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "DELETE" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-        "master_timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout for connection to master node"
-        },
-        "timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout"
-        }
-      }
-    },
-    "body": null
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
deleted file mode 100644
index 6c50657..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
+++ /dev/null
@@ -1,24 +0,0 @@
-{
-  "ingest.get_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Comma separated list of pipeline ids. Wildcards supported",
-          "required" : true
-        }
-      },
-      "params": {
-        "master_timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout for connection to master node"
-        }
-      }
-    },
-    "body": null
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
deleted file mode 100644
index e4c3c2e..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
+++ /dev/null
@@ -1,31 +0,0 @@
-{
-  "ingest.put_pipeline": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "PUT" ],
-    "url": {
-      "path": "/_ingest/pipeline/{id}",
-      "paths": [ "/_ingest/pipeline/{id}" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : true
-        }
-      },
-      "params": {
-        "master_timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout for connection to master node"
-        },
-        "timeout": {
-          "type" : "time",
-          "description" : "Explicit operation timeout"
-        }
-      }
-    },
-    "body": {
-      "description" : "The ingest definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
deleted file mode 100644
index a4904ce..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "ingest.simulate": {
-    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
-    "methods": [ "GET", "POST" ],
-    "url": {
-      "path": "/_ingest/pipeline/_simulate",
-      "paths": [ "/_ingest/pipeline/_simulate", "/_ingest/pipeline/{id}/_simulate/" ],
-      "parts": {
-        "id": {
-          "type" : "string",
-          "description" : "Pipeline ID",
-          "required" : false
-        }
-      },
-      "params": {
-        "verbose": {
-          "type" : "boolean",
-          "description" : "Verbose mode. Display data output for each processor in executed pipeline",
-          "default" : false
-        }
-      }
-    },
-    "body": {
-      "description" : "The simulate definition",
-      "required" : true
-    }    
-  }
-}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.plugins/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.plugins/10_basic.yaml
index bf974c8..86f2a36 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.plugins/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.plugins/10_basic.yaml
@@ -10,7 +10,5 @@
                     name        .+   \n
                     component   .+   \n
                     version     .+   \n
-                    type        .+   \n
-                    url         .+   \n
                     description .+   \n
                $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
deleted file mode 100644
index bf0817f..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
+++ /dev/null
@@ -1,94 +0,0 @@
----
-"Test basic pipeline crud":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { pipelines.0.id: "my_pipeline" }
-  - match: { pipelines.0.config.description: "_description" }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { acknowledged: true }
-
-  - do:
-      catch: missing
-      ingest.get_pipeline:
-        id: "my_pipeline"
-
----
-"Test invalid config":
-  - do:
-      catch: param
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                }
-              }
-            ]
-          }
-
----
-"Test basic pipeline with on_failure in processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "_value",
-                  "on_failure": [
-                    {
-                      "set" : {
-                        "field" : "field2",
-                        "value" : "_failed_value"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.get_pipeline:
-        id: "my_pipeline"
-  - match: { pipelines.0.id: "my_pipeline" }
-  - match: { pipelines.0.config.description: "_description" }
-
-  - do:
-      ingest.delete_pipeline:
-        id: "my_pipeline"
-  - match: { acknowledged: true }
-
-  - do:
-      catch: missing
-      ingest.get_pipeline:
-        id: "my_pipeline"
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
deleted file mode 100644
index 71c5c40..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
+++ /dev/null
@@ -1,37 +0,0 @@
----
-"Test date processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "date" : {
-                  "match_field" : "date_source_field",
-                  "target_field" : "date_target_field",
-                  "match_formats" : ["dd/MM/yyyy"],
-                  "timezone" : "Europe/Amsterdam"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {date_source_field: "12/06/2010"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.date_source_field: "12/06/2010" }
-  - match: { _source.date_target_field: "2010-06-12T00:00:00.000+02:00" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
deleted file mode 100644
index 1e7911e..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
+++ /dev/null
@@ -1,150 +0,0 @@
----
-"Test mutate processors":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "new_field",
-                  "value": "new_value"
-                }
-              },
-              {
-                "append" : {
-                  "field" : "new_field",
-                  "value": ["item2", "item3", "item4"]
-                }
-              },
-              {
-                "rename" : {
-                  "field" : "field_to_rename",
-                  "to": "renamed_field"
-                }
-              },
-              {
-                "remove" : {
-                  "field" : "field_to_remove"
-                }
-              },
-              {
-                "lowercase" : {
-                  "field" : "field_to_lowercase"
-                }
-              },
-              {
-                "uppercase" : {
-                  "field" : "field_to_uppercase"
-                }
-              },
-              {
-                "trim" : {
-                  "field" : "field_to_trim"
-                }
-              },
-              {
-                "split" : {
-                  "field" : "field_to_split",
-                  "separator": "-"
-                }
-              },
-              {
-                "join" : {
-                  "field" : "field_to_join",
-                  "separator": "-"
-                }
-              },
-              {
-                "convert" : {
-                  "field" : "field_to_convert",
-                  "type": "integer"
-                }
-              },
-              {
-                "gsub" : {
-                  "field": "field_to_gsub",
-                  "pattern" : "-",
-                  "replacement" : "."
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {
-          field_to_rename: "value",
-          field_to_remove: "old_value",
-          field_to_lowercase: "LOWERCASE",
-          field_to_uppercase: "uppercase",
-          field_to_trim: "   trimmed   ",
-          field_to_split: "127-0-0-1",
-          field_to_join: ["127","0","0","1"],
-          field_to_convert: ["127","0","0","1"],
-          field_to_gsub: "127-0-0-1"
-        }
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - is_false: _source.field_to_rename
-  - is_false: _source.field_to_remove
-  - match: { _source.new_field: ["new_value", "item2", "item3", "item4"] }
-  - match: { _source.renamed_field: "value" }
-  - match: { _source.field_to_lowercase: "lowercase" }
-  - match: { _source.field_to_uppercase: "UPPERCASE" }
-  - match: { _source.field_to_trim: "trimmed" }
-  - match: { _source.field_to_split: ["127","0","0","1"] }
-  - match: { _source.field_to_join: "127-0-0-1" }
-  - match: { _source.field_to_convert: [127,0,0,1] }
-  - match: { _source.field_to_gsub: "127.0.0.1" }
-
----
-"Test metadata":
-  - do:
-      cluster.health:
-          wait_for_status: green
-
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "_index",
-                  "value" : "surprise"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field: "value"}
-
-  - do:
-      get:
-        index: surprise
-        type: test
-        id: 1
-  - length: { _source: 1 }
-  - match: { _source.field: "value" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
deleted file mode 100644
index 3153ba8..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
+++ /dev/null
@@ -1,421 +0,0 @@
----
-"Test simulate with stored ingest pipeline":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value" : "_value"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      ingest.simulate:
-        id: "my_pipeline"
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test simulate with provided pipeline definition":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-
----
-"Test simulate with provided invalid pipeline definition":
-  - do:
-      catch: request
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { error: 3 }
-  - match: { status: 400 }
-  - match: { error.type: "illegal_argument_exception" }
-  - match: { error.reason: "required property [field] is missing" }
-
----
-"Test simulate without index type and id":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-
----
-"Test simulate with provided pipeline definition with on_failure block":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "rename" : {
-                    "field" : "does_not_exist",
-                    "to" : "field2",
-                    "on_failure" : [
-                      {
-                        "set" : {
-                          "field" : "field2",
-                          "value" : "_value"
-                        }
-                      }
-                    ]
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - match: { docs.0.doc._source.foo: "bar" }
-  - match: { docs.0.doc._source.field2: "_value" }
-  - length: { docs.0.doc._ingest: 1 }
-  - is_true: docs.0.doc._ingest.timestamp
-
----
-"Test simulate with no provided pipeline or pipeline_id":
-  - do:
-      catch: request
-      ingest.simulate:
-        body: >
-          {
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { error: 3 }
-  - match: { status: 400 }
-  - match: { error.type: "illegal_argument_exception" }
-  - match: { error.reason: "required property [pipeline] is missing" }
-
----
-"Test simulate with verbose flag":
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "tag" : "processor[set]-0",
-                    "field" : "field2",
-                    "value" : "_value"
-                  }
-                },
-                {
-                  "set" : {
-                    "field" : "field3",
-                    "value" : "third_val"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - length: { docs.0.processor_results: 2 }
-  - match: { docs.0.processor_results.0.tag: "processor[set]-0" }
-  - length: { docs.0.processor_results.0.doc._source: 2 }
-  - match: { docs.0.processor_results.0.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.0.doc._source.field2: "_value" }
-  - length: { docs.0.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.0.doc._ingest.timestamp
-  - length: { docs.0.processor_results.1.doc._source: 3 }
-  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
-  - match: { docs.0.processor_results.1.doc._source.field2: "_value" }
-  - match: { docs.0.processor_results.1.doc._source.field3: "third_val" }
-  - length: { docs.0.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
-
----
-"Test simulate with exception thrown":
-  - do:
-      ingest.simulate:
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "uppercase" : {
-                    "field" : "foo"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "not_foo": "bar"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "bar"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - match: { docs.0.error.type: "illegal_argument_exception" }
-  - match: { docs.1.doc._source.foo: "BAR" }
-  - length: { docs.1.doc._ingest: 1 }
-  - is_true: docs.1.doc._ingest.timestamp
-
----
-"Test verbose simulate with exception thrown":
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline": {
-              "description": "_description",
-              "processors": [
-                {
-                  "convert" : {
-                    "field" : "foo",
-                    "type" : "integer"
-                  }
-                },
-                {
-                  "uppercase" : {
-                    "field" : "bar"
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "foo": "bar",
-                  "bar": "hello"
-                }
-              },
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id2",
-                "_source": {
-                  "foo": "5",
-                  "bar": "hello"
-                }
-              }
-            ]
-          }
-  - length: { docs: 2 }
-  - length: { docs.0.processor_results: 1 }
-  - match: { docs.0.processor_results.0.error.type: "illegal_argument_exception" }
-  - length: { docs.1.processor_results: 2 }
-  - match: { docs.1.processor_results.0.doc._index: "index" }
-  - match: { docs.1.processor_results.0.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.0.doc._source.bar: "hello" }
-  - length: { docs.1.processor_results.0.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.0.doc._ingest.timestamp
-  - match: { docs.1.processor_results.1.doc._source.foo: 5 }
-  - match: { docs.1.processor_results.1.doc._source.bar: "HELLO" }
-  - length: { docs.1.processor_results.1.doc._ingest: 1 }
-  - is_true: docs.1.processor_results.1.doc._ingest.timestamp
-
----
-"Test verbose simulate with on_failure":
-  - do:
-      ingest.simulate:
-        verbose: true
-        body: >
-          {
-            "pipeline" : {
-              "description": "_description",
-              "processors": [
-                {
-                  "set" : {
-                    "tag" : "setstatus-1",
-                    "field" : "status",
-                    "value" : 200
-                  }
-                },
-                {
-                  "rename" : {
-                    "tag" : "rename-1",
-                    "field" : "foofield",
-                    "to" : "field1",
-                    "on_failure" : [
-                      {
-                        "set" : {
-                          "tag" : "set on_failure rename",
-                          "field" : "foofield",
-                          "value" : "exists"
-                        }
-                      },
-                      {
-                        "rename" : {
-                          "field" : "foofield2",
-                          "to" : "field1",
-                          "on_failure" : [
-                            {
-                              "set" : {
-                                "field" : "foofield2",
-                                "value" : "ran"
-                              }
-                            }
-                          ]
-                        }
-                      }
-                    ]
-                  }
-                }
-              ]
-            },
-            "docs": [
-              {
-                "_index": "index",
-                "_type": "type",
-                "_id": "id",
-                "_source": {
-                  "field1": "123.42 400 <foo>"
-                }
-              }
-            ]
-          }
-  - length: { docs: 1 }
-  - length: { docs.0.processor_results: 5 }
-  - match: { docs.0.processor_results.0.tag: "setstatus-1" }
-  - match: { docs.0.processor_results.0.doc._source.field1: "123.42 400 <foo>" }
-  - match: { docs.0.processor_results.0.doc._source.status: 200 }
-  - match: { docs.0.processor_results.1.tag: "rename-1" }
-  - match: { docs.0.processor_results.1.error.type: "illegal_argument_exception" }
-  - match: { docs.0.processor_results.1.error.reason: "field [foofield] doesn't exist" }
-  - match: { docs.0.processor_results.2.tag: "set on_failure rename" }
-  - is_false: docs.0.processor_results.3.tag
-  - is_false: docs.0.processor_results.4.tag
-  - match: { docs.0.processor_results.4.doc._source.foofield: "exists" }
-  - match: { docs.0.processor_results.4.doc._source.foofield2: "ran" }
-  - match: { docs.0.processor_results.4.doc._source.field1: "123.42 400 <foo>" }
-  - match: { docs.0.processor_results.4.doc._source.status: 200 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
deleted file mode 100644
index 7bce12d..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
+++ /dev/null
@@ -1,108 +0,0 @@
----
-"Test Pipeline With On Failure Block":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "_executed",
-                  "value" : true
-                }
-              },
-              {
-                "date" : {
-                  "match_field" : "date",
-                  "target_field" : "date",
-                  "match_formats" : ["yyyy"]
-                }
-              }
-            ],
-            "on_failure" : [
-              {
-                "set" : {
-                  "field" : "_failed",
-                  "value" : true
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "value1"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "value1" }
-  - match: { _source._executed: true }
-  - match: { _source._failed: true }
-
----
-"Test Pipeline With Nested Processor On Failures":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "rename" : {
-                  "field" : "foofield",
-                  "to" : "field1",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "foofield",
-                        "value" : "exists"
-                      }
-                    },
-                    {
-                      "rename" : {
-                        "field" : "foofield2",
-                        "to" : "field1",
-                        "on_failure" : [
-                          {
-                            "set" : {
-                            "field" : "foofield2",
-                            "value" : "ran"
-                            }
-                          }
-                        ]
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {field1: "value1"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.field1: "value1" }
-  - match: { _source.foofield: "exists" }
-  - match: { _source.foofield2: "ran" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
deleted file mode 100644
index 019c229..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
+++ /dev/null
@@ -1,68 +0,0 @@
----
-"Test Fail Processor":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "fail" : {
-                  "message" : "error_message"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      catch: request
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {}
-
----
-"Test fail with on_failure":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "fail" : {
-                  "message" : "error",
-                  "on_failure" : [
-                    {
-                      "set" : {
-                        "field" : "error_message",
-                        "value" : "fail_processor_ran"
-                      }
-                    }
-                  ]
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.error_message: "fail_processor_ran" }
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
deleted file mode 100644
index b70f05a..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
+++ /dev/null
@@ -1,105 +0,0 @@
-setup:
-  - do:
-      ingest.put_pipeline:
-        id: "pipeline1"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field1",
-                  "value": "value1"
-                }
-              }
-            ]
-          }
-
-  - do:
-      ingest.put_pipeline:
-        id: "pipeline2"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "set" : {
-                  "field" : "field2",
-                  "value": "value2"
-                }
-              }
-            ]
-          }
-
----
-"Test bulk request without default pipeline":
-
-  - do:
-      bulk:
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id1
-              pipeline: pipeline1
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-          - f1: v2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id1
-
-  - match: {_source.field1: value1}
-  - is_false: _source.field2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id2
-
-  - is_false: _source.field1
-  - is_false: _source.field2
-
----
-"Test bulk request with default pipeline":
-
-  - do:
-      bulk:
-        pipeline: pipeline1
-        body:
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id1
-          - f1: v1
-          - index:
-              _index: test_index
-              _type:  test_type
-              _id:    test_id2
-              pipeline: pipeline2
-          - f1: v2
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id1
-
-  - match: {_source.field1: value1}
-  - is_false: _source.field2
-
-  - do:
-      get:
-        index: test_index
-        type: test_type
-        id: test_id2
-
-  - is_false: _source.field1
-  - match: {_source.field2: value2}
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
deleted file mode 100644
index bdc6457..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
+++ /dev/null
@@ -1,64 +0,0 @@
----
-"Test De-Dot Processor With Provided Separator":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "dedot" : {
-                  "separator" : "3"
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {"a.b.c": "hello world"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.a3b3c: "hello world" }
-
----
-"Test De-Dot Processor With Default Separator":
-  - do:
-      ingest.put_pipeline:
-        id: "my_pipeline"
-        body:  >
-          {
-            "description": "_description",
-            "processors": [
-              {
-                "dedot" : {
-                }
-              }
-            ]
-          }
-  - match: { acknowledged: true }
-
-  - do:
-      index:
-        index: test
-        type: test
-        id: 1
-        pipeline: "my_pipeline"
-        body: {"a.b.c": "hello world"}
-
-  - do:
-      get:
-        index: test
-        type: test
-        id: 1
-  - match: { _source.a_b_c: "hello world" }
diff --git a/settings.gradle b/settings.gradle
index 63e78de..682bfec 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -11,7 +11,6 @@ List projects = [
   'test:framework',
   'test:fixtures:example-fixture',
   'test:fixtures:hdfs-fixture',
-  'modules:ingest-grok',
   'modules:lang-expression',
   'modules:lang-groovy',
   'modules:lang-mustache',
@@ -25,7 +24,6 @@ List projects = [
   'plugins:discovery-ec2',
   'plugins:discovery-gce',
   'plugins:discovery-multicast',
-  'plugins:ingest-geoip',
   'plugins:lang-javascript',
   'plugins:lang-plan-a',
   'plugins:lang-python',
@@ -36,14 +34,11 @@ List projects = [
   'plugins:repository-hdfs',
   'plugins:repository-s3',
   'plugins:jvm-example',
-  'plugins:site-example',
   'plugins:store-smb',
   'qa:evil-tests',
   'qa:smoke-test-client',
   'qa:smoke-test-multinode',
   'qa:smoke-test-plugins',
-  'qa:ingest-with-mustache',
-  'qa:ingest-disabled',
   'qa:vagrant',
 ]
 
diff --git a/test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java b/test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
index b2ce5eb..60cde5a 100644
--- a/test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
+++ b/test/framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
@@ -154,11 +154,9 @@ public class BootstrapForTesting {
                     try (InputStream stream = url.openStream()) {
                         properties.load(stream);
                     }
-                    if (Boolean.parseBoolean(properties.getProperty("jvm"))) {
-                        String clazz = properties.getProperty("classname");
-                        if (clazz != null) {
-                            Class.forName(clazz);
-                        }
+                    String clazz = properties.getProperty("classname");
+                    if (clazz != null) {
+                        Class.forName(clazz);
                     }
                 }
             } catch (Exception e) {
diff --git a/test/framework/src/main/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java b/test/framework/src/main/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java
index 99cd417..80b2c32 100644
--- a/test/framework/src/main/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java
+++ b/test/framework/src/main/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.recycler.Recycler.V;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.lang.reflect.Array;
@@ -63,8 +62,10 @@ public class MockPageCacheRecycler extends PageCacheRecycler {
     @Inject
     public MockPageCacheRecycler(Settings settings, ThreadPool threadPool) {
         super(settings, threadPool);
-        final long seed = settings.getAsLong(InternalTestCluster.SETTING_CLUSTER_NODE_SEED, 0L);
-        random = new Random(seed);
+        // we always initialize with 0 here since we really only wanna have some random bytes / ints / longs
+        // and given the fact that it's called concurrently it won't reproduces anyway the same order other than in a unittest
+        // for the latter 0 is just fine
+        random = new Random(0);
     }
 
     private <T> V<T> wrap(final V<T> v) {
diff --git a/test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java b/test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java
index 1c110bc..a9b45a5 100644
--- a/test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java
+++ b/test/framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java
@@ -45,7 +45,7 @@ public class MapperTestUtils {
 
     public static MapperService newMapperService(Path tempDir, Settings settings, IndicesModule indicesModule) throws IOException {
         Settings.Builder settingsBuilder = Settings.builder()
-            .put("path.home", tempDir)
+            .put(Environment.PATH_HOME_SETTING.getKey(), tempDir)
             .put(settings);
         if (settings.get(IndexMetaData.SETTING_VERSION_CREATED) == null) {
             settingsBuilder.put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT);
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java b/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
deleted file mode 100644
index 3f350cf..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.ingest.core.IngestDocument;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.TreeMap;
-
-public final class RandomDocumentPicks {
-
-    private RandomDocumentPicks() {
-
-    }
-
-    /**
-     * Returns a random field name. Can be a leaf field name or the
-     * path to refer to a field name using the dot notation.
-     */
-    public static String randomFieldName(Random random) {
-        int numLevels = RandomInts.randomIntBetween(random, 1, 5);
-        String fieldName = "";
-        for (int i = 0; i < numLevels; i++) {
-            if (i > 0) {
-                fieldName += ".";
-            }
-            fieldName += randomString(random);
-        }
-        return fieldName;
-    }
-
-    /**
-     * Returns a random leaf field name.
-     */
-    public static String randomLeafFieldName(Random random) {
-        String fieldName;
-        do {
-            fieldName = randomString(random);
-        } while (fieldName.contains("."));
-        return fieldName;
-    }
-
-    /**
-     * Returns a randomly selected existing field name out of the fields that are contained
-     * in the document provided as an argument.
-     */
-    public static String randomExistingFieldName(Random random, IngestDocument ingestDocument) {
-        Map<String, Object> source = new TreeMap<>(ingestDocument.getSourceAndMetadata());
-        Map.Entry<String, Object> randomEntry = RandomPicks.randomFrom(random, source.entrySet());
-        String key = randomEntry.getKey();
-        while (randomEntry.getValue() instanceof Map) {
-            @SuppressWarnings("unchecked")
-            Map<String, Object> map = (Map<String, Object>) randomEntry.getValue();
-            Map<String, Object> treeMap = new TreeMap<>(map);
-            randomEntry = RandomPicks.randomFrom(random, treeMap.entrySet());
-            key += "." + randomEntry.getKey();
-        }
-        assert ingestDocument.getFieldValue(key, Object.class) != null;
-        return key;
-    }
-
-    /**
-     * Adds a random non existing field to the provided document and associates it
-     * with the provided value. The field will be added at a random position within the document,
-     * not necessarily at the top level using a leaf field name.
-     */
-    public static String addRandomField(Random random, IngestDocument ingestDocument, Object value) {
-        String fieldName;
-        do {
-            fieldName = randomFieldName(random);
-        } while (canAddField(fieldName, ingestDocument) == false);
-        ingestDocument.setFieldValue(fieldName, value);
-        return fieldName;
-    }
-
-    /**
-     * Checks whether the provided field name can be safely added to the provided document.
-     * When the provided field name holds the path using the dot notation, we have to make sure
-     * that each node of the tree either doesn't exist or is a map, otherwise new fields cannot be added.
-     */
-    public static boolean canAddField(String path, IngestDocument ingestDocument) {
-        String[] pathElements = Strings.splitStringToArray(path, '.');
-        Map<String, Object> innerMap = ingestDocument.getSourceAndMetadata();
-        if (pathElements.length > 1) {
-            for (int i = 0; i < pathElements.length - 1; i++) {
-                Object currentLevel = innerMap.get(pathElements[i]);
-                if (currentLevel == null) {
-                    return true;
-                }
-                if (currentLevel instanceof Map == false) {
-                    return false;
-                }
-                @SuppressWarnings("unchecked")
-                Map<String, Object> map = (Map<String, Object>) currentLevel;
-                innerMap = map;
-            }
-        }
-        String leafKey = pathElements[pathElements.length - 1];
-        return innerMap.containsKey(leafKey) == false;
-    }
-
-    /**
-     * Generates a random document and random metadata
-     */
-    public static IngestDocument randomIngestDocument(Random random) {
-        return randomIngestDocument(random, randomSource(random));
-    }
-
-    /**
-     * Generates a document that holds random metadata and the document provided as a map argument
-     */
-    public static IngestDocument randomIngestDocument(Random random, Map<String, Object> source) {
-        String index = randomString(random);
-        String type = randomString(random);
-        String id = randomString(random);
-        String routing = null;
-        if (random.nextBoolean()) {
-            routing = randomString(random);
-        }
-        String parent = null;
-        if (random.nextBoolean()) {
-            parent = randomString(random);
-        }
-        String timestamp = null;
-        if (random.nextBoolean()) {
-            timestamp = randomString(random);
-        }
-        String ttl = null;
-        if (random.nextBoolean()) {
-            ttl = randomString(random);
-        }
-        return new IngestDocument(index, type, id, routing, parent, timestamp, ttl, source);
-    }
-
-    public static Map<String, Object> randomSource(Random random) {
-        Map<String, Object> document = new HashMap<>();
-        addRandomFields(random, document, 0);
-        return document;
-    }
-
-    /**
-     * Generates a random field value, can be a string, a number, a list of an object itself.
-     */
-    public static Object randomFieldValue(Random random) {
-        return randomFieldValue(random, 0);
-    }
-
-    private static Object randomFieldValue(Random random, int currentDepth) {
-        switch(RandomInts.randomIntBetween(random, 0, 8)) {
-            case 0:
-                return randomString(random);
-            case 1:
-                return random.nextInt();
-            case 2:
-                return random.nextBoolean();
-            case 3:
-                return random.nextDouble();
-            case 4:
-                List<String> stringList = new ArrayList<>();
-                int numStringItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numStringItems; j++) {
-                    stringList.add(randomString(random));
-                }
-                return stringList;
-            case 5:
-                List<Integer> intList = new ArrayList<>();
-                int numIntItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numIntItems; j++) {
-                    intList.add(random.nextInt());
-                }
-                return intList;
-            case 6:
-                List<Boolean> booleanList = new ArrayList<>();
-                int numBooleanItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numBooleanItems; j++) {
-                    booleanList.add(random.nextBoolean());
-                }
-                return booleanList;
-            case 7:
-                List<Double> doubleList = new ArrayList<>();
-                int numDoubleItems = RandomInts.randomIntBetween(random, 1, 10);
-                for (int j = 0; j < numDoubleItems; j++) {
-                    doubleList.add(random.nextDouble());
-                }
-                return doubleList;
-            case 8:
-                Map<String, Object> newNode = new HashMap<>();
-                addRandomFields(random, newNode, ++currentDepth);
-                return newNode;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    public static String randomString(Random random) {
-        if (random.nextBoolean()) {
-            return RandomStrings.randomAsciiOfLengthBetween(random, 1, 10);
-        }
-        return RandomStrings.randomUnicodeOfCodepointLengthBetween(random, 1, 10);
-    }
-
-    private static void addRandomFields(Random random, Map<String, Object> parentNode, int currentDepth) {
-        if (currentDepth > 5) {
-            return;
-        }
-        int numFields = RandomInts.randomIntBetween(random, 1, 10);
-        for (int i = 0; i < numFields; i++) {
-            String fieldName = randomLeafFieldName(random);
-            Object fieldValue = randomFieldValue(random, currentDepth);
-            parentNode.put(fieldName, fieldValue);
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
deleted file mode 100644
index ae13174..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.AbstractProcessorFactory;
-import org.elasticsearch.ingest.core.IngestDocument;
-import org.elasticsearch.ingest.core.Processor;
-
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.function.Consumer;
-
-/**
- * Processor used for testing, keeps track of how many times it is invoked and
- * accepts a {@link Consumer} of {@link IngestDocument} to be called when executed.
- */
-public class TestProcessor implements Processor {
-
-    private final String type;
-    private final String tag;
-    private final Consumer<IngestDocument> ingestDocumentConsumer;
-    private final AtomicInteger invokedCounter = new AtomicInteger();
-
-    public TestProcessor(Consumer<IngestDocument> ingestDocumentConsumer) {
-        this(null, "test-processor", ingestDocumentConsumer);
-    }
-
-    public TestProcessor(String tag, String type, Consumer<IngestDocument> ingestDocumentConsumer) {
-        this.ingestDocumentConsumer = ingestDocumentConsumer;
-        this.type = type;
-        this.tag = tag;
-    }
-
-    @Override
-    public void execute(IngestDocument ingestDocument) throws Exception {
-        invokedCounter.incrementAndGet();
-        ingestDocumentConsumer.accept(ingestDocument);
-    }
-
-    @Override
-    public String getType() {
-        return type;
-    }
-
-    @Override
-    public String getTag() {
-        return tag;
-    }
-
-    public int getInvokedCounter() {
-        return invokedCounter.get();
-    }
-
-    public static final class Factory extends AbstractProcessorFactory<TestProcessor> {
-        @Override
-        public TestProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
-            return new TestProcessor(processorTag, "test-processor", ingestDocument -> {});
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
deleted file mode 100644
index 9330db1..0000000
--- a/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.ingest;
-
-import org.elasticsearch.ingest.core.TemplateService;
-
-import java.util.Map;
-
-public class TestTemplateService implements TemplateService {
-
-    public static TemplateService instance() {
-        return new TestTemplateService();
-    }
-
-    private TestTemplateService() {
-    }
-
-    @Override
-    public Template compile(String template) {
-        return new MockTemplate(template);
-    }
-
-    public static class MockTemplate implements TemplateService.Template {
-
-        private final String expected;
-
-        public MockTemplate(String expected) {
-            this.expected = expected;
-        }
-
-        @Override
-        public String execute(Map<String, Object> model) {
-            return expected;
-        }
-
-        @Override
-        public String getKey() {
-            return expected;
-        }
-    }
-}
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
index ff09ba0..ca11fa2 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -1685,7 +1685,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
                 .put("script.indexed", "on")
                 .put("script.inline", "on")
                         // wait short time for other active shards before actually deleting, default 30s not needed in tests
-                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT, new TimeValue(1, TimeUnit.SECONDS));
+                .put(IndicesStore.INDICES_STORE_DELETE_SHARD_TIMEOUT.getKey(), new TimeValue(1, TimeUnit.SECONDS));
         return builder.build();
     }
 
@@ -1759,7 +1759,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
         NodeConfigurationSource nodeConfigurationSource = new NodeConfigurationSource() {
             @Override
             public Settings nodeSettings(int nodeOrdinal) {
-                return Settings.builder().put(Node.HTTP_ENABLED, false).
+                return Settings.builder().put(NetworkModule.HTTP_ENABLED.getKey(), false).
                         put(ESIntegTestCase.this.nodeSettings(nodeOrdinal)).build();
             }
 
@@ -2091,11 +2091,11 @@ public abstract class ESIntegTestCase extends ESTestCase {
         assertTrue(Files.exists(dest));
         Settings.Builder builder = Settings.builder()
                 .put(settings)
-                .put("path.data", dataDir.toAbsolutePath());
+                .put(Environment.PATH_DATA_SETTING.getKey(), dataDir.toAbsolutePath());
 
         Path configDir = indexDir.resolve("config");
         if (Files.exists(configDir)) {
-            builder.put("path.conf", configDir.toAbsolutePath());
+            builder.put(Environment.PATH_CONF_SETTING.getKey(), configDir.toAbsolutePath());
         }
         return builder.build();
     }
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
index 9b06bae..f73839c 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
@@ -36,6 +36,7 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.node.MockNode;
@@ -158,10 +159,10 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
     private Node newNode() {
         Settings settings = Settings.builder()
             .put(ClusterName.SETTING, InternalTestCluster.clusterName("single-node-cluster", randomLong()))
-            .put("path.home", createTempDir())
+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir())
             // TODO: use a consistent data path for custom paths
             // This needs to tie into the ESIntegTestCase#indexSettings() method
-            .put("path.shared_data", createTempDir().getParent())
+            .put(Environment.PATH_SHARED_DATA_SETTING.getKey(), createTempDir().getParent())
             .put("node.name", nodeName())
             .put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1)
             .put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java b/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
index 3777653..598b621 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ESTestCase.java
@@ -531,8 +531,8 @@ public abstract class ESTestCase extends LuceneTestCase {
     public NodeEnvironment newNodeEnvironment(Settings settings) throws IOException {
         Settings build = Settings.builder()
                 .put(settings)
-                .put("path.home", createTempDir().toAbsolutePath())
-                .putArray("path.data", tmpPaths()).build();
+                .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toAbsolutePath())
+                .putArray(Environment.PATH_DATA_SETTING.getKey(), tmpPaths()).build();
         return new NodeEnvironment(build, new Environment(build));
     }
 
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java b/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
index 05f194f..5d169fc 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ExternalNode.java
@@ -53,7 +53,7 @@ final class ExternalNode implements Closeable {
 
     public static final Settings REQUIRED_SETTINGS = Settings.builder()
             .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
-            .put(DiscoveryModule.DISCOVERY_TYPE_KEY, "zen")
+            .put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "zen")
             .put("node.mode", "network").build(); // we need network mode for this
 
     private final Path path;
@@ -112,7 +112,7 @@ final class ExternalNode implements Closeable {
                 case "node.mode":
                 case "node.local":
                 case NetworkModule.TRANSPORT_TYPE_KEY:
-                case DiscoveryModule.DISCOVERY_TYPE_KEY:
+                case "discovery.type":
                 case NetworkModule.TRANSPORT_SERVICE_TYPE_KEY:
                 case InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING:
                     continue;
diff --git a/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java b/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
index 34b6bfb..0b3facc 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/ExternalTestCluster.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.plugins.Plugin;
 
@@ -74,7 +75,7 @@ public final class ExternalTestCluster extends TestCluster {
                 .put("name", InternalTestCluster.TRANSPORT_CLIENT_PREFIX + EXTERNAL_CLUSTER_PREFIX + counter.getAndIncrement())
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // prevents any settings to be replaced by system properties.
                 .put("client.transport.ignore_cluster_name", true)
-                .put("path.home", tempDir)
+                .put(Environment.PATH_HOME_SETTING.getKey(), tempDir)
                 .put("node.mode", "network").build(); // we require network here!
 
         TransportClient.Builder transportClientBuilder = TransportClient.builder().settings(clientSettings);
diff --git a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
index 01988f6..9fb3eec 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
@@ -62,6 +62,7 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.index.IndexModule;
@@ -146,11 +147,6 @@ public final class InternalTestCluster extends TestCluster {
     private final ESLogger logger = Loggers.getLogger(getClass());
 
     /**
-     * A node level setting that holds a per node random seed that is consistent across node restarts
-     */
-    public static final String SETTING_CLUSTER_NODE_SEED = "test.cluster.node.seed";
-
-    /**
      * The number of ports in the range used for this JVM
      */
     public static final int PORTS_PER_JVM = 100;
@@ -286,12 +282,12 @@ public final class InternalTestCluster extends TestCluster {
                 for (int i = 0; i < numOfDataPaths; i++) {
                     dataPath.append(baseDir.resolve("d" + i).toAbsolutePath()).append(',');
                 }
-                builder.put("path.data", dataPath.toString());
+                builder.put(Environment.PATH_DATA_SETTING.getKey(), dataPath.toString());
             }
         }
-        builder.put("path.shared_data", baseDir.resolve("custom"));
-        builder.put("path.home", baseDir);
-        builder.put("path.repo", baseDir.resolve("repos"));
+        builder.put(Environment.PATH_SHARED_DATA_SETTING.getKey(), baseDir.resolve("custom"));
+        builder.put(Environment.PATH_HOME_SETTING.getKey(), baseDir);
+        builder.put(Environment.PATH_REPO_SETTING.getKey(), baseDir.resolve("repos"));
         builder.put("transport.tcp.port", TRANSPORT_BASE_PORT + "-" + (TRANSPORT_BASE_PORT + PORTS_PER_CLUSTER));
         builder.put("http.port", HTTP_BASE_PORT + "-" + (HTTP_BASE_PORT + PORTS_PER_CLUSTER));
         builder.put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true);
@@ -380,8 +376,7 @@ public final class InternalTestCluster extends TestCluster {
 
     private Settings getRandomNodeSettings(long seed) {
         Random random = new Random(seed);
-        Builder builder = Settings.settingsBuilder()
-                .put(SETTING_CLUSTER_NODE_SEED, seed);
+        Builder builder = Settings.settingsBuilder();
         if (isLocalTransportConfigured() == false) {
             builder.put(Transport.TRANSPORT_TCP_COMPRESS.getKey(), rarely(random));
         }
@@ -389,12 +384,12 @@ public final class InternalTestCluster extends TestCluster {
             builder.put("cache.recycler.page.type", RandomPicks.randomFrom(random, PageCacheRecycler.Type.values()));
         }
         if (random.nextInt(10) == 0) { // 10% of the nodes have a very frequent check interval
-            builder.put(SearchService.KEEPALIVE_INTERVAL_KEY, TimeValue.timeValueMillis(10 + random.nextInt(2000)));
+            builder.put(SearchService.KEEPALIVE_INTERVAL_SETTING.getKey(), TimeValue.timeValueMillis(10 + random.nextInt(2000)));
         } else if (random.nextInt(10) != 0) { // 90% of the time - 10% of the time we don't set anything
-            builder.put(SearchService.KEEPALIVE_INTERVAL_KEY, TimeValue.timeValueSeconds(10 + random.nextInt(5 * 60)));
+            builder.put(SearchService.KEEPALIVE_INTERVAL_SETTING.getKey(), TimeValue.timeValueSeconds(10 + random.nextInt(5 * 60)));
         }
         if (random.nextBoolean()) { // sometimes set a
-            builder.put(SearchService.DEFAULT_KEEPALIVE_KEY, TimeValue.timeValueSeconds(100 + random.nextInt(5 * 60)));
+            builder.put(SearchService.DEFAULT_KEEPALIVE_SETTING.getKey(), TimeValue.timeValueSeconds(100 + random.nextInt(5 * 60)));
         }
 
         if (random.nextInt(10) == 0) {
@@ -457,7 +452,7 @@ public final class InternalTestCluster extends TestCluster {
         }
 
         if (random.nextBoolean()) {
-            builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING, RandomInts.randomIntBetween(random, -100, 2000));
+            builder.put(ScriptService.SCRIPT_CACHE_SIZE_SETTING.getKey(), RandomInts.randomIntBetween(random, 0, 2000));
         }
         if (random.nextBoolean()) {
             builder.put(ScriptService.SCRIPT_CACHE_EXPIRE_SETTING, TimeValue.timeValueMillis(RandomInts.randomIntBetween(random, 750, 10000000)));
@@ -594,10 +589,10 @@ public final class InternalTestCluster extends TestCluster {
         String name = buildNodeName(nodeId);
         assert !nodes.containsKey(name);
         Settings finalSettings = settingsBuilder()
-                .put("path.home", baseDir) // allow overriding path.home
+                .put(Environment.PATH_HOME_SETTING.getKey(), baseDir) // allow overriding path.home
                 .put(settings)
                 .put("name", name)
-                .put(DiscoveryService.SETTING_DISCOVERY_SEED, seed)
+                .put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), seed)
                 .build();
         MockNode node = new MockNode(finalSettings, version, plugins);
         return new NodeAndClient(name, node);
@@ -678,7 +673,7 @@ public final class InternalTestCluster extends TestCluster {
         Builder builder = settingsBuilder().put(settings).put("node.client", true);
         if (size() == 0) {
             // if we are the first node - don't wait for a state
-            builder.put("discovery.initial_state_timeout", 0);
+            builder.put(DiscoveryService.INITIAL_STATE_TIMEOUT_SETTING.getKey(), 0);
         }
         String name = startNode(builder);
         return nodes.get(name).nodeClient();
@@ -844,8 +839,8 @@ public final class InternalTestCluster extends TestCluster {
                     IOUtils.rm(nodeEnv.nodeDataPaths());
                 }
             }
-            final long newIdSeed = node.settings().getAsLong(DiscoveryService.SETTING_DISCOVERY_SEED, 0l) + 1; // use a new seed to make sure we have new node id
-            Settings finalSettings = Settings.builder().put(node.settings()).put(newSettings).put(DiscoveryService.SETTING_DISCOVERY_SEED, newIdSeed).build();
+            final long newIdSeed = DiscoveryService.DISCOVERY_SEED_SETTING.get(node.settings()) + 1; // use a new seed to make sure we have new node id
+            Settings finalSettings = Settings.builder().put(node.settings()).put(newSettings).put(DiscoveryService.DISCOVERY_SEED_SETTING.getKey(), newIdSeed).build();
             Collection<Class<? extends Plugin>> plugins = node.getPlugins();
             Version version = node.getVersion();
             node = new MockNode(finalSettings, version, plugins);
@@ -890,7 +885,7 @@ public final class InternalTestCluster extends TestCluster {
             Settings nodeSettings = node.settings();
             Builder builder = settingsBuilder()
                     .put("client.transport.nodes_sampler_interval", "1s")
-                    .put("path.home", baseDir)
+                    .put(Environment.PATH_HOME_SETTING.getKey(), baseDir)
                     .put("name", TRANSPORT_CLIENT_PREFIX + node.settings().get("name"))
                     .put(ClusterName.SETTING, clusterName).put("client.transport.sniff", sniff)
                     .put("node.mode", nodeSettings.get("node.mode", nodeMode))
diff --git a/test/framework/src/main/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java b/test/framework/src/main/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
index e549c18..71c1cc2 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java
@@ -24,6 +24,7 @@ import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.network.NetworkUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.NodeConfigurationSource;
 
@@ -35,7 +36,7 @@ import java.util.Set;
 
 public class ClusterDiscoveryConfiguration extends NodeConfigurationSource {
 
-    static Settings DEFAULT_NODE_SETTINGS = Settings.settingsBuilder().put("discovery.type", "zen").build();
+    static Settings DEFAULT_NODE_SETTINGS = Settings.settingsBuilder().put(DiscoveryModule.DISCOVERY_TYPE_SETTING.getKey(), "zen").build();
     private static final String IP_ADDR = "127.0.0.1";
 
     final int numOfNodes;
diff --git a/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 61755f7..cb3bbc7 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -731,82 +731,6 @@ public class ElasticsearchAssertions {
         return response;
     }
 
-    public static void assertNodeContainsPlugins(NodesInfoResponse response, String nodeId,
-                                                 List<String> expectedJvmPluginNames,
-                                                 List<String> expectedJvmPluginDescriptions,
-                                                 List<String> expectedJvmVersions,
-                                                 List<String> expectedSitePluginNames,
-                                                 List<String> expectedSitePluginDescriptions,
-                                                 List<String> expectedSiteVersions) {
-
-        Assert.assertThat(response.getNodesMap().get(nodeId), notNullValue());
-
-        PluginsAndModules plugins = response.getNodesMap().get(nodeId).getPlugins();
-        Assert.assertThat(plugins, notNullValue());
-
-        List<String> pluginNames = filterAndMap(plugins, jvmPluginPredicate, nameFunction);
-        for (String expectedJvmPluginName : expectedJvmPluginNames) {
-            Assert.assertThat(pluginNames, hasItem(expectedJvmPluginName));
-        }
-
-        List<String> pluginDescriptions = filterAndMap(plugins, jvmPluginPredicate, descriptionFunction);
-        for (String expectedJvmPluginDescription : expectedJvmPluginDescriptions) {
-            Assert.assertThat(pluginDescriptions, hasItem(expectedJvmPluginDescription));
-        }
-
-        List<String> jvmPluginVersions = filterAndMap(plugins, jvmPluginPredicate, versionFunction);
-        for (String pluginVersion : expectedJvmVersions) {
-            Assert.assertThat(jvmPluginVersions, hasItem(pluginVersion));
-        }
-
-        boolean anyHaveUrls =
-                plugins
-                        .getPluginInfos()
-                        .stream()
-                        .filter(jvmPluginPredicate.and(sitePluginPredicate.negate()))
-                        .map(urlFunction)
-                        .anyMatch(p -> p != null);
-        assertFalse(anyHaveUrls);
-
-        List<String> sitePluginNames = filterAndMap(plugins, sitePluginPredicate, nameFunction);
-
-        Assert.assertThat(sitePluginNames.isEmpty(), is(expectedSitePluginNames.isEmpty()));
-        for (String expectedSitePluginName : expectedSitePluginNames) {
-            Assert.assertThat(sitePluginNames, hasItem(expectedSitePluginName));
-        }
-
-        List<String> sitePluginDescriptions = filterAndMap(plugins, sitePluginPredicate, descriptionFunction);
-        Assert.assertThat(sitePluginDescriptions.isEmpty(), is(expectedSitePluginDescriptions.isEmpty()));
-        for (String sitePluginDescription : expectedSitePluginDescriptions) {
-            Assert.assertThat(sitePluginDescriptions, hasItem(sitePluginDescription));
-        }
-
-        List<String> sitePluginUrls = filterAndMap(plugins, sitePluginPredicate, urlFunction);
-        Assert.assertThat(sitePluginUrls, not(contains(nullValue())));
-
-        List<String> sitePluginVersions = filterAndMap(plugins, sitePluginPredicate, versionFunction);
-        Assert.assertThat(sitePluginVersions.isEmpty(), is(expectedSiteVersions.isEmpty()));
-        for (String pluginVersion : expectedSiteVersions) {
-            Assert.assertThat(sitePluginVersions, hasItem(pluginVersion));
-        }
-    }
-
-    private static List<String> filterAndMap(PluginsAndModules pluginsInfo, Predicate<PluginInfo> predicate, Function<PluginInfo, String> function) {
-        return pluginsInfo.getPluginInfos().stream().filter(predicate).map(function).collect(Collectors.toList());
-    }
-
-    private static Predicate<PluginInfo> jvmPluginPredicate = p -> p.isJvm();
-
-    private static Predicate<PluginInfo> sitePluginPredicate = p -> p.isSite();
-
-    private static Function<PluginInfo, String> nameFunction = p -> p.getName();
-
-    private static Function<PluginInfo, String> descriptionFunction = p -> p.getDescription();
-
-    private static Function<PluginInfo, String> urlFunction = p -> p.getUrl();
-
-    private static Function<PluginInfo, String> versionFunction = p -> p.getVersion();
-
     /**
      * Check if a file exists
      */
diff --git a/test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java b/test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
index 1c9bddb..2347fc4 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
@@ -147,7 +147,7 @@ public class ReproduceInfoPrinter extends RunListener {
             if (System.getProperty("tests.jvm.argline") != null && !System.getProperty("tests.jvm.argline").isEmpty()) {
                 appendOpt("tests.jvm.argline", "\"" + System.getProperty("tests.jvm.argline") + "\"");
             }
-            appendOpt("tests.locale", Locale.getDefault().toString());
+            appendOpt("tests.locale", Locale.getDefault().toLanguageTag());
             appendOpt("tests.timezone", TimeZone.getDefault().getID());
             return this;
         }
diff --git a/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index 9899693..d66acb7 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -24,6 +24,7 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.network.NetworkModule;
+import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -60,8 +61,10 @@ public class AssertingLocalTransport extends LocalTransport {
         }
     }
 
-    public static final String ASSERTING_TRANSPORT_MIN_VERSION_KEY = "transport.asserting.version.min";
-    public static final String ASSERTING_TRANSPORT_MAX_VERSION_KEY = "transport.asserting.version.max";
+    public static final Setting<Version> ASSERTING_TRANSPORT_MIN_VERSION_KEY = new Setting<>("transport.asserting.version.min",
+            Version.CURRENT.minimumCompatibilityVersion().toString(), Version::fromString, false, Setting.Scope.CLUSTER);
+    public static final Setting<Version> ASSERTING_TRANSPORT_MAX_VERSION_KEY = new Setting<>("transport.asserting.version.max",
+            Version.CURRENT.toString(), Version::fromString, false, Setting.Scope.CLUSTER);
     private final Random random;
     private final Version minVersion;
     private final Version maxVersion;
@@ -71,8 +74,8 @@ public class AssertingLocalTransport extends LocalTransport {
         super(settings, threadPool, version, namedWriteableRegistry);
         final long seed = ESIntegTestCase.INDEX_TEST_SEED_SETTING.get(settings);
         random = new Random(seed);
-        minVersion = settings.getAsVersion(ASSERTING_TRANSPORT_MIN_VERSION_KEY, Version.V_0_18_0);
-        maxVersion = settings.getAsVersion(ASSERTING_TRANSPORT_MAX_VERSION_KEY, Version.CURRENT);
+        minVersion = ASSERTING_TRANSPORT_MIN_VERSION_KEY.get(settings);
+        maxVersion = ASSERTING_TRANSPORT_MAX_VERSION_KEY.get(settings);
     }
 
     @Override
diff --git a/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java b/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
index 0a8869b..985c8a8 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/transport/MockTransportService.java
@@ -34,7 +34,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.tasks.TaskManager;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.ConnectTransportException;
 import org.elasticsearch.transport.RequestHandlerRegistry;
@@ -283,7 +282,7 @@ public class MockTransportService extends TransportService {
                 }
 
                 // TODO: Replace with proper setting
-                TimeValue connectingTimeout = NetworkService.TcpSettings.TCP_DEFAULT_CONNECT_TIMEOUT;
+                TimeValue connectingTimeout = NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT.getDefault(Settings.EMPTY);
                 try {
                     if (delay.millis() < connectingTimeout.millis()) {
                         Thread.sleep(delay.millis());
@@ -306,7 +305,7 @@ public class MockTransportService extends TransportService {
                 }
 
                 // TODO: Replace with proper setting
-                TimeValue connectingTimeout = NetworkService.TcpSettings.TCP_DEFAULT_CONNECT_TIMEOUT;
+                TimeValue connectingTimeout = NetworkService.TcpSettings.TCP_CONNECT_TIMEOUT.getDefault(Settings.EMPTY);
                 try {
                     if (delay.millis() < connectingTimeout.millis()) {
                         Thread.sleep(delay.millis());
